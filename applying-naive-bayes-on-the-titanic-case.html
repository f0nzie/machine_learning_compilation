<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 28 Applying Naive-Bayes on the Titanic case | A Machine Learning Book</title>
<meta name="author" content="Alfonso R. Reyes">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.5.3/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.5.3/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.2.9000/tabs.js"></script><script src="libs/bs3compat-0.2.2.9000/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">A Machine Learning Book</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Prerequisites</a></li>
<li class="book-part">The Basics of Machine Learning</li>
<li><a class="" href="introduction-to-pca.html"><span class="header-section-number">1</span> Introduction to PCA</a></li>
<li><a class="" href="comparison-of-two-pca-packages.html"><span class="header-section-number">2</span> Comparison of two PCA packages</a></li>
<li><a class="" href="detailed-study-of-principal-component-analysis.html"><span class="header-section-number">3</span> Detailed study of Principal Component Analysis</a></li>
<li><a class="" href="detection-of-diabetes-using-logistic-regression.html"><span class="header-section-number">4</span> Detection of diabetes using Logistic Regression</a></li>
<li><a class="" href="sensitivity-analysis-for-a-neural-network.html"><span class="header-section-number">5</span> Sensitivity analysis for a neural network</a></li>
<li><a class="" href="data-visualization-for-ml-models.html"><span class="header-section-number">6</span> Data Visualization for ML models</a></li>
<li class="book-part">Feature Engineering</li>
<li><a class="" href="ten-methods-to-assess-variable-importance.html"><span class="header-section-number">7</span> Ten methods to assess Variable Importance</a></li>
<li><a class="" href="employee-attrition-using-feature-importance.html"><span class="header-section-number">8</span> Employee Attrition using Feature Importance</a></li>
<li><a class="" href="feature-selection-to-enhance-cancer-detection.html"><span class="header-section-number">9</span> Feature Selection to enhance cancer detection</a></li>
<li><a class="" href="dealing-with-unbalanced-data.html"><span class="header-section-number">10</span> Dealing with unbalanced data</a></li>
<li><a class="" href="imputting-missing-values-with-random-forest.html"><span class="header-section-number">11</span> Imputting missing values with Random Forest</a></li>
<li class="book-part">Algorithms Comparison</li>
<li><a class="" href="introduction-to-algorithms-for-classification.html"><span class="header-section-number">12</span> Introduction to algorithms for Classification</a></li>
<li><a class="" href="comparing-classification-algorithms.html"><span class="header-section-number">13</span> Comparing Classification algorithms</a></li>
<li><a class="" href="linear-and-non-linear-algorithms-for-classification.html"><span class="header-section-number">14</span> Linear and Non-Linear Algorithms for Classification</a></li>
<li><a class="" href="predicting-flu-outcome-comparing-eight-classification-algorithms.html"><span class="header-section-number">15</span> Predicting Flu outcome comparing eight classification algorithms</a></li>
<li><a class="" href="classification-on-bad-loans.html"><span class="header-section-number">16</span> Classification on bad loans</a></li>
<li><a class="" href="comparison-of-six-linear-regression-algorithms.html"><span class="header-section-number">17</span> Comparison of six Linear Regression algorithms</a></li>
<li class="book-part">Classification</li>
<li><a class="" href="a-gentle-introduction-to-support-vector-machines.html"><span class="header-section-number">18</span> A gentle introduction to Support Vector Machines</a></li>
<li><a class="" href="broad-view-of-svm.html"><span class="header-section-number">19</span> Broad view of SVM</a></li>
<li><a class="" href="tuning-of-support-vector-machine-prediction.html"><span class="header-section-number">20</span> Tuning of Support Vector Machine prediction</a></li>
<li><a class="" href="who-buys-social-network-ads.html"><span class="header-section-number">21</span> Who buys Social Network ads</a></li>
<li><a class="" href="predicting-ozone-levels.html"><span class="header-section-number">22</span> Predicting Ozone levels</a></li>
<li><a class="" href="building-a-naive-bayes-classifier.html"><span class="header-section-number">23</span> Building a Naive Bayes Classifier</a></li>
<li><a class="" href="detect-mines-vs-rocks-with-random-forest.html"><span class="header-section-number">24</span> Detect mines vs rocks with Random Forest</a></li>
<li><a class="" href="predicting-the-type-of-glass.html"><span class="header-section-number">25</span> Predicting the type of glass</a></li>
<li><a class="" href="naive-bayes-for-sms-spam.html"><span class="header-section-number">26</span> Naive Bayes for SMS spam</a></li>
<li><a class="" href="vehicles-classiification-with-decision-trees.html"><span class="header-section-number">27</span> Vehicles classiification with Decision Trees</a></li>
<li><a class="active" href="applying-naive-bayes-on-the-titanic-case.html"><span class="header-section-number">28</span> Applying Naive-Bayes on the Titanic case</a></li>
<li><a class="" href="a-detailed-study-of-bike-sharing-demand.html"><span class="header-section-number">29</span> A detailed study of bike sharing demand</a></li>
<li class="book-part">Linear Regression</li>
<li><a class="" href="linear-regression-with-islr.html"><span class="header-section-number">30</span> Linear Regression with ISLR</a></li>
<li><a class="" href="evaluation-of-three-linear-regression-models.html"><span class="header-section-number">31</span> Evaluation of three linear regression models</a></li>
<li><a class="" href="comparing-regression-models.html"><span class="header-section-number">32</span> Comparing regression models</a></li>
<li><a class="" href="finding-the-factors-of-happiness.html"><span class="header-section-number">33</span> Finding the factors of happiness</a></li>
<li><a class="" href="regression-with-a-neural-network.html"><span class="header-section-number">34</span> Regression with a neural network</a></li>
<li><a class="" href="comparing-multiple-regression-vs-a-neural-network.html"><span class="header-section-number">35</span> Comparing Multiple Regression vs a Neural Network</a></li>
<li><a class="" href="temperature-modeling-using-nested-dataframes.html"><span class="header-section-number">36</span> Temperature modeling using nested dataframes</a></li>
<li class="book-part">Neural Networks</li>
<li><a class="" href="credit-scoring-with-neuralnet.html"><span class="header-section-number">37</span> Credit Scoring with neuralnet</a></li>
<li><a class="" href="wine-classification-with-neuralnet.html"><span class="header-section-number">38</span> Wine classification with neuralnet</a></li>
<li><a class="" href="predicting-the-rating-of-cereals.html"><span class="header-section-number">39</span> Predicting the rating of cereals</a></li>
<li><a class="" href="fitting-a-linear-model-with-neural-networks.html"><span class="header-section-number">40</span> Fitting a linear model with neural networks</a></li>
<li><a class="" href="visualization-of-neural-networks.html"><span class="header-section-number">41</span> Visualization of neural networks</a></li>
<li><a class="" href="build-a-fully-connected-r-neural-network-from-scratch.html"><span class="header-section-number">42</span> Build a fully connected R neural network from scratch</a></li>
<li><a class="" href="tuning-hyperparameters-in-a-neural-network.html"><span class="header-section-number">43</span> Tuning Hyperparameters in a Neural Network</a></li>
<li><a class="" href="deep-learning-tips-for-classification-and-regression.html"><span class="header-section-number">44</span> Deep Learning tips for Classification and Regression</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="what-is-dot-hat-in-a-regression-output.html"><span class="header-section-number">A</span> What is dot hat in a regression output</a></li>
<li><a class="" href="q-q-normal-to-compare-data-to-distributions.html"><span class="header-section-number">B</span> Q-Q normal to compare data to distributions</a></li>
<li><a class="" href="qq-and-pp-plots.html"><span class="header-section-number">C</span> QQ and PP Plots</a></li>
<li><a class="" href="visualizing-residuals.html"><span class="header-section-number">D</span> Visualizing residuals</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/hadley/r-pkgs">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="applying-naive-bayes-on-the-titanic-case" class="section level1">
<h1>
<span class="header-section-number">28</span> Applying Naive-Bayes on the Titanic case<a class="anchor" aria-label="anchor" href="#applying-naive-bayes-on-the-titanic-case"><i class="fas fa-link"></i></a>
</h1>
<ul>
<li>Datasets: <code>Titanic</code>
</li>
<li>Algorithms:
<ul>
<li>Naive Bayes</li>
</ul>
</li>
</ul>
<p>The Titanic dataset in R is a table for about 2200 passengers summarised according to four factors – economic status ranging from 1st class, 2nd class, 3rd class and crew; gender which is either male or female; Age category which is either Child or Adult and whether the type of passenger survived. For each combination of Age, Gender, Class and Survived status, the table gives the number of passengers who fall into the combination. We will use the Naive Bayes Technique to classify such passengers and check how well it performs.</p>
<div class="sourceCode" id="cb683"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">#Getting started with Naive Bayes</span>
<span class="co">#Install the package</span>
<span class="co">#install.packages(“e1071”)</span>
<span class="co">#Loading the library</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">e1071</span><span class="op">)</span>

<span class="co">#Next load the Titanic dataset</span>
<span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"Titanic"</span><span class="op">)</span>
<span class="co">#Save into a data frame and view it</span>
<span class="va">Titanic_df</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame</a></span><span class="op">(</span><span class="va">Titanic</span><span class="op">)</span></code></pre></div>
<p>We see that there are 32 observations which represent all possible combinations of Class, Sex, Age and Survived with their frequency. Since it is summarised, this table is not suitable for modelling purposes. We need to expand the table into individual rows. Let’s create a repeating sequence of rows based on the frequencies in the table</p>
<div class="sourceCode" id="cb684"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">#Creating data from table</span>
<span class="va">repeating_sequence</span><span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep.int</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq_len</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">Titanic_df</span><span class="op">)</span><span class="op">)</span>, <span class="va">Titanic_df</span><span class="op">$</span><span class="va">Freq</span><span class="op">)</span> <span class="co">#This will repeat each combination equal to the frequency of each combination</span>

<span class="co"># Create the dataset by row repetition created</span>
<span class="va">Titanic_dataset</span><span class="op">=</span><span class="va">Titanic_df</span><span class="op">[</span><span class="va">repeating_sequence</span>,<span class="op">]</span>

<span class="co"># We no longer need the frequency, drop the feature</span>
<span class="va">Titanic_dataset</span><span class="op">$</span><span class="va">Freq</span><span class="op">=</span><span class="cn">NULL</span></code></pre></div>
<p>The data is now ready for Naive Bayes to process. Let’s fit the model</p>
<div class="sourceCode" id="cb685"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Fitting the Naive Bayes model</span>
<span class="va">Naive_Bayes_Model</span><span class="op">=</span><span class="fu"><a href="https://rdrr.io/pkg/e1071/man/naiveBayes.html">naiveBayes</a></span><span class="op">(</span><span class="va">Survived</span> <span class="op">~</span><span class="va">.</span>, data<span class="op">=</span><span class="va">Titanic_dataset</span><span class="op">)</span>

<span class="co"># What does the model say? Print the model summary</span>
<span class="va">Naive_Bayes_Model</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Naive Bayes Classifier for Discrete Predictors</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; naiveBayes.default(x = X, y = Y, laplace = laplace)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; A-priori probabilities:</span>
<span class="co">#&gt; Y</span>
<span class="co">#&gt;    No   Yes </span>
<span class="co">#&gt; 0.677 0.323 </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Conditional probabilities:</span>
<span class="co">#&gt;      Class</span>
<span class="co">#&gt; Y        1st    2nd    3rd   Crew</span>
<span class="co">#&gt;   No  0.0819 0.1121 0.3544 0.4517</span>
<span class="co">#&gt;   Yes 0.2855 0.1660 0.2504 0.2982</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;      Sex</span>
<span class="co">#&gt; Y       Male Female</span>
<span class="co">#&gt;   No  0.9154 0.0846</span>
<span class="co">#&gt;   Yes 0.5162 0.4838</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;      Age</span>
<span class="co">#&gt; Y      Child  Adult</span>
<span class="co">#&gt;   No  0.0349 0.9651</span>
<span class="co">#&gt;   Yes 0.0802 0.9198</span></code></pre></div>
<p>The model creates the conditional probability for each feature separately. We also have the a-priori probabilities which indicates the distribution of our data. Let’s calculate how we perform on the data.</p>
<div class="sourceCode" id="cb686"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Prediction on the dataset</span>
<span class="va">NB_Predictions</span><span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">Naive_Bayes_Model</span>,<span class="va">Titanic_dataset</span><span class="op">)</span>
<span class="co"># Confusion matrix to check accuracy</span>
<span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">NB_Predictions</span>,<span class="va">Titanic_dataset</span><span class="op">$</span><span class="va">Survived</span><span class="op">)</span>
<span class="co">#&gt;               </span>
<span class="co">#&gt; NB_Predictions   No  Yes</span>
<span class="co">#&gt;            No  1364  362</span>
<span class="co">#&gt;            Yes  126  349</span></code></pre></div>
<p>We have the results! We are able to classify 1364 out of 1490 “No” cases correctly and 349 out of 711 “Yes” cases correctly. This means the ability of Naive Bayes algorithm to predict “No” cases is about 91.5% but it falls down to only 49% of the “Yes” cases resulting in an overall accuracy of 77.8%</p>
<div id="can-we-do-any-better" class="section level2">
<h2>
<span class="header-section-number">28.1</span> Can we Do any Better?<a class="anchor" aria-label="anchor" href="#can-we-do-any-better"><i class="fas fa-link"></i></a>
</h2>
<p>Naive Bayes is a parametric algorithm which implies that you cannot perform differently in different runs as long as the data remains the same. We will, however, learn another implementation of Naive Bayes algorithm using the ‘mlr’ package. Assuming the same session is going on for the readers, I will install and load the package and start fitting a model</p>
<div class="sourceCode" id="cb687"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Getting started with Naive Bayes in mlr</span>
<span class="co"># install.packages(“mlr”)</span>
<span class="co"># Loading the library</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://mlr.mlr-org.com">mlr</a></span><span class="op">)</span>
<span class="co">#&gt; Loading required package: ParamHelpers</span>
<span class="co">#&gt; 'mlr' is in maintenance mode since July 2019. Future development</span>
<span class="co">#&gt; efforts will go into its successor 'mlr3' (&lt;https://mlr3.mlr-org.com&gt;).</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Attaching package: 'mlr'</span>
<span class="co">#&gt; The following object is masked from 'package:e1071':</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     impute</span></code></pre></div>
<p>The mlr package consists of a lot of models and works by creating tasks and learners which are then trained. Let’s create a classification task using the titanic dataset and fit a model with the naive bayes algorithm.</p>
<div class="sourceCode" id="cb688"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Create a classification task for learning on Titanic Dataset and specify the target feature</span>
<span class="va">task</span> <span class="op">=</span> <span class="fu"><a href="https://mlr.mlr-org.com/reference/ClassifTask.html">makeClassifTask</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">Titanic_dataset</span>, target <span class="op">=</span> <span class="st">"Survived"</span><span class="op">)</span>

<span class="co"># Initialize the Naive Bayes classifier</span>
<span class="va">selected_model</span> <span class="op">=</span> <span class="fu"><a href="https://mlr.mlr-org.com/reference/makeLearner.html">makeLearner</a></span><span class="op">(</span><span class="st">"classif.naiveBayes"</span><span class="op">)</span>

<span class="co"># Train the model</span>
<span class="va">NB_mlr</span> <span class="op">=</span> <span class="fu"><a href="https://mlr.mlr-org.com/reference/train.html">train</a></span><span class="op">(</span><span class="va">selected_model</span>, <span class="va">task</span><span class="op">)</span></code></pre></div>
<p>The summary of the model which was printed in e3071 package is stored in learner model. Let’s print it and compare</p>
<div class="sourceCode" id="cb689"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Read the model learned  </span>
<span class="va">NB_mlr</span><span class="op">$</span><span class="va">learner.model</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Naive Bayes Classifier for Discrete Predictors</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; naiveBayes.default(x = X, y = Y, laplace = laplace)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; A-priori probabilities:</span>
<span class="co">#&gt; Y</span>
<span class="co">#&gt;    No   Yes </span>
<span class="co">#&gt; 0.677 0.323 </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Conditional probabilities:</span>
<span class="co">#&gt;      Class</span>
<span class="co">#&gt; Y        1st    2nd    3rd   Crew</span>
<span class="co">#&gt;   No  0.0819 0.1121 0.3544 0.4517</span>
<span class="co">#&gt;   Yes 0.2855 0.1660 0.2504 0.2982</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;      Sex</span>
<span class="co">#&gt; Y       Male Female</span>
<span class="co">#&gt;   No  0.9154 0.0846</span>
<span class="co">#&gt;   Yes 0.5162 0.4838</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;      Age</span>
<span class="co">#&gt; Y      Child  Adult</span>
<span class="co">#&gt;   No  0.0349 0.9651</span>
<span class="co">#&gt;   Yes 0.0802 0.9198</span></code></pre></div>
<p>The a-priori probabilities and the conditional probabilities for the model are similar to the one calculated by e3071 package as was expected. This means that our predictions will also be the same.</p>
<div class="sourceCode" id="cb690"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Predict on the dataset without passing the target feature</span>
<span class="va">predictions_mlr</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html">as.data.frame</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">NB_mlr</span>, newdata <span class="op">=</span> <span class="va">Titanic_dataset</span><span class="op">[</span>,<span class="fl">1</span><span class="op">:</span><span class="fl">3</span><span class="op">]</span><span class="op">)</span><span class="op">)</span>

<span class="co">## Confusion matrix to check accuracy</span>
<span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">predictions_mlr</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>,<span class="va">Titanic_dataset</span><span class="op">$</span><span class="va">Survived</span><span class="op">)</span>
<span class="co">#&gt;      </span>
<span class="co">#&gt;         No  Yes</span>
<span class="co">#&gt;   No  1364  362</span>
<span class="co">#&gt;   Yes  126  349</span></code></pre></div>
<p>As we see, the predictions are exactly same. The only way to improve is to have more features or more data. Perhaps, if we have more features such as the exact age, size of family, number of parents in the ship and siblings then we may arrive at a better model using Naive Bayes. In essence, Naive Bayes has an advantage of a strong foundation build and is very robust. I know of the ‘caret’ package which also consists of Naive Bayes function but it will also give us the same predictions and probability.</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="vehicles-classiification-with-decision-trees.html"><span class="header-section-number">27</span> Vehicles classiification with Decision Trees</a></div>
<div class="next"><a href="a-detailed-study-of-bike-sharing-demand.html"><span class="header-section-number">29</span> A detailed study of bike sharing demand</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#applying-naive-bayes-on-the-titanic-case"><span class="header-section-number">28</span> Applying Naive-Bayes on the Titanic case</a></li>
<li><a class="nav-link" href="#can-we-do-any-better"><span class="header-section-number">28.1</span> Can we Do any Better?</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/hadley/r-pkgs/blob/master/302-classification_252a-titanic-naives_bayes.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/hadley/r-pkgs/edit/master/302-classification_252a-titanic-naives_bayes.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>A Machine Learning Book</strong>" was written by Alfonso R. Reyes. It was last built on 2020-11-18.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
