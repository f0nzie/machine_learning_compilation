<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 40 Wine classification with neuralnet | A Machine Learning Compilation</title>
<meta name="author" content="Several authors. Compiled by Alfonso R. Reyes">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.5.3/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.5.3/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.2.9000/tabs.js"></script><script src="libs/bs3compat-0.2.2.9000/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">A Machine Learning Compilation</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Preface</a></li>
<li class="book-part">The Basics of Machine Learning</li>
<li><a class="" href="introduction-to-pca.html"><span class="header-section-number">2</span> Introduction to PCA</a></li>
<li><a class="" href="comparison-of-two-pca-packages.html"><span class="header-section-number">3</span> Comparison of two PCA packages</a></li>
<li><a class="" href="detailed-study-of-principal-component-analysis.html"><span class="header-section-number">4</span> Detailed study of Principal Component Analysis</a></li>
<li><a class="" href="detection-of-diabetes-using-logistic-regression.html"><span class="header-section-number">5</span> Detection of diabetes using Logistic Regression</a></li>
<li><a class="" href="sensitivity-analysis-for-a-neural-network.html"><span class="header-section-number">6</span> Sensitivity analysis for a neural network</a></li>
<li><a class="" href="data-visualization-for-ml-models.html"><span class="header-section-number">7</span> Data Visualization for ML models</a></li>
<li class="book-part">Feature Engineering</li>
<li><a class="" href="ten-methods-to-assess-variable-importance.html"><span class="header-section-number">8</span> Ten methods to assess Variable Importance</a></li>
<li><a class="" href="employee-attrition-using-feature-importance.html"><span class="header-section-number">9</span> Employee Attrition using Feature Importance</a></li>
<li class="book-part">Classification</li>
<li><a class="" href="a-gentle-introduction-to-support-vector-machines.html"><span class="header-section-number">10</span> A gentle introduction to Support Vector Machines</a></li>
<li><a class="" href="broad-view-of-svm.html"><span class="header-section-number">11</span> Broad view of SVM</a></li>
<li><a class="" href="feature-selection-to-enhance-cancer-detection.html"><span class="header-section-number">12</span> Feature Selection to enhance cancer detection</a></li>
<li><a class="" href="dealing-with-unbalanced-data.html"><span class="header-section-number">13</span> Dealing with unbalanced data</a></li>
<li><a class="" href="imputting-missing-values-with-random-forest.html"><span class="header-section-number">14</span> Imputting missing values with Random Forest</a></li>
<li><a class="" href="tuning-of-support-vector-machine-prediction.html"><span class="header-section-number">15</span> Tuning of Support Vector Machine prediction</a></li>
<li class="book-part">Classification</li>
<li><a class="" href="introduction-to-algorithms-for-classification.html"><span class="header-section-number">16</span> Introduction to algorithms for Classification</a></li>
<li><a class="" href="comparing-classification-algorithms.html"><span class="header-section-number">17</span> Comparing Classification algorithms</a></li>
<li><a class="" href="who-buys-social-network-ads.html"><span class="header-section-number">18</span> Who buys Social Network ads</a></li>
<li><a class="" href="predicting-ozone-levels.html"><span class="header-section-number">19</span> Predicting Ozone levels</a></li>
<li><a class="" href="building-a-naive-bayes-classifier.html"><span class="header-section-number">20</span> Building a Naive Bayes Classifier</a></li>
<li><a class="" href="linear-and-non-linear-algorithms-for-classification.html"><span class="header-section-number">21</span> Linear and Non-Linear Algorithms for Classification</a></li>
<li><a class="" href="detect-mines-vs-rocks-with-random-forest.html"><span class="header-section-number">22</span> Detect mines vs rocks with Random Forest</a></li>
<li><a class="" href="predicting-the-type-of-glass.html"><span class="header-section-number">23</span> Predicting the type of glass</a></li>
<li><a class="" href="naive-bayes-for-sms-spam.html"><span class="header-section-number">24</span> Naive Bayes for SMS spam</a></li>
<li><a class="" href="vehicles-classiification-with-decision-trees.html"><span class="header-section-number">25</span> Vehicles classiification with Decision Trees</a></li>
<li><a class="" href="applying-naive-bayes-on-the-titanic-case.html"><span class="header-section-number">26</span> Applying Naive-Bayes on the Titanic case</a></li>
<li><a class="" href="classification-on-bad-loans.html"><span class="header-section-number">27</span> Classification on bad loans</a></li>
<li><a class="" href="predicting-flu-outcome-comparing-eight-classification-algorithms.html"><span class="header-section-number">28</span> Predicting Flu outcome comparing eight classification algorithms</a></li>
<li><a class="" href="a-detailed-study-of-bike-sharing-demand.html"><span class="header-section-number">29</span> A detailed study of bike sharing demand</a></li>
<li><a class="" href="prediction-of-arrhythmia-with-deep-neural-nets.html"><span class="header-section-number">30</span> Prediction of arrhythmia with deep neural nets</a></li>
<li class="book-part">Linear Regression</li>
<li><a class="" href="linear-regression-with-islr.html"><span class="header-section-number">31</span> Linear Regression with ISLR</a></li>
<li><a class="" href="evaluation-of-three-linear-regression-models.html"><span class="header-section-number">32</span> Evaluation of three linear regression models</a></li>
<li><a class="" href="comparison-of-six-linear-regression-algorithms.html"><span class="header-section-number">33</span> Comparison of six Linear Regression algorithms</a></li>
<li><a class="" href="comparing-regression-models.html"><span class="header-section-number">34</span> Comparing regression models</a></li>
<li><a class="" href="finding-the-factors-of-happiness.html"><span class="header-section-number">35</span> Finding the factors of happiness</a></li>
<li><a class="" href="regression-with-a-neural-network.html"><span class="header-section-number">36</span> Regression with a neural network</a></li>
<li><a class="" href="comparing-multiple-regression-vs-a-neural-network.html"><span class="header-section-number">37</span> Comparing Multiple Regression vs a Neural Network</a></li>
<li><a class="" href="temperature-modeling-using-nested-dataframes.html"><span class="header-section-number">38</span> Temperature modeling using nested dataframes</a></li>
<li class="book-part">Neural Networks</li>
<li><a class="" href="credit-scoring-with-neuralnet.html"><span class="header-section-number">39</span> Credit Scoring with neuralnet</a></li>
<li><a class="active" href="wine-classification-with-neuralnet.html"><span class="header-section-number">40</span> Wine classification with neuralnet</a></li>
<li><a class="" href="predicting-the-rating-of-cereals.html"><span class="header-section-number">41</span> Predicting the rating of cereals</a></li>
<li><a class="" href="fitting-a-linear-model-with-neural-networks.html"><span class="header-section-number">42</span> Fitting a linear model with neural networks</a></li>
<li><a class="" href="visualization-of-neural-networks.html"><span class="header-section-number">43</span> Visualization of neural networks</a></li>
<li><a class="" href="build-a-fully-connected-r-neural-network-from-scratch.html"><span class="header-section-number">44</span> Build a fully connected R neural network from scratch</a></li>
<li><a class="" href="tuning-hyperparameters-in-a-neural-network.html"><span class="header-section-number">45</span> Tuning Hyperparameters in a Neural Network</a></li>
<li><a class="" href="deep-learning-tips-for-classification-and-regression.html"><span class="header-section-number">46</span> Deep Learning tips for Classification and Regression</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="what-is-dot-hat-in-a-regression-output.html"><span class="header-section-number">A</span> What is dot hat in a regression output</a></li>
<li><a class="" href="q-q-normal-to-compare-data-to-distributions.html"><span class="header-section-number">B</span> Q-Q normal to compare data to distributions</a></li>
<li><a class="" href="qq-and-pp-plots.html"><span class="header-section-number">C</span> QQ and PP Plots</a></li>
<li><a class="" href="visualizing-residuals.html"><span class="header-section-number">D</span> Visualizing residuals</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/hadley/r-pkgs">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="wine-classification-with-neuralnet" class="section level1">
<h1>
<span class="header-section-number">40</span> Wine classification with neuralnet<a class="anchor" aria-label="anchor" href="#wine-classification-with-neuralnet"><i class="fas fa-link"></i></a>
</h1>
<p>Source: <a href="https://www.r-bloggers.com/multilabel-classification-with-neuralnet-package/" class="uri">https://www.r-bloggers.com/multilabel-classification-with-neuralnet-package/</a></p>
<p>The <code>neuralnet</code> package is perhaps not the best option in R for using neural networks. If you ask why, for starters it does not recognize the typical formula y~., it does not support factors, it does not provide a lot of models other than a standard MLP, and it has great competitors in the nnet package that seems to be better integrated in R and can be used with the caret package, and in the MXnet package that is a high level deep learning library which provides a wide variety of neural networks.</p>
<p>But still, I think there is some value in the ease of use of the neuralnet package, especially for a beginner, therefore I’ll be using it.</p>
<p>I’m going to be using both the neuralnet and, curiously enough, the nnet package. Let’s load them:</p>
<div class="sourceCode" id="cb977"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># load libs</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">require</a></span><span class="op">(</span><span class="va"><a href="https://github.com/bips-hb/neuralnet">neuralnet</a></span><span class="op">)</span>
<span class="co">#&gt; Loading required package: neuralnet</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">require</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">nnet</a></span><span class="op">)</span>
<span class="co">#&gt; Loading required package: nnet</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">require</a></span><span class="op">(</span><span class="va"><a href="http://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span>
<span class="co">#&gt; Loading required package: ggplot2</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">10</span><span class="op">)</span></code></pre></div>
<div id="the-dataset" class="section level2">
<h2>
<span class="header-section-number">40.1</span> The dataset<a class="anchor" aria-label="anchor" href="#the-dataset"><i class="fas fa-link"></i></a>
</h2>
<p>I looked in the UCI Machine Learning Repository1 and found the wine dataset.</p>
<p>This dataset contains the results of a chemical analysis on 3 different kind of wines. The target variable is the label of the wine which is a factor with 3 (unordered) levels. The predictors are all continuous and represent 13 variables obtained as a result of chemical measurements.</p>
<div class="sourceCode" id="cb978"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># get the data file from the package location</span>
<span class="va">wine_dataset_path</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/file.path.html">file.path</a></span><span class="op">(</span><span class="va">data_raw_dir</span>, <span class="st">"wine.data"</span><span class="op">)</span>
<span class="va">wine_dataset_path</span>
<span class="co">#&gt; [1] "../data/wine.data"</span></code></pre></div>
<div class="sourceCode" id="cb979"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">wines</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.csv</a></span><span class="op">(</span><span class="va">wine_dataset_path</span><span class="op">)</span>
<span class="va">wines</span>
<span class="co">#&gt;     X1 X14.23 X1.71 X2.43 X15.6 X127 X2.8 X3.06 X.28 X2.29 X5.64 X1.04 X3.92</span>
<span class="co">#&gt; 1    1   13.2  1.78  2.14  11.2  100 2.65  2.76 0.26  1.28  4.38 1.050  3.40</span>
<span class="co">#&gt; 2    1   13.2  2.36  2.67  18.6  101 2.80  3.24 0.30  2.81  5.68 1.030  3.17</span>
<span class="co">#&gt; 3    1   14.4  1.95  2.50  16.8  113 3.85  3.49 0.24  2.18  7.80 0.860  3.45</span>
<span class="co">#&gt; 4    1   13.2  2.59  2.87  21.0  118 2.80  2.69 0.39  1.82  4.32 1.040  2.93</span>
<span class="co">#&gt; 5    1   14.2  1.76  2.45  15.2  112 3.27  3.39 0.34  1.97  6.75 1.050  2.85</span>
<span class="co">#&gt; 6    1   14.4  1.87  2.45  14.6   96 2.50  2.52 0.30  1.98  5.25 1.020  3.58</span>
<span class="co">#&gt; 7    1   14.1  2.15  2.61  17.6  121 2.60  2.51 0.31  1.25  5.05 1.060  3.58</span>
<span class="co">#&gt; 8    1   14.8  1.64  2.17  14.0   97 2.80  2.98 0.29  1.98  5.20 1.080  2.85</span>
<span class="co">#&gt; 9    1   13.9  1.35  2.27  16.0   98 2.98  3.15 0.22  1.85  7.22 1.010  3.55</span>
<span class="co">#&gt; 10   1   14.1  2.16  2.30  18.0  105 2.95  3.32 0.22  2.38  5.75 1.250  3.17</span>
<span class="co">#&gt; 11   1   14.1  1.48  2.32  16.8   95 2.20  2.43 0.26  1.57  5.00 1.170  2.82</span>
<span class="co">#&gt; 12   1   13.8  1.73  2.41  16.0   89 2.60  2.76 0.29  1.81  5.60 1.150  2.90</span>
<span class="co">#&gt; 13   1   14.8  1.73  2.39  11.4   91 3.10  3.69 0.43  2.81  5.40 1.250  2.73</span>
<span class="co">#&gt; 14   1   14.4  1.87  2.38  12.0  102 3.30  3.64 0.29  2.96  7.50 1.200  3.00</span>
<span class="co">#&gt; 15   1   13.6  1.81  2.70  17.2  112 2.85  2.91 0.30  1.46  7.30 1.280  2.88</span>
<span class="co">#&gt; 16   1   14.3  1.92  2.72  20.0  120 2.80  3.14 0.33  1.97  6.20 1.070  2.65</span>
<span class="co">#&gt; 17   1   13.8  1.57  2.62  20.0  115 2.95  3.40 0.40  1.72  6.60 1.130  2.57</span>
<span class="co">#&gt; 18   1   14.2  1.59  2.48  16.5  108 3.30  3.93 0.32  1.86  8.70 1.230  2.82</span>
<span class="co">#&gt; 19   1   13.6  3.10  2.56  15.2  116 2.70  3.03 0.17  1.66  5.10 0.960  3.36</span>
<span class="co">#&gt; 20   1   14.1  1.63  2.28  16.0  126 3.00  3.17 0.24  2.10  5.65 1.090  3.71</span>
<span class="co">#&gt; 21   1   12.9  3.80  2.65  18.6  102 2.41  2.41 0.25  1.98  4.50 1.030  3.52</span>
<span class="co">#&gt; 22   1   13.7  1.86  2.36  16.6  101 2.61  2.88 0.27  1.69  3.80 1.110  4.00</span>
<span class="co">#&gt; 23   1   12.8  1.60  2.52  17.8   95 2.48  2.37 0.26  1.46  3.93 1.090  3.63</span>
<span class="co">#&gt; 24   1   13.5  1.81  2.61  20.0   96 2.53  2.61 0.28  1.66  3.52 1.120  3.82</span>
<span class="co">#&gt; 25   1   13.1  2.05  3.22  25.0  124 2.63  2.68 0.47  1.92  3.58 1.130  3.20</span>
<span class="co">#&gt; 26   1   13.4  1.77  2.62  16.1   93 2.85  2.94 0.34  1.45  4.80 0.920  3.22</span>
<span class="co">#&gt; 27   1   13.3  1.72  2.14  17.0   94 2.40  2.19 0.27  1.35  3.95 1.020  2.77</span>
<span class="co">#&gt; 28   1   13.9  1.90  2.80  19.4  107 2.95  2.97 0.37  1.76  4.50 1.250  3.40</span>
<span class="co">#&gt; 29   1   14.0  1.68  2.21  16.0   96 2.65  2.33 0.26  1.98  4.70 1.040  3.59</span>
<span class="co">#&gt; 30   1   13.7  1.50  2.70  22.5  101 3.00  3.25 0.29  2.38  5.70 1.190  2.71</span>
<span class="co">#&gt; 31   1   13.6  1.66  2.36  19.1  106 2.86  3.19 0.22  1.95  6.90 1.090  2.88</span>
<span class="co">#&gt; 32   1   13.7  1.83  2.36  17.2  104 2.42  2.69 0.42  1.97  3.84 1.230  2.87</span>
<span class="co">#&gt; 33   1   13.8  1.53  2.70  19.5  132 2.95  2.74 0.50  1.35  5.40 1.250  3.00</span>
<span class="co">#&gt; 34   1   13.5  1.80  2.65  19.0  110 2.35  2.53 0.29  1.54  4.20 1.100  2.87</span>
<span class="co">#&gt; 35   1   13.5  1.81  2.41  20.5  100 2.70  2.98 0.26  1.86  5.10 1.040  3.47</span>
<span class="co">#&gt; 36   1   13.3  1.64  2.84  15.5  110 2.60  2.68 0.34  1.36  4.60 1.090  2.78</span>
<span class="co">#&gt; 37   1   13.1  1.65  2.55  18.0   98 2.45  2.43 0.29  1.44  4.25 1.120  2.51</span>
<span class="co">#&gt; 38   1   13.1  1.50  2.10  15.5   98 2.40  2.64 0.28  1.37  3.70 1.180  2.69</span>
<span class="co">#&gt; 39   1   14.2  3.99  2.51  13.2  128 3.00  3.04 0.20  2.08  5.10 0.890  3.53</span>
<span class="co">#&gt; 40   1   13.6  1.71  2.31  16.2  117 3.15  3.29 0.34  2.34  6.13 0.950  3.38</span>
<span class="co">#&gt; 41   1   13.4  3.84  2.12  18.8   90 2.45  2.68 0.27  1.48  4.28 0.910  3.00</span>
<span class="co">#&gt; 42   1   13.9  1.89  2.59  15.0  101 3.25  3.56 0.17  1.70  5.43 0.880  3.56</span>
<span class="co">#&gt; 43   1   13.2  3.98  2.29  17.5  103 2.64  2.63 0.32  1.66  4.36 0.820  3.00</span>
<span class="co">#&gt; 44   1   13.1  1.77  2.10  17.0  107 3.00  3.00 0.28  2.03  5.04 0.880  3.35</span>
<span class="co">#&gt; 45   1   14.2  4.04  2.44  18.9  111 2.85  2.65 0.30  1.25  5.24 0.870  3.33</span>
<span class="co">#&gt; 46   1   14.4  3.59  2.28  16.0  102 3.25  3.17 0.27  2.19  4.90 1.040  3.44</span>
<span class="co">#&gt; 47   1   13.9  1.68  2.12  16.0  101 3.10  3.39 0.21  2.14  6.10 0.910  3.33</span>
<span class="co">#&gt; 48   1   14.1  2.02  2.40  18.8  103 2.75  2.92 0.32  2.38  6.20 1.070  2.75</span>
<span class="co">#&gt; 49   1   13.9  1.73  2.27  17.4  108 2.88  3.54 0.32  2.08  8.90 1.120  3.10</span>
<span class="co">#&gt; 50   1   13.1  1.73  2.04  12.4   92 2.72  3.27 0.17  2.91  7.20 1.120  2.91</span>
<span class="co">#&gt; 51   1   13.8  1.65  2.60  17.2   94 2.45  2.99 0.22  2.29  5.60 1.240  3.37</span>
<span class="co">#&gt; 52   1   13.8  1.75  2.42  14.0  111 3.88  3.74 0.32  1.87  7.05 1.010  3.26</span>
<span class="co">#&gt; 53   1   13.8  1.90  2.68  17.1  115 3.00  2.79 0.39  1.68  6.30 1.130  2.93</span>
<span class="co">#&gt; 54   1   13.7  1.67  2.25  16.4  118 2.60  2.90 0.21  1.62  5.85 0.920  3.20</span>
<span class="co">#&gt; 55   1   13.6  1.73  2.46  20.5  116 2.96  2.78 0.20  2.45  6.25 0.980  3.03</span>
<span class="co">#&gt; 56   1   14.2  1.70  2.30  16.3  118 3.20  3.00 0.26  2.03  6.38 0.940  3.31</span>
<span class="co">#&gt; 57   1   13.3  1.97  2.68  16.8  102 3.00  3.23 0.31  1.66  6.00 1.070  2.84</span>
<span class="co">#&gt; 58   1   13.7  1.43  2.50  16.7  108 3.40  3.67 0.19  2.04  6.80 0.890  2.87</span>
<span class="co">#&gt; 59   2   12.4  0.94  1.36  10.6   88 1.98  0.57 0.28  0.42  1.95 1.050  1.82</span>
<span class="co">#&gt; 60   2   12.3  1.10  2.28  16.0  101 2.05  1.09 0.63  0.41  3.27 1.250  1.67</span>
<span class="co">#&gt; 61   2   12.6  1.36  2.02  16.8  100 2.02  1.41 0.53  0.62  5.75 0.980  1.59</span>
<span class="co">#&gt; 62   2   13.7  1.25  1.92  18.0   94 2.10  1.79 0.32  0.73  3.80 1.230  2.46</span>
<span class="co">#&gt; 63   2   12.4  1.13  2.16  19.0   87 3.50  3.10 0.19  1.87  4.45 1.220  2.87</span>
<span class="co">#&gt; 64   2   12.2  1.45  2.53  19.0  104 1.89  1.75 0.45  1.03  2.95 1.450  2.23</span>
<span class="co">#&gt; 65   2   12.4  1.21  2.56  18.1   98 2.42  2.65 0.37  2.08  4.60 1.190  2.30</span>
<span class="co">#&gt; 66   2   13.1  1.01  1.70  15.0   78 2.98  3.18 0.26  2.28  5.30 1.120  3.18</span>
<span class="co">#&gt; 67   2   12.4  1.17  1.92  19.6   78 2.11  2.00 0.27  1.04  4.68 1.120  3.48</span>
<span class="co">#&gt; 68   2   13.3  0.94  2.36  17.0  110 2.53  1.30 0.55  0.42  3.17 1.020  1.93</span>
<span class="co">#&gt; 69   2   12.2  1.19  1.75  16.8  151 1.85  1.28 0.14  2.50  2.85 1.280  3.07</span>
<span class="co">#&gt; 70   2   12.3  1.61  2.21  20.4  103 1.10  1.02 0.37  1.46  3.05 0.906  1.82</span>
<span class="co">#&gt; 71   2   13.9  1.51  2.67  25.0   86 2.95  2.86 0.21  1.87  3.38 1.360  3.16</span>
<span class="co">#&gt; 72   2   13.5  1.66  2.24  24.0   87 1.88  1.84 0.27  1.03  3.74 0.980  2.78</span>
<span class="co">#&gt; 73   2   13.0  1.67  2.60  30.0  139 3.30  2.89 0.21  1.96  3.35 1.310  3.50</span>
<span class="co">#&gt; 74   2   12.0  1.09  2.30  21.0  101 3.38  2.14 0.13  1.65  3.21 0.990  3.13</span>
<span class="co">#&gt; 75   2   11.7  1.88  1.92  16.0   97 1.61  1.57 0.34  1.15  3.80 1.230  2.14</span>
<span class="co">#&gt; 76   2   13.0  0.90  1.71  16.0   86 1.95  2.03 0.24  1.46  4.60 1.190  2.48</span>
<span class="co">#&gt; 77   2   11.8  2.89  2.23  18.0  112 1.72  1.32 0.43  0.95  2.65 0.960  2.52</span>
<span class="co">#&gt; 78   2   12.3  0.99  1.95  14.8  136 1.90  1.85 0.35  2.76  3.40 1.060  2.31</span>
<span class="co">#&gt; 79   2   12.7  3.87  2.40  23.0  101 2.83  2.55 0.43  1.95  2.57 1.190  3.13</span>
<span class="co">#&gt; 80   2   12.0  0.92  2.00  19.0   86 2.42  2.26 0.30  1.43  2.50 1.380  3.12</span>
<span class="co">#&gt; 81   2   12.7  1.81  2.20  18.8   86 2.20  2.53 0.26  1.77  3.90 1.160  3.14</span>
<span class="co">#&gt; 82   2   12.1  1.13  2.51  24.0   78 2.00  1.58 0.40  1.40  2.20 1.310  2.72</span>
<span class="co">#&gt; 83   2   13.1  3.86  2.32  22.5   85 1.65  1.59 0.61  1.62  4.80 0.840  2.01</span>
<span class="co">#&gt; 84   2   11.8  0.89  2.58  18.0   94 2.20  2.21 0.22  2.35  3.05 0.790  3.08</span>
<span class="co">#&gt; 85   2   12.7  0.98  2.24  18.0   99 2.20  1.94 0.30  1.46  2.62 1.230  3.16</span>
<span class="co">#&gt; 86   2   12.2  1.61  2.31  22.8   90 1.78  1.69 0.43  1.56  2.45 1.330  2.26</span>
<span class="co">#&gt; 87   2   11.7  1.67  2.62  26.0   88 1.92  1.61 0.40  1.34  2.60 1.360  3.21</span>
<span class="co">#&gt; 88   2   11.6  2.06  2.46  21.6   84 1.95  1.69 0.48  1.35  2.80 1.000  2.75</span>
<span class="co">#&gt; 89   2   12.1  1.33  2.30  23.6   70 2.20  1.59 0.42  1.38  1.74 1.070  3.21</span>
<span class="co">#&gt; 90   2   12.1  1.83  2.32  18.5   81 1.60  1.50 0.52  1.64  2.40 1.080  2.27</span>
<span class="co">#&gt; 91   2   12.0  1.51  2.42  22.0   86 1.45  1.25 0.50  1.63  3.60 1.050  2.65</span>
<span class="co">#&gt; 92   2   12.7  1.53  2.26  20.7   80 1.38  1.46 0.58  1.62  3.05 0.960  2.06</span>
<span class="co">#&gt; 93   2   12.3  2.83  2.22  18.0   88 2.45  2.25 0.25  1.99  2.15 1.150  3.30</span>
<span class="co">#&gt; 94   2   11.6  1.99  2.28  18.0   98 3.02  2.26 0.17  1.35  3.25 1.160  2.96</span>
<span class="co">#&gt; 95   2   12.5  1.52  2.20  19.0  162 2.50  2.27 0.32  3.28  2.60 1.160  2.63</span>
<span class="co">#&gt; 96   2   11.8  2.12  2.74  21.5  134 1.60  0.99 0.14  1.56  2.50 0.950  2.26</span>
<span class="co">#&gt; 97   2   12.3  1.41  1.98  16.0   85 2.55  2.50 0.29  1.77  2.90 1.230  2.74</span>
<span class="co">#&gt; 98   2   12.4  1.07  2.10  18.5   88 3.52  3.75 0.24  1.95  4.50 1.040  2.77</span>
<span class="co">#&gt; 99   2   12.3  3.17  2.21  18.0   88 2.85  2.99 0.45  2.81  2.30 1.420  2.83</span>
<span class="co">#&gt; 100  2   12.1  2.08  1.70  17.5   97 2.23  2.17 0.26  1.40  3.30 1.270  2.96</span>
<span class="co">#&gt; 101  2   12.6  1.34  1.90  18.5   88 1.45  1.36 0.29  1.35  2.45 1.040  2.77</span>
<span class="co">#&gt; 102  2   12.3  2.45  2.46  21.0   98 2.56  2.11 0.34  1.31  2.80 0.800  3.38</span>
<span class="co">#&gt; 103  2   11.8  1.72  1.88  19.5   86 2.50  1.64 0.37  1.42  2.06 0.940  2.44</span>
<span class="co">#&gt; 104  2   12.5  1.73  1.98  20.5   85 2.20  1.92 0.32  1.48  2.94 1.040  3.57</span>
<span class="co">#&gt; 105  2   12.4  2.55  2.27  22.0   90 1.68  1.84 0.66  1.42  2.70 0.860  3.30</span>
<span class="co">#&gt; 106  2   12.2  1.73  2.12  19.0   80 1.65  2.03 0.37  1.63  3.40 1.000  3.17</span>
<span class="co">#&gt; 107  2   12.7  1.75  2.28  22.5   84 1.38  1.76 0.48  1.63  3.30 0.880  2.42</span>
<span class="co">#&gt; 108  2   12.2  1.29  1.94  19.0   92 2.36  2.04 0.39  2.08  2.70 0.860  3.02</span>
<span class="co">#&gt; 109  2   11.6  1.35  2.70  20.0   94 2.74  2.92 0.29  2.49  2.65 0.960  3.26</span>
<span class="co">#&gt; 110  2   11.5  3.74  1.82  19.5  107 3.18  2.58 0.24  3.58  2.90 0.750  2.81</span>
<span class="co">#&gt; 111  2   12.5  2.43  2.17  21.0   88 2.55  2.27 0.26  1.22  2.00 0.900  2.78</span>
<span class="co">#&gt; 112  2   11.8  2.68  2.92  20.0  103 1.75  2.03 0.60  1.05  3.80 1.230  2.50</span>
<span class="co">#&gt; 113  2   11.4  0.74  2.50  21.0   88 2.48  2.01 0.42  1.44  3.08 1.100  2.31</span>
<span class="co">#&gt; 114  2   12.1  1.39  2.50  22.5   84 2.56  2.29 0.43  1.04  2.90 0.930  3.19</span>
<span class="co">#&gt; 115  2   11.0  1.51  2.20  21.5   85 2.46  2.17 0.52  2.01  1.90 1.710  2.87</span>
<span class="co">#&gt; 116  2   11.8  1.47  1.99  20.8   86 1.98  1.60 0.30  1.53  1.95 0.950  3.33</span>
<span class="co">#&gt; 117  2   12.4  1.61  2.19  22.5  108 2.00  2.09 0.34  1.61  2.06 1.060  2.96</span>
<span class="co">#&gt; 118  2   12.8  3.43  1.98  16.0   80 1.63  1.25 0.43  0.83  3.40 0.700  2.12</span>
<span class="co">#&gt; 119  2   12.0  3.43  2.00  19.0   87 2.00  1.64 0.37  1.87  1.28 0.930  3.05</span>
<span class="co">#&gt; 120  2   11.4  2.40  2.42  20.0   96 2.90  2.79 0.32  1.83  3.25 0.800  3.39</span>
<span class="co">#&gt; 121  2   11.6  2.05  3.23  28.5  119 3.18  5.08 0.47  1.87  6.00 0.930  3.69</span>
<span class="co">#&gt; 122  2   12.4  4.43  2.73  26.5  102 2.20  2.13 0.43  1.71  2.08 0.920  3.12</span>
<span class="co">#&gt; 123  2   13.1  5.80  2.13  21.5   86 2.62  2.65 0.30  2.01  2.60 0.730  3.10</span>
<span class="co">#&gt; 124  2   11.9  4.31  2.39  21.0   82 2.86  3.03 0.21  2.91  2.80 0.750  3.64</span>
<span class="co">#&gt; 125  2   12.1  2.16  2.17  21.0   85 2.60  2.65 0.37  1.35  2.76 0.860  3.28</span>
<span class="co">#&gt; 126  2   12.4  1.53  2.29  21.5   86 2.74  3.15 0.39  1.77  3.94 0.690  2.84</span>
<span class="co">#&gt; 127  2   11.8  2.13  2.78  28.5   92 2.13  2.24 0.58  1.76  3.00 0.970  2.44</span>
<span class="co">#&gt; 128  2   12.4  1.63  2.30  24.5   88 2.22  2.45 0.40  1.90  2.12 0.890  2.78</span>
<span class="co">#&gt; 129  2   12.0  4.30  2.38  22.0   80 2.10  1.75 0.42  1.35  2.60 0.790  2.57</span>
<span class="co">#&gt; 130  3   12.9  1.35  2.32  18.0  122 1.51  1.25 0.21  0.94  4.10 0.760  1.29</span>
<span class="co">#&gt; 131  3   12.9  2.99  2.40  20.0  104 1.30  1.22 0.24  0.83  5.40 0.740  1.42</span>
<span class="co">#&gt; 132  3   12.8  2.31  2.40  24.0   98 1.15  1.09 0.27  0.83  5.70 0.660  1.36</span>
<span class="co">#&gt; 133  3   12.7  3.55  2.36  21.5  106 1.70  1.20 0.17  0.84  5.00 0.780  1.29</span>
<span class="co">#&gt; 134  3   12.5  1.24  2.25  17.5   85 2.00  0.58 0.60  1.25  5.45 0.750  1.51</span>
<span class="co">#&gt; 135  3   12.6  2.46  2.20  18.5   94 1.62  0.66 0.63  0.94  7.10 0.730  1.58</span>
<span class="co">#&gt; 136  3   12.2  4.72  2.54  21.0   89 1.38  0.47 0.53  0.80  3.85 0.750  1.27</span>
<span class="co">#&gt; 137  3   12.5  5.51  2.64  25.0   96 1.79  0.60 0.63  1.10  5.00 0.820  1.69</span>
<span class="co">#&gt; 138  3   13.5  3.59  2.19  19.5   88 1.62  0.48 0.58  0.88  5.70 0.810  1.82</span>
<span class="co">#&gt; 139  3   12.8  2.96  2.61  24.0  101 2.32  0.60 0.53  0.81  4.92 0.890  2.15</span>
<span class="co">#&gt; 140  3   12.9  2.81  2.70  21.0   96 1.54  0.50 0.53  0.75  4.60 0.770  2.31</span>
<span class="co">#&gt; 141  3   13.4  2.56  2.35  20.0   89 1.40  0.50 0.37  0.64  5.60 0.700  2.47</span>
<span class="co">#&gt; 142  3   13.5  3.17  2.72  23.5   97 1.55  0.52 0.50  0.55  4.35 0.890  2.06</span>
<span class="co">#&gt; 143  3   13.6  4.95  2.35  20.0   92 2.00  0.80 0.47  1.02  4.40 0.910  2.05</span>
<span class="co">#&gt; 144  3   12.2  3.88  2.20  18.5  112 1.38  0.78 0.29  1.14  8.21 0.650  2.00</span>
<span class="co">#&gt; 145  3   13.2  3.57  2.15  21.0  102 1.50  0.55 0.43  1.30  4.00 0.600  1.68</span>
<span class="co">#&gt; 146  3   13.9  5.04  2.23  20.0   80 0.98  0.34 0.40  0.68  4.90 0.580  1.33</span>
<span class="co">#&gt; 147  3   12.9  4.61  2.48  21.5   86 1.70  0.65 0.47  0.86  7.65 0.540  1.86</span>
<span class="co">#&gt; 148  3   13.3  3.24  2.38  21.5   92 1.93  0.76 0.45  1.25  8.42 0.550  1.62</span>
<span class="co">#&gt; 149  3   13.1  3.90  2.36  21.5  113 1.41  1.39 0.34  1.14  9.40 0.570  1.33</span>
<span class="co">#&gt; 150  3   13.5  3.12  2.62  24.0  123 1.40  1.57 0.22  1.25  8.60 0.590  1.30</span>
<span class="co">#&gt; 151  3   12.8  2.67  2.48  22.0  112 1.48  1.36 0.24  1.26 10.80 0.480  1.47</span>
<span class="co">#&gt; 152  3   13.1  1.90  2.75  25.5  116 2.20  1.28 0.26  1.56  7.10 0.610  1.33</span>
<span class="co">#&gt; 153  3   13.2  3.30  2.28  18.5   98 1.80  0.83 0.61  1.87 10.52 0.560  1.51</span>
<span class="co">#&gt; 154  3   12.6  1.29  2.10  20.0  103 1.48  0.58 0.53  1.40  7.60 0.580  1.55</span>
<span class="co">#&gt; 155  3   13.2  5.19  2.32  22.0   93 1.74  0.63 0.61  1.55  7.90 0.600  1.48</span>
<span class="co">#&gt; 156  3   13.8  4.12  2.38  19.5   89 1.80  0.83 0.48  1.56  9.01 0.570  1.64</span>
<span class="co">#&gt; 157  3   12.4  3.03  2.64  27.0   97 1.90  0.58 0.63  1.14  7.50 0.670  1.73</span>
<span class="co">#&gt; 158  3   14.3  1.68  2.70  25.0   98 2.80  1.31 0.53  2.70 13.00 0.570  1.96</span>
<span class="co">#&gt; 159  3   13.5  1.67  2.64  22.5   89 2.60  1.10 0.52  2.29 11.75 0.570  1.78</span>
<span class="co">#&gt; 160  3   12.4  3.83  2.38  21.0   88 2.30  0.92 0.50  1.04  7.65 0.560  1.58</span>
<span class="co">#&gt; 161  3   13.7  3.26  2.54  20.0  107 1.83  0.56 0.50  0.80  5.88 0.960  1.82</span>
<span class="co">#&gt; 162  3   12.8  3.27  2.58  22.0  106 1.65  0.60 0.60  0.96  5.58 0.870  2.11</span>
<span class="co">#&gt; 163  3   13.0  3.45  2.35  18.5  106 1.39  0.70 0.40  0.94  5.28 0.680  1.75</span>
<span class="co">#&gt; 164  3   13.8  2.76  2.30  22.0   90 1.35  0.68 0.41  1.03  9.58 0.700  1.68</span>
<span class="co">#&gt; 165  3   13.7  4.36  2.26  22.5   88 1.28  0.47 0.52  1.15  6.62 0.780  1.75</span>
<span class="co">#&gt; 166  3   13.4  3.70  2.60  23.0  111 1.70  0.92 0.43  1.46 10.68 0.850  1.56</span>
<span class="co">#&gt; 167  3   12.8  3.37  2.30  19.5   88 1.48  0.66 0.40  0.97 10.26 0.720  1.75</span>
<span class="co">#&gt; 168  3   13.6  2.58  2.69  24.5  105 1.55  0.84 0.39  1.54  8.66 0.740  1.80</span>
<span class="co">#&gt; 169  3   13.4  4.60  2.86  25.0  112 1.98  0.96 0.27  1.11  8.50 0.670  1.92</span>
<span class="co">#&gt; 170  3   12.2  3.03  2.32  19.0   96 1.25  0.49 0.40  0.73  5.50 0.660  1.83</span>
<span class="co">#&gt; 171  3   12.8  2.39  2.28  19.5   86 1.39  0.51 0.48  0.64  9.90 0.570  1.63</span>
<span class="co">#&gt; 172  3   14.2  2.51  2.48  20.0   91 1.68  0.70 0.44  1.24  9.70 0.620  1.71</span>
<span class="co">#&gt; 173  3   13.7  5.65  2.45  20.5   95 1.68  0.61 0.52  1.06  7.70 0.640  1.74</span>
<span class="co">#&gt; 174  3   13.4  3.91  2.48  23.0  102 1.80  0.75 0.43  1.41  7.30 0.700  1.56</span>
<span class="co">#&gt; 175  3   13.3  4.28  2.26  20.0  120 1.59  0.69 0.43  1.35 10.20 0.590  1.56</span>
<span class="co">#&gt; 176  3   13.2  2.59  2.37  20.0  120 1.65  0.68 0.53  1.46  9.30 0.600  1.62</span>
<span class="co">#&gt; 177  3   14.1  4.10  2.74  24.5   96 2.05  0.76 0.56  1.35  9.20 0.610  1.60</span>
<span class="co">#&gt;     X1065</span>
<span class="co">#&gt; 1    1050</span>
<span class="co">#&gt; 2    1185</span>
<span class="co">#&gt; 3    1480</span>
<span class="co">#&gt; 4     735</span>
<span class="co">#&gt; 5    1450</span>
<span class="co">#&gt; 6    1290</span>
<span class="co">#&gt; 7    1295</span>
<span class="co">#&gt; 8    1045</span>
<span class="co">#&gt; 9    1045</span>
<span class="co">#&gt; 10   1510</span>
<span class="co">#&gt; 11   1280</span>
<span class="co">#&gt; 12   1320</span>
<span class="co">#&gt; 13   1150</span>
<span class="co">#&gt; 14   1547</span>
<span class="co">#&gt; 15   1310</span>
<span class="co">#&gt; 16   1280</span>
<span class="co">#&gt; 17   1130</span>
<span class="co">#&gt; 18   1680</span>
<span class="co">#&gt; 19    845</span>
<span class="co">#&gt; 20    780</span>
<span class="co">#&gt; 21    770</span>
<span class="co">#&gt; 22   1035</span>
<span class="co">#&gt; 23   1015</span>
<span class="co">#&gt; 24    845</span>
<span class="co">#&gt; 25    830</span>
<span class="co">#&gt; 26   1195</span>
<span class="co">#&gt; 27   1285</span>
<span class="co">#&gt; 28    915</span>
<span class="co">#&gt; 29   1035</span>
<span class="co">#&gt; 30   1285</span>
<span class="co">#&gt; 31   1515</span>
<span class="co">#&gt; 32    990</span>
<span class="co">#&gt; 33   1235</span>
<span class="co">#&gt; 34   1095</span>
<span class="co">#&gt; 35    920</span>
<span class="co">#&gt; 36    880</span>
<span class="co">#&gt; 37   1105</span>
<span class="co">#&gt; 38   1020</span>
<span class="co">#&gt; 39    760</span>
<span class="co">#&gt; 40    795</span>
<span class="co">#&gt; 41   1035</span>
<span class="co">#&gt; 42   1095</span>
<span class="co">#&gt; 43    680</span>
<span class="co">#&gt; 44    885</span>
<span class="co">#&gt; 45   1080</span>
<span class="co">#&gt; 46   1065</span>
<span class="co">#&gt; 47    985</span>
<span class="co">#&gt; 48   1060</span>
<span class="co">#&gt; 49   1260</span>
<span class="co">#&gt; 50   1150</span>
<span class="co">#&gt; 51   1265</span>
<span class="co">#&gt; 52   1190</span>
<span class="co">#&gt; 53   1375</span>
<span class="co">#&gt; 54   1060</span>
<span class="co">#&gt; 55   1120</span>
<span class="co">#&gt; 56    970</span>
<span class="co">#&gt; 57   1270</span>
<span class="co">#&gt; 58   1285</span>
<span class="co">#&gt; 59    520</span>
<span class="co">#&gt; 60    680</span>
<span class="co">#&gt; 61    450</span>
<span class="co">#&gt; 62    630</span>
<span class="co">#&gt; 63    420</span>
<span class="co">#&gt; 64    355</span>
<span class="co">#&gt; 65    678</span>
<span class="co">#&gt; 66    502</span>
<span class="co">#&gt; 67    510</span>
<span class="co">#&gt; 68    750</span>
<span class="co">#&gt; 69    718</span>
<span class="co">#&gt; 70    870</span>
<span class="co">#&gt; 71    410</span>
<span class="co">#&gt; 72    472</span>
<span class="co">#&gt; 73    985</span>
<span class="co">#&gt; 74    886</span>
<span class="co">#&gt; 75    428</span>
<span class="co">#&gt; 76    392</span>
<span class="co">#&gt; 77    500</span>
<span class="co">#&gt; 78    750</span>
<span class="co">#&gt; 79    463</span>
<span class="co">#&gt; 80    278</span>
<span class="co">#&gt; 81    714</span>
<span class="co">#&gt; 82    630</span>
<span class="co">#&gt; 83    515</span>
<span class="co">#&gt; 84    520</span>
<span class="co">#&gt; 85    450</span>
<span class="co">#&gt; 86    495</span>
<span class="co">#&gt; 87    562</span>
<span class="co">#&gt; 88    680</span>
<span class="co">#&gt; 89    625</span>
<span class="co">#&gt; 90    480</span>
<span class="co">#&gt; 91    450</span>
<span class="co">#&gt; 92    495</span>
<span class="co">#&gt; 93    290</span>
<span class="co">#&gt; 94    345</span>
<span class="co">#&gt; 95    937</span>
<span class="co">#&gt; 96    625</span>
<span class="co">#&gt; 97    428</span>
<span class="co">#&gt; 98    660</span>
<span class="co">#&gt; 99    406</span>
<span class="co">#&gt; 100   710</span>
<span class="co">#&gt; 101   562</span>
<span class="co">#&gt; 102   438</span>
<span class="co">#&gt; 103   415</span>
<span class="co">#&gt; 104   672</span>
<span class="co">#&gt; 105   315</span>
<span class="co">#&gt; 106   510</span>
<span class="co">#&gt; 107   488</span>
<span class="co">#&gt; 108   312</span>
<span class="co">#&gt; 109   680</span>
<span class="co">#&gt; 110   562</span>
<span class="co">#&gt; 111   325</span>
<span class="co">#&gt; 112   607</span>
<span class="co">#&gt; 113   434</span>
<span class="co">#&gt; 114   385</span>
<span class="co">#&gt; 115   407</span>
<span class="co">#&gt; 116   495</span>
<span class="co">#&gt; 117   345</span>
<span class="co">#&gt; 118   372</span>
<span class="co">#&gt; 119   564</span>
<span class="co">#&gt; 120   625</span>
<span class="co">#&gt; 121   465</span>
<span class="co">#&gt; 122   365</span>
<span class="co">#&gt; 123   380</span>
<span class="co">#&gt; 124   380</span>
<span class="co">#&gt; 125   378</span>
<span class="co">#&gt; 126   352</span>
<span class="co">#&gt; 127   466</span>
<span class="co">#&gt; 128   342</span>
<span class="co">#&gt; 129   580</span>
<span class="co">#&gt; 130   630</span>
<span class="co">#&gt; 131   530</span>
<span class="co">#&gt; 132   560</span>
<span class="co">#&gt; 133   600</span>
<span class="co">#&gt; 134   650</span>
<span class="co">#&gt; 135   695</span>
<span class="co">#&gt; 136   720</span>
<span class="co">#&gt; 137   515</span>
<span class="co">#&gt; 138   580</span>
<span class="co">#&gt; 139   590</span>
<span class="co">#&gt; 140   600</span>
<span class="co">#&gt; 141   780</span>
<span class="co">#&gt; 142   520</span>
<span class="co">#&gt; 143   550</span>
<span class="co">#&gt; 144   855</span>
<span class="co">#&gt; 145   830</span>
<span class="co">#&gt; 146   415</span>
<span class="co">#&gt; 147   625</span>
<span class="co">#&gt; 148   650</span>
<span class="co">#&gt; 149   550</span>
<span class="co">#&gt; 150   500</span>
<span class="co">#&gt; 151   480</span>
<span class="co">#&gt; 152   425</span>
<span class="co">#&gt; 153   675</span>
<span class="co">#&gt; 154   640</span>
<span class="co">#&gt; 155   725</span>
<span class="co">#&gt; 156   480</span>
<span class="co">#&gt; 157   880</span>
<span class="co">#&gt; 158   660</span>
<span class="co">#&gt; 159   620</span>
<span class="co">#&gt; 160   520</span>
<span class="co">#&gt; 161   680</span>
<span class="co">#&gt; 162   570</span>
<span class="co">#&gt; 163   675</span>
<span class="co">#&gt; 164   615</span>
<span class="co">#&gt; 165   520</span>
<span class="co">#&gt; 166   695</span>
<span class="co">#&gt; 167   685</span>
<span class="co">#&gt; 168   750</span>
<span class="co">#&gt; 169   630</span>
<span class="co">#&gt; 170   510</span>
<span class="co">#&gt; 171   470</span>
<span class="co">#&gt; 172   660</span>
<span class="co">#&gt; 173   740</span>
<span class="co">#&gt; 174   750</span>
<span class="co">#&gt; 175   835</span>
<span class="co">#&gt; 176   840</span>
<span class="co">#&gt; 177   560</span></code></pre></div>
<div class="sourceCode" id="cb980"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">wines</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"label"</span>,
                  <span class="st">"Alcohol"</span>,
                  <span class="st">"Malic_acid"</span>,
                  <span class="st">"Ash"</span>,
                  <span class="st">"Alcalinity_of_ash"</span>,
                  <span class="st">"Magnesium"</span>,
                  <span class="st">"Total_phenols"</span>,
                  <span class="st">"Flavanoids"</span>,
                  <span class="st">"Nonflavanoid_phenols"</span>,
                  <span class="st">"Proanthocyanins"</span>,
                  <span class="st">"Color_intensity"</span>,
                  <span class="st">"Hue"</span>,
                  <span class="st">"OD280_OD315_of_diluted_wines"</span>,
                  <span class="st">"Proline"</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb981"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">wines</span><span class="op">)</span>
<span class="co">#&gt;   label Alcohol Malic_acid  Ash Alcalinity_of_ash Magnesium Total_phenols</span>
<span class="co">#&gt; 1     1    13.2       1.78 2.14              11.2       100          2.65</span>
<span class="co">#&gt; 2     1    13.2       2.36 2.67              18.6       101          2.80</span>
<span class="co">#&gt; 3     1    14.4       1.95 2.50              16.8       113          3.85</span>
<span class="co">#&gt; 4     1    13.2       2.59 2.87              21.0       118          2.80</span>
<span class="co">#&gt; 5     1    14.2       1.76 2.45              15.2       112          3.27</span>
<span class="co">#&gt; 6     1    14.4       1.87 2.45              14.6        96          2.50</span>
<span class="co">#&gt;   Flavanoids Nonflavanoid_phenols Proanthocyanins Color_intensity  Hue</span>
<span class="co">#&gt; 1       2.76                 0.26            1.28            4.38 1.05</span>
<span class="co">#&gt; 2       3.24                 0.30            2.81            5.68 1.03</span>
<span class="co">#&gt; 3       3.49                 0.24            2.18            7.80 0.86</span>
<span class="co">#&gt; 4       2.69                 0.39            1.82            4.32 1.04</span>
<span class="co">#&gt; 5       3.39                 0.34            1.97            6.75 1.05</span>
<span class="co">#&gt; 6       2.52                 0.30            1.98            5.25 1.02</span>
<span class="co">#&gt;   OD280_OD315_of_diluted_wines Proline</span>
<span class="co">#&gt; 1                         3.40    1050</span>
<span class="co">#&gt; 2                         3.17    1185</span>
<span class="co">#&gt; 3                         3.45    1480</span>
<span class="co">#&gt; 4                         2.93     735</span>
<span class="co">#&gt; 5                         2.85    1450</span>
<span class="co">#&gt; 6                         3.58    1290</span></code></pre></div>
<div class="sourceCode" id="cb982"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">plt1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">wines</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">Alcohol</span>, y <span class="op">=</span> <span class="va">Magnesium</span>, colour <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/h2o/man/as.factor.html">as.factor</a></span><span class="op">(</span><span class="va">label</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>size<span class="op">=</span><span class="fl">3</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="st">"Wines"</span><span class="op">)</span>

<span class="va">plt1</span></code></pre></div>
<div class="inline-figure"><img src="601-nn-classification_904-wine_selection_nn_files/figure-html/unnamed-chunk-6-1.png" width="70%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb983"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">plt2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">wines</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">Alcohol</span>, y <span class="op">=</span> <span class="va">Proline</span>, colour <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/pkg/h2o/man/as.factor.html">as.factor</a></span><span class="op">(</span><span class="va">label</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>size<span class="op">=</span><span class="fl">3</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="st">"Wines"</span><span class="op">)</span>
<span class="va">plt2</span></code></pre></div>
<div class="inline-figure"><img src="601-nn-classification_904-wine_selection_nn_files/figure-html/unnamed-chunk-7-1.png" width="70%" style="display: block; margin: auto;"></div>
</div>
<div id="preprocessing" class="section level2">
<h2>
<span class="header-section-number">40.2</span> Preprocessing<a class="anchor" aria-label="anchor" href="#preprocessing"><i class="fas fa-link"></i></a>
</h2>
<p>During the preprocessing phase, I have to do at least the following two things:</p>
<p>Encode the categorical variables. Standardize the predictors. First of all, let’s encode our target variable. The encoding of the categorical variables is needed when using neuralnet since it does not like factors at all. It will shout at you if you try to feed in a factor (I am told nnet likes factors though).</p>
<p>In the wine dataset the variable label contains three different labels: 1,2 and 3.</p>
<p>The usual practice, as far as I know, is to encode categorical variables as a “one hot” vector. For instance, if I had three classes, like in this case, I’d need to replace the label variable with three variables like these:</p>
<pre><code>#   l1,l2,l3
#   1,0,0
#   0,0,1
#   ...</code></pre>
<p>In this case the first observation would be labeled as a 1, the second would be labeled as a 2, and so on. Ironically, the <code>nnet</code> package provides a function to perform this encoding in a painless way:</p>
<div class="sourceCode" id="cb985"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Encode as a one hot vector multilabel data</span>
<span class="va">train</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/mice/man/cbind.html">cbind</a></span><span class="op">(</span><span class="va">wines</span><span class="op">[</span>, <span class="fl">2</span><span class="op">:</span><span class="fl">14</span><span class="op">]</span>, <span class="fu"><a href="https://rdrr.io/pkg/nnet/man/class.ind.html">class.ind</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/h2o/man/as.factor.html">as.factor</a></span><span class="op">(</span><span class="va">wines</span><span class="op">$</span><span class="va">label</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>

<span class="co"># Set labels name</span>
<span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">train</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">wines</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="fl">14</span><span class="op">]</span>,<span class="st">"l1"</span>,<span class="st">"l2"</span>,<span class="st">"l3"</span><span class="op">)</span></code></pre></div>
<p>By the way, since the predictors are all continuous, you do not need to encode any of them, however, in case you needed to, you could apply the same strategy applied above to all the categorical predictors. Unless of course you’d like to try some other kind of custom encoding.</p>
<p>Now let’s standardize the predictors in the [0−1]"&gt;[0−1] interval by leveraging the <code>lapply</code> function:</p>
<div class="sourceCode" id="cb986"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Scale data</span>
<span class="va">scl</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">{</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span><span class="op">/</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">}</span>
<span class="va">train</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">13</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/lapply.html">lapply</a></span><span class="op">(</span><span class="va">train</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">13</span><span class="op">]</span>, <span class="va">scl</span><span class="op">)</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">train</span><span class="op">)</span>
<span class="co">#&gt;   Alcohol Malic_acid   Ash Alcalinity_of_ash Magnesium Total_phenols Flavanoids</span>
<span class="co">#&gt; 1   0.571      0.206 0.417            0.0309     0.326         0.576      0.511</span>
<span class="co">#&gt; 2   0.561      0.320 0.701            0.4124     0.337         0.628      0.612</span>
<span class="co">#&gt; 3   0.879      0.239 0.610            0.3196     0.467         0.990      0.665</span>
<span class="co">#&gt; 4   0.582      0.366 0.807            0.5361     0.522         0.628      0.496</span>
<span class="co">#&gt; 5   0.834      0.202 0.583            0.2371     0.457         0.790      0.643</span>
<span class="co">#&gt; 6   0.884      0.223 0.583            0.2062     0.283         0.524      0.460</span>
<span class="co">#&gt;   Nonflavanoid_phenols Proanthocyanins Color_intensity   Hue</span>
<span class="co">#&gt; 1                0.245           0.274           0.265 0.463</span>
<span class="co">#&gt; 2                0.321           0.757           0.375 0.447</span>
<span class="co">#&gt; 3                0.208           0.558           0.556 0.309</span>
<span class="co">#&gt; 4                0.491           0.445           0.259 0.455</span>
<span class="co">#&gt; 5                0.396           0.492           0.467 0.463</span>
<span class="co">#&gt; 6                0.321           0.495           0.339 0.439</span>
<span class="co">#&gt;   OD280_OD315_of_diluted_wines Proline l1 l2 l3</span>
<span class="co">#&gt; 1                        0.780   0.551  1  0  0</span>
<span class="co">#&gt; 2                        0.696   0.647  1  0  0</span>
<span class="co">#&gt; 3                        0.799   0.857  1  0  0</span>
<span class="co">#&gt; 4                        0.608   0.326  1  0  0</span>
<span class="co">#&gt; 5                        0.579   0.836  1  0  0</span>
<span class="co">#&gt; 6                        0.846   0.722  1  0  0</span></code></pre></div>
</div>
<div id="fitting-the-model-with-neuralnet" class="section level2">
<h2>
<span class="header-section-number">40.3</span> Fitting the model with neuralnet<a class="anchor" aria-label="anchor" href="#fitting-the-model-with-neuralnet"><i class="fas fa-link"></i></a>
</h2>
<p>Now it is finally time to fit the model. As you might remember from the old post I wrote, <code>neuralnet</code> does not like the formula <code>y~.</code>. Fear not, you can build the formula to be used in a simple step:</p>
<div class="sourceCode" id="cb987"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Set up formula</span>
<span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">train</span><span class="op">)</span>
<span class="va">f</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/formula.html">as.formula</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"l1 + l2 + l3 ~"</span>, <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="va">n</span><span class="op">[</span><span class="op">!</span><span class="va">n</span> <span class="op">%in%</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"l1"</span>,<span class="st">"l2"</span>,<span class="st">"l3"</span><span class="op">)</span><span class="op">]</span>, collapse <span class="op">=</span> <span class="st">" + "</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>
<span class="va">f</span>
<span class="co">#&gt; l1 + l2 + l3 ~ Alcohol + Malic_acid + Ash + Alcalinity_of_ash + </span>
<span class="co">#&gt;     Magnesium + Total_phenols + Flavanoids + Nonflavanoid_phenols + </span>
<span class="co">#&gt;     Proanthocyanins + Color_intensity + Hue + OD280_OD315_of_diluted_wines + </span>
<span class="co">#&gt;     Proline</span></code></pre></div>
<p>Note that the characters in the vector are not pasted to the right of the “~” symbol. Just remember to check that the formula is indeed correct and then you are good to go.</p>
<p>Let’s train the neural network with the full dataset. It should take very little time to converge. If you did not standardize the predictors it could take a lot more though.</p>
<div class="sourceCode" id="cb988"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">nn</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/neuralnet/man/neuralnet.html">neuralnet</a></span><span class="op">(</span><span class="va">f</span>,
                data <span class="op">=</span> <span class="va">train</span>,
                hidden <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">13</span>, <span class="fl">10</span>, <span class="fl">3</span><span class="op">)</span>,
                act.fct <span class="op">=</span> <span class="st">"logistic"</span>,
                linear.output <span class="op">=</span> <span class="cn">FALSE</span>,
                lifesign <span class="op">=</span> <span class="st">"minimal"</span><span class="op">)</span>
<span class="co">#&gt; hidden: 13, 10, 3    thresh: 0.01    rep: 1/1    steps:      88  error: 0.03039  time: 0.06 secs</span></code></pre></div>
<p>Note that I set the argument linear.output to <code>FALSE</code> in order to tell the model that I want to apply the activation function act.fct and that I am not doing a regression task. Then I set the activation function to logistic (which by the way is the default option) in order to apply the logistic function. The other available option is <code>tanh</code> but the model seems to perform a little worse with it so I opted for the default option. As far as I know these two are the only two available options, there is no <code>relu</code> function available although it seems to be a common activation function in other packages.</p>
<p>As far as the number of hidden neurons, I tried some combination and the one used seems to perform slightly better than the others (around 1% of accuracy difference in cross validation score).</p>
<p>By using the in-built <code>plot</code> method you can get a visual take on what is actually happening inside the model, however the plot is not that helpful I think</p>
<div class="sourceCode" id="cb989"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span><span class="op">(</span><span class="va">nn</span><span class="op">)</span></code></pre></div>
<p>Let’s have a look at the accuracy on the training set:</p>
<div class="sourceCode" id="cb990"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Compute predictions</span>
<span class="va">pr.nn</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/neuralnet/man/compute.html">compute</a></span><span class="op">(</span><span class="va">nn</span>, <span class="va">train</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">13</span><span class="op">]</span><span class="op">)</span>

<span class="co"># Extract results</span>
<span class="va">pr.nn_</span> <span class="op">&lt;-</span> <span class="va">pr.nn</span><span class="op">$</span><span class="va">net.result</span>
<span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">pr.nn_</span><span class="op">)</span>
<span class="co">#&gt;       [,1]    [,2]     [,3]</span>
<span class="co">#&gt; [1,] 0.990 0.00317 6.99e-06</span>
<span class="co">#&gt; [2,] 0.991 0.00233 8.69e-06</span>
<span class="co">#&gt; [3,] 0.991 0.00210 8.65e-06</span>
<span class="co">#&gt; [4,] 0.986 0.00442 8.74e-06</span>
<span class="co">#&gt; [5,] 0.992 0.00212 8.32e-06</span>
<span class="co">#&gt; [6,] 0.992 0.00214 8.34e-06</span></code></pre></div>
<div class="sourceCode" id="cb991"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Accuracy (training set)</span>
<span class="va">original_values</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/maxCol.html">max.col</a></span><span class="op">(</span><span class="va">train</span><span class="op">[</span>, <span class="fl">14</span><span class="op">:</span><span class="fl">16</span><span class="op">]</span><span class="op">)</span>
<span class="va">pr.nn_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/maxCol.html">max.col</a></span><span class="op">(</span><span class="va">pr.nn_</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">pr.nn_2</span> <span class="op">==</span> <span class="va">original_values</span><span class="op">)</span>
<span class="co">#&gt; [1] 1</span></code></pre></div>
<p>100% not bad! But wait, this may be because our model over fitted the data, furthermore evaluating accuracy on the training set is kind of cheating since the model already “knows” (or should know) the answers. In order to assess the “true accuracy” of the model you need to perform some kind of cross validation.</p>
</div>
<div id="cross-validating-the-classifier" class="section level2">
<h2>
<span class="header-section-number">40.4</span> Cross validating the classifier<a class="anchor" aria-label="anchor" href="#cross-validating-the-classifier"><i class="fas fa-link"></i></a>
</h2>
<p>Let’s cross-validate the model using the evergreen 10 fold cross validation with the following train and test split: 95% of the dataset will be used as training set while the remaining 5% as test set.</p>
<p>Just out of curiosity, I decided to run a LOOCV round too. In case you’d like to run this cross validation technique, just set the proportion variable to 0.995: this will select just one observation for as test set and leave all the other observations as training set. Running LOOCV you should get similar results to the 10 fold cross validation.</p>
<div class="sourceCode" id="cb992"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Set seed for reproducibility purposes</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">500</span><span class="op">)</span>
<span class="co"># 10 fold cross validation</span>
<span class="va">k</span> <span class="op">&lt;-</span> <span class="fl">10</span>
<span class="co"># Results from cv</span>
<span class="va">outs</span> <span class="op">&lt;-</span> <span class="cn">NULL</span>
<span class="co"># Train test split proportions</span>
<span class="va">proportion</span> <span class="op">&lt;-</span> <span class="fl">0.95</span> <span class="co"># Set to 0.995 for LOOCV</span>

<span class="co"># Crossvalidate, go!</span>
<span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">k</span><span class="op">)</span>
<span class="op">{</span>
    <span class="va">index</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/sample.html">sample</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">train</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/pkg/h2o/man/h2o.round.html">round</a></span><span class="op">(</span><span class="va">proportion</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">train</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>
    <span class="va">train_cv</span> <span class="op">&lt;-</span> <span class="va">train</span><span class="op">[</span><span class="va">index</span>, <span class="op">]</span>
    <span class="va">test_cv</span> <span class="op">&lt;-</span> <span class="va">train</span><span class="op">[</span><span class="op">-</span><span class="va">index</span>, <span class="op">]</span>
    <span class="va">nn_cv</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/neuralnet/man/neuralnet.html">neuralnet</a></span><span class="op">(</span><span class="va">f</span>,
                        data <span class="op">=</span> <span class="va">train_cv</span>,
                        hidden <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">13</span>, <span class="fl">10</span>, <span class="fl">3</span><span class="op">)</span>,
                        act.fct <span class="op">=</span> <span class="st">"logistic"</span>,
                        linear.output <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span>
    
    <span class="co"># Compute predictions</span>
    <span class="va">pr.nn</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/neuralnet/man/compute.html">compute</a></span><span class="op">(</span><span class="va">nn_cv</span>, <span class="va">test_cv</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">13</span><span class="op">]</span><span class="op">)</span>
    <span class="co"># Extract results</span>
    <span class="va">pr.nn_</span> <span class="op">&lt;-</span> <span class="va">pr.nn</span><span class="op">$</span><span class="va">net.result</span>
    <span class="co"># Accuracy (test set)</span>
    <span class="va">original_values</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/maxCol.html">max.col</a></span><span class="op">(</span><span class="va">test_cv</span><span class="op">[</span>, <span class="fl">14</span><span class="op">:</span><span class="fl">16</span><span class="op">]</span><span class="op">)</span>
    <span class="va">pr.nn_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/maxCol.html">max.col</a></span><span class="op">(</span><span class="va">pr.nn_</span><span class="op">)</span>
    <span class="va">outs</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">pr.nn_2</span> <span class="op">==</span> <span class="va">original_values</span><span class="op">)</span>
<span class="op">}</span>

<span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">outs</span><span class="op">)</span>
<span class="co">#&gt; [1] 0.978</span></code></pre></div>
<p>98.8%, awesome! Next time when you are invited to a relaxing evening that includes a wine tasting competition I think you should definitely bring your laptop as a contestant!</p>
<p>Aside from that poor taste joke, (I made it again!), indeed this dataset is not the most challenging, I think with some more tweaking a better cross validation score could be achieved. Nevertheless I hope you found this tutorial useful. A gist with the entire code for this tutorial can be found here.</p>
<p>Thank you for reading this article, please feel free to leave a comment if you have any questions or suggestions and share the post with others if you find it useful.</p>
<p>Notes:</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="credit-scoring-with-neuralnet.html"><span class="header-section-number">39</span> Credit Scoring with neuralnet</a></div>
<div class="next"><a href="predicting-the-rating-of-cereals.html"><span class="header-section-number">41</span> Predicting the rating of cereals</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#wine-classification-with-neuralnet"><span class="header-section-number">40</span> Wine classification with neuralnet</a></li>
<li><a class="nav-link" href="#the-dataset"><span class="header-section-number">40.1</span> The dataset</a></li>
<li><a class="nav-link" href="#preprocessing"><span class="header-section-number">40.2</span> Preprocessing</a></li>
<li><a class="nav-link" href="#fitting-the-model-with-neuralnet"><span class="header-section-number">40.3</span> Fitting the model with neuralnet</a></li>
<li><a class="nav-link" href="#cross-validating-the-classifier"><span class="header-section-number">40.4</span> Cross validating the classifier</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/hadley/r-pkgs/blob/master/601-nn-classification_904-wine_selection_nn.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/hadley/r-pkgs/edit/master/601-nn-classification_904-wine_selection_nn.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>A Machine Learning Compilation</strong>" was written by Several authors. Compiled by Alfonso R. Reyes. It was last built on 2020-11-19.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
