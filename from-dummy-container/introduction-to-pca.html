<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 2 Introduction to PCA | A Machine Learning Compilation</title>
<meta name="author" content="Several authors. Compiled by Alfonso R. Reyes">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.5.3/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.5.3/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.2.9000/tabs.js"></script><script src="libs/bs3compat-0.2.2.9000/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">A Machine Learning Compilation</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Preface</a></li>
<li class="book-part">The Basics of Machine Learning</li>
<li><a class="active" href="introduction-to-pca.html"><span class="header-section-number">2</span> Introduction to PCA</a></li>
<li><a class="" href="comparison-of-two-pca-packages.html"><span class="header-section-number">3</span> Comparison of two PCA packages</a></li>
<li><a class="" href="detailed-study-of-principal-component-analysis.html"><span class="header-section-number">4</span> Detailed study of Principal Component Analysis</a></li>
<li><a class="" href="detection-of-diabetes-using-logistic-regression.html"><span class="header-section-number">5</span> Detection of diabetes using Logistic Regression</a></li>
<li><a class="" href="sensitivity-analysis-for-a-neural-network.html"><span class="header-section-number">6</span> Sensitivity analysis for a neural network</a></li>
<li><a class="" href="data-visualization-for-ml-models.html"><span class="header-section-number">7</span> Data Visualization for ML models</a></li>
<li class="book-part">Feature Engineering</li>
<li><a class="" href="ten-methods-to-assess-variable-importance.html"><span class="header-section-number">8</span> Ten methods to assess Variable Importance</a></li>
<li><a class="" href="employee-attrition-using-feature-importance.html"><span class="header-section-number">9</span> Employee Attrition using Feature Importance</a></li>
<li class="book-part">Classification</li>
<li><a class="" href="a-gentle-introduction-to-support-vector-machines.html"><span class="header-section-number">10</span> A gentle introduction to Support Vector Machines</a></li>
<li><a class="" href="broad-view-of-svm.html"><span class="header-section-number">11</span> Broad view of SVM</a></li>
<li><a class="" href="feature-selection-to-enhance-cancer-detection.html"><span class="header-section-number">12</span> Feature Selection to enhance cancer detection</a></li>
<li><a class="" href="dealing-with-unbalanced-data.html"><span class="header-section-number">13</span> Dealing with unbalanced data</a></li>
<li><a class="" href="imputting-missing-values-with-random-forest.html"><span class="header-section-number">14</span> Imputting missing values with Random Forest</a></li>
<li><a class="" href="tuning-of-support-vector-machine-prediction.html"><span class="header-section-number">15</span> Tuning of Support Vector Machine prediction</a></li>
<li class="book-part">Classification</li>
<li><a class="" href="introduction-to-algorithms-for-classification.html"><span class="header-section-number">16</span> Introduction to algorithms for Classification</a></li>
<li><a class="" href="comparing-classification-algorithms.html"><span class="header-section-number">17</span> Comparing Classification algorithms</a></li>
<li><a class="" href="who-buys-social-network-ads.html"><span class="header-section-number">18</span> Who buys Social Network ads</a></li>
<li><a class="" href="predicting-ozone-levels.html"><span class="header-section-number">19</span> Predicting Ozone levels</a></li>
<li><a class="" href="building-a-naive-bayes-classifier.html"><span class="header-section-number">20</span> Building a Naive Bayes Classifier</a></li>
<li><a class="" href="linear-and-non-linear-algorithms-for-classification.html"><span class="header-section-number">21</span> Linear and Non-Linear Algorithms for Classification</a></li>
<li><a class="" href="detect-mines-vs-rocks-with-random-forest.html"><span class="header-section-number">22</span> Detect mines vs rocks with Random Forest</a></li>
<li><a class="" href="predicting-the-type-of-glass.html"><span class="header-section-number">23</span> Predicting the type of glass</a></li>
<li><a class="" href="naive-bayes-for-sms-spam.html"><span class="header-section-number">24</span> Naive Bayes for SMS spam</a></li>
<li><a class="" href="vehicles-classiification-with-decision-trees.html"><span class="header-section-number">25</span> Vehicles classiification with Decision Trees</a></li>
<li><a class="" href="applying-naive-bayes-on-the-titanic-case.html"><span class="header-section-number">26</span> Applying Naive-Bayes on the Titanic case</a></li>
<li><a class="" href="classification-on-bad-loans.html"><span class="header-section-number">27</span> Classification on bad loans</a></li>
<li><a class="" href="predicting-flu-outcome-comparing-eight-classification-algorithms.html"><span class="header-section-number">28</span> Predicting Flu outcome comparing eight classification algorithms</a></li>
<li><a class="" href="a-detailed-study-of-bike-sharing-demand.html"><span class="header-section-number">29</span> A detailed study of bike sharing demand</a></li>
<li><a class="" href="prediction-of-arrhythmia-with-deep-neural-nets.html"><span class="header-section-number">30</span> Prediction of arrhythmia with deep neural nets</a></li>
<li class="book-part">Linear Regression</li>
<li><a class="" href="linear-regression-with-islr.html"><span class="header-section-number">31</span> Linear Regression with ISLR</a></li>
<li><a class="" href="evaluation-of-three-linear-regression-models.html"><span class="header-section-number">32</span> Evaluation of three linear regression models</a></li>
<li><a class="" href="comparison-of-six-linear-regression-algorithms.html"><span class="header-section-number">33</span> Comparison of six Linear Regression algorithms</a></li>
<li><a class="" href="comparing-regression-models.html"><span class="header-section-number">34</span> Comparing regression models</a></li>
<li><a class="" href="finding-the-factors-of-happiness.html"><span class="header-section-number">35</span> Finding the factors of happiness</a></li>
<li><a class="" href="regression-with-a-neural-network.html"><span class="header-section-number">36</span> Regression with a neural network</a></li>
<li><a class="" href="comparing-multiple-regression-vs-a-neural-network.html"><span class="header-section-number">37</span> Comparing Multiple Regression vs a Neural Network</a></li>
<li><a class="" href="temperature-modeling-using-nested-dataframes.html"><span class="header-section-number">38</span> Temperature modeling using nested dataframes</a></li>
<li class="book-part">Neural Networks</li>
<li><a class="" href="credit-scoring-with-neuralnet.html"><span class="header-section-number">39</span> Credit Scoring with neuralnet</a></li>
<li><a class="" href="wine-classification-with-neuralnet.html"><span class="header-section-number">40</span> Wine classification with neuralnet</a></li>
<li><a class="" href="predicting-the-rating-of-cereals.html"><span class="header-section-number">41</span> Predicting the rating of cereals</a></li>
<li><a class="" href="fitting-a-linear-model-with-neural-networks.html"><span class="header-section-number">42</span> Fitting a linear model with neural networks</a></li>
<li><a class="" href="visualization-of-neural-networks.html"><span class="header-section-number">43</span> Visualization of neural networks</a></li>
<li><a class="" href="build-a-fully-connected-r-neural-network-from-scratch.html"><span class="header-section-number">44</span> Build a fully connected R neural network from scratch</a></li>
<li><a class="" href="tuning-hyperparameters-in-a-neural-network.html"><span class="header-section-number">45</span> Tuning Hyperparameters in a Neural Network</a></li>
<li><a class="" href="deep-learning-tips-for-classification-and-regression.html"><span class="header-section-number">46</span> Deep Learning tips for Classification and Regression</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="what-is-dot-hat-in-a-regression-output.html"><span class="header-section-number">A</span> What is dot hat in a regression output</a></li>
<li><a class="" href="q-q-normal-to-compare-data-to-distributions.html"><span class="header-section-number">B</span> Q-Q normal to compare data to distributions</a></li>
<li><a class="" href="qq-and-pp-plots.html"><span class="header-section-number">C</span> QQ and PP Plots</a></li>
<li><a class="" href="visualizing-residuals.html"><span class="header-section-number">D</span> Visualizing residuals</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/hadley/r-pkgs">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="introduction-to-pca" class="section level1">
<h1>
<span class="header-section-number">2</span> Introduction to PCA<a class="anchor" aria-label="anchor" href="#introduction-to-pca"><i class="fas fa-link"></i></a>
</h1>
<ul>
<li><p>Dataset: <code>iris</code></p></li>
<li>
<p>Algorithms:</p>
<ul>
<li>PCA</li>
</ul>
</li>
</ul>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># devtools::install_github("vqv/ggbiplot")</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://github.com/vqv/ggbiplot">ggbiplot</a></span><span class="op">)</span>
<span class="co">#&gt; Loading required package: ggplot2</span>
<span class="co">#&gt; Loading required package: plyr</span>
<span class="co">#&gt; Loading required package: scales</span>
<span class="co">#&gt; Loading required package: grid</span>

<span class="va">iris.pca</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html">prcomp</a></span><span class="op">(</span><span class="va">iris</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span>, center <span class="op">=</span> <span class="cn">TRUE</span>, scale <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">iris.pca</span><span class="op">)</span>
<span class="co">#&gt; Standard deviations (1, .., p=4):</span>
<span class="co">#&gt; [1] 1.708 0.956 0.383 0.144</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Rotation (n x k) = (4 x 4):</span>
<span class="co">#&gt;                 PC1     PC2    PC3    PC4</span>
<span class="co">#&gt; Sepal.Length  0.521 -0.3774  0.720  0.261</span>
<span class="co">#&gt; Sepal.Width  -0.269 -0.9233 -0.244 -0.124</span>
<span class="co">#&gt; Petal.Length  0.580 -0.0245 -0.142 -0.801</span>
<span class="co">#&gt; Petal.Width   0.565 -0.0669 -0.634  0.524</span></code></pre></div>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">iris.pca</span><span class="op">)</span>
<span class="co">#&gt; Importance of components:</span>
<span class="co">#&gt;                         PC1   PC2    PC3     PC4</span>
<span class="co">#&gt; Standard deviation     1.71 0.956 0.3831 0.14393</span>
<span class="co">#&gt; Proportion of Variance 0.73 0.229 0.0367 0.00518</span>
<span class="co">#&gt; Cumulative Proportion  0.73 0.958 0.9948 1.00000</span></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">g</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/ggbiplot/man/ggbiplot.html">ggbiplot</a></span><span class="op">(</span><span class="va">iris.pca</span>,
              obs.scale <span class="op">=</span> <span class="fl">1</span>,
              var.scale <span class="op">=</span> <span class="fl">1</span>,
              groups <span class="op">=</span> <span class="va">iris</span><span class="op">$</span><span class="va">Species</span>,
              ellipse <span class="op">=</span> <span class="cn">TRUE</span>,
              circle <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu">scale_color_discrete</span><span class="op">(</span>name <span class="op">=</span> <span class="st">""</span><span class="op">)</span> <span class="op">+</span>
    <span class="fu">theme</span><span class="op">(</span>legend.direction <span class="op">=</span> <span class="st">"horizontal"</span>, legend.position <span class="op">=</span> <span class="st">"top"</span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">g</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="001-meta_109c-pca-introduction_files/figure-html/unnamed-chunk-3-1.png" width="70%" style="display: block; margin: auto;"></div>
<p>The PC1 axis explains 0.730 of the variance, while the PC2 axis explains 0.229 of the variance.</p>
<div id="underlying-principal-components" class="section level2">
<h2>
<span class="header-section-number">2.1</span> Underlying principal components<a class="anchor" aria-label="anchor" href="#underlying-principal-components"><i class="fas fa-link"></i></a>
</h2>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Run PCA here with prcomp ()</span>
<span class="va">iris.pca</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html">prcomp</a></span><span class="op">(</span><span class="va">iris</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span>, center <span class="op">=</span> <span class="cn">TRUE</span>, scale <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">iris.pca</span><span class="op">)</span>
<span class="co">#&gt; Standard deviations (1, .., p=4):</span>
<span class="co">#&gt; [1] 1.708 0.956 0.383 0.144</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Rotation (n x k) = (4 x 4):</span>
<span class="co">#&gt;                 PC1     PC2    PC3    PC4</span>
<span class="co">#&gt; Sepal.Length  0.521 -0.3774  0.720  0.261</span>
<span class="co">#&gt; Sepal.Width  -0.269 -0.9233 -0.244 -0.124</span>
<span class="co">#&gt; Petal.Length  0.580 -0.0245 -0.142 -0.801</span>
<span class="co">#&gt; Petal.Width   0.565 -0.0669 -0.634  0.524</span></code></pre></div>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Now, compute the new dataset aligned to the PCs by</span>
<span class="co"># using the predict() function .</span>
<span class="va">df.new</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">iris.pca</span>, <span class="va">iris</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">df.new</span><span class="op">)</span>
<span class="co">#&gt;        PC1    PC2     PC3      PC4</span>
<span class="co">#&gt; [1,] -2.26 -0.478  0.1273  0.02409</span>
<span class="co">#&gt; [2,] -2.07  0.672  0.2338  0.10266</span>
<span class="co">#&gt; [3,] -2.36  0.341 -0.0441  0.02828</span>
<span class="co">#&gt; [4,] -2.29  0.595 -0.0910 -0.06574</span>
<span class="co">#&gt; [5,] -2.38 -0.645 -0.0157 -0.03580</span>
<span class="co">#&gt; [6,] -2.07 -1.484 -0.0269  0.00659</span></code></pre></div>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Show the PCA model’s sdev values are the square root</span>
<span class="co"># of the projected variances, which are along the diagonal</span>
<span class="co"># of the covariance matrix of the projected data.</span>
<span class="va">iris.pca</span><span class="op">$</span><span class="va">sdev</span><span class="op">^</span><span class="fl">2</span>
<span class="co">#&gt; [1] 2.9185 0.9140 0.1468 0.0207</span></code></pre></div>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># # Compute covariance matrix for new dataset.</span>
<span class="co"># Recall that the standard deviation is the square root of the variance.</span>
<span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cov</a></span><span class="op">(</span><span class="va">df.new</span><span class="op">)</span>, <span class="fl">5</span><span class="op">)</span>
<span class="co">#&gt;      PC1   PC2   PC3    PC4</span>
<span class="co">#&gt; PC1 2.92 0.000 0.000 0.0000</span>
<span class="co">#&gt; PC2 0.00 0.914 0.000 0.0000</span>
<span class="co">#&gt; PC3 0.00 0.000 0.147 0.0000</span>
<span class="co">#&gt; PC4 0.00 0.000 0.000 0.0207</span></code></pre></div>
</div>
<div id="compute-eigenvectors-and-eigenvalues" class="section level2">
<h2>
<span class="header-section-number">2.2</span> Compute eigenvectors and eigenvalues<a class="anchor" aria-label="anchor" href="#compute-eigenvectors-and-eigenvalues"><i class="fas fa-link"></i></a>
</h2>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Scale and center the data.</span>
<span class="va">df.scaled</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/scale.html">scale</a></span><span class="op">(</span><span class="va">iris</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span>, center <span class="op">=</span> <span class="cn">TRUE</span>, scale <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>

<span class="co"># Compute the covariance matrix.</span>
<span class="va">cov.df.scaled</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cov</a></span><span class="op">(</span><span class="va">df.scaled</span><span class="op">)</span>

<span class="co"># Compute the eigenvectors and eigen values.</span>
<span class="co"># Each eigenvector (column) is a principal component.</span>
<span class="co"># Each eigenvalue is the variance explained by the</span>
<span class="co"># associated eigenvector.</span>
<span class="va">eigenInformation</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/eigen.html">eigen</a></span><span class="op">(</span><span class="va">cov.df.scaled</span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">eigenInformation</span><span class="op">)</span>
<span class="co">#&gt; eigen() decomposition</span>
<span class="co">#&gt; $values</span>
<span class="co">#&gt; [1] 2.9185 0.9140 0.1468 0.0207</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $vectors</span>
<span class="co">#&gt;        [,1]    [,2]   [,3]   [,4]</span>
<span class="co">#&gt; [1,]  0.521 -0.3774  0.720  0.261</span>
<span class="co">#&gt; [2,] -0.269 -0.9233 -0.244 -0.124</span>
<span class="co">#&gt; [3,]  0.580 -0.0245 -0.142 -0.801</span>
<span class="co">#&gt; [4,]  0.565 -0.0669 -0.634  0.524</span></code></pre></div>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Now, compute the new dataset aligned to the PCs by</span>
<span class="co"># multiplying the eigenvector and data matrices.</span>


<span class="co"># Create transposes in preparation for matrix multiplication</span>
<span class="va">eigenvectors.t</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">eigenInformation</span><span class="op">$</span><span class="va">vectors</span><span class="op">)</span>     <span class="co"># 4x4</span>
<span class="va">df.scaled.t</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">df.scaled</span><span class="op">)</span>    <span class="co"># 4x150</span>

<span class="co"># Perform matrix multiplication.</span>
<span class="va">df.new</span> <span class="op">&lt;-</span> <span class="va">eigenvectors.t</span> <span class="op">%*%</span> <span class="va">df.scaled.t</span>   <span class="co"># 4x150</span>

<span class="co"># Create new data frame. First take transpose and</span>
<span class="co"># then add column names.</span>
<span class="va">df.new.t</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html">t</a></span><span class="op">(</span><span class="va">df.new</span><span class="op">)</span>    <span class="co"># 150x4</span>
<span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">df.new.t</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"PC1"</span>, <span class="st">"PC2"</span>, <span class="st">"PC3"</span>, <span class="st">"PC4"</span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">df.new.t</span><span class="op">)</span>
<span class="co">#&gt;        PC1    PC2     PC3      PC4</span>
<span class="co">#&gt; [1,] -2.26 -0.478  0.1273  0.02409</span>
<span class="co">#&gt; [2,] -2.07  0.672  0.2338  0.10266</span>
<span class="co">#&gt; [3,] -2.36  0.341 -0.0441  0.02828</span>
<span class="co">#&gt; [4,] -2.29  0.595 -0.0910 -0.06574</span>
<span class="co">#&gt; [5,] -2.38 -0.645 -0.0157 -0.03580</span>
<span class="co">#&gt; [6,] -2.07 -1.484 -0.0269  0.00659</span></code></pre></div>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Compute covariance matrix for new dataset </span>
<span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cov</a></span><span class="op">(</span><span class="va">df.new.t</span><span class="op">)</span>, <span class="fl">5</span><span class="op">)</span>
<span class="co">#&gt;      PC1   PC2   PC3    PC4</span>
<span class="co">#&gt; PC1 2.92 0.000 0.000 0.0000</span>
<span class="co">#&gt; PC2 0.00 0.914 0.000 0.0000</span>
<span class="co">#&gt; PC3 0.00 0.000 0.147 0.0000</span>
<span class="co">#&gt; PC4 0.00 0.000 0.000 0.0207</span></code></pre></div>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="index.html"><span class="header-section-number">1</span> Preface</a></div>
<div class="next"><a href="comparison-of-two-pca-packages.html"><span class="header-section-number">3</span> Comparison of two PCA packages</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#introduction-to-pca"><span class="header-section-number">2</span> Introduction to PCA</a></li>
<li><a class="nav-link" href="#underlying-principal-components"><span class="header-section-number">2.1</span> Underlying principal components</a></li>
<li><a class="nav-link" href="#compute-eigenvectors-and-eigenvalues"><span class="header-section-number">2.2</span> Compute eigenvectors and eigenvalues</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/hadley/r-pkgs/blob/master/001-meta_109c-pca-introduction.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/hadley/r-pkgs/edit/master/001-meta_109c-pca-introduction.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>A Machine Learning Compilation</strong>" was written by Several authors. Compiled by Alfonso R. Reyes. It was last built on 2020-11-19.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
