<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 8 Ten methods to assess Variable Importance | A Machine Learning Compilation</title>
<meta name="author" content="Several authors. Compiled by Alfonso R. Reyes">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.5.3/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.5.3/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.2.9000/tabs.js"></script><script src="libs/bs3compat-0.2.2.9000/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">A Machine Learning Compilation</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Preface</a></li>
<li class="book-part">The Basics of Machine Learning</li>
<li><a class="" href="introduction-to-pca.html"><span class="header-section-number">2</span> Introduction to PCA</a></li>
<li><a class="" href="comparison-of-two-pca-packages.html"><span class="header-section-number">3</span> Comparison of two PCA packages</a></li>
<li><a class="" href="detailed-study-of-principal-component-analysis.html"><span class="header-section-number">4</span> Detailed study of Principal Component Analysis</a></li>
<li><a class="" href="detection-of-diabetes-using-logistic-regression.html"><span class="header-section-number">5</span> Detection of diabetes using Logistic Regression</a></li>
<li><a class="" href="sensitivity-analysis-for-a-neural-network.html"><span class="header-section-number">6</span> Sensitivity analysis for a neural network</a></li>
<li><a class="" href="data-visualization-for-ml-models.html"><span class="header-section-number">7</span> Data Visualization for ML models</a></li>
<li class="book-part">Feature Engineering</li>
<li><a class="active" href="ten-methods-to-assess-variable-importance.html"><span class="header-section-number">8</span> Ten methods to assess Variable Importance</a></li>
<li><a class="" href="employee-attrition-using-feature-importance.html"><span class="header-section-number">9</span> Employee Attrition using Feature Importance</a></li>
<li class="book-part">Classification</li>
<li><a class="" href="a-gentle-introduction-to-support-vector-machines.html"><span class="header-section-number">10</span> A gentle introduction to Support Vector Machines</a></li>
<li><a class="" href="broad-view-of-svm.html"><span class="header-section-number">11</span> Broad view of SVM</a></li>
<li><a class="" href="feature-selection-to-enhance-cancer-detection.html"><span class="header-section-number">12</span> Feature Selection to enhance cancer detection</a></li>
<li><a class="" href="dealing-with-unbalanced-data.html"><span class="header-section-number">13</span> Dealing with unbalanced data</a></li>
<li><a class="" href="imputting-missing-values-with-random-forest.html"><span class="header-section-number">14</span> Imputting missing values with Random Forest</a></li>
<li><a class="" href="tuning-of-support-vector-machine-prediction.html"><span class="header-section-number">15</span> Tuning of Support Vector Machine prediction</a></li>
<li class="book-part">Classification</li>
<li><a class="" href="introduction-to-algorithms-for-classification.html"><span class="header-section-number">16</span> Introduction to algorithms for Classification</a></li>
<li><a class="" href="comparing-classification-algorithms.html"><span class="header-section-number">17</span> Comparing Classification algorithms</a></li>
<li><a class="" href="who-buys-social-network-ads.html"><span class="header-section-number">18</span> Who buys Social Network ads</a></li>
<li><a class="" href="predicting-ozone-levels.html"><span class="header-section-number">19</span> Predicting Ozone levels</a></li>
<li><a class="" href="building-a-naive-bayes-classifier.html"><span class="header-section-number">20</span> Building a Naive Bayes Classifier</a></li>
<li><a class="" href="linear-and-non-linear-algorithms-for-classification.html"><span class="header-section-number">21</span> Linear and Non-Linear Algorithms for Classification</a></li>
<li><a class="" href="detect-mines-vs-rocks-with-random-forest.html"><span class="header-section-number">22</span> Detect mines vs rocks with Random Forest</a></li>
<li><a class="" href="predicting-the-type-of-glass.html"><span class="header-section-number">23</span> Predicting the type of glass</a></li>
<li><a class="" href="naive-bayes-for-sms-spam.html"><span class="header-section-number">24</span> Naive Bayes for SMS spam</a></li>
<li><a class="" href="vehicles-classiification-with-decision-trees.html"><span class="header-section-number">25</span> Vehicles classiification with Decision Trees</a></li>
<li><a class="" href="applying-naive-bayes-on-the-titanic-case.html"><span class="header-section-number">26</span> Applying Naive-Bayes on the Titanic case</a></li>
<li><a class="" href="classification-on-bad-loans.html"><span class="header-section-number">27</span> Classification on bad loans</a></li>
<li><a class="" href="predicting-flu-outcome-comparing-eight-classification-algorithms.html"><span class="header-section-number">28</span> Predicting Flu outcome comparing eight classification algorithms</a></li>
<li><a class="" href="a-detailed-study-of-bike-sharing-demand.html"><span class="header-section-number">29</span> A detailed study of bike sharing demand</a></li>
<li><a class="" href="prediction-of-arrhythmia-with-deep-neural-nets.html"><span class="header-section-number">30</span> Prediction of arrhythmia with deep neural nets</a></li>
<li class="book-part">Linear Regression</li>
<li><a class="" href="linear-regression-with-islr.html"><span class="header-section-number">31</span> Linear Regression with ISLR</a></li>
<li><a class="" href="evaluation-of-three-linear-regression-models.html"><span class="header-section-number">32</span> Evaluation of three linear regression models</a></li>
<li><a class="" href="comparison-of-six-linear-regression-algorithms.html"><span class="header-section-number">33</span> Comparison of six Linear Regression algorithms</a></li>
<li><a class="" href="comparing-regression-models.html"><span class="header-section-number">34</span> Comparing regression models</a></li>
<li><a class="" href="finding-the-factors-of-happiness.html"><span class="header-section-number">35</span> Finding the factors of happiness</a></li>
<li><a class="" href="regression-with-a-neural-network.html"><span class="header-section-number">36</span> Regression with a neural network</a></li>
<li><a class="" href="comparing-multiple-regression-vs-a-neural-network.html"><span class="header-section-number">37</span> Comparing Multiple Regression vs a Neural Network</a></li>
<li><a class="" href="temperature-modeling-using-nested-dataframes.html"><span class="header-section-number">38</span> Temperature modeling using nested dataframes</a></li>
<li class="book-part">Neural Networks</li>
<li><a class="" href="credit-scoring-with-neuralnet.html"><span class="header-section-number">39</span> Credit Scoring with neuralnet</a></li>
<li><a class="" href="wine-classification-with-neuralnet.html"><span class="header-section-number">40</span> Wine classification with neuralnet</a></li>
<li><a class="" href="predicting-the-rating-of-cereals.html"><span class="header-section-number">41</span> Predicting the rating of cereals</a></li>
<li><a class="" href="fitting-a-linear-model-with-neural-networks.html"><span class="header-section-number">42</span> Fitting a linear model with neural networks</a></li>
<li><a class="" href="visualization-of-neural-networks.html"><span class="header-section-number">43</span> Visualization of neural networks</a></li>
<li><a class="" href="build-a-fully-connected-r-neural-network-from-scratch.html"><span class="header-section-number">44</span> Build a fully connected R neural network from scratch</a></li>
<li><a class="" href="tuning-hyperparameters-in-a-neural-network.html"><span class="header-section-number">45</span> Tuning Hyperparameters in a Neural Network</a></li>
<li><a class="" href="deep-learning-tips-for-classification-and-regression.html"><span class="header-section-number">46</span> Deep Learning tips for Classification and Regression</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="what-is-dot-hat-in-a-regression-output.html"><span class="header-section-number">A</span> What is dot hat in a regression output</a></li>
<li><a class="" href="q-q-normal-to-compare-data-to-distributions.html"><span class="header-section-number">B</span> Q-Q normal to compare data to distributions</a></li>
<li><a class="" href="qq-and-pp-plots.html"><span class="header-section-number">C</span> QQ and PP Plots</a></li>
<li><a class="" href="visualizing-residuals.html"><span class="header-section-number">D</span> Visualizing residuals</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/f0nzie/machine_learning_compilation">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="ten-methods-to-assess-variable-importance" class="section level1">
<h1>
<span class="header-section-number">8</span> Ten methods to assess Variable Importance<a class="anchor" aria-label="anchor" href="#ten-methods-to-assess-variable-importance"><i class="fas fa-link"></i></a>
</h1>
<ul>
<li><p>Datasets: <code>GlaucomaM</code></p></li>
<li>
<p>Algorithms:</p>
<ul>
<li>Partition Trees</li>
<li>Regularized Random Forest (RRF)</li>
<li>Lasso Regression</li>
<li>Linear Regression</li>
<li>Recursive Feature Elimination (RFE)</li>
<li>Genetic Algorithm</li>
<li>Simulated Annealing</li>
</ul>
</li>
</ul>
<p>Source:
<a href="https://www.machinelearningplus.com/machine-learning/feature-selection/" class="uri">https://www.machinelearningplus.com/machine-learning/feature-selection/</a></p>
<p>In real-world datasets, it is fairly common to have columns that are
nothing but noise.</p>
<p>You are better off getting rid of such variables because of the memory
space they occupy, the time and the computational esources it is going
to cost, especially in large datasets.</p>
<p>Sometimes, you have a variable that makes business sense, but you are
not sure if it actually helps in predicting the Y. You also need to
consider the fact that, a feature that could be useful in one ML
algorithm (say a decision tree) may go underrepresented or unused by
another (like a regression model).</p>
<p>Having said that, it is still possible that a variable that shows poor
signs of helping to explain the response variable (Y), can turn out to
be significantly useful in the presence of (or combination with) other
predictors. What I mean by that is, a variable might have a low
correlation value of (~0.2) with Y. But in the presence of other
variables, it can help to explain certain patterns/phenomenon that other
variables can’t explain.</p>
<p>In such cases, it can be hard to make a call whether to include or
exclude such variables.</p>
<p>The strategies we are about to discuss can help fix such problems. Not
only that, it will also help understand if a particular variable is
important or not and how much it is contributing to the model</p>
<p>An important caveat. It is always best to have variables that have sound
business logic backing the inclusion of a variable and rely solely on
variable importance metrics.</p>
<p>Alright. Let’s load up the ‘Glaucoma’ dataset where the goal is to
predict if a patient has Glaucoma or not based on 63 different
physiological measurements. You can directly run the codes or download
the dataset here.</p>
<p>A lot of interesting examples ahead. Let’s get started.</p>
<div class="sourceCode" id="cb194"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Load Packages and prepare dataset</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ModelOriented.github.io/DALEX/">DALEX</a></span><span class="op">)</span>
<span class="co">#&gt; Welcome to DALEX (version: 1.2.0).</span>
<span class="co">#&gt; Find examples and detailed introduction at: https://pbiecek.github.io/ema/</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">TH.data</span><span class="op">)</span>
<span class="co">#&gt; Loading required package: survival</span>
<span class="co">#&gt; Loading required package: MASS</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Attaching package: 'TH.data'</span>
<span class="co">#&gt; The following object is masked from 'package:MASS':</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     geyser</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/topepo/caret/">caret</a></span><span class="op">)</span>
<span class="co">#&gt; Loading required package: lattice</span>
<span class="co">#&gt; Loading required package: ggplot2</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Attaching package: 'caret'</span>
<span class="co">#&gt; The following object is masked from 'package:survival':</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     cluster</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://github.com/collectivemedia/tictoc">tictoc</a></span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"GlaucomaM"</span>, package <span class="op">=</span> <span class="st">"TH.data"</span><span class="op">)</span>
<span class="va">trainData</span> <span class="op">&lt;-</span> <span class="va">GlaucomaM</span>
<span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">trainData</span><span class="op">)</span>
<span class="co">#&gt;      ag    at    as    an    ai   eag   eat   eas   ean   eai  abrg  abrt  abrs</span>
<span class="co">#&gt; 2  2.22 0.354 0.580 0.686 0.601 1.267 0.336 0.346 0.255 0.331 0.479 0.260 0.107</span>
<span class="co">#&gt; 43 2.68 0.475 0.672 0.868 0.667 2.053 0.440 0.520 0.639 0.454 1.090 0.377 0.257</span>
<span class="co">#&gt; 25 1.98 0.343 0.508 0.624 0.504 1.200 0.299 0.396 0.259 0.246 0.465 0.209 0.112</span>
<span class="co">#&gt; 65 1.75 0.269 0.476 0.525 0.476 0.612 0.147 0.017 0.044 0.405 0.170 0.062 0.000</span>
<span class="co">#&gt; 70 2.99 0.599 0.686 1.039 0.667 2.513 0.543 0.607 0.871 0.492 1.800 0.431 0.494</span>
<span class="co">#&gt; 16 2.92 0.483 0.763 0.901 0.770 2.200 0.462 0.637 0.504 0.597 1.311 0.394 0.365</span>
<span class="co">#&gt;     abrn  abri    hic   mhcg  mhct   mhcs   mhcn   mhci   phcg   phct   phcs</span>
<span class="co">#&gt; 2  0.014 0.098  0.214  0.111 0.412  0.036  0.105 -0.022 -0.139  0.242 -0.053</span>
<span class="co">#&gt; 43 0.212 0.245  0.382  0.140 0.338  0.104  0.080  0.109 -0.015  0.296 -0.015</span>
<span class="co">#&gt; 25 0.041 0.103  0.195  0.062 0.356  0.045 -0.009 -0.048 -0.149  0.206 -0.092</span>
<span class="co">#&gt; 65 0.000 0.108 -0.030 -0.015 0.074 -0.084 -0.050  0.035 -0.182 -0.097 -0.125</span>
<span class="co">#&gt; 70 0.601 0.274  0.383  0.089 0.233  0.145  0.023  0.007 -0.131  0.163  0.055</span>
<span class="co">#&gt; 16 0.251 0.301  0.442  0.128 0.375  0.049  0.111  0.052 -0.088  0.281 -0.067</span>
<span class="co">#&gt;      phcn   phci   hvc  vbsg  vbst  vbss  vbsn  vbsi  vasg  vast  vass  vasn</span>
<span class="co">#&gt; 2   0.010 -0.139 0.613 0.303 0.103 0.088 0.022 0.090 0.062 0.000 0.011 0.032</span>
<span class="co">#&gt; 43 -0.015  0.036 0.382 0.676 0.181 0.186 0.141 0.169 0.029 0.001 0.007 0.011</span>
<span class="co">#&gt; 25 -0.081 -0.149 0.557 0.300 0.084 0.088 0.046 0.082 0.036 0.002 0.004 0.016</span>
<span class="co">#&gt; 65 -0.138 -0.182 0.373 0.048 0.011 0.000 0.000 0.036 0.070 0.005 0.030 0.033</span>
<span class="co">#&gt; 70 -0.131 -0.115 0.405 0.889 0.151 0.253 0.330 0.155 0.020 0.001 0.004 0.008</span>
<span class="co">#&gt; 16 -0.062 -0.088 0.507 0.972 0.213 0.316 0.197 0.246 0.043 0.001 0.005 0.028</span>
<span class="co">#&gt;     vasi  vbrg  vbrt  vbrs  vbrn  vbri  varg  vart  vars  varn  vari   mdg</span>
<span class="co">#&gt; 2  0.018 0.075 0.039 0.021 0.002 0.014 0.756 0.009 0.209 0.298 0.240 0.705</span>
<span class="co">#&gt; 43 0.010 0.370 0.127 0.099 0.050 0.093 0.410 0.006 0.105 0.181 0.117 0.898</span>
<span class="co">#&gt; 25 0.013 0.081 0.034 0.019 0.007 0.021 0.565 0.014 0.132 0.243 0.177 0.687</span>
<span class="co">#&gt; 65 0.002 0.005 0.001 0.000 0.000 0.004 0.380 0.032 0.147 0.151 0.050 0.207</span>
<span class="co">#&gt; 70 0.007 0.532 0.103 0.173 0.181 0.075 0.228 0.011 0.026 0.105 0.087 0.721</span>
<span class="co">#&gt; 16 0.009 0.467 0.136 0.148 0.078 0.104 0.540 0.008 0.133 0.232 0.167 0.927</span>
<span class="co">#&gt;      mdt   mds   mdn   mdi    tmg    tmt    tms    tmn    tmi    mr   rnf  mdic</span>
<span class="co">#&gt; 2  0.637 0.738 0.596 0.691 -0.236 -0.018 -0.230 -0.510 -0.158 0.841 0.410 0.137</span>
<span class="co">#&gt; 43 0.850 0.907 0.771 0.940 -0.211 -0.014 -0.165 -0.317 -0.192 0.924 0.256 0.252</span>
<span class="co">#&gt; 25 0.643 0.689 0.684 0.700 -0.185 -0.097 -0.235 -0.337 -0.020 0.795 0.378 0.152</span>
<span class="co">#&gt; 65 0.171 0.022 0.046 0.221 -0.148 -0.035 -0.449 -0.217 -0.091 0.746 0.200 0.027</span>
<span class="co">#&gt; 70 0.638 0.730 0.730 0.640 -0.052 -0.105  0.084 -0.012 -0.054 0.977 0.193 0.297</span>
<span class="co">#&gt; 16 0.842 0.953 0.906 0.898 -0.040  0.087  0.018 -0.094 -0.051 0.965 0.339 0.333</span>
<span class="co">#&gt;      emd    mv  Class</span>
<span class="co">#&gt; 2  0.239 0.035 normal</span>
<span class="co">#&gt; 43 0.329 0.022 normal</span>
<span class="co">#&gt; 25 0.250 0.029 normal</span>
<span class="co">#&gt; 65 0.078 0.023 normal</span>
<span class="co">#&gt; 70 0.354 0.034 normal</span>
<span class="co">#&gt; 16 0.442 0.028 normal</span></code></pre></div>
<div id="boruta" class="section level2">
<h2>
<span class="header-section-number">8.1</span> 1. Boruta<a class="anchor" aria-label="anchor" href="#boruta"><i class="fas fa-link"></i></a>
</h2>
<p>Boruta is a feature ranking and selection algorithm based on random
forests algorithm. The advantage with Boruta is that it clearly decides
if a variable is important or not and helps to select variables that are
statistically significant. Besides, you can adjust the strictness of the
algorithm by adjusting the <span class="math inline">\(p\)</span> values that defaults to 0.01 and the
<code>maxRuns</code>.</p>
<p><code>maxRuns</code> is the number of times the algorithm is run. The higher the
<code>maxRuns</code> the more selective you get in picking the variables. The
default value is 100.</p>
<p>In the process of deciding if a feature is important or not, some
features may be marked by Boruta as ‘Tentative’. Sometimes increasing
the maxRuns can help resolve the ‘Tentativeness’ of the feature.</p>
<p>Lets see an example based on the Glaucoma dataset from <code>TH.data</code> package
that I created earlier.</p>
<div class="sourceCode" id="cb195"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># install.packages('Boruta')</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://notabug.org/mbq/Boruta/">Boruta</a></span><span class="op">)</span>
<span class="co">#&gt; Loading required package: ranger</span></code></pre></div>
<p>The <code>boruta</code> function uses a formula interface just like most predictive
modeling functions. So the first argument to <code>boruta()</code> is the formula
with the response variable on the left and all the predictors on the
right. By placing a dot, all the variables in <code>trainData</code> other than
Class will be included in the model.</p>
<p>The <code>doTrace</code> argument controls the amount of output printed to the
console. Higher the value, more the log details you get. So save space I
have set it to 0, but try setting it to 1 and 2 if you are running the
code.</p>
<p>Finally the output is stored in <code>boruta_output</code>.</p>
<div class="sourceCode" id="cb196"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Perform Boruta search</span>
<span class="va">boruta_output</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/Boruta/man/Boruta.html">Boruta</a></span><span class="op">(</span><span class="va">Class</span> <span class="op">~</span> <span class="va">.</span>, data<span class="op">=</span><span class="fu"><a href="https://Rdatatable.gitlab.io/data.table/reference/na.omit.data.table.html">na.omit</a></span><span class="op">(</span><span class="va">trainData</span><span class="op">)</span>, doTrace<span class="op">=</span><span class="fl">0</span><span class="op">)</span>  </code></pre></div>
<p>Let’s see what the boruta_output contains.</p>
<div class="sourceCode" id="cb197"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">boruta_output</span><span class="op">)</span>
<span class="co">#&gt;  [1] "finalDecision" "ImpHistory"    "pValue"        "maxRuns"      </span>
<span class="co">#&gt;  [5] "light"         "mcAdj"         "timeTaken"     "roughfixed"   </span>
<span class="co">#&gt;  [9] "call"          "impSource"</span></code></pre></div>
<div class="sourceCode" id="cb198"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Get significant variables including tentatives</span>
<span class="va">boruta_signif</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/Boruta/man/getSelectedAttributes.html">getSelectedAttributes</a></span><span class="op">(</span><span class="va">boruta_output</span>, withTentative <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">boruta_signif</span><span class="op">)</span>  
<span class="co">#&gt;  [1] "as"   "ai"   "eas"  "ean"  "abrg" "abrs" "abrn" "abri" "hic"  "mhcg"</span>
<span class="co">#&gt; [11] "mhcs" "mhcn" "mhci" "phcg" "phcn" "phci" "hvc"  "vbsg" "vbss" "vbsn"</span>
<span class="co">#&gt; [21] "vbsi" "vasg" "vass" "vasi" "vbrg" "vbrs" "vbrn" "vbri" "varg" "vart"</span>
<span class="co">#&gt; [31] "vars" "varn" "vari" "mdn"  "tmg"  "tmt"  "tms"  "tmi"  "mr"   "rnf" </span>
<span class="co">#&gt; [41] "mdic" "emd"</span></code></pre></div>
<p>If you are not sure about the tentative variables being selected for
granted, you can choose a <code>TentativeRoughFix</code> on <code>boruta_output</code>.</p>
<div class="sourceCode" id="cb199"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Do a tentative rough fix</span>
<span class="va">roughFixMod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/Boruta/man/TentativeRoughFix.html">TentativeRoughFix</a></span><span class="op">(</span><span class="va">boruta_output</span><span class="op">)</span>
<span class="va">boruta_signif</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/Boruta/man/getSelectedAttributes.html">getSelectedAttributes</a></span><span class="op">(</span><span class="va">roughFixMod</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">boruta_signif</span><span class="op">)</span>
<span class="co">#&gt;  [1] "as"   "ai"   "ean"  "abrg" "abrs" "abrn" "abri" "hic"  "mhcg" "mhcn"</span>
<span class="co">#&gt; [11] "mhci" "phcg" "phcn" "phci" "hvc"  "vbsn" "vbsi" "vasg" "vass" "vasi"</span>
<span class="co">#&gt; [21] "vbrg" "vbrs" "vbrn" "vbri" "varg" "vart" "vars" "varn" "vari" "mdn" </span>
<span class="co">#&gt; [31] "tmg"  "tms"  "tmi"  "mr"   "rnf"  "mdic"</span></code></pre></div>
<p>There you go. Boruta has decided on the ‘Tentative’ variables on our
behalf. Let’s find out the importance scores of these variables.</p>
<div class="sourceCode" id="cb200"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Variable Importance Scores</span>
<span class="va">imps</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/Boruta/man/attStats.html">attStats</a></span><span class="op">(</span><span class="va">roughFixMod</span><span class="op">)</span>
<span class="va">imps2</span> <span class="op">=</span> <span class="va">imps</span><span class="op">[</span><span class="va">imps</span><span class="op">$</span><span class="va">decision</span> <span class="op">!=</span> <span class="st">'Rejected'</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">'meanImp'</span>, <span class="st">'decision'</span><span class="op">)</span><span class="op">]</span>
<span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">imps2</span><span class="op">[</span><span class="fu"><a href="https://Rdatatable.gitlab.io/data.table/reference/setorder.html">order</a></span><span class="op">(</span><span class="op">-</span><span class="va">imps2</span><span class="op">$</span><span class="va">meanImp</span><span class="op">)</span>, <span class="op">]</span><span class="op">)</span>  <span class="co"># descending sort</span>
<span class="co">#&gt;      meanImp  decision</span>
<span class="co">#&gt; vari   12.37 Confirmed</span>
<span class="co">#&gt; varg   11.74 Confirmed</span>
<span class="co">#&gt; vars   10.74 Confirmed</span>
<span class="co">#&gt; phci    8.34 Confirmed</span>
<span class="co">#&gt; hic     8.21 Confirmed</span>
<span class="co">#&gt; varn    7.88 Confirmed</span></code></pre></div>
<p>Let’s plot it to see the importances of these variables.</p>
<div class="sourceCode" id="cb201"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Plot variable importance</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span><span class="op">(</span><span class="va">boruta_output</span>, cex.axis<span class="op">=</span><span class="fl">.7</span>, las<span class="op">=</span><span class="fl">2</span>, xlab<span class="op">=</span><span class="st">""</span>, main<span class="op">=</span><span class="st">"Variable Importance"</span><span class="op">)</span>  </code></pre></div>
<div class="inline-figure"><img src="100-fe-meta_133-ten_methods_variable_importance_files/figure-html/boruta-plot-1.png" width="70%" style="display: block; margin: auto;"></div>
<p>This plot reveals the importance of each of the features.</p>
<p>The columns in green are ‘confirmed’ and the ones in red are not. There
are couple of blue bars representing <code>ShadowMax</code> and <code>ShadowMin.</code> They
are not actual features, but are used by the <code>boruta</code> algorithm to
decide if a variable is important or not.</p>
</div>
<div id="variable-importance" class="section level2">
<h2>
<span class="header-section-number">8.2</span> 2. Variable Importance<a class="anchor" aria-label="anchor" href="#variable-importance"><i class="fas fa-link"></i></a>
</h2>
<p>Another way to look at feature selection is to consider variables most
used by various ML algorithms the most to be important.</p>
<p>Depending on how the machine learning algorithm learns the relationship
between <span class="math inline">\(X\)</span>’s and $Y$, different machine learning algorithms may
possibly end up using different variables (but mostly common vars) to
various degrees.</p>
<p>What I mean by that is, the variables that proved useful in a tree-based
algorithm like <code>rpart</code>, can turn out to be less useful in a
regression-based model. <strong>So all variables need not be equally useful to
all algorithms.</strong></p>
<p>So how do we find the variable importance for a given ML algo?</p>
<p><code><a href="https://rdrr.io/pkg/caret/man/train.html">train()</a></code> the desired model using the caret package. Then, use
<code><a href="https://rdrr.io/pkg/caret/man/varImp.html">varImp()</a></code> to determine the feature importance.</p>
<p>You may want to try out multiple algorithms, to get a feel of the
usefulness of the features across algos.</p>
</div>
<div id="rpart" class="section level2">
<h2>
<span class="header-section-number">8.3</span> 3. rpart<a class="anchor" aria-label="anchor" href="#rpart"><i class="fas fa-link"></i></a>
</h2>
<div class="sourceCode" id="cb202"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Train an rpart model and compute variable importance.</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/topepo/caret/">caret</a></span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">100</span><span class="op">)</span>
<span class="va">rPartMod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/train.html">train</a></span><span class="op">(</span><span class="va">Class</span> <span class="op">~</span> <span class="va">.</span>, 
                  data<span class="op">=</span><span class="va">trainData</span>, 
                  method<span class="op">=</span><span class="st">"rpart"</span><span class="op">)</span>

<span class="va">rpartImp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/varImp.html">varImp</a></span><span class="op">(</span><span class="va">rPartMod</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">rpartImp</span><span class="op">)</span>
<span class="co">#&gt; rpart variable importance</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;   only 20 most important variables shown (out of 62)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;      Overall</span>
<span class="co">#&gt; varg   100.0</span>
<span class="co">#&gt; vari    93.2</span>
<span class="co">#&gt; vars    85.2</span>
<span class="co">#&gt; varn    76.9</span>
<span class="co">#&gt; tmi     72.3</span>
<span class="co">#&gt; mhcn     0.0</span>
<span class="co">#&gt; as       0.0</span>
<span class="co">#&gt; phcs     0.0</span>
<span class="co">#&gt; vbst     0.0</span>
<span class="co">#&gt; abrt     0.0</span>
<span class="co">#&gt; vbsg     0.0</span>
<span class="co">#&gt; eai      0.0</span>
<span class="co">#&gt; vbrs     0.0</span>
<span class="co">#&gt; vbsi     0.0</span>
<span class="co">#&gt; eag      0.0</span>
<span class="co">#&gt; tmt      0.0</span>
<span class="co">#&gt; phcn     0.0</span>
<span class="co">#&gt; vart     0.0</span>
<span class="co">#&gt; mds      0.0</span>
<span class="co">#&gt; an       0.0</span></code></pre></div>
<p>Only 5 of the 63 features was used by rpart and if you look closely, the
5 variables used here are in the top 6 that boruta selected.</p>
<p>Let’s do one more: the variable importances from Regularized Random
Forest (RRF) algorithm.</p>
</div>
<div id="regularized-random-forest-rrf" class="section level2">
<h2>
<span class="header-section-number">8.4</span> 4. Regularized Random Forest (RRF)<a class="anchor" aria-label="anchor" href="#regularized-random-forest-rrf"><i class="fas fa-link"></i></a>
</h2>
<div class="sourceCode" id="cb203"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/pkg/tictoc/man/tic.html">tic</a></span><span class="op">(</span><span class="op">)</span>
<span class="co"># Train an RRF model and compute variable importance.</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">100</span><span class="op">)</span>
<span class="va">rrfMod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/train.html">train</a></span><span class="op">(</span><span class="va">Class</span> <span class="op">~</span> <span class="va">.</span>, 
                data <span class="op">=</span> <span class="va">trainData</span>, 
                method <span class="op">=</span> <span class="st">"RRF"</span><span class="op">)</span>
<span class="co">#&gt; Registered S3 method overwritten by 'RRF':</span>
<span class="co">#&gt;   method      from        </span>
<span class="co">#&gt;   plot.margin randomForest</span>

<span class="va">rrfImp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/varImp.html">varImp</a></span><span class="op">(</span><span class="va">rrfMod</span>, scale<span class="op">=</span><span class="cn">F</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/pkg/tictoc/man/tic.html">toc</a></span><span class="op">(</span><span class="op">)</span>
<span class="co">#&gt; 383.138 sec elapsed</span>
<span class="va">rrfImp</span>
<span class="co">#&gt; RRF variable importance</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;   only 20 most important variables shown (out of 62)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;      Overall</span>
<span class="co">#&gt; varg   25.07</span>
<span class="co">#&gt; vari   18.78</span>
<span class="co">#&gt; vars    5.29</span>
<span class="co">#&gt; tmi     4.09</span>
<span class="co">#&gt; mhcg    3.25</span>
<span class="co">#&gt; mhci    2.81</span>
<span class="co">#&gt; hic     2.69</span>
<span class="co">#&gt; hvc     2.50</span>
<span class="co">#&gt; mv      2.00</span>
<span class="co">#&gt; vasg    1.99</span>
<span class="co">#&gt; phci    1.77</span>
<span class="co">#&gt; phcn    1.53</span>
<span class="co">#&gt; phct    1.43</span>
<span class="co">#&gt; vass    1.37</span>
<span class="co">#&gt; phcg    1.37</span>
<span class="co">#&gt; tms     1.32</span>
<span class="co">#&gt; tmg     1.16</span>
<span class="co">#&gt; abrs    1.16</span>
<span class="co">#&gt; tmt     1.13</span>
<span class="co">#&gt; mdic    1.13</span></code></pre></div>
<div class="sourceCode" id="cb204"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span><span class="op">(</span><span class="va">rrfImp</span>, top <span class="op">=</span> <span class="fl">20</span>, main<span class="op">=</span><span class="st">'Variable Importance'</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="100-fe-meta_133-ten_methods_variable_importance_files/figure-html/unnamed-chunk-7-1.png" width="70%" style="display: block; margin: auto;"></div>
<p>The topmost important variables are pretty much from the top tier of
Boruta’s selections.</p>
<p>Some of the other algorithms available in <code><a href="https://rdrr.io/pkg/caret/man/train.html">train()</a></code> that you can use to
compute varImp are the following:</p>
<pre><code>ada, AdaBag, AdaBoost.M1, adaboost, bagEarth, bagEarthGCV, bagFDA, bagFDAGCV, bartMachine, blasso, BstLm, bstSm, C5.0, C5.0Cost, C5.0Rules, C5.0Tree, cforest, chaid, ctree, ctree2, cubist, deepboost, earth, enet, evtree, extraTrees, fda, gamboost, gbm_h2o, gbm, gcvEarth, glmnet_h2o, glmnet, glmStepAIC, J48, JRip, lars, lars2, lasso, LMT, LogitBoost, M5, M5Rules, msaenet, nodeHarvest, OneR, ordinalNet, ORFlog, ORFpls, ORFridge, ORFsvm, pam, parRF, PART, penalized, PenalizedLDA, qrf, ranger, Rborist, relaxo, rf, rFerns, rfRules, rotationForest, rotationForestCp, rpart, rpart1SE, rpart2, rpartCost, rpartScore, rqlasso, rqnc, RRF, RRFglobal, sdwd, smda, sparseLDA, spikeslab, wsrf, xgbLinear, xgbTree.</code></pre>
</div>
<div id="lasso-regression" class="section level2">
<h2>
<span class="header-section-number">8.5</span> 5. Lasso Regression<a class="anchor" aria-label="anchor" href="#lasso-regression"><i class="fas fa-link"></i></a>
</h2>
<p>Least Absolute Shrinkage and Selection Operator (LASSO) regression is a
type of regularization method that penalizes with L1-norm.</p>
<p>It basically imposes a cost to having large weights (value of
coefficients). And its called L1 regularization, because the cost added,
is proportional to the absolute value of weight coefficients.</p>
<p>As a result, in the process of shrinking the coefficients, it eventually
reduces the coefficients of certain unwanted features all the to zero.
That is, it removes the unneeded variables altogether.</p>
<p>So effectively, LASSO regression can be considered as a variable
selection technique as well.</p>
<div class="sourceCode" id="cb206"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://glmnet.stanford.edu">glmnet</a></span><span class="op">)</span>
<span class="co">#&gt; Loading required package: Matrix</span>
<span class="co">#&gt; Loaded glmnet 3.0-2</span>

<span class="co"># online data</span>
<span class="co"># trainData &lt;- read.csv('https://raw.githubusercontent.com/selva86/datasets/master/GlaucomaM.csv')</span>

<span class="va">trainData</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.csv</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/file.path.html">file.path</a></span><span class="op">(</span><span class="va">data_raw_dir</span>, <span class="st">"glaucoma.csv"</span><span class="op">)</span><span class="op">)</span>

<span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://Rdatatable.gitlab.io/data.table/reference/as.matrix.html">as.matrix</a></span><span class="op">(</span><span class="va">trainData</span><span class="op">[</span>,<span class="op">-</span><span class="fl">63</span><span class="op">]</span><span class="op">)</span> <span class="co"># all X vars</span>
<span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/double.html">as.double</a></span><span class="op">(</span><span class="fu"><a href="https://Rdatatable.gitlab.io/data.table/reference/as.matrix.html">as.matrix</a></span><span class="op">(</span><span class="fu"><a href="https://Rdatatable.gitlab.io/data.table/reference/fifelse.html">ifelse</a></span><span class="op">(</span><span class="va">trainData</span><span class="op">[</span>, <span class="fl">63</span><span class="op">]</span><span class="op">==</span><span class="st">'normal'</span>, <span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="co"># Only Class</span>

<span class="co"># Fit the LASSO model (Lasso: Alpha = 1)</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">100</span><span class="op">)</span>
<span class="va">cv.lasso</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://glmnet.stanford.edu/reference/cv.glmnet.html">cv.glmnet</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, family<span class="op">=</span><span class="st">'binomial'</span>, alpha<span class="op">=</span><span class="fl">1</span>, parallel<span class="op">=</span><span class="cn">TRUE</span>, standardize<span class="op">=</span><span class="cn">TRUE</span>, type.measure<span class="op">=</span><span class="st">'auc'</span><span class="op">)</span>
<span class="co">#&gt; Warning: executing %dopar% sequentially: no parallel backend registered</span>

<span class="co"># Results</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span><span class="op">(</span><span class="va">cv.lasso</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="100-fe-meta_133-ten_methods_variable_importance_files/figure-html/lasso-train-1.png" width="70%" style="display: block; margin: auto;"></div>
<p>Let’s see how to interpret this plot.</p>
<p>The X axis of the plot is the log of <code>lambda</code>. That means when it is 2
here, the lambda value is actually 100.</p>
<p>The numbers at the top of the plot show how many predictors were
included in the model. The position of red dots along the Y-axis tells
what <code>AUC</code> we got when you include as many variables shown on the top
x-axis.</p>
<p>You can also see two dashed vertical lines.</p>
<p>The first one on the left points to the lambda with the lowest mean
squared error. The one on the right point to the number of variables
with the highest deviance within 1 standard deviation.</p>
<p>The best lambda value is stored inside ‘cv.lasso$lambda.min’.</p>
<div class="sourceCode" id="cb207"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># plot(cv.lasso$glmnet.fit, xvar="lambda", label=TRUE)</span>
<span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">'Min Lambda: '</span>, <span class="va">cv.lasso</span><span class="op">$</span><span class="va">lambda.min</span>, <span class="st">'\n 1Sd Lambda: '</span>, <span class="va">cv.lasso</span><span class="op">$</span><span class="va">lambda.1se</span><span class="op">)</span>
<span class="co">#&gt; Min Lambda:  0.0224 </span>
<span class="co">#&gt;  1Sd Lambda:  0.144</span>
<span class="va">df_coef</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://Rdatatable.gitlab.io/data.table/reference/as.matrix.html">as.matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">cv.lasso</span>, s<span class="op">=</span><span class="va">cv.lasso</span><span class="op">$</span><span class="va">lambda.min</span><span class="op">)</span><span class="op">)</span>, <span class="fl">2</span><span class="op">)</span>

<span class="co"># See all contributing variables</span>
<span class="va">df_coef</span><span class="op">[</span><span class="va">df_coef</span><span class="op">[</span>, <span class="fl">1</span><span class="op">]</span> <span class="op">!=</span> <span class="fl">0</span>, <span class="op">]</span>
<span class="co">#&gt; (Intercept)          as        mhci        phci         hvc        vast </span>
<span class="co">#&gt;        2.68       -1.59        3.85        5.60       -2.41      -13.90 </span>
<span class="co">#&gt;        vars        vari         mdn         mdi         tmg         tms </span>
<span class="co">#&gt;      -20.18       -1.58        0.50        0.99        0.06        2.56 </span>
<span class="co">#&gt;         tmi </span>
<span class="co">#&gt;        2.23</span></code></pre></div>
<p>The above output shows what variables LASSO considered important. A high
positive or low negative implies more important is that variable.</p>
</div>
<div id="step-wise-forward-and-backward-selection" class="section level2">
<h2>
<span class="header-section-number">8.6</span> 6. Step wise Forward and Backward Selection<a class="anchor" aria-label="anchor" href="#step-wise-forward-and-backward-selection"><i class="fas fa-link"></i></a>
</h2>
<p>Stepwise regression can be used to select features if the Y variable is
a numeric variable. It is particularly used in selecting best linear
regression models.</p>
<p>It searches for the best possible regression model by iteratively
selecting and dropping variables to arrive at a model with the lowest
possible AIC.</p>
<p>It can be implemented using the <code><a href="https://rdrr.io/r/stats/step.html">step()</a></code> function and you need to
provide it with a lower model, which is the base model from which it
won’t remove any features and an upper model, which is a full model that
has all possible features you want to have.</p>
<p>Our case is not so complicated (&lt; 20 vars), so lets just do a simple
stepwise in ‘both’ directions.</p>
<p>I will use the <code>ozone</code> dataset for this where the objective is to
predict the <code>ozone_reading</code> based on other weather related observations.</p>
<div class="sourceCode" id="cb208"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Load data</span>
<span class="co"># online</span>
<span class="co"># trainData &lt;- read.csv("http://rstatistics.net/wp-content/uploads/2015/09/ozone1.csv",</span>
<span class="co">#                      stringsAsFactors=F)</span>
<span class="va">trainData</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.csv</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/file.path.html">file.path</a></span><span class="op">(</span><span class="va">data_raw_dir</span>, <span class="st">"ozone1.csv"</span><span class="op">)</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">trainData</span><span class="op">)</span><span class="op">)</span>
<span class="co">#&gt;   Month Day_of_month Day_of_week ozone_reading pressure_height Wind_speed</span>
<span class="co">#&gt; 1     1            1           4             3            5480          8</span>
<span class="co">#&gt; 2     1            2           5             3            5660          6</span>
<span class="co">#&gt; 3     1            3           6             3            5710          4</span>
<span class="co">#&gt; 4     1            4           7             5            5700          3</span>
<span class="co">#&gt; 5     1            5           1             5            5760          3</span>
<span class="co">#&gt; 6     1            6           2             6            5720          4</span>
<span class="co">#&gt;   Humidity Temperature_Sandburg Temperature_ElMonte Inversion_base_height</span>
<span class="co">#&gt; 1       20                 40.5                39.8                  5000</span>
<span class="co">#&gt; 2       41                 38.0                46.7                  4109</span>
<span class="co">#&gt; 3       28                 40.0                49.5                  2693</span>
<span class="co">#&gt; 4       37                 45.0                52.3                   590</span>
<span class="co">#&gt; 5       51                 54.0                45.3                  1450</span>
<span class="co">#&gt; 6       69                 35.0                49.6                  1568</span>
<span class="co">#&gt;   Pressure_gradient Inversion_temperature Visibility</span>
<span class="co">#&gt; 1               -15                  30.6        200</span>
<span class="co">#&gt; 2               -14                  48.0        300</span>
<span class="co">#&gt; 3               -25                  47.7        250</span>
<span class="co">#&gt; 4               -24                  55.0        100</span>
<span class="co">#&gt; 5                25                  57.0         60</span>
<span class="co">#&gt; 6                15                  53.8         60</span></code></pre></div>
<p>The data is ready. Let’s perform the stepwise.</p>
<div class="sourceCode" id="cb209"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Step 1: Define base intercept only model</span>
<span class="va">base.mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">ozone_reading</span> <span class="op">~</span> <span class="fl">1</span> , data<span class="op">=</span><span class="va">trainData</span><span class="op">)</span>  

<span class="co"># Step 2: Full model with all predictors</span>
<span class="va">all.mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">ozone_reading</span> <span class="op">~</span> <span class="va">.</span> , data<span class="op">=</span> <span class="va">trainData</span><span class="op">)</span> 

<span class="co"># Step 3: Perform step-wise algorithm. direction='both' implies both forward and backward stepwise</span>
<span class="va">stepMod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/step.html">step</a></span><span class="op">(</span><span class="va">base.mod</span>, scope <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>lower <span class="op">=</span> <span class="va">base.mod</span>, upper <span class="op">=</span> <span class="va">all.mod</span><span class="op">)</span>, direction <span class="op">=</span> <span class="st">"both"</span>, trace <span class="op">=</span> <span class="fl">0</span>, steps <span class="op">=</span> <span class="fl">1000</span><span class="op">)</span>  

<span class="co"># Step 4: Get the shortlisted variable.</span>
<span class="va">shortlistedVars</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/unlist.html">unlist</a></span><span class="op">(</span><span class="va">stepMod</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">)</span><span class="op">)</span> 
<span class="va">shortlistedVars</span> <span class="op">&lt;-</span> <span class="va">shortlistedVars</span><span class="op">[</span><span class="op">!</span><span class="va">shortlistedVars</span> <span class="op">%in%</span> <span class="st">"(Intercept)"</span><span class="op">]</span> <span class="co"># remove intercept</span>

<span class="co"># Show</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">shortlistedVars</span><span class="op">)</span>
<span class="co">#&gt; [1] "Temperature_Sandburg"  "Humidity"              "Temperature_ElMonte"  </span>
<span class="co">#&gt; [4] "Month"                 "pressure_height"       "Inversion_base_height"</span></code></pre></div>
<p>The selected model has the above 6 features in it.</p>
<p>But if you have too many features (&gt; 100) in training data, then it
might be a good idea to split the dataset into chunks of 10 variables
each with Y as mandatory in each dataset. Loop through all the chunks
and collect the best features.</p>
<p>We are doing it this way because some variables that came as important
in a training data with fewer features may not show up in a linear reg
model built on lots of features.</p>
<p>Finally, from a pool of shortlisted features (from small chunk models),
run a full stepwise model to get the final set of selected features.</p>
<p>You can take this as a learning assignment to be solved within 20
minutes.</p>
</div>
<div id="relative-importance-from-linear-regression" class="section level2">
<h2>
<span class="header-section-number">8.7</span> 7. Relative Importance from Linear Regression<a class="anchor" aria-label="anchor" href="#relative-importance-from-linear-regression"><i class="fas fa-link"></i></a>
</h2>
<p>This technique is specific to linear regression models.</p>
<p>Relative importance can be used to assess which variables contributed
how much in explaining the linear model’s R-squared value. So, if you
sum up the produced importances, it will add up to the model’s R-sq
value.</p>
<p>In essence, it is not directly a feature selection method, because you
have already provided the features that go in the model. But after
building the model, the <code>relaimpo</code> can provide a sense of how important
each feature is in contributing to the R-sq, or in other words, in
‘explaining the Y variable’.</p>
<p>So, how to calculate relative importance?</p>
<p>It is implemented in the <code>relaimpo</code> package. Basically, you build a
linear regression model and pass that as the main argument to
<code><a href="https://rdrr.io/pkg/relaimpo/man/calc.relimp.html">calc.relimp()</a></code>. relaimpo has multiple options to compute the relative
importance, but the recommended method is to use <code>type='lmg'</code>, as I have
done below.</p>
<div class="sourceCode" id="cb210"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># install.packages('relaimpo')</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://prof.beuth-hochschule.de/groemping/relaimpo/">relaimpo</a></span><span class="op">)</span>
<span class="co">#&gt; Loading required package: boot</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Attaching package: 'boot'</span>
<span class="co">#&gt; The following object is masked from 'package:lattice':</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     melanoma</span>
<span class="co">#&gt; The following object is masked from 'package:survival':</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     aml</span>
<span class="co">#&gt; Loading required package: survey</span>
<span class="co">#&gt; Loading required package: grid</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Attaching package: 'survey'</span>
<span class="co">#&gt; The following object is masked from 'package:graphics':</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     dotchart</span>
<span class="co">#&gt; Loading required package: mitools</span>
<span class="co">#&gt; This is the global version of package relaimpo.</span>
<span class="co">#&gt; If you are a non-US user, a version with the interesting additional metric pmvd is available</span>
<span class="co">#&gt; from Ulrike Groempings web site at prof.beuth-hochschule.de/groemping.</span>

<span class="co"># Build linear regression model</span>
<span class="va">model_formula</span> <span class="op">=</span> <span class="va">ozone_reading</span> <span class="op">~</span> <span class="va">Temperature_Sandburg</span> <span class="op">+</span> <span class="va">Humidity</span> <span class="op">+</span> <span class="va">Temperature_ElMonte</span> <span class="op">+</span> <span class="va">Month</span> <span class="op">+</span> <span class="va">pressure_height</span> <span class="op">+</span> <span class="va">Inversion_base_height</span>
<span class="va">lmMod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">model_formula</span>, data<span class="op">=</span><span class="va">trainData</span><span class="op">)</span>

<span class="co"># calculate relative importance</span>
<span class="va">relImportance</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/relaimpo/man/calc.relimp.html">calc.relimp</a></span><span class="op">(</span><span class="va">lmMod</span>, type <span class="op">=</span> <span class="st">"lmg"</span>, rela <span class="op">=</span> <span class="cn">F</span><span class="op">)</span>  

<span class="co"># Sort</span>
<span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">'Relative Importances: \n'</span><span class="op">)</span>
<span class="co">#&gt; Relative Importances:</span>
<span class="fu"><a href="https://rdrr.io/r/base/sort.html">sort</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">relImportance</span><span class="op">$</span><span class="va">lmg</span>, <span class="fl">3</span><span class="op">)</span>, decreasing<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span>
<span class="co">#&gt;   Temperature_ElMonte  Temperature_Sandburg       pressure_height </span>
<span class="co">#&gt;                 0.214                 0.203                 0.104 </span>
<span class="co">#&gt; Inversion_base_height              Humidity                 Month </span>
<span class="co">#&gt;                 0.096                 0.086                 0.012</span></code></pre></div>
<p>Additionally, you can use bootstrapping (using <code>boot.relimp</code>) to compute
the confidence intervals of the produced relative importances.</p>
<div class="sourceCode" id="cb211"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">bootsub</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/relaimpo/man/booteval.relimp.html">boot.relimp</a></span><span class="op">(</span><span class="va">ozone_reading</span> <span class="op">~</span> <span class="va">Temperature_Sandburg</span> <span class="op">+</span> <span class="va">Humidity</span> <span class="op">+</span> <span class="va">Temperature_ElMonte</span> <span class="op">+</span> <span class="va">Month</span> <span class="op">+</span> <span class="va">pressure_height</span> <span class="op">+</span> <span class="va">Inversion_base_height</span>, data<span class="op">=</span><span class="va">trainData</span>,
                       b <span class="op">=</span> <span class="fl">1000</span>, type <span class="op">=</span> <span class="st">'lmg'</span>, rank <span class="op">=</span> <span class="cn">TRUE</span>, diff <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/relaimpo/man/booteval.relimp.html">booteval.relimp</a></span><span class="op">(</span><span class="va">bootsub</span>, level<span class="op">=</span><span class="fl">.95</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="100-fe-meta_133-ten_methods_variable_importance_files/figure-html/unnamed-chunk-9-1.png" width="70%" style="display: block; margin: auto;"></div>
</div>
<div id="recursive-feature-elimination-rfe" class="section level2">
<h2>
<span class="header-section-number">8.8</span> 8. Recursive Feature Elimination (RFE)<a class="anchor" aria-label="anchor" href="#recursive-feature-elimination-rfe"><i class="fas fa-link"></i></a>
</h2>
<p>Recursive feature elimnation (rfe) offers a rigorous way to determine
the important variables before you even feed them into a ML algo.</p>
<p>It can be implemented using the <code><a href="https://rdrr.io/pkg/caret/man/rfe.html">rfe()</a></code> from caret package.</p>
<p>The rfe() also takes two important parameters.</p>
<ul>
<li><code>sizes</code></li>
<li><code>rfeControl</code></li>
</ul>
<p>So, what does <code>sizes</code> and <code>rfeControl</code> represent?</p>
<p>The sizes determines the number of most important features the rfe
should iterate. Below, I have set the size as 1 to 5, 10, 15 and 18.</p>
<p>Secondly, the <code>rfeControl</code> parameter receives the output of the
<code><a href="https://rdrr.io/pkg/caret/man/rfeControl.html">rfeControl()</a></code>. You can set what type of variable evaluation algorithm
must be used. Here, I have used random forests based rfFuncs. The
<code>method='repeatedCV'</code> means it will do a repeated k-Fold cross
validation with <code>repeats=5</code>.</p>
<p>Once complete, you get the accuracy and kappa for each model size you
provided. The final selected model subset size is marked with a * in
the rightmost selected column.</p>
<div class="sourceCode" id="cb212"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="va">trainData</span><span class="op">)</span>
<span class="co">#&gt; 'data.frame':    366 obs. of  13 variables:</span>
<span class="co">#&gt;  $ Month                : int  1 1 1 1 1 1 1 1 1 1 ...</span>
<span class="co">#&gt;  $ Day_of_month         : int  1 2 3 4 5 6 7 8 9 10 ...</span>
<span class="co">#&gt;  $ Day_of_week          : int  4 5 6 7 1 2 3 4 5 6 ...</span>
<span class="co">#&gt;  $ ozone_reading        : num  3 3 3 5 5 6 4 4 6 7 ...</span>
<span class="co">#&gt;  $ pressure_height      : num  5480 5660 5710 5700 5760 5720 5790 5790 5700 5700 ...</span>
<span class="co">#&gt;  $ Wind_speed           : int  8 6 4 3 3 4 6 3 3 3 ...</span>
<span class="co">#&gt;  $ Humidity             : num  20 41 28 37 51 ...</span>
<span class="co">#&gt;  $ Temperature_Sandburg : num  40.5 38 40 45 54 ...</span>
<span class="co">#&gt;  $ Temperature_ElMonte  : num  39.8 46.7 49.5 52.3 45.3 ...</span>
<span class="co">#&gt;  $ Inversion_base_height: num  5000 4109 2693 590 1450 ...</span>
<span class="co">#&gt;  $ Pressure_gradient    : num  -15 -14 -25 -24 25 15 -33 -28 23 -2 ...</span>
<span class="co">#&gt;  $ Inversion_temperature: num  30.6 48 47.7 55 57 ...</span>
<span class="co">#&gt;  $ Visibility           : int  200 300 250 100 60 60 100 250 120 120 ...</span></code></pre></div>
<div class="sourceCode" id="cb213"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/pkg/tictoc/man/tic.html">tic</a></span><span class="op">(</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">100</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/options.html">options</a></span><span class="op">(</span>warn<span class="op">=</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span>

<span class="va">subsets</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>, <span class="fl">10</span>, <span class="fl">15</span>, <span class="fl">18</span><span class="op">)</span>

<span class="va">ctrl</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/rfeControl.html">rfeControl</a></span><span class="op">(</span>functions <span class="op">=</span> <span class="va">rfFuncs</span>,
                   method <span class="op">=</span> <span class="st">"repeatedcv"</span>,
                   repeats <span class="op">=</span> <span class="fl">5</span>,
                   verbose <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span>

<span class="va">lmProfile</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/rfe.html">rfe</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">trainData</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">3</span>, <span class="fl">5</span><span class="op">:</span><span class="fl">13</span><span class="op">)</span><span class="op">]</span>, y<span class="op">=</span><span class="va">trainData</span><span class="op">$</span><span class="va">ozone_reading</span>,
                 sizes <span class="op">=</span> <span class="va">subsets</span>,
                 rfeControl <span class="op">=</span> <span class="va">ctrl</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/pkg/tictoc/man/tic.html">toc</a></span><span class="op">(</span><span class="op">)</span>
<span class="co">#&gt; 95.356 sec elapsed</span>
<span class="va">lmProfile</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Recursive feature selection</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Outer resampling method: Cross-Validated (10 fold, repeated 5 times) </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Resampling performance over subset size:</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;  Variables RMSE Rsquared  MAE RMSESD RsquaredSD MAESD Selected</span>
<span class="co">#&gt;          1 5.13    0.595 3.92  0.826     0.1275 0.586         </span>
<span class="co">#&gt;          2 4.03    0.746 3.11  0.542     0.0743 0.416         </span>
<span class="co">#&gt;          3 3.95    0.756 3.06  0.472     0.0670 0.380         </span>
<span class="co">#&gt;          4 3.93    0.759 3.01  0.468     0.0683 0.361         </span>
<span class="co">#&gt;          5 3.90    0.763 2.98  0.467     0.0659 0.350         </span>
<span class="co">#&gt;         10 3.77    0.782 2.85  0.496     0.0734 0.393        *</span>
<span class="co">#&gt;         12 3.77    0.781 2.86  0.508     0.0756 0.401         </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; The top 5 variables (out of 10):</span>
<span class="co">#&gt;    Temperature_ElMonte, Pressure_gradient, Temperature_Sandburg, Inversion_temperature, Humidity</span></code></pre></div>
<p>So, it says, Temperature_ElMonte, Pressure_gradient,
Temperature_Sandburg, Inversion_temperature, Humidity are the top 5
variables in that order.</p>
<p>And the best model size out of the provided models sizes (in subsets) is
10.</p>
<p>You can see all of the top 10 variables from ‘<code>lmProfile$optVariables</code>’
that was created using <code>rfe</code> function above.</p>
</div>
<div id="genetic-algorithm" class="section level2">
<h2>
<span class="header-section-number">8.9</span> 9. Genetic Algorithm<a class="anchor" aria-label="anchor" href="#genetic-algorithm"><i class="fas fa-link"></i></a>
</h2>
<p>You can perform a supervised feature selection with genetic algorithms
using the <code><a href="https://rdrr.io/pkg/caret/man/gafs.default.html">gafs()</a></code>. This is <strong>quite resource expensive</strong> so consider
that before choosing the number of iterations (iters) and the number of
repeats in <code><a href="https://rdrr.io/pkg/caret/man/safsControl.html">gafsControl()</a></code>.</p>
<div class="sourceCode" id="cb214"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/pkg/tictoc/man/tic.html">tic</a></span><span class="op">(</span><span class="op">)</span>
<span class="co"># Define control function</span>
<span class="va">ga_ctrl</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/safsControl.html">gafsControl</a></span><span class="op">(</span>functions <span class="op">=</span> <span class="va">rfGA</span>,  <span class="co"># another option is `caretGA`.</span>
                        method <span class="op">=</span> <span class="st">"cv"</span>,
                        repeats <span class="op">=</span> <span class="fl">3</span><span class="op">)</span>

<span class="co"># Genetic Algorithm feature selection</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">100</span><span class="op">)</span>
<span class="va">ga_obj</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/gafs.default.html">gafs</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">trainData</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">3</span>, <span class="fl">5</span><span class="op">:</span><span class="fl">13</span><span class="op">)</span><span class="op">]</span>, 
               y<span class="op">=</span><span class="va">trainData</span><span class="op">[</span>, <span class="fl">4</span><span class="op">]</span>, 
               iters <span class="op">=</span> <span class="fl">3</span>,   <span class="co"># normally much higher (100+)</span>
               gafsControl <span class="op">=</span> <span class="va">ga_ctrl</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/pkg/tictoc/man/tic.html">toc</a></span><span class="op">(</span><span class="op">)</span>
<span class="co">#&gt; 635.279 sec elapsed</span>
<span class="va">ga_obj</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Genetic Algorithm Feature Selection</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; 366 samples</span>
<span class="co">#&gt; 12 predictors</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Maximum generations: 3 </span>
<span class="co">#&gt; Population per generation: 50 </span>
<span class="co">#&gt; Crossover probability: 0.8 </span>
<span class="co">#&gt; Mutation probability: 0.1 </span>
<span class="co">#&gt; Elitism: 0 </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Internal performance values: RMSE, Rsquared</span>
<span class="co">#&gt; Subset selection driven to minimize internal RMSE </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; External performance values: RMSE, Rsquared, MAE</span>
<span class="co">#&gt; Best iteration chose by minimizing external RMSE </span>
<span class="co">#&gt; External resampling method: Cross-Validated (10 fold) </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; During resampling:</span>
<span class="co">#&gt;   * the top 5 selected variables (out of a possible 12):</span>
<span class="co">#&gt;     Month (100%), Pressure_gradient (100%), Temperature_ElMonte (100%), Humidity (80%), Visibility (80%)</span>
<span class="co">#&gt;   * on average, 6.8 variables were selected (min = 5, max = 9)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; In the final search using the entire training set:</span>
<span class="co">#&gt;    * 9 features selected at iteration 2 including:</span>
<span class="co">#&gt;      Month, Day_of_month, pressure_height, Wind_speed, Humidity ... </span>
<span class="co">#&gt;    * external performance at this iteration is</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;       RMSE   Rsquared        MAE </span>
<span class="co">#&gt;      3.721      0.788      2.800</span></code></pre></div>
<div class="sourceCode" id="cb215"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Optimal variables</span>
<span class="va">ga_obj</span><span class="op">$</span><span class="va">optVariables</span>
<span class="co">#&gt; [1] "Month"                 "Day_of_month"          "pressure_height"      </span>
<span class="co">#&gt; [4] "Wind_speed"            "Humidity"              "Temperature_ElMonte"  </span>
<span class="co">#&gt; [7] "Inversion_base_height" "Pressure_gradient"     "Inversion_temperature"</span></code></pre></div>
<p>‘Month’ ‘Day_of_month’ ‘Wind_speed’ ‘Temperature_ElMonte’
‘Pressure_gradient’ ‘Visibility’</p>
<p>So the optimal variables according to the genetic algorithms are listed
above. But, I wouldn’t use it just yet because, the above variant was
tuned for only 3 iterations, which is quite low. I had to set it so low
to save computing time.</p>
</div>
<div id="simulated-annealing" class="section level2">
<h2>
<span class="header-section-number">8.10</span> 10. Simulated Annealing<a class="anchor" aria-label="anchor" href="#simulated-annealing"><i class="fas fa-link"></i></a>
</h2>
<p>Simulated annealing is a global search algorithm that allows a
suboptimal solution to be accepted in hope that a better solution will
show up eventually.</p>
<p>It works by making small random changes to an initial solution and sees
if the performance improved. The change is accepted if it improves, else
it can still be accepted if the difference of performances meet an
acceptance criteria.</p>
<p>In caret it has been implemented in the <code><a href="https://rdrr.io/pkg/caret/man/safs.html">safs()</a></code> which accepts a control
parameter that can be set using the <code><a href="https://rdrr.io/pkg/caret/man/safsControl.html">safsControl()</a></code> function.</p>
<p><code>safsControl</code> is similar to other control functions in caret (like you
saw in rfe and ga), and additionally it accepts an improve parameter
which is the number of iterations it should wait without improvement
until the values are reset to previous iteration.</p>
<div class="sourceCode" id="cb216"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/pkg/tictoc/man/tic.html">tic</a></span><span class="op">(</span><span class="op">)</span>
<span class="co"># Define control function</span>
<span class="va">sa_ctrl</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/safsControl.html">safsControl</a></span><span class="op">(</span>functions <span class="op">=</span> <span class="va">rfSA</span>,
                        method <span class="op">=</span> <span class="st">"repeatedcv"</span>,
                        repeats <span class="op">=</span> <span class="fl">3</span>,
                        improve <span class="op">=</span> <span class="fl">5</span><span class="op">)</span> <span class="co"># n iterations without improvement before a reset</span>

<span class="co"># Genetic Algorithm feature selection</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">100</span><span class="op">)</span>
<span class="va">sa_obj</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/caret/man/safs.html">safs</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">trainData</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">3</span>, <span class="fl">5</span><span class="op">:</span><span class="fl">13</span><span class="op">)</span><span class="op">]</span>, 
               y<span class="op">=</span><span class="va">trainData</span><span class="op">[</span>, <span class="fl">4</span><span class="op">]</span>,
               safsControl <span class="op">=</span> <span class="va">sa_ctrl</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/pkg/tictoc/man/tic.html">toc</a></span><span class="op">(</span><span class="op">)</span>
<span class="co">#&gt; 108.312 sec elapsed</span>
<span class="va">sa_obj</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Simulated Annealing Feature Selection</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; 366 samples</span>
<span class="co">#&gt; 12 predictors</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Maximum search iterations: 10 </span>
<span class="co">#&gt; Restart after 5 iterations without improvement (0.3 restarts on average)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Internal performance values: RMSE, Rsquared</span>
<span class="co">#&gt; Subset selection driven to minimize internal RMSE </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; External performance values: RMSE, Rsquared, MAE</span>
<span class="co">#&gt; Best iteration chose by minimizing external RMSE </span>
<span class="co">#&gt; External resampling method: Cross-Validated (10 fold, repeated 3 times) </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; During resampling:</span>
<span class="co">#&gt;   * the top 5 selected variables (out of a possible 12):</span>
<span class="co">#&gt;     Temperature_Sandburg (80%), Month (66.7%), Pressure_gradient (66.7%), Temperature_ElMonte (63.3%), Visibility (60%)</span>
<span class="co">#&gt;   * on average, 6.5 variables were selected (min = 3, max = 11)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; In the final search using the entire training set:</span>
<span class="co">#&gt;    * 6 features selected at iteration 9 including:</span>
<span class="co">#&gt;      Day_of_week, pressure_height, Wind_speed, Humidity, Inversion_base_height ... </span>
<span class="co">#&gt;    * external performance at this iteration is</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;       RMSE   Rsquared        MAE </span>
<span class="co">#&gt;      4.108      0.743      3.111</span></code></pre></div>
<div class="sourceCode" id="cb217"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Optimal variables</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">sa_obj</span><span class="op">$</span><span class="va">optVariables</span><span class="op">)</span>
<span class="co">#&gt; [1] "Day_of_week"           "pressure_height"       "Wind_speed"           </span>
<span class="co">#&gt; [4] "Humidity"              "Inversion_base_height" "Pressure_gradient"</span></code></pre></div>
</div>
<div id="information-value-and-weights-of-evidence" class="section level2">
<h2>
<span class="header-section-number">8.11</span> Information Value and Weights of Evidence<a class="anchor" aria-label="anchor" href="#information-value-and-weights-of-evidence"><i class="fas fa-link"></i></a>
</h2>
<p>The Information Value can be used to judge how important a given
categorical variable is in explaining the binary Y variable. It goes
well with logistic regression and other classification models that can
model binary variables.</p>
<p>Let’s try to find out how important the categorical variables are in
predicting if an individual will earn &gt; 50k from the <code>adult.csv</code>
dataset. Just run the code below to import the dataset.</p>
<div class="sourceCode" id="cb218"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://r-statistics.co/Information-Value-With-R.html">InformationValue</a></span><span class="op">)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Attaching package: 'InformationValue'</span>
<span class="co">#&gt; The following objects are masked from 'package:caret':</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     confusionMatrix, precision, sensitivity, specificity</span>

<span class="co"># online data</span>
<span class="co"># inputData &lt;- read.csv("http://rstatistics.net/wp-content/uploads/2015/09/adult.csv")</span>

<span class="va">inputData</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.csv</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/file.path.html">file.path</a></span><span class="op">(</span><span class="va">data_raw_dir</span>, <span class="st">"adult.csv"</span><span class="op">)</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">inputData</span><span class="op">)</span><span class="op">)</span>
<span class="co">#&gt;   AGE        WORKCLASS FNLWGT EDUCATION EDUCATIONNUM      MARITALSTATUS</span>
<span class="co">#&gt; 1  39        State-gov  77516 Bachelors           13      Never-married</span>
<span class="co">#&gt; 2  50 Self-emp-not-inc  83311 Bachelors           13 Married-civ-spouse</span>
<span class="co">#&gt; 3  38          Private 215646   HS-grad            9           Divorced</span>
<span class="co">#&gt; 4  53          Private 234721      11th            7 Married-civ-spouse</span>
<span class="co">#&gt; 5  28          Private 338409 Bachelors           13 Married-civ-spouse</span>
<span class="co">#&gt; 6  37          Private 284582   Masters           14 Married-civ-spouse</span>
<span class="co">#&gt;          OCCUPATION  RELATIONSHIP  RACE    SEX CAPITALGAIN CAPITALLOSS</span>
<span class="co">#&gt; 1      Adm-clerical Not-in-family White   Male        2174           0</span>
<span class="co">#&gt; 2   Exec-managerial       Husband White   Male           0           0</span>
<span class="co">#&gt; 3 Handlers-cleaners Not-in-family White   Male           0           0</span>
<span class="co">#&gt; 4 Handlers-cleaners       Husband Black   Male           0           0</span>
<span class="co">#&gt; 5    Prof-specialty          Wife Black Female           0           0</span>
<span class="co">#&gt; 6   Exec-managerial          Wife White Female           0           0</span>
<span class="co">#&gt;   HOURSPERWEEK NATIVECOUNTRY ABOVE50K</span>
<span class="co">#&gt; 1           40 United-States        0</span>
<span class="co">#&gt; 2           13 United-States        0</span>
<span class="co">#&gt; 3           40 United-States        0</span>
<span class="co">#&gt; 4           40 United-States        0</span>
<span class="co">#&gt; 5           40          Cuba        0</span>
<span class="co">#&gt; 6           40 United-States        0</span></code></pre></div>
<div class="sourceCode" id="cb219"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Choose Categorical Variables to compute Info Value.</span>
<span class="va">cat_vars</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span> <span class="op">(</span><span class="st">"WORKCLASS"</span>, <span class="st">"EDUCATION"</span>, <span class="st">"MARITALSTATUS"</span>, <span class="st">"OCCUPATION"</span>, <span class="st">"RELATIONSHIP"</span>, <span class="st">"RACE"</span>, <span class="st">"SEX"</span>, <span class="st">"NATIVECOUNTRY"</span><span class="op">)</span>  <span class="co"># get all categorical variables</span>

<span class="va">factor_vars</span> <span class="op">&lt;-</span> <span class="va">cat_vars</span>


<span class="co"># Init Output</span>
<span class="va">df_iv</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>VARS<span class="op">=</span><span class="va">cat_vars</span>, IV<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/numeric.html">numeric</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">cat_vars</span><span class="op">)</span><span class="op">)</span>, STRENGTH<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/character.html">character</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">cat_vars</span><span class="op">)</span><span class="op">)</span>, stringsAsFactors <span class="op">=</span> <span class="cn">F</span><span class="op">)</span>  <span class="co"># init output dataframe</span>

<span class="co"># Get Information Value for each variable</span>
<span class="kw">for</span> <span class="op">(</span><span class="va">factor_var</span> <span class="kw">in</span> <span class="va">factor_vars</span><span class="op">)</span><span class="op">{</span>
  <span class="va">df_iv</span><span class="op">[</span><span class="va">df_iv</span><span class="op">$</span><span class="va">VARS</span> <span class="op">==</span> <span class="va">factor_var</span>, <span class="st">"IV"</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu">InformationValue</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/InformationValue/man/IV.html">IV</a></span><span class="op">(</span>X<span class="op">=</span><span class="va">inputData</span><span class="op">[</span>, <span class="va">factor_var</span><span class="op">]</span>, Y<span class="op">=</span><span class="va">inputData</span><span class="op">$</span><span class="va">ABOVE50K</span><span class="op">)</span>
  <span class="va">df_iv</span><span class="op">[</span><span class="va">df_iv</span><span class="op">$</span><span class="va">VARS</span> <span class="op">==</span> <span class="va">factor_var</span>, <span class="st">"STRENGTH"</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/attr.html">attr</a></span><span class="op">(</span><span class="fu">InformationValue</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/InformationValue/man/IV.html">IV</a></span><span class="op">(</span>X<span class="op">=</span><span class="va">inputData</span><span class="op">[</span>, <span class="va">factor_var</span><span class="op">]</span>, Y<span class="op">=</span><span class="va">inputData</span><span class="op">$</span><span class="va">ABOVE50K</span><span class="op">)</span>, <span class="st">"howgood"</span><span class="op">)</span>
<span class="op">}</span>

<span class="co"># Sort</span>
<span class="va">df_iv</span> <span class="op">&lt;-</span> <span class="va">df_iv</span><span class="op">[</span><span class="fu"><a href="https://Rdatatable.gitlab.io/data.table/reference/setorder.html">order</a></span><span class="op">(</span><span class="op">-</span><span class="va">df_iv</span><span class="op">$</span><span class="va">IV</span><span class="op">)</span>, <span class="op">]</span>

<span class="va">df_iv</span>
<span class="co">#&gt;            VARS     IV            STRENGTH</span>
<span class="co">#&gt; 5  RELATIONSHIP 1.5356   Highly Predictive</span>
<span class="co">#&gt; 3 MARITALSTATUS 1.3388   Highly Predictive</span>
<span class="co">#&gt; 4    OCCUPATION 0.7762   Highly Predictive</span>
<span class="co">#&gt; 2     EDUCATION 0.7411   Highly Predictive</span>
<span class="co">#&gt; 7           SEX 0.3033   Highly Predictive</span>
<span class="co">#&gt; 1     WORKCLASS 0.1634   Highly Predictive</span>
<span class="co">#&gt; 8 NATIVECOUNTRY 0.0794 Somewhat Predictive</span>
<span class="co">#&gt; 6          RACE 0.0693 Somewhat Predictive</span></code></pre></div>
<p>Here is what the quantum of Information Value means:</p>
<p>Less than 0.02, then the predictor is not useful for modeling
(separating the Goods from the Bads)</p>
<p>0.02 to 0.1, then the predictor has only a weak relationship. 0.1 to
0.3, then the predictor has a medium strength relationship. 0.3 or
higher, then the predictor has a strong relationship. That was about IV.
Then what is Weight of Evidence?</p>
<p>Weights of evidence can be useful to find out how important a given
categorical variable is in explaining the ‘events’ (called ‘Goods’ in
below table.)</p>
<p>The ‘Information Value’ of the categorical variable can then be derived
from the respective WOE values.</p>
<p>IV=(perc good of all goods−perc bad of all bads) *WOE</p>
<p>The ‘WOETable’ below given the computation in more detail.</p>
<div class="sourceCode" id="cb220"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/pkg/InformationValue/man/WOETable.html">WOETable</a></span><span class="op">(</span>X<span class="op">=</span><span class="va">inputData</span><span class="op">[</span>, <span class="st">'WORKCLASS'</span><span class="op">]</span>, Y<span class="op">=</span><span class="va">inputData</span><span class="op">$</span><span class="va">ABOVE50K</span><span class="op">)</span>
<span class="co">#&gt;                CAT GOODS  BADS TOTAL   PCT_G    PCT_B    WOE       IV</span>
<span class="co">#&gt; 1                ?   191  1645  1836 0.02429 0.066545 -1.008 0.042574</span>
<span class="co">#&gt; 2      Federal-gov   371   589   960 0.04719 0.023827  0.683 0.015964</span>
<span class="co">#&gt; 3        Local-gov   617  1476  2093 0.07848 0.059709  0.273 0.005131</span>
<span class="co">#&gt; 4     Never-worked     7     7     7 0.00089 0.000283  1.146 0.000696</span>
<span class="co">#&gt; 5          Private  4963 17733 22696 0.63126 0.717354 -0.128 0.011006</span>
<span class="co">#&gt; 6     Self-emp-inc   622   494  1116 0.07911 0.019984  1.376 0.081363</span>
<span class="co">#&gt; 7 Self-emp-not-inc   724  1817  2541 0.09209 0.073503  0.225 0.004190</span>
<span class="co">#&gt; 8        State-gov   353   945  1298 0.04490 0.038228  0.161 0.001073</span>
<span class="co">#&gt; 9      Without-pay    14    14    14 0.00178 0.000566  1.146 0.001391</span></code></pre></div>
<p>The total IV of a variable is the sum of IV’s of its categories.</p>
</div>
<div id="dalex-package" class="section level2">
<h2>
<span class="header-section-number">8.12</span> DALEX Package<a class="anchor" aria-label="anchor" href="#dalex-package"><i class="fas fa-link"></i></a>
</h2>
<p>The <code>DALEX</code> is a powerful package that explains various things about the
variables used in an ML model.</p>
<p>For example, using the <code>variable_dropout()</code> function you can find out
how important a variable is based on a dropout loss, that is how much
loss is incurred by removing a variable from the model.</p>
<p>Apart from this, it also has the <code><a href="https://ModelOriented.github.io/DALEX/reference/model_profile.html">single_variable()</a></code> function that gives
you an idea of how the model’s output will change by changing the values
of one of the X’s in the model.</p>
<p>It also has the <code>single_prediction()</code> that can decompose a single model
prediction so as to understand which variable caused what effect in
predicting the value of Y.</p>
<div class="sourceCode" id="cb221"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.stat.berkeley.edu/~breiman/RandomForests/">randomForest</a></span><span class="op">)</span>
<span class="co">#&gt; randomForest 4.6-14</span>
<span class="co">#&gt; Type rfNews() to see new features/changes/bug fixes.</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Attaching package: 'randomForest'</span>
<span class="co">#&gt; The following object is masked from 'package:dplyr':</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     combine</span>
<span class="co">#&gt; The following object is masked from 'package:ranger':</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     importance</span>
<span class="co">#&gt; The following object is masked from 'package:ggplot2':</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     margin</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ModelOriented.github.io/DALEX/">DALEX</a></span><span class="op">)</span>

<span class="co"># Load data</span>
<span class="co"># inputData &lt;- read.csv("http://rstatistics.net/wp-content/uploads/2015/09/adult.csv")</span>

<span class="va">inputData</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.csv</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/file.path.html">file.path</a></span><span class="op">(</span><span class="va">data_raw_dir</span>, <span class="st">"adult.csv"</span><span class="op">)</span><span class="op">)</span>

<span class="co"># Train random forest model</span>
<span class="va">rf_mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/randomForest/man/randomForest.html">randomForest</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">ABOVE50K</span><span class="op">)</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">inputData</span>, ntree<span class="op">=</span><span class="fl">100</span><span class="op">)</span>
<span class="va">rf_mod</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt;  randomForest(formula = factor(ABOVE50K) ~ ., data = inputData,      ntree = 100) </span>
<span class="co">#&gt;                Type of random forest: classification</span>
<span class="co">#&gt;                      Number of trees: 100</span>
<span class="co">#&gt; No. of variables tried at each split: 3</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;         OOB estimate of  error rate: 13.6%</span>
<span class="co">#&gt; Confusion matrix:</span>
<span class="co">#&gt;       0    1 class.error</span>
<span class="co">#&gt; 0 23051 1669      0.0675</span>
<span class="co">#&gt; 1  2754 5087      0.3512</span>

<span class="co"># Variable importance with DALEX</span>
<span class="va">explained_rf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ModelOriented.github.io/DALEX/reference/explain.html">explain</a></span><span class="op">(</span><span class="va">rf_mod</span>, data<span class="op">=</span><span class="va">inputData</span>, y<span class="op">=</span><span class="va">inputData</span><span class="op">$</span><span class="va">ABOVE50K</span><span class="op">)</span>
<span class="co">#&gt; Preparation of a new explainer is initiated</span>
<span class="co">#&gt;   -&gt; model label       :  randomForest  ( <span style="color: #BBBB00;"> default </span><span> )</span></span>
<span class="co">#&gt;   -&gt; data              :  32561  rows  15  cols </span>
<span class="co">#&gt;   -&gt; target variable   :  32561  values </span>
<span class="co">#&gt;   -&gt; model_info        :  package randomForest , ver. 4.6.14 , task classification ( <span style="color: #BBBB00;"> default </span><span> ) </span></span>
<span class="co">#&gt;   -&gt; predict function  :  yhat.randomForest  will be used ( <span style="color: #BBBB00;"> default </span><span> )</span></span>
<span class="co">#&gt;   -&gt; predicted values  :  numerical, min =  0 , mean =  0.237 , max =  1  </span>
<span class="co">#&gt;   -&gt; residual function :  difference between y and yhat ( <span style="color: #BBBB00;"> default </span><span> )</span></span>
<span class="co">#&gt;   -&gt; residuals         :  numerical, min =  -0.94 , mean =  0.00374 , max =  0.93  </span>
<span class="co">#&gt;  <span style="color: #00BB00;"> A new explainer has been created! </span></span>

<span class="co"># Get the variable importances</span>
<span class="co"># varimps = variable_dropout(explained_rf, type='raw')</span>
<span class="va">varimps</span> <span class="op">=</span> <span class="fu"><a href="https://ModelOriented.github.io/DALEX/reference/model_parts.html">variable_importance</a></span><span class="op">(</span><span class="va">explained_rf</span>, type<span class="op">=</span><span class="st">'raw'</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">varimps</span><span class="op">)</span>
<span class="co">#&gt;         variable mean_dropout_loss        label</span>
<span class="co">#&gt; 1   _full_model_              36.8 randomForest</span>
<span class="co">#&gt; 2       ABOVE50K              36.8 randomForest</span>
<span class="co">#&gt; 3           RACE              41.1 randomForest</span>
<span class="co">#&gt; 4  NATIVECOUNTRY              43.6 randomForest</span>
<span class="co">#&gt; 5            SEX              46.4 randomForest</span>
<span class="co">#&gt; 6    CAPITALLOSS              46.9 randomForest</span>
<span class="co">#&gt; 7      WORKCLASS              55.8 randomForest</span>
<span class="co">#&gt; 8      EDUCATION              62.1 randomForest</span>
<span class="co">#&gt; 9   EDUCATIONNUM              63.3 randomForest</span>
<span class="co">#&gt; 10        FNLWGT              64.0 randomForest</span>
<span class="co">#&gt; 11  RELATIONSHIP              64.9 randomForest</span>
<span class="co">#&gt; 12  HOURSPERWEEK              68.6 randomForest</span>
<span class="co">#&gt; 13   CAPITALGAIN              69.3 randomForest</span>
<span class="co">#&gt; 14 MARITALSTATUS              73.3 randomForest</span>
<span class="co">#&gt; 15    OCCUPATION              85.1 randomForest</span>
<span class="co">#&gt; 16           AGE              85.2 randomForest</span>
<span class="co">#&gt; 17    _baseline_             299.5 randomForest</span></code></pre></div>
<div class="sourceCode" id="cb222"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span><span class="op">(</span><span class="va">varimps</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="100-fe-meta_133-ten_methods_variable_importance_files/figure-html/unnamed-chunk-13-1.png" width="70%" style="display: block; margin: auto;"></div>
</div>
<div id="conclusion" class="section level2">
<h2>
<span class="header-section-number">8.13</span> Conclusion<a class="anchor" aria-label="anchor" href="#conclusion"><i class="fas fa-link"></i></a>
</h2>
<p>Hope you find these methods useful. As it turns out different methods
showed different variables as important, or at least the degree of
importance changed. This need not be a conflict, because each method
gives a different perspective of how the variable can be useful
depending on how the algorithms learn <code>Y ~ x</code>. So its cool.</p>
<p>If you find any code breaks or bugs, report the issue here or just write
it below.</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="data-visualization-for-ml-models.html"><span class="header-section-number">7</span> Data Visualization for ML models</a></div>
<div class="next"><a href="employee-attrition-using-feature-importance.html"><span class="header-section-number">9</span> Employee Attrition using Feature Importance</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#ten-methods-to-assess-variable-importance"><span class="header-section-number">8</span> Ten methods to assess Variable Importance</a></li>
<li><a class="nav-link" href="#boruta"><span class="header-section-number">8.1</span> 1. Boruta</a></li>
<li><a class="nav-link" href="#variable-importance"><span class="header-section-number">8.2</span> 2. Variable Importance</a></li>
<li><a class="nav-link" href="#rpart"><span class="header-section-number">8.3</span> 3. rpart</a></li>
<li><a class="nav-link" href="#regularized-random-forest-rrf"><span class="header-section-number">8.4</span> 4. Regularized Random Forest (RRF)</a></li>
<li><a class="nav-link" href="#lasso-regression"><span class="header-section-number">8.5</span> 5. Lasso Regression</a></li>
<li><a class="nav-link" href="#step-wise-forward-and-backward-selection"><span class="header-section-number">8.6</span> 6. Step wise Forward and Backward Selection</a></li>
<li><a class="nav-link" href="#relative-importance-from-linear-regression"><span class="header-section-number">8.7</span> 7. Relative Importance from Linear Regression</a></li>
<li><a class="nav-link" href="#recursive-feature-elimination-rfe"><span class="header-section-number">8.8</span> 8. Recursive Feature Elimination (RFE)</a></li>
<li><a class="nav-link" href="#genetic-algorithm"><span class="header-section-number">8.9</span> 9. Genetic Algorithm</a></li>
<li><a class="nav-link" href="#simulated-annealing"><span class="header-section-number">8.10</span> 10. Simulated Annealing</a></li>
<li><a class="nav-link" href="#information-value-and-weights-of-evidence"><span class="header-section-number">8.11</span> Information Value and Weights of Evidence</a></li>
<li><a class="nav-link" href="#dalex-package"><span class="header-section-number">8.12</span> DALEX Package</a></li>
<li><a class="nav-link" href="#conclusion"><span class="header-section-number">8.13</span> Conclusion</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/f0nzie/machine_learning_compilation/blob/master/100-fe-meta_133-ten_methods_variable_importance.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/f0nzie/machine_learning_compilation/edit/master/100-fe-meta_133-ten_methods_variable_importance.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>A Machine Learning Compilation</strong>" was written by Several authors. Compiled by Alfonso R. Reyes. It was last built on 2020-11-20.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
