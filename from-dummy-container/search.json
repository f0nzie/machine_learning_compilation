[{"path":"index.html","id":"preface","chapter":"1 Preface","heading":"1 Preface","text":"compilation machine learning examples found. easy understand, address fundamental principle, explain chose particular algorithm.find detailed; others short straight point.","code":""},{"path":"index.html","id":"prerequisites","chapter":"1 Preface","heading":"Prerequisites","text":"used R-3.6.3 RStudio Preview 1.4. also plan use Anaconda, Miniconda GNU Python parts make use Python code.code reproducible.","code":"\ninstall.packages(\"bookdown\")\n# or the development version\n# devtools::install_github(\"rstudio/bookdown\")"},{"path":"introduction-to-pca.html","id":"introduction-to-pca","chapter":"2 Introduction to PCA","heading":"2 Introduction to PCA","text":"Dataset: irisDataset: irisAlgorithms:\nPCA\nAlgorithms:PCAThe PC1 axis explains 0.730 variance, PC2 axis explains 0.229 variance.","code":"\n# devtools::install_github(\"vqv/ggbiplot\")\nlibrary(ggbiplot)\n#> Loading required package: ggplot2\n#> Loading required package: plyr\n#> Loading required package: scales\n#> Loading required package: grid\n\niris.pca <- prcomp(iris[, 1:4], center = TRUE, scale = TRUE)\nprint(iris.pca)\n#> Standard deviations (1, .., p=4):\n#> [1] 1.708 0.956 0.383 0.144\n#> \n#> Rotation (n x k) = (4 x 4):\n#>                 PC1     PC2    PC3    PC4\n#> Sepal.Length  0.521 -0.3774  0.720  0.261\n#> Sepal.Width  -0.269 -0.9233 -0.244 -0.124\n#> Petal.Length  0.580 -0.0245 -0.142 -0.801\n#> Petal.Width   0.565 -0.0669 -0.634  0.524\nsummary(iris.pca)\n#> Importance of components:\n#>                         PC1   PC2    PC3     PC4\n#> Standard deviation     1.71 0.956 0.3831 0.14393\n#> Proportion of Variance 0.73 0.229 0.0367 0.00518\n#> Cumulative Proportion  0.73 0.958 0.9948 1.00000\ng <- ggbiplot(iris.pca,\n              obs.scale = 1,\n              var.scale = 1,\n              groups = iris$Species,\n              ellipse = TRUE,\n              circle = TRUE) +\n    scale_color_discrete(name = \"\") +\n    theme(legend.direction = \"horizontal\", legend.position = \"top\")\n\nprint(g)"},{"path":"introduction-to-pca.html","id":"underlying-principal-components","chapter":"2 Introduction to PCA","heading":"2.1 Underlying principal components","text":"","code":"\n# Run PCA here with prcomp ()\niris.pca <- prcomp(iris[, 1:4], center = TRUE, scale = TRUE)\n\nprint(iris.pca)\n#> Standard deviations (1, .., p=4):\n#> [1] 1.708 0.956 0.383 0.144\n#> \n#> Rotation (n x k) = (4 x 4):\n#>                 PC1     PC2    PC3    PC4\n#> Sepal.Length  0.521 -0.3774  0.720  0.261\n#> Sepal.Width  -0.269 -0.9233 -0.244 -0.124\n#> Petal.Length  0.580 -0.0245 -0.142 -0.801\n#> Petal.Width   0.565 -0.0669 -0.634  0.524\n# Now, compute the new dataset aligned to the PCs by\n# using the predict() function .\ndf.new <- predict(iris.pca, iris[, 1:4])\nhead(df.new)\n#>        PC1    PC2     PC3      PC4\n#> [1,] -2.26 -0.478  0.1273  0.02409\n#> [2,] -2.07  0.672  0.2338  0.10266\n#> [3,] -2.36  0.341 -0.0441  0.02828\n#> [4,] -2.29  0.595 -0.0910 -0.06574\n#> [5,] -2.38 -0.645 -0.0157 -0.03580\n#> [6,] -2.07 -1.484 -0.0269  0.00659\n# Show the PCA model’s sdev values are the square root\n# of the projected variances, which are along the diagonal\n# of the covariance matrix of the projected data.\niris.pca$sdev^2\n#> [1] 2.9185 0.9140 0.1468 0.0207\n# # Compute covariance matrix for new dataset.\n# Recall that the standard deviation is the square root of the variance.\nround(cov(df.new), 5)\n#>      PC1   PC2   PC3    PC4\n#> PC1 2.92 0.000 0.000 0.0000\n#> PC2 0.00 0.914 0.000 0.0000\n#> PC3 0.00 0.000 0.147 0.0000\n#> PC4 0.00 0.000 0.000 0.0207"},{"path":"introduction-to-pca.html","id":"compute-eigenvectors-and-eigenvalues","chapter":"2 Introduction to PCA","heading":"2.2 Compute eigenvectors and eigenvalues","text":"","code":"\n# Scale and center the data.\ndf.scaled <- scale(iris[, 1:4], center = TRUE, scale = TRUE)\n\n# Compute the covariance matrix.\ncov.df.scaled <- cov(df.scaled)\n\n# Compute the eigenvectors and eigen values.\n# Each eigenvector (column) is a principal component.\n# Each eigenvalue is the variance explained by the\n# associated eigenvector.\neigenInformation <- eigen(cov.df.scaled)\n\nprint(eigenInformation)\n#> eigen() decomposition\n#> $values\n#> [1] 2.9185 0.9140 0.1468 0.0207\n#> \n#> $vectors\n#>        [,1]    [,2]   [,3]   [,4]\n#> [1,]  0.521 -0.3774  0.720  0.261\n#> [2,] -0.269 -0.9233 -0.244 -0.124\n#> [3,]  0.580 -0.0245 -0.142 -0.801\n#> [4,]  0.565 -0.0669 -0.634  0.524\n# Now, compute the new dataset aligned to the PCs by\n# multiplying the eigenvector and data matrices.\n\n\n# Create transposes in preparation for matrix multiplication\neigenvectors.t <- t(eigenInformation$vectors)     # 4x4\ndf.scaled.t <- t(df.scaled)    # 4x150\n\n# Perform matrix multiplication.\ndf.new <- eigenvectors.t %*% df.scaled.t   # 4x150\n\n# Create new data frame. First take transpose and\n# then add column names.\ndf.new.t <- t(df.new)    # 150x4\ncolnames(df.new.t) <- c(\"PC1\", \"PC2\", \"PC3\", \"PC4\")\n\nhead(df.new.t)\n#>        PC1    PC2     PC3      PC4\n#> [1,] -2.26 -0.478  0.1273  0.02409\n#> [2,] -2.07  0.672  0.2338  0.10266\n#> [3,] -2.36  0.341 -0.0441  0.02828\n#> [4,] -2.29  0.595 -0.0910 -0.06574\n#> [5,] -2.38 -0.645 -0.0157 -0.03580\n#> [6,] -2.07 -1.484 -0.0269  0.00659\n# Compute covariance matrix for new dataset \nround(cov(df.new.t), 5)\n#>      PC1   PC2   PC3    PC4\n#> PC1 2.92 0.000 0.000 0.0000\n#> PC2 0.00 0.914 0.000 0.0000\n#> PC3 0.00 0.000 0.147 0.0000\n#> PC4 0.00 0.000 0.000 0.0207"},{"path":"comparison-of-two-pca-packages.html","id":"comparison-of-two-pca-packages","chapter":"3 Comparison of two PCA packages","heading":"3 Comparison of two PCA packages","text":"Datasets: decathlon2Datasets: decathlon2Algorithms:\nPCA\nAlgorithms:PCA","code":""},{"path":"comparison-of-two-pca-packages.html","id":"prcomp-vs-princomp","chapter":"3 Comparison of two PCA packages","heading":"3.1 prcomp vs princomp","text":"http://www.sthda.com/english/articles/31-principal-component-methods--r-practical-guide/118-principal-component-analysis--r-prcomp-vs-princomp/","code":""},{"path":"comparison-of-two-pca-packages.html","id":"general-methods-for-principal-component-analysis","chapter":"3 Comparison of two PCA packages","heading":"3.2 General methods for principal component analysis","text":"two general methods perform PCA R :Spectral decomposition examines covariances / correlations variablesSingular value decomposition examines covariances / correlations individualsThe function princomp() uses spectral decomposition approach. functions prcomp() PCA()[FactoMineR] use singular value decomposition (SVD).","code":""},{"path":"comparison-of-two-pca-packages.html","id":"prcomp-and-princomp-functions","chapter":"3 Comparison of two PCA packages","heading":"3.2.1 prcomp() and princomp() functions","text":"simplified format 2 functions :Arguments prcomp():x: numeric matrix data framescale: logical value indicating whether variables scaled unit variance analysis takes placeArguments prcomp():x: numeric matrix data framescale: logical value indicating whether variables scaled unit variance analysis takes placeArguments princomp():x: numeric matrix data frame cor: logical value. TRUE, data centered scaled analysis scores: logical value. TRUE, coordinates principal component calculatedArguments princomp():x: numeric matrix data frame cor: logical value. TRUE, data centered scaled analysis scores: logical value. TRUE, coordinates principal component calculated","code":"prcomp(x, scale = FALSE)\nprincomp(x, cor = FALSE, scores = TRUE)"},{"path":"comparison-of-two-pca-packages.html","id":"loading-factoextra","chapter":"3 Comparison of two PCA packages","heading":"3.2.2 Loading factoextra","text":"","code":"\n# install.packages(\"factoextra\")\nlibrary(factoextra)\n#> Loading required package: ggplot2\n#> Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa"},{"path":"comparison-of-two-pca-packages.html","id":"loading-the-decathlon-dataset","chapter":"3 Comparison of two PCA packages","heading":"3.2.3 Loading the decathlon dataset","text":"’ll use data sets decathlon2 [factoextra], already described : PCA - Data format.Briefly, contains:Active individuals (rows 1 23) active variables (columns 1 10), used perform principal component analysisSupplementary individuals (rows 24 27) supplementary variables (columns 11 13), coordinates predicted using PCA information parameters obtained active individuals/variables.","code":"\nlibrary(\"factoextra\")\ndata(decathlon2)\ndecathlon2.active <- decathlon2[1:23, 1:10]\nhead(decathlon2.active[, 1:6])\n#>           X100m Long.jump Shot.put High.jump X400m X110m.hurdle\n#> SEBRLE     11.0      7.58     14.8      2.07  49.8         14.7\n#> CLAY       10.8      7.40     14.3      1.86  49.4         14.1\n#> BERNARD    11.0      7.23     14.2      1.92  48.9         15.0\n#> YURKOV     11.3      7.09     15.2      2.10  50.4         15.3\n#> ZSIVOCZKY  11.1      7.30     13.5      2.01  48.6         14.2\n#> McMULLEN   10.8      7.31     13.8      2.13  49.9         14.4\ndecathlon2.supplementary <- decathlon2[24:27, 1:10]\nhead(decathlon2.supplementary[, 1:6])\n#>         X100m Long.jump Shot.put High.jump X400m X110m.hurdle\n#> KARPOV   11.0      7.30     14.8      2.04  48.4         14.1\n#> WARNERS  11.1      7.60     14.3      1.98  48.7         14.2\n#> Nool     10.8      7.53     14.3      1.88  48.8         14.8\n#> Drews    10.9      7.38     13.1      1.88  48.5         14.0"},{"path":"comparison-of-two-pca-packages.html","id":"compute-pca-in-r-using-prcomp","chapter":"3 Comparison of two PCA packages","heading":"3.3 Compute PCA in R using prcomp()","text":"section ’ll provide easy--use R code compute visualize PCA R using prcomp() function factoextra package.`. Load factoextra visualizationcompute PCAVisualize eigenvalues (scree plot). Show percentage variances explained principal component.plot , might want stop fifth principal component. 87% information (variances) contained data retained first five principal components.","code":"\nlibrary(factoextra)\n# compute PCA\nres.pca <- prcomp(decathlon2.active, scale = TRUE)\n# Visualize eigenvalues (scree plot).\nfviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 50))"},{"path":"comparison-of-two-pca-packages.html","id":"plots-quality-and-contribution","chapter":"3 Comparison of two PCA packages","heading":"3.4 Plots: quality and contribution","text":"Graph individuals. Individuals similar profile grouped together.Graph variables. Positive correlated variables point side plot. Negative correlated variables point opposite sides graph.Biplot individuals variables","code":"\n# Graph of individuals.\nfviz_pca_ind(res.pca,\n             col.ind = \"cos2\", # Color by the quality of representation\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE     # Avoid text overlapping\n             )\n# Graph of variables.\nfviz_pca_var(res.pca,\n             col.var = \"contrib\", # Color by contributions to the PC\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE     # Avoid text overlapping\n             )\n# Biplot of individuals and variables\nfviz_pca_biplot(res.pca, repel = TRUE,\n                col.var = \"#2E9FDF\", # Variables color\n                col.ind = \"#696969\"  # Individuals color\n                )"},{"path":"comparison-of-two-pca-packages.html","id":"access-to-the-pca-results","chapter":"3 Comparison of two PCA packages","heading":"3.5 Access to the PCA results","text":"","code":"\nlibrary(factoextra)\n# Eigenvalues\neig.val <- get_eigenvalue(res.pca)\neig.val\n#>        eigenvalue variance.percent cumulative.variance.percent\n#> Dim.1       4.124            41.24                        41.2\n#> Dim.2       1.839            18.39                        59.6\n#> Dim.3       1.239            12.39                        72.0\n#> Dim.4       0.819             8.19                        80.2\n#> Dim.5       0.702             7.02                        87.2\n#> Dim.6       0.423             4.23                        91.5\n#> Dim.7       0.303             3.03                        94.5\n#> Dim.8       0.274             2.74                        97.2\n#> Dim.9       0.155             1.55                        98.8\n#> Dim.10      0.122             1.22                       100.0\n  \n# Results for Variables\nres.var <- get_pca_var(res.pca)\nres.var$coord          # Coordinates\n#>                 Dim.1   Dim.2   Dim.3   Dim.4  Dim.5    Dim.6    Dim.7    Dim.8\n#> X100m        -0.85063  0.1794 -0.3016  0.0336 -0.194  0.03537 -0.09134 -0.10472\n#> Long.jump     0.79418 -0.2809  0.1905 -0.1154  0.233 -0.03373 -0.15433 -0.39738\n#> Shot.put      0.73391 -0.0854 -0.5176  0.1285 -0.249 -0.23979 -0.00989  0.02436\n#> High.jump     0.61008  0.4652 -0.3301  0.1446  0.403 -0.28464  0.02816  0.08441\n#> X400m        -0.70160 -0.2902 -0.2835  0.4308  0.104 -0.04929  0.28611 -0.23355\n#> X110m.hurdle -0.76413  0.0247 -0.4489 -0.0169  0.224  0.00263 -0.37007 -0.00834\n#> Discus        0.74321 -0.0497 -0.1765  0.3950 -0.408  0.19854 -0.14273 -0.03956\n#> Pole.vault   -0.21727 -0.8075 -0.0941 -0.3390 -0.222 -0.32746 -0.01039  0.03291\n#> Javeline      0.42823 -0.3861 -0.6041 -0.3317  0.198  0.36210  0.13356  0.05284\n#> X1500m        0.00428 -0.7845  0.2195  0.4480  0.263  0.04205 -0.11137  0.19447\n#>                Dim.9   Dim.10\n#> X100m        -0.3031  0.04442\n#> Long.jump    -0.0516  0.02972\n#> Shot.put      0.0478  0.21745\n#> High.jump    -0.1121 -0.13357\n#> X400m         0.0822 -0.03417\n#> X110m.hurdle  0.1618 -0.01563\n#> Discus        0.0134 -0.17259\n#> Pole.vault   -0.0258 -0.13721\n#> Javeline     -0.0405 -0.00385\n#> X1500m       -0.1022  0.06283\nres.var$contrib        # Contributions to the PCs\n#>                 Dim.1   Dim.2  Dim.3   Dim.4 Dim.5    Dim.6   Dim.7   Dim.8\n#> X100m        1.75e+01  1.7505  7.339  0.1376  5.39  0.29592  2.7571  3.9952\n#> Long.jump    1.53e+01  4.2904  2.930  1.6249  7.75  0.26900  7.8716 57.5332\n#> Shot.put     1.31e+01  0.3967 21.620  2.0141  8.82 13.59686  0.0323  0.2162\n#> High.jump    9.02e+00 11.7716  8.793  2.5499 23.12 19.15961  0.2620  2.5957\n#> X400m        1.19e+01  4.5799  6.488 22.6509  1.54  0.57451 27.0527 19.8734\n#> X110m.hurdle 1.42e+01  0.0333 16.261  0.0348  7.17  0.00164 45.2616  0.0254\n#> Discus       1.34e+01  0.1341  2.515 19.0413 23.76  9.32175  6.7323  0.5702\n#> Pole.vault   1.14e+00 35.4619  0.714 14.0231  7.01 25.35762  0.0357  0.3947\n#> Javeline     4.45e+00  8.1087 29.453 13.4296  5.58 31.00496  5.8957  1.0173\n#> X1500m       4.44e-04 33.4729  3.887 24.4939  9.88  0.41813  4.0989 13.7787\n#>               Dim.9  Dim.10\n#> X100m        59.174  1.6176\n#> Long.jump     1.715  0.7241\n#> Shot.put      1.471 38.7677\n#> High.jump     8.102 14.6265\n#> X400m         4.349  0.9573\n#> X110m.hurdle 16.858  0.2003\n#> Discus        0.115 24.4217\n#> Pole.vault    0.428 15.4356\n#> Javeline      1.054  0.0122\n#> X1500m        6.734  3.2370\nres.var$cos2           # Quality of representation \n#>                 Dim.1    Dim.2   Dim.3    Dim.4  Dim.5    Dim.6    Dim.7\n#> X100m        7.24e-01 0.032184 0.09094 0.001127 0.0378 1.25e-03 8.34e-03\n#> Long.jump    6.31e-01 0.078881 0.03631 0.013315 0.0544 1.14e-03 2.38e-02\n#> Shot.put     5.39e-01 0.007294 0.26791 0.016504 0.0619 5.75e-02 9.77e-05\n#> High.jump    3.72e-01 0.216424 0.10896 0.020895 0.1622 8.10e-02 7.93e-04\n#> X400m        4.92e-01 0.084203 0.08039 0.185611 0.0108 2.43e-03 8.19e-02\n#> X110m.hurdle 5.84e-01 0.000612 0.20150 0.000285 0.0503 6.93e-06 1.37e-01\n#> Discus       5.52e-01 0.002466 0.03116 0.156032 0.1667 3.94e-02 2.04e-02\n#> Pole.vault   4.72e-02 0.651977 0.00885 0.114911 0.0491 1.07e-01 1.08e-04\n#> Javeline     1.83e-01 0.149080 0.36497 0.110048 0.0391 1.31e-01 1.78e-02\n#> X1500m       1.83e-05 0.615409 0.04817 0.200713 0.0693 1.77e-03 1.24e-02\n#>                 Dim.8    Dim.9   Dim.10\n#> X100m        1.10e-02 0.091848 1.97e-03\n#> Long.jump    1.58e-01 0.002661 8.83e-04\n#> Shot.put     5.93e-04 0.002284 4.73e-02\n#> High.jump    7.12e-03 0.012575 1.78e-02\n#> X400m        5.45e-02 0.006750 1.17e-03\n#> X110m.hurdle 6.96e-05 0.026166 2.44e-04\n#> Discus       1.56e-03 0.000179 2.98e-02\n#> Pole.vault   1.08e-03 0.000664 1.88e-02\n#> Javeline     2.79e-03 0.001637 1.49e-05\n#> X1500m       3.78e-02 0.010453 3.95e-03\n# Results for individuals\nres.ind <- get_pca_ind(res.pca)\nres.ind$coord          # Coordinates\n#>              Dim.1  Dim.2  Dim.3   Dim.4     Dim.5   Dim.6   Dim.7    Dim.8\n#> SEBRLE       0.191 -1.554 -0.628  0.0821  1.142614 -0.4639 -0.2080  0.04346\n#> CLAY         0.790 -2.420  1.357  1.2698 -0.806848  1.3042 -0.2129  0.61724\n#> BERNARD     -1.329 -1.612 -0.196 -1.9209  0.082343 -0.4006 -0.4064  0.70386\n#> YURKOV      -0.869  0.433 -2.474  0.6972  0.398858  0.1029 -0.3249  0.11500\n#> ZSIVOCZKY   -0.106  2.023  1.305 -0.0993 -0.197024  0.8955  0.0883 -0.20234\n#> McMULLEN     0.119  0.992  0.844  1.3122  1.585871  0.1866  0.4783  0.29309\n#> MARTINEAU   -2.392  1.285 -0.898  0.3731 -2.243352 -0.4567 -0.2998 -0.29163\n#> HERNU       -1.891 -1.178 -0.156  0.8913 -0.126741  0.4362 -0.5661 -1.52940\n#> BARRAS      -1.774  0.413  0.658  0.2287 -0.233837  0.0903  0.2159  0.68258\n#> NOOL        -2.777  1.573  0.607 -1.5555  1.424184  0.4972 -0.5321 -0.43339\n#> BOURGUIGNON -4.414 -1.264 -0.010  0.6668  0.419152 -0.0820 -0.5983  0.56362\n#> Sebrle       3.451 -1.217 -1.678 -0.8087 -0.025053 -0.0828  0.0102 -0.03059\n#> Clay         3.316 -1.623 -0.618 -0.3168  0.569165  0.7772  0.2575 -0.58064\n#> Karpov       4.070  0.798  1.015  0.3134 -0.797426 -0.3296 -1.3637  0.34531\n#> Macey        1.848  2.064 -0.979  0.5847 -0.000216 -0.1973 -0.2693 -0.36322\n#> Warners      1.387 -0.282  2.000 -1.0196 -0.040540 -0.5567 -0.2674 -0.10947\n#> Zsivoczky    0.472  0.927 -1.728 -0.1848  0.407303 -0.1138  0.0399  0.53804\n#> Hernu        0.276  1.166  0.171 -0.8487 -0.689480 -0.3317  0.4431  0.24729\n#> Bernard      1.367  1.478  0.831  0.7453  0.859802 -0.3281  0.3636  0.00617\n#> Schwarzl    -0.710 -0.658  1.041 -0.9272 -0.288757 -0.6889  0.5657 -0.68705\n#> Pogorelov   -0.214 -0.861  0.298  1.3556 -0.015053 -1.5938  0.7837 -0.03762\n#> Schoenbeck  -0.495 -1.300  0.103 -0.2493 -0.645226  0.1617  0.8575 -0.25585\n#> Barras      -0.316  0.819 -0.862 -0.5894 -0.779739  1.1742  0.9451  0.36555\n#>                Dim.9  Dim.10\n#> SEBRLE      -0.65934  0.0327\n#> CLAY        -0.06013 -0.3172\n#> BERNARD      0.17008 -0.0991\n#> YURKOV      -0.10952 -0.1197\n#> ZSIVOCZKY   -0.52310 -0.3484\n#> McMULLEN    -0.10562 -0.3932\n#> MARTINEAU   -0.22342 -0.6164\n#> HERNU        0.00618  0.5537\n#> BARRAS      -0.66928  0.5309\n#> NOOL        -0.11578 -0.0962\n#> BOURGUIGNON  0.52581  0.0586\n#> Sebrle      -0.84721  0.2197\n#> Clay         0.40978 -0.6160\n#> Karpov       0.19306  0.2172\n#> Macey        0.36826  0.2125\n#> Warners      0.18028  0.2421\n#> Zsivoczky    0.58597 -0.1427\n#> Hernu        0.06691 -0.2087\n#> Bernard      0.27949  0.3207\n#> Schwarzl    -0.00836 -0.3021\n#> Pogorelov   -0.13053 -0.0370\n#> Schoenbeck   0.56422  0.2968\n#> Barras       0.10226  0.6119\nres.ind$contrib        # Contributions to the PCs\n#>               Dim.1  Dim.2    Dim.3   Dim.4    Dim.5   Dim.6    Dim.7    Dim.8\n#> SEBRLE       0.0385  5.712 1.39e+00  0.0357 8.09e+00  2.2126  0.62143 2.99e-02\n#> CLAY         0.6581 13.854 6.46e+00  8.5557 4.03e+00 17.4880  0.65141 6.04e+00\n#> BERNARD      1.8627  6.144 1.35e-01 19.5783 4.20e-02  1.6502  2.37365 7.85e+00\n#> YURKOV       0.7969  0.443 2.15e+01  2.5794 9.86e-01  0.1088  1.51656 2.09e-01\n#> ZSIVOCZKY    0.0118  9.682 5.97e+00  0.0523 2.41e-01  8.2456  0.11192 6.49e-01\n#> McMULLEN     0.0148  2.325 2.50e+00  9.1353 1.56e+01  0.3579  3.28702 1.36e+00\n#> MARTINEAU    6.0337  3.904 2.83e+00  0.7386 3.12e+01  2.1441  1.29111 1.35e+00\n#> HERNU        3.7700  3.284 8.58e-02  4.2151 9.96e-02  1.9566  4.60485 3.71e+01\n#> BARRAS       3.3194  0.402 1.52e+00  0.2776 3.39e-01  0.0838  0.67004 7.38e+00\n#> NOOL         8.1299  5.849 1.29e+00 12.8376 1.26e+01  2.5413  4.06767 2.98e+00\n#> BOURGUIGNON 20.5373  3.776 3.53e-04  2.3588 1.09e+00  0.0691  5.14425 5.03e+00\n#> Sebrle      12.5584  3.502 9.88e+00  3.4701 3.89e-03  0.0705  0.00148 1.48e-02\n#> Clay        11.5936  6.232 1.34e+00  0.5325 2.01e+00  6.2097  0.95282 5.34e+00\n#> Karpov      17.4661  1.507 3.61e+00  0.5210 3.94e+00  1.1168 26.72016 1.89e+00\n#> Macey        3.6021 10.073 3.36e+00  1.8139 2.89e-07  0.4001  1.04191 2.09e+00\n#> Warners      2.0291  0.188 1.40e+01  5.5159 1.02e-02  3.1867  1.02738 1.90e-01\n#> Zsivoczky    0.2344  2.031 1.05e+01  0.1813 1.03e+00  0.1332  0.02289 4.59e+00\n#> Hernu        0.0805  3.214 1.02e-01  3.8217 2.95e+00  1.1311  2.82103 9.69e-01\n#> Bernard      1.9708  5.166 2.43e+00  2.9474 4.58e+00  1.1066  1.89945 6.02e-04\n#> Schwarzl     0.5318  1.025 3.80e+00  4.5612 5.17e-01  4.8796  4.59812 7.48e+00\n#> Pogorelov    0.0484  1.753 3.11e-01  9.7503 1.40e-03 26.1167  8.82532 2.24e-02\n#> Schoenbeck   0.2586  3.997 3.72e-02  0.3297 2.58e+00  0.2689 10.56627 1.04e+00\n#> Barras       0.1052  1.588 2.61e+00  1.8430 3.77e+00 14.1743 12.83542 2.12e+00\n#>                Dim.9  Dim.10\n#> SEBRLE      12.17748  0.0382\n#> CLAY         0.10126  3.5857\n#> BERNARD      0.81032  0.3499\n#> YURKOV       0.33601  0.5107\n#> ZSIVOCZKY    7.66492  4.3274\n#> McMULLEN     0.31250  5.5105\n#> MARTINEAU    1.39820 13.5440\n#> HERNU        0.00107 10.9278\n#> BARRAS      12.54733 10.0454\n#> NOOL         0.37548  0.3300\n#> BOURGUIGNON  7.74457  0.1222\n#> Sebrle      20.10555  1.7206\n#> Clay         4.70357 13.5271\n#> Karpov       1.04399  1.6819\n#> Macey        3.79877  1.6096\n#> Warners      0.91042  2.0890\n#> Zsivoczky    9.61785  0.7261\n#> Hernu        0.12540  1.5523\n#> Bernard      2.18807  3.6657\n#> Schwarzl     0.00196  3.2536\n#> Pogorelov    0.47727  0.0487\n#> Schoenbeck   8.91730  3.1402\n#> Barras       0.29289 13.3453\nres.ind$cos2           # Quality of representation \n#>               Dim.1  Dim.2    Dim.3   Dim.4    Dim.5    Dim.6    Dim.7    Dim.8\n#> SEBRLE      0.00753 0.4975 8.13e-02 0.00139 2.69e-01 0.044324 8.91e-03 3.89e-04\n#> CLAY        0.04870 0.4570 1.44e-01 0.12579 5.08e-02 0.132691 3.54e-03 2.97e-02\n#> BERNARD     0.19720 0.2900 4.29e-03 0.41182 7.57e-04 0.017913 1.84e-02 5.53e-02\n#> YURKOV      0.09611 0.0238 7.78e-01 0.06181 2.02e-02 0.001345 1.34e-02 1.68e-03\n#> ZSIVOCZKY   0.00157 0.5764 2.40e-01 0.00139 5.47e-03 0.112918 1.10e-03 5.76e-03\n#> McMULLEN    0.00218 0.1522 1.10e-01 0.26649 3.89e-01 0.005388 3.54e-02 1.33e-02\n#> MARTINEAU   0.40401 0.1165 5.69e-02 0.00983 3.55e-01 0.014721 6.34e-03 6.00e-03\n#> HERNU       0.39928 0.1551 2.73e-03 0.08870 1.79e-03 0.021248 3.58e-02 2.61e-01\n#> BARRAS      0.61624 0.0333 8.48e-02 0.01024 1.07e-02 0.001594 9.13e-03 9.12e-02\n#> NOOL        0.48987 0.1571 2.34e-02 0.15369 1.29e-01 0.015701 1.80e-02 1.19e-02\n#> BOURGUIGNON 0.85970 0.0705 4.45e-06 0.01962 7.75e-03 0.000297 1.58e-02 1.40e-02\n#> Sebrle      0.67538 0.0840 1.60e-01 0.03708 3.56e-05 0.000389 5.85e-06 5.30e-05\n#> Clay        0.68759 0.1648 2.39e-02 0.00627 2.03e-02 0.037763 4.15e-03 2.11e-02\n#> Karpov      0.78367 0.0301 4.87e-02 0.00464 3.01e-02 0.005138 8.80e-02 5.64e-03\n#> Macey       0.36344 0.4531 1.02e-01 0.03636 4.95e-09 0.004140 7.71e-03 1.40e-02\n#> Warners     0.25565 0.0106 5.31e-01 0.13808 2.18e-04 0.041169 9.50e-03 1.59e-03\n#> Zsivoczky   0.04505 0.1740 6.05e-01 0.00692 3.36e-02 0.002625 3.23e-04 5.87e-02\n#> Hernu       0.02482 0.4418 9.46e-03 0.23420 1.55e-01 0.035771 6.38e-02 1.99e-02\n#> Bernard     0.28935 0.3381 1.07e-01 0.08598 1.14e-01 0.016659 2.05e-02 5.88e-06\n#> Schwarzl    0.11672 0.1003 2.51e-01 0.19889 1.93e-02 0.109806 7.40e-02 1.09e-01\n#> Pogorelov   0.00780 0.1259 1.50e-02 0.31210 3.85e-05 0.431416 1.04e-01 2.40e-04\n#> Schoenbeck  0.06707 0.4620 2.90e-03 0.01699 1.14e-01 0.007150 2.01e-01 1.79e-02\n#> Barras      0.01897 0.1277 1.41e-01 0.06604 1.16e-01 0.262130 1.70e-01 2.54e-02\n#>                Dim.9   Dim.10\n#> SEBRLE      8.95e-02 0.000221\n#> CLAY        2.82e-04 0.007847\n#> BERNARD     3.23e-03 0.001096\n#> YURKOV      1.53e-03 0.001822\n#> ZSIVOCZKY   3.85e-02 0.017092\n#> McMULLEN    1.73e-03 0.023927\n#> MARTINEAU   3.52e-03 0.026821\n#> HERNU       4.27e-06 0.034229\n#> BARRAS      8.77e-02 0.055153\n#> NOOL        8.51e-04 0.000588\n#> BOURGUIGNON 1.22e-02 0.000151\n#> Sebrle      4.07e-02 0.002737\n#> Clay        1.05e-02 0.023726\n#> Karpov      1.76e-03 0.002232\n#> Macey       1.44e-02 0.004803\n#> Warners     4.32e-03 0.007784\n#> Zsivoczky   6.96e-02 0.004127\n#> Hernu       1.46e-03 0.014160\n#> Bernard     1.21e-02 0.015917\n#> Schwarzl    1.62e-05 0.021117\n#> Pogorelov   2.89e-03 0.000232\n#> Schoenbeck  8.70e-02 0.024083\n#> Barras      1.99e-03 0.071184"},{"path":"comparison-of-two-pca-packages.html","id":"predict-using-pca","chapter":"3 Comparison of two PCA packages","heading":"3.6 Predict using PCA","text":"section, ’ll show predict coordinates supplementary individuals variables using information provided previously performed PCA.Data: rows 24 27 columns 1 10 [decathlon2 data sets]. new data must contain columns (variables) names order active data used compute PCA.Predict coordinates new individuals data. Use R base function predict():Graph individuals including supplementary individuals:predicted coordinates individuals can manually calculated follow:Center scale new individuals data using center scale PCACalculate predicted coordinates multiplying scaled values eigenvectors (loadings) principal components. R code can used :","code":"\n# Data for the supplementary individuals\nind.sup <- decathlon2[24:27, 1:10]\nind.sup[, 1:6]\n#>         X100m Long.jump Shot.put High.jump X400m X110m.hurdle\n#> KARPOV   11.0      7.30     14.8      2.04  48.4         14.1\n#> WARNERS  11.1      7.60     14.3      1.98  48.7         14.2\n#> Nool     10.8      7.53     14.3      1.88  48.8         14.8\n#> Drews    10.9      7.38     13.1      1.88  48.5         14.0\nind.sup.coord <- predict(res.pca, newdata = ind.sup)\nind.sup.coord[, 1:4]\n#>            PC1    PC2   PC3    PC4\n#> KARPOV   0.777 -0.762 1.597  1.686\n#> WARNERS -0.378  0.119 1.701 -0.691\n#> Nool    -0.547 -1.934 0.472 -2.228\n#> Drews   -1.085 -0.017 2.982 -1.501\n# Plot of active individuals\np <- fviz_pca_ind(res.pca, repel = TRUE)\n# Add supplementary individuals\nfviz_add(p, ind.sup.coord, color =\"blue\")\n# Centering and scaling the supplementary individuals\nind.scaled <- scale(ind.sup, \n                    center = res.pca$center,\n                    scale = res.pca$scale)\n# Coordinates of the individividuals\ncoord_func <- function(ind, loadings){\n  r <- loadings*ind\n  apply(r, 2, sum)\n}\npca.loadings <- res.pca$rotation\nind.sup.coord <- t(apply(ind.scaled, 1, coord_func, pca.loadings ))\nind.sup.coord[, 1:4]\n#>            PC1    PC2   PC3    PC4\n#> KARPOV   0.777 -0.762 1.597  1.686\n#> WARNERS -0.378  0.119 1.701 -0.691\n#> Nool    -0.547 -1.934 0.472 -2.228\n#> Drews   -1.085 -0.017 2.982 -1.501"},{"path":"comparison-of-two-pca-packages.html","id":"supplementary-variables","chapter":"3 Comparison of two PCA packages","heading":"3.7 Supplementary variables","text":"","code":""},{"path":"comparison-of-two-pca-packages.html","id":"qualitative-categorical-variables","chapter":"3 Comparison of two PCA packages","heading":"3.7.1 Qualitative / categorical variables","text":"data sets decathlon2 contain supplementary qualitative variable columns 13 corresponding type competitions.Qualitative / categorical variables can used color individuals groups. grouping variable length number active individuals (23).Calculate coordinates levels grouping variables. coordinates given group calculated mean coordinates individuals group.","code":"\ngroups <- as.factor(decathlon2$Competition[1:23])\nfviz_pca_ind(res.pca,\n             col.ind = groups, # color by groups\n             palette = c(\"#00AFBB\",  \"#FC4E07\"),\n             addEllipses = TRUE, # Concentration ellipses\n             ellipse.type = \"confidence\",\n             legend.title = \"Groups\",\n             repel = TRUE\n             )\nlibrary(magrittr) # for pipe %>%\nlibrary(dplyr)   # everything else\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\n\n# 1. Individual coordinates\nres.ind <- get_pca_ind(res.pca)\n# 2. Coordinate of groups\ncoord.groups <- res.ind$coord %>%\n  as_data_frame() %>%\n  select(Dim.1, Dim.2) %>%\n  mutate(competition = groups) %>%\n  group_by(competition) %>%\n  summarise(\n    Dim.1 = mean(Dim.1),\n    Dim.2 = mean(Dim.2)\n    )\n#> Warning: `as_data_frame()` is deprecated as of tibble 2.0.0.\n#> Please use `as_tibble()` instead.\n#> The signature and semantics have changed, see `?as_tibble`.\n#> This warning is displayed once every 8 hours.\n#> Call `lifecycle::last_warnings()` to see where this warning was generated.\ncoord.groups\n#> # A tibble: 2 x 3\n#>   competition Dim.1  Dim.2\n#>   <fct>       <dbl>  <dbl>\n#> 1 Decastar    -1.31 -0.119\n#> 2 OlympicG     1.20  0.109"},{"path":"comparison-of-two-pca-packages.html","id":"quantitative-variables","chapter":"3 Comparison of two PCA packages","heading":"3.7.2 Quantitative variables","text":"Data: columns 11:12. length number active individuals (23)coordinates given quantitative variable calculated correlation quantitative variables principal components.","code":"\nquanti.sup <- decathlon2[1:23, 11:12, drop = FALSE]\nhead(quanti.sup)\n#>           Rank Points\n#> SEBRLE       1   8217\n#> CLAY         2   8122\n#> BERNARD      4   8067\n#> YURKOV       5   8036\n#> ZSIVOCZKY    7   8004\n#> McMULLEN     8   7995\n# Predict coordinates and compute cos2\nquanti.coord <- cor(quanti.sup, res.pca$x)\nquanti.cos2 <- quanti.coord^2\n# Graph of variables including supplementary variables\np <- fviz_pca_var(res.pca)\nfviz_add(p, quanti.coord, color =\"blue\", geom=\"arrow\")"},{"path":"comparison-of-two-pca-packages.html","id":"theory-behind-pca-results","chapter":"3 Comparison of two PCA packages","heading":"3.8 Theory behind PCA results","text":"","code":""},{"path":"comparison-of-two-pca-packages.html","id":"pca-results-for-variables","chapter":"3 Comparison of two PCA packages","heading":"3.8.1 PCA results for variables","text":"’ll show calculate PCA results variables: coordinates, cos2 contributions:var.coord = loadings * component standard deviations var.cos2 = var.coord^2 var.contrib. contribution variable given principal component (percentage) : (var.cos2 * 100) / (total cos2 component)","code":"\n# Helper function \n#::::::::::::::::::::::::::::::::::::::::\nvar_coord_func <- function(loadings, comp.sdev){\n  loadings*comp.sdev\n}\n# Compute Coordinates\n#::::::::::::::::::::::::::::::::::::::::\nloadings <- res.pca$rotation\nsdev <- res.pca$sdev\nvar.coord <- t(apply(loadings, 1, var_coord_func, sdev)) \nhead(var.coord[, 1:4])\n#>                 PC1     PC2    PC3     PC4\n#> X100m        -0.851  0.1794 -0.302  0.0336\n#> Long.jump     0.794 -0.2809  0.191 -0.1154\n#> Shot.put      0.734 -0.0854 -0.518  0.1285\n#> High.jump     0.610  0.4652 -0.330  0.1446\n#> X400m        -0.702 -0.2902 -0.284  0.4308\n#> X110m.hurdle -0.764  0.0247 -0.449 -0.0169\n# Compute Cos2\n#::::::::::::::::::::::::::::::::::::::::\nvar.cos2 <- var.coord^2\nhead(var.cos2[, 1:4])\n#>                PC1      PC2    PC3      PC4\n#> X100m        0.724 0.032184 0.0909 0.001127\n#> Long.jump    0.631 0.078881 0.0363 0.013315\n#> Shot.put     0.539 0.007294 0.2679 0.016504\n#> High.jump    0.372 0.216424 0.1090 0.020895\n#> X400m        0.492 0.084203 0.0804 0.185611\n#> X110m.hurdle 0.584 0.000612 0.2015 0.000285\n# Compute contributions\n#::::::::::::::::::::::::::::::::::::::::\ncomp.cos2 <- apply(var.cos2, 2, sum)\ncontrib <- function(var.cos2, comp.cos2){var.cos2*100/comp.cos2}\nvar.contrib <- t(apply(var.cos2,1, contrib, comp.cos2))\nhead(var.contrib[, 1:4])\n#>                PC1     PC2   PC3     PC4\n#> X100m        17.54  1.7505  7.34  0.1376\n#> Long.jump    15.29  4.2904  2.93  1.6249\n#> Shot.put     13.06  0.3967 21.62  2.0141\n#> High.jump     9.02 11.7716  8.79  2.5499\n#> X400m        11.94  4.5799  6.49 22.6509\n#> X110m.hurdle 14.16  0.0333 16.26  0.0348"},{"path":"comparison-of-two-pca-packages.html","id":"pca-results-for-individuals","chapter":"3 Comparison of two PCA packages","heading":"3.8.2 PCA results for individuals","text":"ind.coord = res.pca$xind.coord = res.pca$xCos2 individuals. Two steps:\nCalculate square distance individual PCA center gravity: d2 = [(var1_ind_i - mean_var1)/sd_var1]^2 + …+ [(var10_ind_i - mean_var10)/sd_var10]^2 + …+..\nCalculate cos2 ind.coord^2/d2\nCos2 individuals. Two steps:Calculate square distance individual PCA center gravity: d2 = [(var1_ind_i - mean_var1)/sd_var1]^2 + …+ [(var10_ind_i - mean_var10)/sd_var10]^2 + …+..Calculate cos2 ind.coord^2/d2Contributions individuals principal components: 100 * (1 / number_of_individuals)*(ind.coord^2 / comp_sdev^2). Note sum contributions per column 100Contributions individuals principal components: 100 * (1 / number_of_individuals)*(ind.coord^2 / comp_sdev^2). Note sum contributions per column 100","code":"\n# Coordinates of individuals\n#::::::::::::::::::::::::::::::::::\nind.coord <- res.pca$x\nhead(ind.coord[, 1:4])\n#>              PC1    PC2    PC3     PC4\n#> SEBRLE     0.191 -1.554 -0.628  0.0821\n#> CLAY       0.790 -2.420  1.357  1.2698\n#> BERNARD   -1.329 -1.612 -0.196 -1.9209\n#> YURKOV    -0.869  0.433 -2.474  0.6972\n#> ZSIVOCZKY -0.106  2.023  1.305 -0.0993\n#> McMULLEN   0.119  0.992  0.844  1.3122\n# Cos2 of individuals\n#:::::::::::::::::::::::::::::::::\n# 1. square of the distance between an individual and the\n# PCA center of gravity\ncenter <- res.pca$center\nscale<- res.pca$scale\n\ngetdistance <- function(ind_row, center, scale){\n  return(sum(((ind_row-center)/scale)^2))\n}\n\nd2 <- apply(decathlon2.active,1, getdistance, center, scale)\n# 2. Compute the cos2. The sum of each row is 1\ncos2 <- function(ind.coord, d2){return(ind.coord^2/d2)}\nind.cos2 <- apply(ind.coord, 2, cos2, d2)\nhead(ind.cos2[, 1:4])\n#>               PC1    PC2     PC3     PC4\n#> SEBRLE    0.00753 0.4975 0.08133 0.00139\n#> CLAY      0.04870 0.4570 0.14363 0.12579\n#> BERNARD   0.19720 0.2900 0.00429 0.41182\n#> YURKOV    0.09611 0.0238 0.77823 0.06181\n#> ZSIVOCZKY 0.00157 0.5764 0.23975 0.00139\n#> McMULLEN  0.00218 0.1522 0.11014 0.26649\n# Contributions of individuals\n#:::::::::::::::::::::::::::::::\ncontrib <- function(ind.coord, comp.sdev, n.ind){\n  100*(1/n.ind)*ind.coord^2/comp.sdev^2\n}\nind.contrib <- t(apply(ind.coord, 1, contrib, \n                       res.pca$sdev, nrow(ind.coord)))\nhead(ind.contrib[, 1:4])\n#>              PC1    PC2    PC3     PC4\n#> SEBRLE    0.0385  5.712  1.385  0.0357\n#> CLAY      0.6581 13.854  6.460  8.5557\n#> BERNARD   1.8627  6.144  0.135 19.5783\n#> YURKOV    0.7969  0.443 21.476  2.5794\n#> ZSIVOCZKY 0.0118  9.682  5.975  0.0523\n#> McMULLEN  0.0148  2.325  2.497  9.1353"},{"path":"detailed-study-of-principal-component-analysis.html","id":"detailed-study-of-principal-component-analysis","chapter":"4 Detailed study of Principal Component Analysis","heading":"4 Detailed study of Principal Component Analysis","text":"Datasets: decathlon2Datasets: decathlon2Algorithms:\nPCA\nAlgorithms:PCAhttp://www.sthda.com/english/articles/31-principal-component-methods--r-practical-guide/112-pca-principal-component-analysis-essentials/Principal component analysis (PCA) allows us summarize \nvisualize information data set containing\nindividuals/observations described multiple inter-correlated\nquantitative variables. variable considered different\ndimension. 3 variables data sets, \ndifficult visualize multi-dimensional hyperspace.Principal component analysis used extract important\ninformation multivariate data table express \ninformation set new variables called principal components.\nnew variables correspond linear combination originals.\nnumber principal components less equal number\noriginal variables.information given data set corresponds total variation\ncontains. goal PCA identify directions (principal\ncomponents) along variation data maximal.words, PCA reduces dimensionality multivariate data \ntwo three principal components, can visualized graphically,\nminimal loss information.PCA terminology, data contains :Active individuals (light blue, rows 1:23) : Individuals \nused principal component analysis.Active individuals (light blue, rows 1:23) : Individuals \nused principal component analysis.Supplementary individuals (dark blue, rows 24:27) : \ncoordinates individuals predicted using PCA\ninformation parameters obtained active\nindividuals/variablesSupplementary individuals (dark blue, rows 24:27) : \ncoordinates individuals predicted using PCA\ninformation parameters obtained active\nindividuals/variablesActive variables (pink, columns 1:10) : Variables used\nprincipal component analysis.Active variables (pink, columns 1:10) : Variables used\nprincipal component analysis.Supplementary variables: supplementary individuals, \ncoordinates variables predicted also. can :\nSupplementary continuous variables (red): Columns 11 12\ncorresponding respectively rank points \nathletes.\nSupplementary qualitative variables (green): Column 13\ncorresponding two athlete-tic meetings (2004 Olympic Game\n2004 Decastar). categorical (factor) variable\nfactor. can used color individuals groups.\nSupplementary variables: supplementary individuals, \ncoordinates variables predicted also. can :Supplementary continuous variables (red): Columns 11 12\ncorresponding respectively rank points \nathletes.Supplementary qualitative variables (green): Column 13\ncorresponding two athlete-tic meetings (2004 Olympic Game\n2004 Decastar). categorical (factor) variable\nfactor. can used color individuals groups.start subsetting active individuals active variables \nprincipal component analysis:","code":"\n# install.packages(c(\"FactoMineR\", \"factoextra\"))\nlibrary(FactoMineR)\nlibrary(factoextra)\n#> Loading required package: ggplot2\n#> Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\ndata(decathlon2)\n# head(decathlon2)\ndecathlon2.active <- decathlon2[1:23, 1:10]\nhead(decathlon2.active[, 1:6], 4)\n#>         X100m Long.jump Shot.put High.jump X400m X110m.hurdle\n#> SEBRLE   11.0      7.58     14.8      2.07  49.8         14.7\n#> CLAY     10.8      7.40     14.3      1.86  49.4         14.1\n#> BERNARD  11.0      7.23     14.2      1.92  48.9         15.0\n#> YURKOV   11.3      7.09     15.2      2.10  50.4         15.3"},{"path":"detailed-study-of-principal-component-analysis.html","id":"data-standardization","chapter":"4 Detailed study of Principal Component Analysis","heading":"4.1 Data standardization","text":"principal component analysis, variables often scaled (\n.e. standardized). particularly recommended variables \nmeasured different scales (e.g: kilograms, kilometers, centimeters,\n…); otherwise, PCA outputs obtained severely affected.goal make variables comparable. Generally variables \nscaled ) standard deviation one ii) mean zero.function PCA() [FactoMineR package] can used. simplified format\n:object created using function PCA() contains many\ninformation found many different lists matrices. values\ndescribed next section.","code":"\nlibrary(FactoMineR)\nres.pca <- PCA(decathlon2.active, graph = FALSE)\nprint(res.pca)\n#> **Results for the Principal Component Analysis (PCA)**\n#> The analysis was performed on 23 individuals, described by 10 variables\n#> *The results are available in the following objects:\n#> \n#>    name               description                          \n#> 1  \"$eig\"             \"eigenvalues\"                        \n#> 2  \"$var\"             \"results for the variables\"          \n#> 3  \"$var$coord\"       \"coord. for the variables\"           \n#> 4  \"$var$cor\"         \"correlations variables - dimensions\"\n#> 5  \"$var$cos2\"        \"cos2 for the variables\"             \n#> 6  \"$var$contrib\"     \"contributions of the variables\"     \n#> 7  \"$ind\"             \"results for the individuals\"        \n#> 8  \"$ind$coord\"       \"coord. for the individuals\"         \n#> 9  \"$ind$cos2\"        \"cos2 for the individuals\"           \n#> 10 \"$ind$contrib\"     \"contributions of the individuals\"   \n#> 11 \"$call\"            \"summary statistics\"                 \n#> 12 \"$call$centre\"     \"mean of the variables\"              \n#> 13 \"$call$ecart.type\" \"standard error of the variables\"    \n#> 14 \"$call$row.w\"      \"weights for the individuals\"        \n#> 15 \"$call$col.w\"      \"weights for the variables\""},{"path":"detailed-study-of-principal-component-analysis.html","id":"eigenvalues-variances","chapter":"4 Detailed study of Principal Component Analysis","heading":"4.2 Eigenvalues / Variances","text":"described previous sections, eigenvalues measure amount \nvariation retained principal component. Eigenvalues large\nfirst PCs small subsequent PCs. , first\nPCs corresponds directions maximum amount variation\ndata set.examine eigenvalues determine number principal\ncomponents considered. eigenvalues proportion \nvariances (.e., information) retained principal components (PCs)\ncan extracted using function get_eigenvalue() [factoextra\npackage].sum eigenvalues give total variance 10.proportion variation explained eigenvalue given \nsecond column. example, 4.124 divided 10 equals 0.4124, , \n41.24% variation explained first eigenvalue. \ncumulative percentage explained obtained adding successive\nproportions variation explained obtain running total. \ninstance, 41.242% plus 18.385% equals 59.627%, forth. Therefore,\n59.627% variation explained first two eigenvalues\ntogether.Unfortunately, well-accepted objective way decide \nmany principal components enough. depend specific\nfield application specific data set. practice, tend \nlook first principal components order find interesting\npatterns data.analysis, first three principal components explain 72% \nvariation. acceptably large percentage.alternative method determine number principal components \nlook Scree Plot, plot eigenvalues ordered \nlargest smallest. number component determined \npoint, beyond remaining eigenvalues relatively small\ncomparable size (Jollife 2002, Peres-Neto, Jackson, Somers\n(2005)).scree plot can produced using function fviz_eig() \nfviz_screeplot() [factoextra package].plot , might want stop fifth principal\ncomponent. 87% information (variances) contained data \nretained first five principal components.","code":"\nlibrary(factoextra)\neig.val <- get_eigenvalue(res.pca)\neig.val\n#>        eigenvalue variance.percent cumulative.variance.percent\n#> Dim.1       4.124            41.24                        41.2\n#> Dim.2       1.839            18.39                        59.6\n#> Dim.3       1.239            12.39                        72.0\n#> Dim.4       0.819             8.19                        80.2\n#> Dim.5       0.702             7.02                        87.2\n#> Dim.6       0.423             4.23                        91.5\n#> Dim.7       0.303             3.03                        94.5\n#> Dim.8       0.274             2.74                        97.2\n#> Dim.9       0.155             1.55                        98.8\n#> Dim.10      0.122             1.22                       100.0\nfviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 50))"},{"path":"detailed-study-of-principal-component-analysis.html","id":"graph-of-variables","chapter":"4 Detailed study of Principal Component Analysis","heading":"4.3 Graph of variables","text":"Results simple method extract results, variables, \nPCA output use function get_pca_var() [factoextra package].\nfunction provides list matrices containing results \nactive variables (coordinates, correlation variables \naxes, squared cosine contributions)components get_pca_var() can used plot variables\nfollow:var$coord: coordinates variables create scatter plotvar$cos2: represents quality representation variables \nfactor map. ’s calculated squared coordinates: var.cos2\n= var.coord * var.coord.var$contrib: contains contributions (percentage) \nvariables principal components. contribution \nvariable (var) given principal component (percentage) :\n(var.cos2 * 100) / (total cos2 component).Note , ’s possible plot variables color according\neither ) quality factor map (cos2) ii) \ncontribution values principal components (contrib).different components can accessed follow:section, describe visualize variables draw\nconclusions correlations. Next, highlight variables\naccording either ) quality representation factor map\nii) contributions principal components.","code":"\nvar <- get_pca_var(res.pca)\nvar\n#> Principal Component Analysis Results for variables\n#>  ===================================================\n#>   Name       Description                                    \n#> 1 \"$coord\"   \"Coordinates for the variables\"                \n#> 2 \"$cor\"     \"Correlations between variables and dimensions\"\n#> 3 \"$cos2\"    \"Cos2 for the variables\"                       \n#> 4 \"$contrib\" \"contributions of the variables\"\n# Coordinates\nhead(var$coord)\n#>               Dim.1   Dim.2  Dim.3   Dim.4  Dim.5\n#> X100m        -0.851 -0.1794  0.302  0.0336 -0.194\n#> Long.jump     0.794  0.2809 -0.191 -0.1154  0.233\n#> Shot.put      0.734  0.0854  0.518  0.1285 -0.249\n#> High.jump     0.610 -0.4652  0.330  0.1446  0.403\n#> X400m        -0.702  0.2902  0.284  0.4308  0.104\n#> X110m.hurdle -0.764 -0.0247  0.449 -0.0169  0.224\n# Cos2: quality on the factore map\nhead(var$cos2)\n#>              Dim.1    Dim.2  Dim.3    Dim.4  Dim.5\n#> X100m        0.724 0.032184 0.0909 0.001127 0.0378\n#> Long.jump    0.631 0.078881 0.0363 0.013315 0.0544\n#> Shot.put     0.539 0.007294 0.2679 0.016504 0.0619\n#> High.jump    0.372 0.216424 0.1090 0.020895 0.1622\n#> X400m        0.492 0.084203 0.0804 0.185611 0.0108\n#> X110m.hurdle 0.584 0.000612 0.2015 0.000285 0.0503\n# Contributions to the principal components\nhead(var$contrib)\n#>              Dim.1   Dim.2 Dim.3   Dim.4 Dim.5\n#> X100m        17.54  1.7505  7.34  0.1376  5.39\n#> Long.jump    15.29  4.2904  2.93  1.6249  7.75\n#> Shot.put     13.06  0.3967 21.62  2.0141  8.82\n#> High.jump     9.02 11.7716  8.79  2.5499 23.12\n#> X400m        11.94  4.5799  6.49 22.6509  1.54\n#> X110m.hurdle 14.16  0.0333 16.26  0.0348  7.17"},{"path":"detailed-study-of-principal-component-analysis.html","id":"correlation-circle","chapter":"4 Detailed study of Principal Component Analysis","heading":"4.4 Correlation circle","text":"correlation variable principal component (PC) \nused coordinates variable PC. representation \nvariables differs plot observations: observations\nrepresented projections, variables represented\ncorrelations (Abdi Williams 2010).plot variables, type :plot also known variable correlation plots. shows \nrelationships variables. can interpreted follow:Positively correlated variables grouped together.Negatively correlated variables positioned opposite sides \nplot origin (opposed quadrants).distance variables origin measures quality\nvariables factor map. Variables away \norigin well represented factor map.","code":"\n# Coordinates of variables\nhead(var$coord, 4)\n#>            Dim.1   Dim.2  Dim.3   Dim.4  Dim.5\n#> X100m     -0.851 -0.1794  0.302  0.0336 -0.194\n#> Long.jump  0.794  0.2809 -0.191 -0.1154  0.233\n#> Shot.put   0.734  0.0854  0.518  0.1285 -0.249\n#> High.jump  0.610 -0.4652  0.330  0.1446  0.403\nfviz_pca_var(res.pca, col.var = \"black\")"},{"path":"detailed-study-of-principal-component-analysis.html","id":"quality-of-representation","chapter":"4 Detailed study of Principal Component Analysis","heading":"4.5 Quality of representation","text":"quality representation variables factor map called\ncos2 (square cosine, squared coordinates) . can access cos2\nfollow:can visualize cos2 variables dimensions using \ncorrplot package:’s also possible create bar plot variables cos2 using \nfunction fviz_cos2() [factoextra]:Note ,high cos2 indicates good representation variable \nprincipal component. case variable positioned close\ncircumference correlation circle.high cos2 indicates good representation variable \nprincipal component. case variable positioned close\ncircumference correlation circle.low cos2 indicates variable perfectly represented\nPCs. case variable close center \ncircle.low cos2 indicates variable perfectly represented\nPCs. case variable close center \ncircle.given variable, sum cos2 principal\ncomponents equal one.variable perfectly represented two principal components\n(Dim.1 & Dim.2), sum cos2 two PCs equal one.\ncase variables positioned circle \ncorrelations.variables, 2 components might required \nperfectly represent data. case variables positioned\ninside circle correlations.summary:cos2 values used estimate quality \nrepresentationThe closer variable circle correlations, better\nrepresentation factor map (important \ninterpret components)Variables closed center plot less\nimportant first components.’s possible color variables cos2 values using \nargument col.var = \"cos2\". produces gradient colors. \ncase, argument gradient.cols can used provide custom color.\ninstance, gradient.cols = c(“white”, “blue”, “red”) means :variables low cos2 values colored “white”variables mid cos2 values colored “blue”variables high cos2 values colored redNote , ’s also possible change transparency \nvariables according cos2 values using option alpha.var =\n“cos2”. example, type :","code":"\nhead(var$cos2, 4)\n#>           Dim.1   Dim.2  Dim.3   Dim.4  Dim.5\n#> X100m     0.724 0.03218 0.0909 0.00113 0.0378\n#> Long.jump 0.631 0.07888 0.0363 0.01331 0.0544\n#> Shot.put  0.539 0.00729 0.2679 0.01650 0.0619\n#> High.jump 0.372 0.21642 0.1090 0.02089 0.1622\nlibrary(corrplot)\n#> corrplot 0.84 loaded\ncorrplot(var$cos2, is.corr=FALSE)\n# Total cos2 of variables on Dim.1 and Dim.2\nfviz_cos2(res.pca, choice = \"var\", axes = 1:2)\n# Color by cos2 values: quality on the factor map\nfviz_pca_var(res.pca, col.var = \"cos2\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), \n             repel = TRUE # Avoid text overlapping\n             )\n# Change the transparency by cos2 values\nfviz_pca_var(res.pca, alpha.var = \"cos2\")"},{"path":"detailed-study-of-principal-component-analysis.html","id":"contributions-of-variables-to-pcs","chapter":"4 Detailed study of Principal Component Analysis","heading":"4.6 Contributions of variables to PCs","text":"contributions variables accounting variability \ngiven principal component expressed percentage.Variables correlated PC1 (.e., Dim.1) PC2 (.e.,\nDim.2) important explaining variability \ndata set.Variables correlated PC correlated \nlast dimensions variables low contribution might \nremoved simplify overall analysis.contribution variables can extracted follow :larger value contribution, variable\ncontributes component.’s possible use function corrplot() [corrplot package] \nhighlight contributing variables dimension:function fviz_contrib() [factoextra package] can used draw\nbar plot variable contributions. data contains many\nvariables, can decide show top contributing variables.\nR code shows top 10 variables contributing \nprincipal components:total contribution PC1 PC2 obtained following R\ncode:red dashed line graph indicates expected average\ncontribution. contribution variables uniform, \nexpected value 1/length(variables) = 1/10 = 10%. given\ncomponent, variable contribution larger cutoff \nconsidered important contributing component.Note , total contribution given variable, explaining \nvariations retained two principal components, say PC1 PC2, \ncalculated contrib = [(C1 * Eig1) + (C2 * Eig2)]/(Eig1 + Eig2),\nwhereC1 C2 contributions variable PC1 PC2,\nrespectivelyEig1 Eig2 eigenvalues PC1 PC2, respectively.\nRecall eigenvalues measure amount variation retained \nPC.case, expected average contribution (cutoff) calculated\nfollow: mentioned , contributions 10 variables\nuniform, expected average contribution given PC \n1/10 = 10%. expected average contribution variable PC1 \nPC2 : [(10* Eig1) + (10 * Eig2)]/(Eig1 + Eig2)can seen variables - X100m, Long.jump Pole.vault -\ncontribute dimensions 1 2.important (, contributing) variables can highlighted \ncorrelation plot follow:Note , ’s also possible change transparency variables\naccording contrib values using option\nalpha.var = \"contrib\". example, type :","code":"\nhead(var$contrib, 4)\n#>           Dim.1  Dim.2 Dim.3 Dim.4 Dim.5\n#> X100m     17.54  1.751  7.34 0.138  5.39\n#> Long.jump 15.29  4.290  2.93 1.625  7.75\n#> Shot.put  13.06  0.397 21.62 2.014  8.82\n#> High.jump  9.02 11.772  8.79 2.550 23.12\nlibrary(\"corrplot\")\n\ncorrplot(var$contrib, is.corr=FALSE)    \n# Contributions of variables to PC1\nfviz_contrib(res.pca, choice = \"var\", axes = 1, top = 10)\n# Contributions of variables to PC2\nfviz_contrib(res.pca, choice = \"var\", axes = 2, top = 10)\nfviz_contrib(res.pca, choice = \"var\", axes = 1:2, top = 10)\nfviz_pca_var(res.pca, col.var = \"contrib\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\")\n             )\n# Change the transparency by contrib values\nfviz_pca_var(res.pca, alpha.var = \"contrib\")"},{"path":"detailed-study-of-principal-component-analysis.html","id":"color-by-a-custom-continuous-variable","chapter":"4 Detailed study of Principal Component Analysis","heading":"4.7 Color by a custom continuous variable","text":"previous sections, showed color variables \ncontributions cos2. Note , ’s possible color\nvariables custom continuous variable. coloring variable\nlength number active variables PCA\n(n = 10).example, type :","code":"\n# Create a random continuous variable of length 10\nset.seed(123)\nmy.cont.var <- rnorm(10)\n# Color variables by the continuous variable\nfviz_pca_var(res.pca, col.var = my.cont.var,\n             gradient.cols = c(\"blue\", \"yellow\", \"red\"),\n             legend.title = \"Cont.Var\")"},{"path":"detailed-study-of-principal-component-analysis.html","id":"color-by-groups","chapter":"4 Detailed study of Principal Component Analysis","heading":"4.8 Color by groups","text":"’s also possible change color variables groups defined \nqualitative/categorical variable, also called factor R terminology.don’t grouping variable data sets classifying\nvariables, ’ll create .following demo example, start classifying variables\n3 groups using kmeans clustering algorithm. Next, use \nclusters returned kmeans algorithm color variables.","code":"\n# Create a grouping variable using kmeans\n# Create 3 groups of variables (centers = 3)\nset.seed(123)\nres.km <- kmeans(var$coord, centers = 3, nstart = 25)\ngrp <- as.factor(res.km$cluster)\n# Color variables by groups\nfviz_pca_var(res.pca, col.var = grp, \n             palette = c(\"#0073C2FF\", \"#EFC000FF\", \"#868686FF\"),\n             legend.title = \"Cluster\")"},{"path":"detailed-study-of-principal-component-analysis.html","id":"dimension-description","chapter":"4 Detailed study of Principal Component Analysis","heading":"4.9 Dimension description","text":"section (???)(pca-variable-contributions), described \nhighlight variables according contributions principal\ncomponents.Note also , function dimdesc() [FactoMineR], dimension\ndescription, can used identify significantly associated\nvariables given principal component . can used follow:","code":"\nres.desc <- dimdesc(res.pca, axes = c(1,2), proba = 0.05)\n# Description of dimension 1\nres.desc$Dim.1\n#> $quanti\n#>              correlation  p.value\n#> Long.jump          0.794 6.06e-06\n#> Discus             0.743 4.84e-05\n#> Shot.put           0.734 6.72e-05\n#> High.jump          0.610 1.99e-03\n#> Javeline           0.428 4.15e-02\n#> X400m             -0.702 1.91e-04\n#> X110m.hurdle      -0.764 2.20e-05\n#> X100m             -0.851 2.73e-07\n#> \n#> attr(,\"class\")\n#> [1] \"condes\" \"list \"\n# Description of dimension 2\nres.desc$Dim.2\n#> $quanti\n#>            correlation  p.value\n#> Pole.vault       0.807 3.21e-06\n#> X1500m           0.784 9.38e-06\n#> High.jump       -0.465 2.53e-02\n#> \n#> attr(,\"class\")\n#> [1] \"condes\" \"list \""},{"path":"detailed-study-of-principal-component-analysis.html","id":"graph-of-individuals","chapter":"4 Detailed study of Principal Component Analysis","heading":"4.10 Graph of individuals","text":"Results results, individuals can extracted using function\nget_pca_ind()[factoextra package]. Similarly get_pca_var(),\nfunction get_pca_ind() provides list matrices containing \nresults individuals (coordinates, correlation \nindividuals axes, squared cosine contributions)get access different components, use :","code":"\nind <- get_pca_ind(res.pca)\nind\n#> Principal Component Analysis Results for individuals\n#>  ===================================================\n#>   Name       Description                       \n#> 1 \"$coord\"   \"Coordinates for the individuals\" \n#> 2 \"$cos2\"    \"Cos2 for the individuals\"        \n#> 3 \"$contrib\" \"contributions of the individuals\"\n# Coordinates of individuals\nhead(ind$coord)\n#>            Dim.1  Dim.2  Dim.3   Dim.4   Dim.5\n#> SEBRLE     0.196  1.589  0.642  0.0839  1.1683\n#> CLAY       0.808  2.475 -1.387  1.2984 -0.8250\n#> BERNARD   -1.359  1.648  0.201 -1.9641  0.0842\n#> YURKOV    -0.889 -0.443  2.530  0.7129  0.4078\n#> ZSIVOCZKY -0.108 -2.069 -1.334 -0.1015 -0.2015\n#> McMULLEN   0.121 -1.014 -0.863  1.3416  1.6215\n# Quality of individuals\nhead(ind$cos2)\n#>             Dim.1  Dim.2   Dim.3   Dim.4    Dim.5\n#> SEBRLE    0.00753 0.4975 0.08133 0.00139 0.268903\n#> CLAY      0.04870 0.4570 0.14363 0.12579 0.050785\n#> BERNARD   0.19720 0.2900 0.00429 0.41182 0.000757\n#> YURKOV    0.09611 0.0238 0.77823 0.06181 0.020228\n#> ZSIVOCZKY 0.00157 0.5764 0.23975 0.00139 0.005465\n#> McMULLEN  0.00218 0.1522 0.11014 0.26649 0.389262\n# Contributions of individuals\nhead(ind$contrib)\n#>            Dim.1  Dim.2  Dim.3   Dim.4   Dim.5\n#> SEBRLE    0.0403  5.971  1.448  0.0373  8.4589\n#> CLAY      0.6881 14.484  6.754  8.9446  4.2179\n#> BERNARD   1.9474  6.423  0.141 20.4682  0.0439\n#> YURKOV    0.8331  0.463 22.452  2.6966  1.0308\n#> ZSIVOCZKY 0.0123 10.122  6.246  0.0547  0.2515\n#> McMULLEN  0.0155  2.431  2.610  9.5506 16.2949"},{"path":"detailed-study-of-principal-component-analysis.html","id":"plots-quality-and-contribution-1","chapter":"4 Detailed study of Principal Component Analysis","heading":"4.11 Plots: quality and contribution","text":"fviz_pca_ind() used produce graph individuals. \ncreate simple plot, type :Like variables, ’s also possible color individuals cos2\nvalues:Note , individuals similar grouped together \nplot.can also change point size according cos2 \ncorresponding individuals:change point size color cos2, try :create bar plot quality representation (cos2) \nindividuals factor map, can use function fviz_cos2() \npreviously described variables:visualize contribution individuals first two principal\ncomponents, type :","code":"\nfviz_pca_ind(res.pca)\nfviz_pca_ind(res.pca, col.ind = \"cos2\", \n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE # Avoid text overlapping (slow if many points)\n             )\nfviz_pca_ind(res.pca, pointsize = \"cos2\", \n             pointshape = 21, fill = \"#E7B800\",\n             repel = TRUE # Avoid text overlapping (slow if many points)\n             )\nfviz_pca_ind(res.pca, col.ind = \"cos2\", pointsize = \"cos2\",\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE # Avoid text overlapping (slow if many points)\n             )\nfviz_cos2(res.pca, choice = \"ind\")\n# Total contribution on PC1 and PC2\nfviz_contrib(res.pca, choice = \"ind\", axes = 1:2)"},{"path":"detailed-study-of-principal-component-analysis.html","id":"color-by-a-custom-continuous-variable-1","chapter":"4 Detailed study of Principal Component Analysis","heading":"4.12 Color by a custom continuous variable","text":"variables, individuals can colored custom continuous\nvariable specifying argument col.ind.example, type :","code":"\n# Create a random continuous variable of length 23,\n# Same length as the number of active individuals in the PCA\nset.seed(123)\nmy.cont.var <- rnorm(23)\n# Color individuals by the continuous variable\nfviz_pca_ind(res.pca, col.ind = my.cont.var,\n             gradient.cols = c(\"blue\", \"yellow\", \"red\"),\n             legend.title = \"Cont.Var\")"},{"path":"detailed-study-of-principal-component-analysis.html","id":"color-by-groups-1","chapter":"4 Detailed study of Principal Component Analysis","heading":"4.13 Color by groups","text":", describe color individuals group. Additionally, \nshow add concentration ellipses confidence ellipses \ngroups. , ’ll use iris data demo data sets.Iris data sets look like :column “Species” used grouping variable. start \ncomputing principal component analysis follow:R code : argument habillage col.ind can used \nspecify factor variable coloring individuals groups.add concentration ellipse around group, specify argument\naddEllipses = TRUE. argument palette can used change group\ncolors.remove group mean point, specify argument mean.point =\nFALSE. want confidence ellipses instead concentration\nellipses, use ellipse.type = “confidence”.Note , allowed values palette include:“grey” grey color palettes;brewer palettes e.g. “RdBu”, “Blues”, …; view , type \nR: RColorBrewer::display.brewer.().custom color palette e.g. c(“blue”, “red”); scientific journal\npalettes ggsci R package, e.g.: “npg”, “aaas”, * “lancet”,\n“jco”, “ucscgb”, “uchicago”, “simpsons” “rickandmorty”. \nexample, use jco (journal clinical oncology) color\npalette, type :","code":"\nhead(iris, 3)\n#>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#> 1          5.1         3.5          1.4         0.2  setosa\n#> 2          4.9         3.0          1.4         0.2  setosa\n#> 3          4.7         3.2          1.3         0.2  setosa\n# The variable Species (index = 5) is removed\n# before PCA analysis\niris.pca <- PCA(iris[,-5], graph = FALSE)\nfviz_pca_ind(iris.pca,\n             geom.ind = \"point\", # show points only (nbut not \"text\")\n             col.ind = iris$Species, # color by groups\n             palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             addEllipses = TRUE, # Concentration ellipses\n             legend.title = \"Groups\"\n             )\n# Add confidence ellipses\nfviz_pca_ind(iris.pca, geom.ind = \"point\", col.ind = iris$Species, \n             palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             addEllipses = TRUE, ellipse.type = \"confidence\",\n             legend.title = \"Groups\"\n             )\nfviz_pca_ind(iris.pca,\n             label = \"none\", # hide individual labels\n             habillage = iris$Species, # color by groups\n             addEllipses = TRUE, # Concentration ellipses\n             palette = \"jco\"\n             )"},{"path":"detailed-study-of-principal-component-analysis.html","id":"graph-customization","chapter":"4 Detailed study of Principal Component Analysis","heading":"4.14 Graph customization","text":"Note , fviz_pca_ind() fviz_pca_var() related functions\nwrapper around core function fviz() [factoextra]. fviz() \nwrapper around function ggscatter() [ggpubr]. Therefore,\narguments, passed function fviz() \nggscatter(), can specified fviz_pca_ind() fviz_pca_var()., present additional arguments customize PCA\ngraph variables individuals.","code":""},{"path":"detailed-study-of-principal-component-analysis.html","id":"dimensions","chapter":"4 Detailed study of Principal Component Analysis","heading":"4.14.1 Dimensions","text":"default, variables/individuals represented dimensions 1 2.\nwant visualize dimensions 2 3, example, \nspecify argument axes = c(2, 3).Plot elements: point, text, arrow argument geom (geometry) \nderivatives used specify geometry elements graphical\nelements used plotting.geom.var: text specifying geometry used plotting\nvariables. Allowed values combination c(“point”, “arrow”,\n“text”).Use geom.var = “point”, show points;Use geom.var = “text” show text labels;Use geom.var = c(“point”, “text”) show points text\nlabelsUse geom.var = c(“arrow”, “text”) show arrows labels\n(default).example, type :","code":"\n# Variables on dimensions 2 and 3\nfviz_pca_var(res.pca, axes = c(2, 3))\n# Individuals on dimensions 2 and 3\nfviz_pca_ind(res.pca, axes = c(2, 3))\n# Show variable points and text labels\nfviz_pca_var(res.pca, geom.var = c(\"point\", \"text\"))\n# Show individuals text labels only\nfviz_pca_ind(res.pca, geom.ind =  \"text\")"},{"path":"detailed-study-of-principal-component-analysis.html","id":"size-and-shape-of-plot-elements","chapter":"4 Detailed study of Principal Component Analysis","heading":"4.15 Size and shape of plot elements","text":"","code":"\n# Change the size of arrows an labels\nfviz_pca_var(res.pca, arrowsize = 1, labelsize = 5, \n             repel = TRUE)\n# Change points size, shape and fill color\n# Change labelsize\nfviz_pca_ind(res.pca, \n             pointsize = 3, pointshape = 21, fill = \"lightblue\",\n             labelsize = 5, repel = TRUE)"},{"path":"detailed-study-of-principal-component-analysis.html","id":"ellipses","chapter":"4 Detailed study of Principal Component Analysis","heading":"4.16 Ellipses","text":"","code":"\n# Add confidence ellipses\nfviz_pca_ind(iris.pca, geom.ind = \"point\", \n             col.ind = iris$Species, # color by groups\n             palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             addEllipses = TRUE, ellipse.type = \"confidence\",\n             legend.title = \"Groups\"\n             )\n# Convex hull\nfviz_pca_ind(iris.pca, geom.ind = \"point\",\n             col.ind = iris$Species, # color by groups\n             palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             addEllipses = TRUE, ellipse.type = \"convex\",\n             legend.title = \"Groups\"\n             )"},{"path":"detailed-study-of-principal-component-analysis.html","id":"group-mean-points","chapter":"4 Detailed study of Principal Component Analysis","heading":"4.17 Group mean points","text":"coloring individuals groups (section (???)(color-ind--groups)),\nmean points groups (barycenters) also displayed default.remove mean points, use argument mean.point = FALSE.","code":"\nfviz_pca_ind(iris.pca,\n             geom.ind = \"point\", # show points only (but not \"text\")\n             group.ind = iris$Species, # color by groups\n             legend.title = \"Groups\",\n             mean.point = FALSE)"},{"path":"detailed-study-of-principal-component-analysis.html","id":"axis-lines","chapter":"4 Detailed study of Principal Component Analysis","heading":"4.18 Axis lines","text":"","code":"\nfviz_pca_var(res.pca, axes.linetype = \"blank\")"},{"path":"detailed-study-of-principal-component-analysis.html","id":"graphical-parameters","chapter":"4 Detailed study of Principal Component Analysis","heading":"4.19 Graphical parameters","text":"change easily graphical ggplots, can use function\nggpar() [ggpubr package]","code":"\nind.p <- fviz_pca_ind(iris.pca, geom = \"point\", col.ind = iris$Species)\nggpubr::ggpar(ind.p,\n              title = \"Principal Component Analysis\",\n              subtitle = \"Iris data set\",\n              caption = \"Source: factoextra\",\n              xlab = \"PC1\", ylab = \"PC2\",\n              legend.title = \"Species\", legend.position = \"top\",\n              ggtheme = theme_gray(), palette = \"jco\"\n              )"},{"path":"detailed-study-of-principal-component-analysis.html","id":"biplot","chapter":"4 Detailed study of Principal Component Analysis","heading":"4.20 Biplot","text":"make simple biplot individuals variables, type :Note , biplot might useful low\nnumber variables individuals data set; otherwise \nfinal plot unreadable.Note also , coordinate individuals variables \nconstructed space. Therefore, biplot, \nmainly focus direction variables absolute\npositions plot.Roughly speaking biplot can interpreted follow: * \nindividual side given variable high\nvalue variable; * individual opposite side\ngiven variable low value variable.following example, want color individuals \nvariables groups. trick use pointshape = 21 \nindividual points. particular point shape can filled color\nusing argument fill.ind. border line color individual points\nset “black” using col.ind. color variable groups, \nargument col.var used.customize individuals variable colors, use helper\nfunctions fill_palette() color_palette() [ggpubr package].Another complex example color individuals groups (discrete\ncolor) variables contributions principal components\n(gradient colors). Additionally, ’ll change transparency \nvariables contributions using argument alpha.var.","code":"\nfviz_pca_biplot(res.pca, repel = TRUE,\n                col.var = \"#2E9FDF\", # Variables color\n                col.ind = \"#696969\"  # Individuals color\n                )\nfviz_pca_biplot(iris.pca, \n                col.ind = iris$Species, palette = \"jco\", \n                addEllipses = TRUE, label = \"var\",\n                col.var = \"black\", repel = TRUE,\n                legend.title = \"Species\") \nfviz_pca_biplot(iris.pca, \n                # Fill individuals by groups\n                geom.ind = \"point\",\n                pointshape = 21,\n                pointsize = 2.5,\n                fill.ind = iris$Species,\n                col.ind = \"black\",\n                # Color variable by groups\n                col.var = factor(c(\"sepal\", \"sepal\", \"petal\", \"petal\")),\n                \n                legend.title = list(fill = \"Species\", color = \"Clusters\"),\n                repel = TRUE        # Avoid label overplotting\n             )+\n  ggpubr::fill_palette(\"jco\")+      # Indiviual fill color\n  ggpubr::color_palette(\"npg\")      # Variable colors\nfviz_pca_biplot(iris.pca, \n                # Individuals\n                geom.ind = \"point\",\n                fill.ind = iris$Species, col.ind = \"black\",\n                pointshape = 21, pointsize = 2,\n                palette = \"jco\",\n                addEllipses = TRUE,\n                # Variables\n                alpha.var =\"contrib\", col.var = \"contrib\",\n                gradient.cols = \"RdYlBu\",\n                \n                legend.title = list(fill = \"Species\", color = \"Contrib\",\n                                    alpha = \"Contrib\")\n                )"},{"path":"detailed-study-of-principal-component-analysis.html","id":"supplementary-elements","chapter":"4 Detailed study of Principal Component Analysis","heading":"4.21 Supplementary elements","text":"Definition types described (section (???)(pca-data-format)),\ndecathlon2 datasets contain supplementary continuous variables\n(quanti.sup, columns 11:12), supplementary qualitative variables\n(quali.sup, column 13) supplementary individuals (ind.sup, rows\n24:27).Supplementary variables individuals used \ndetermination principal components. coordinates \npredicted using information provided performed principal\ncomponent analysis active variables/individuals.Specification PCA specify supplementary individuals variables,\nfunction PCA() can used follow:","code":"\nres.pca <- PCA(decathlon2, ind.sup = 24:27, \n               quanti.sup = 11:12, quali.sup = 13, graph=FALSE)"},{"path":"detailed-study-of-principal-component-analysis.html","id":"quantitative-variables-1","chapter":"4 Detailed study of Principal Component Analysis","heading":"4.22 Quantitative variables","text":"Predicted results (coordinates, correlation cos2) \nsupplementary quantitative variables:Visualize variables (active supplementary ones):Note , default, supplementary quantitative variables shown\nblue color dashed lines.arguments customize plot:Using fviz_pca_var(), quantitative supplementary variables \ndisplayed automatically correlation circle plot. Note , \ncan add quanti.sup variables manually, using fviz_add()\nfunction, customization. example shown .","code":"\nres.pca$quanti.sup\n#> $coord\n#>         Dim.1   Dim.2  Dim.3   Dim.4   Dim.5\n#> Rank   -0.701 -0.2452 -0.183  0.0558 -0.0738\n#> Points  0.964  0.0777  0.158 -0.1662 -0.0311\n#> \n#> $cor\n#>         Dim.1   Dim.2  Dim.3   Dim.4   Dim.5\n#> Rank   -0.701 -0.2452 -0.183  0.0558 -0.0738\n#> Points  0.964  0.0777  0.158 -0.1662 -0.0311\n#> \n#> $cos2\n#>        Dim.1   Dim.2  Dim.3   Dim.4   Dim.5\n#> Rank   0.492 0.06012 0.0336 0.00311 0.00545\n#> Points 0.929 0.00603 0.0250 0.02763 0.00097\nfviz_pca_var(res.pca)\n# Change color of variables\nfviz_pca_var(res.pca,\n             col.var = \"black\",     # Active variables\n             col.quanti.sup = \"red\" # Suppl. quantitative variables\n             )\n# Hide active variables on the plot, \n# show only supplementary variables\nfviz_pca_var(res.pca, invisible = \"var\")\n# Hide supplementary variables\nfviz_pca_var(res.pca, invisible = \"quanti.sup\")\n# Plot of active variables\np <- fviz_pca_var(res.pca, invisible = \"quanti.sup\")\n# Add supplementary active variables\nfviz_add(p, res.pca$quanti.sup$coord, \n         geom = c(\"arrow\", \"text\"), \n         color = \"red\")"},{"path":"detailed-study-of-principal-component-analysis.html","id":"individuals","chapter":"4 Detailed study of Principal Component Analysis","heading":"4.23 Individuals","text":"Predicted results supplementary individuals (ind.sup):Visualize individuals (active supplementary ones). graph,\ncan add also supplementary qualitative variables (quali.sup),\ncoordinates accessible using res.pca\\(quali.supp\\)coord.Supplementary individuals shown blue. levels \nsupplementary qualitative variable shown red color.","code":"\nres.pca$ind.sup\n#> $coord\n#>          Dim.1   Dim.2  Dim.3  Dim.4   Dim.5\n#> KARPOV   0.795  0.7795 -1.633  1.724 -0.7507\n#> WARNERS -0.386 -0.1216 -1.739 -0.706 -0.0323\n#> Nool    -0.559  1.9775 -0.483 -2.278 -0.2546\n#> Drews   -1.109  0.0174 -3.049 -1.534 -0.3264\n#> \n#> $cos2\n#>          Dim.1    Dim.2  Dim.3  Dim.4    Dim.5\n#> KARPOV  0.0510 4.91e-02 0.2155 0.2403 0.045549\n#> WARNERS 0.0242 2.40e-03 0.4904 0.0809 0.000169\n#> Nool    0.0290 3.62e-01 0.0216 0.4811 0.006008\n#> Drews   0.0921 2.27e-05 0.6956 0.1762 0.007974\n#> \n#> $dist\n#>  KARPOV WARNERS    Nool   Drews \n#>    3.52    2.48    3.28    3.66\np <- fviz_pca_ind(res.pca, col.ind.sup = \"blue\", repel = TRUE)\np <- fviz_add(p, res.pca$quali.sup$coord, color = \"red\")\np"},{"path":"detailed-study-of-principal-component-analysis.html","id":"qualitative-variables","chapter":"4 Detailed study of Principal Component Analysis","heading":"4.24 Qualitative variables","text":"previous section, showed can add supplementary\nqualitative variables individuals plot using fviz_add().Note , supplementary qualitative variables can also used \ncoloring individuals groups. can help interpret data. \ndata sets decathlon2 contain supplementary qualitative variable \ncolumns 13 corresponding type competitions.results concerning supplementary qualitative variable :color individuals supplementary qualitative variable, \nargument habillage used specify index supplementary\nqualitative variable. Historically, argument name comes \nFactoMineR package. ’s french word meaning “dressing” \nenglish. keep consistency FactoMineR factoextra,\ndecided keep argument nameRecall , remove mean points groups, specify argument\nmean.point = FALSE.","code":"\nres.pca$quali\n#> $coord\n#>          Dim.1  Dim.2   Dim.3  Dim.4  Dim.5\n#> Decastar -1.34  0.122 -0.0379  0.181  0.134\n#> OlympicG  1.23 -0.112  0.0347 -0.166 -0.123\n#> \n#> $cos2\n#>          Dim.1   Dim.2   Dim.3  Dim.4   Dim.5\n#> Decastar 0.905 0.00744 0.00072 0.0164 0.00905\n#> OlympicG 0.905 0.00744 0.00072 0.0164 0.00905\n#> \n#> $v.test\n#>          Dim.1  Dim.2  Dim.3  Dim.4 Dim.5\n#> Decastar -2.97  0.403 -0.153  0.897  0.72\n#> OlympicG  2.97 -0.403  0.153 -0.897 -0.72\n#> \n#> $dist\n#> Decastar OlympicG \n#>     1.41     1.29 \n#> \n#> $eta2\n#>             Dim.1  Dim.2   Dim.3  Dim.4  Dim.5\n#> Competition 0.401 0.0074 0.00106 0.0366 0.0236\nfviz_pca_ind(res.pca, habillage = 13,\n             addEllipses =TRUE, ellipse.type = \"confidence\",\n             palette = \"jco\", repel = TRUE) "},{"path":"detailed-study-of-principal-component-analysis.html","id":"filtering-results","chapter":"4 Detailed study of Principal Component Analysis","heading":"4.25 Filtering results","text":"many individuals/variable, ’s possible visualize \nusing arguments select.ind select.var.selection done according contribution values,\nsupplementary individuals/variables shown don’t\ncontribute construction axes.","code":"\n# Visualize variable with cos2 >= 0.6\nfviz_pca_var(res.pca, select.var = list(cos2 = 0.6))\n# Top 5 active variables with the highest cos2\nfviz_pca_var(res.pca, select.var= list(cos2 = 5))\n# Select by names\nname <- list(name = c(\"Long.jump\", \"High.jump\", \"X100m\"))\nfviz_pca_var(res.pca, select.var = name)\n# top 5 contributing individuals and variable\nfviz_pca_biplot(res.pca, select.ind = list(contrib = 5), \n               select.var = list(contrib = 5),\n               ggtheme = theme_minimal())"},{"path":"detailed-study-of-principal-component-analysis.html","id":"exporting-results","chapter":"4 Detailed study of Principal Component Analysis","heading":"4.26 Exporting results","text":"Export plots PDF/PNG files factoextra package produces \nggplot2-based graphs. save ggplots, standard R code \nfollow:following examples, ’ll show save different\ngraphs pdf png files.first step create plots want R object:Note , using R code create PDF file \ncurrent working directory. see path current working\ndirectory, type getwd() R console.print plot specific png file, R code looks like :Another alternative, export ggplots, use function\nggexport() [ggpubr package]. like ggexport(), ’s \nsimple. one line R code, allows us export individual plots \nfile (pdf, eps png) (one plot per page). can also arrange \nplots (2 plot per page, example) exporting . examples\ndemonstrates export ggplots using ggexport().Export individual plots pdf file (one plot per page):Arrange export. Specify nrow ncol display multiple plots \npage:Export plots png files. specify list plots, multiple\npng files automatically created hold plot.","code":"# Print the plot to a pdf file\npdf(\"myplot.pdf\")\nprint(myplot)\ndev.off()\n# Scree plot\nscree.plot <- fviz_eig(res.pca)\n# Plot of individuals\nind.plot <- fviz_pca_ind(res.pca)\n# Plot of variables\nvar.plot <- fviz_pca_var(res.pca)\npdf(file.path(data_out_dir, \"PCA.pdf\"))   # Create a new pdf device\nprint(scree.plot)\nprint(ind.plot)\nprint(var.plot)\ndev.off() # Close the pdf device\n#> png \n#>   2\n# Print scree plot to a png file\npng(file.path(data_out_dir, \"pca-scree-plot.png\"))\nprint(scree.plot)\ndev.off()\n#> png \n#>   2\n# Print individuals plot to a png file\npng(file.path(data_out_dir, \"pca-variables.png\"))\nprint(var.plot)\ndev.off()\n#> png \n#>   2\n# Print variables plot to a png file\npng(file.path(data_out_dir, \"pca-individuals.png\"))\nprint(ind.plot)\ndev.off()\n#> png \n#>   2\nlibrary(ggpubr)\n#> Loading required package: magrittr\nggexport(plotlist = list(scree.plot, ind.plot, var.plot), \n         filename = file.path(data_out_dir, \"PCA.pdf\"))\n#> file saved to ../output/data/PCA.pdf\nggexport(plotlist = list(scree.plot, ind.plot, var.plot), \n         nrow = 2, ncol = 2,\n         filename = file.path(data_out_dir, \"PCA.pdf\"))\n#> file saved to ../output/data/PCA.pdf\nggexport(plotlist = list(scree.plot, ind.plot, var.plot),\n         filename = file.path(data_out_dir, \"PCA.png\"))\n#> [1] \"../output/data/PCA%03d.png\"\n#> file saved to ../output/data/PCA%03d.png"},{"path":"detailed-study-of-principal-component-analysis.html","id":"export-results-to-txtcsv-files","chapter":"4 Detailed study of Principal Component Analysis","heading":"4.27 Export results to txt/csv files","text":"outputs PCA (individuals/variables coordinates,\ncontributions, etc) can exported , TXT/CSV file, using\nfunction write.infile() [FactoMineR] package:","code":"\n# Export into a TXT file\nwrite.infile(res.pca, file.path(data_out_dir, \"pca.txt\"), sep = \"\\t\")\n# Export into a CSV file\nwrite.infile(res.pca, file.path(data_out_dir, \"pca.csv\"), sep = \";\")"},{"path":"detailed-study-of-principal-component-analysis.html","id":"summary","chapter":"4 Detailed study of Principal Component Analysis","heading":"4.28 Summary","text":"conclusion, described perform interpret principal\ncomponent analysis (PCA). computed PCA using PCA() function\n[FactoMineR]. Next, used factoextra R package produce\nggplot2-based visualization PCA results.functions [packages] compute PCA R:Using prcomp() [stats]Using dudi.pca() [ade4]Using epPCA() [ExPosition]matter functions decide use, list , \nfactoextra package can handle output creating beautiful plots\nsimilar described previous sections FactoMineR:","code":"\nres.pca <- prcomp(iris[, -5], scale. = TRUE)\nres.pca <- princomp(iris[, -5], cor = TRUE)\nlibrary(ade4)\n#> \n#> Attaching package: 'ade4'\n#> The following object is masked from 'package:FactoMineR':\n#> \n#>     reconst\nres.pca <- dudi.pca(iris[, -5], scannf = FALSE, nf = 5)\nlibrary(ExPosition)\n#> Loading required package: prettyGraphs\nres.pca <- epPCA(iris[, -5], graph = FALSE)\nfviz_eig(res.pca)     # Scree plot\nfviz_pca_ind(res.pca) # Graph of individuals\nfviz_pca_var(res.pca) # Graph of variables"},{"path":"detection-of-diabetes-using-logistic-regression.html","id":"detection-of-diabetes-using-logistic-regression","chapter":"5 Detection of diabetes using Logistic Regression","heading":"5 Detection of diabetes using Logistic Regression","text":"Datasets: diabetes.csvAlgorithms:\nLogistic Regression\nFeature Engineering\nLogistic RegressionFeature Engineering","code":""},{"path":"detection-of-diabetes-using-logistic-regression.html","id":"introduction","chapter":"5 Detection of diabetes using Logistic Regression","heading":"5.1 Introduction","text":"Source: https://github.com/AntoineGuillot2/Logistic-Regression-R/blob/master/script.R\nSource: http://enhancedatascience.com/2017/04/26/r-basics-logistic-regression--r/\nData: https://www.kaggle.com/uciml/pima-indians-diabetes-databaseThe goal logistic regression predict whether outcome positive (aka 1) negative (.e: 0). real life example :Emmanuel Macron win French Presidential election lose?Mr.X illness ?visitor click link ?, logistic regression can used lot binary classification cases often run advanced methods. tutorial, use diabetes detection dataset Kaggle.dataset contains data Pima Indians Women number pregnancies, blood pressure, skin thickness, … goal tutorial able detect diabetes using measures.","code":""},{"path":"detection-of-diabetes-using-logistic-regression.html","id":"exploring-the-data","chapter":"5 Detection of diabetes using Logistic Regression","heading":"5.2 Exploring the data","text":"usual, first, let’s take look data. can download data please put file diabetes.csv working directory. summary function, can easily summarise different variables:mean outcome 0.35 shows imbalance classes. However, imbalance strong problem.understand relationship variables, Scatter Plot Matrix used. plot , GGally package used.correlations explanatory variables seem strong. Hence model likely suffer multicollinearity. explanatory variable correlated Outcome. first sight, glucose rate important factor detect outcome.","code":"\nlibrary(ggplot2)\nlibrary(data.table)\n\nDiabetesData <- data.table(read.csv(file.path(data_raw_dir, 'diabetes.csv')))\n\n# Quick data summary\nsummary(DiabetesData)\n#>   Pregnancies       Glucose    BloodPressure   SkinThickness     Insulin   \n#>  Min.   : 0.00   Min.   :  0   Min.   :  0.0   Min.   : 0.0   Min.   :  0  \n#>  1st Qu.: 1.00   1st Qu.: 99   1st Qu.: 62.0   1st Qu.: 0.0   1st Qu.:  0  \n#>  Median : 3.00   Median :117   Median : 72.0   Median :23.0   Median : 30  \n#>  Mean   : 3.85   Mean   :121   Mean   : 69.1   Mean   :20.5   Mean   : 80  \n#>  3rd Qu.: 6.00   3rd Qu.:140   3rd Qu.: 80.0   3rd Qu.:32.0   3rd Qu.:127  \n#>  Max.   :17.00   Max.   :199   Max.   :122.0   Max.   :99.0   Max.   :846  \n#>       BMI       DiabetesPedigreeFunction      Age          Outcome     \n#>  Min.   : 0.0   Min.   :0.078            Min.   :21.0   Min.   :0.000  \n#>  1st Qu.:27.3   1st Qu.:0.244            1st Qu.:24.0   1st Qu.:0.000  \n#>  Median :32.0   Median :0.372            Median :29.0   Median :0.000  \n#>  Mean   :32.0   Mean   :0.472            Mean   :33.2   Mean   :0.349  \n#>  3rd Qu.:36.6   3rd Qu.:0.626            3rd Qu.:41.0   3rd Qu.:1.000  \n#>  Max.   :67.1   Max.   :2.420            Max.   :81.0   Max.   :1.000\n# Scatter plot matrix\nlibrary(GGally)\n#> Registered S3 method overwritten by 'GGally':\n#>   method from   \n#>   +.gg   ggplot2\nggpairs(DiabetesData, lower = list(continuous='smooth'))"},{"path":"detection-of-diabetes-using-logistic-regression.html","id":"logistic-regression-with-r","chapter":"5 Detection of diabetes using Logistic Regression","heading":"5.3 Logistic regression with R","text":"variable exploration, first model can fitted using glm function. stargazer, easy get nice output ASCII even Latex.overall model significant. expected glucose rate lowest p-value variables. However, Age, Insulin Skin Thickness good predictors Diabetes.","code":"\n# first model: all features\nglm1 = glm(Outcome~., \n           DiabetesData, \n           family = binomial(link=\"logit\"))\n\nsummary(glm1)\n#> \n#> Call:\n#> glm(formula = Outcome ~ ., family = binomial(link = \"logit\"), \n#>     data = DiabetesData)\n#> \n#> Deviance Residuals: \n#>    Min      1Q  Median      3Q     Max  \n#> -2.557  -0.727  -0.416   0.727   2.930  \n#> \n#> Coefficients:\n#>                           Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)              -8.404696   0.716636  -11.73  < 2e-16 ***\n#> Pregnancies               0.123182   0.032078    3.84  0.00012 ***\n#> Glucose                   0.035164   0.003709    9.48  < 2e-16 ***\n#> BloodPressure            -0.013296   0.005234   -2.54  0.01107 *  \n#> SkinThickness             0.000619   0.006899    0.09  0.92852    \n#> Insulin                  -0.001192   0.000901   -1.32  0.18607    \n#> BMI                       0.089701   0.015088    5.95  2.8e-09 ***\n#> DiabetesPedigreeFunction  0.945180   0.299147    3.16  0.00158 ** \n#> Age                       0.014869   0.009335    1.59  0.11119    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 993.48  on 767  degrees of freedom\n#> Residual deviance: 723.45  on 759  degrees of freedom\n#> AIC: 741.4\n#> \n#> Number of Fisher Scoring iterations: 5\nrequire(stargazer)\n#> Loading required package: stargazer\n#> \n#> Please cite as:\n#>  Hlavac, Marek (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n#>  R package version 5.2.2. https://CRAN.R-project.org/package=stargazer\nstargazer(glm1,type='text')\n#> \n#> ====================================================\n#>                              Dependent variable:    \n#>                          ---------------------------\n#>                                    Outcome          \n#> ----------------------------------------------------\n#> Pregnancies                       0.123***          \n#>                                    (0.032)          \n#>                                                     \n#> Glucose                           0.035***          \n#>                                    (0.004)          \n#>                                                     \n#> BloodPressure                     -0.013**          \n#>                                    (0.005)          \n#>                                                     \n#> SkinThickness                       0.001           \n#>                                    (0.007)          \n#>                                                     \n#> Insulin                            -0.001           \n#>                                    (0.001)          \n#>                                                     \n#> BMI                               0.090***          \n#>                                    (0.015)          \n#>                                                     \n#> DiabetesPedigreeFunction          0.945***          \n#>                                    (0.299)          \n#>                                                     \n#> Age                                 0.015           \n#>                                    (0.009)          \n#>                                                     \n#> Constant                          -8.400***         \n#>                                    (0.717)          \n#>                                                     \n#> ----------------------------------------------------\n#> Observations                         768            \n#> Log Likelihood                    -362.000          \n#> Akaike Inf. Crit.                  741.000          \n#> ====================================================\n#> Note:                    *p<0.1; **p<0.05; ***p<0.01"},{"path":"detection-of-diabetes-using-logistic-regression.html","id":"a-second-model","chapter":"5 Detection of diabetes using Logistic Regression","heading":"5.4 A second model","text":"Since variables significant, removing good way improve model robustness. second model, SkinThickness, Insulin, Age removed.","code":"\n# second model: selected features\nglm2 = glm(Outcome~., \n         data = DiabetesData[,c(1:3,6:7,9), with=F], \n         family = binomial(link=\"logit\"))\n\nsummary(glm2)\n#> \n#> Call:\n#> glm(formula = Outcome ~ ., family = binomial(link = \"logit\"), \n#>     data = DiabetesData[, c(1:3, 6:7, 9), with = F])\n#> \n#> Deviance Residuals: \n#>    Min      1Q  Median      3Q     Max  \n#> -2.793  -0.736  -0.419   0.725   2.955  \n#> \n#> Coefficients:\n#>                          Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)              -7.95495    0.67582  -11.77  < 2e-16 ***\n#> Pregnancies               0.15349    0.02784    5.51  3.5e-08 ***\n#> Glucose                   0.03466    0.00339   10.21  < 2e-16 ***\n#> BloodPressure            -0.01201    0.00503   -2.39    0.017 *  \n#> BMI                       0.08483    0.01412    6.01  1.9e-09 ***\n#> DiabetesPedigreeFunction  0.91063    0.29403    3.10    0.002 ** \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 993.48  on 767  degrees of freedom\n#> Residual deviance: 728.56  on 762  degrees of freedom\n#> AIC: 740.6\n#> \n#> Number of Fisher Scoring iterations: 5"},{"path":"detection-of-diabetes-using-logistic-regression.html","id":"classification-rate-and-confusion-matrix","chapter":"5 Detection of diabetes using Logistic Regression","heading":"5.5 Classification rate and confusion matrix","text":"Now model, let’s access performance.Around 77.4% observations correctly classified. Due class imbalance, need go confusion matrix.model good detect people diabetes. However, performance ill people great (154 268 correctly classified).can also get percentage Real/False Positive/Negative:matrix, 57.5% people diabetes correctly classified. way improve false negative rate lower detection threshold. However, consequence, false positive rate increase.","code":"\n# Correctly classified observations\nmean((glm2$fitted.values>0.5)==DiabetesData$Outcome)\n#> [1] 0.775\n###Confusion matrix count\nRP=sum((glm2$fitted.values>=0.5)==DiabetesData$Outcome & DiabetesData$Outcome==1)\nFP=sum((glm2$fitted.values>=0.5)!=DiabetesData$Outcome & DiabetesData$Outcome==0)\nRN=sum((glm2$fitted.values>=0.5)==DiabetesData$Outcome & DiabetesData$Outcome==0)\nFN=sum((glm2$fitted.values>=0.5)!=DiabetesData$Outcome & DiabetesData$Outcome==1)\nconfMat<-matrix(c(RP,FP,FN,RN),ncol = 2)\ncolnames(confMat)<-c(\"Pred Diabetes\",'Pred no diabetes')\nrownames(confMat)<-c(\"Real Diabetes\",'Real no diabetes')\nconfMat\n#>                  Pred Diabetes Pred no diabetes\n#> Real Diabetes              154              114\n#> Real no diabetes            59              441\n# Confusion matrix proportion\nRPR=RP/sum(DiabetesData$Outcome==1)*100\nFNR=FN/sum(DiabetesData$Outcome==1)*100\nFPR=FP/sum(DiabetesData$Outcome==0)*100\nRNR=RN/sum(DiabetesData$Outcome==0)*100\nconfMat<-matrix(c(RPR,FPR,FNR,RNR),ncol = 2)\ncolnames(confMat)<-c(\"Pred Diabetes\",'Pred no diabetes')\nrownames(confMat)<-c(\"Real Diabetes\",'Real no diabetes')\nconfMat\n#>                  Pred Diabetes Pred no diabetes\n#> Real Diabetes             57.5             42.5\n#> Real no diabetes          11.8             88.2"},{"path":"detection-of-diabetes-using-logistic-regression.html","id":"plots-and-decision-boundaries","chapter":"5 Detection of diabetes using Logistic Regression","heading":"5.6 Plots and decision boundaries","text":"two strongest predictors outcome Glucose rate BMI. High glucose rate high BMI strong indicators Diabetes.can also plot BMI glucose outcomes, variables taken mean level.","code":"\n# Plot and decision boundaries\nDiabetesData$Predicted <- glm2$fitted.values\n\nggplot(DiabetesData, aes(x = BMI, y = Glucose, color = Predicted > 0.5)) + \n    geom_point(size=2, alpha=0.5)\nggplot(DiabetesData, aes(x=BMI, y = Glucose, color=Outcome == (Predicted > 0.5))) + \n    geom_point(size=2, alpha=0.5)\nrange(DiabetesData$BMI)\n#> [1]  0.0 67.1\n# BMI vs predicted\nBMI_plot = data.frame(BMI = ((min(DiabetesData$BMI-2)*100):\n                               (max(DiabetesData$BMI+2)*100))/100,\n                    Glucose = mean(DiabetesData$Glucose),\n                    Pregnancies = mean(DiabetesData$Pregnancies),\n                    BloodPressure = mean(DiabetesData$BloodPressure),\n                    DiabetesPedigreeFunction = mean(DiabetesData$DiabetesPedigreeFunction))\n\nBMI_plot$Predicted = predict(glm2, newdata = BMI_plot, type = 'response')\nggplot(BMI_plot, aes(x = BMI, y = Predicted)) + \n    geom_line()\nrange(BMI_plot$BMI)\n#> [1] -2.0 69.1\nrange(DiabetesData$Glucose)\n#> [1]   0 199\n# Glucose vs predicted\nGlucose_plot=data.frame(Glucose = \n                        ((min(DiabetesData$Glucose-2)*100):\n                             (max(DiabetesData$Glucose+2)*100))/100,\n                    BMI=mean(DiabetesData$BMI),\n                    Pregnancies=mean(DiabetesData$Pregnancies),\n                    BloodPressure=mean(DiabetesData$BloodPressure),\n                    DiabetesPedigreeFunction=mean(DiabetesData$DiabetesPedigreeFunction))\n\nGlucose_plot$Predicted = predict(glm2, newdata = Glucose_plot, type = 'response')\nggplot(Glucose_plot, aes(x = Glucose, y = Predicted)) + \n    geom_line()\nrange(Glucose_plot$Glucose)\n#> [1]  -2 201"},{"path":"sensitivity-analysis-for-a-neural-network.html","id":"sensitivity-analysis-for-a-neural-network","chapter":"6 Sensitivity analysis for a neural network","heading":"6 Sensitivity analysis for a neural network","text":"Datasets: Simulated data normal distributionAlgorithms:\nNeural Networks\nNeural Networks","code":""},{"path":"sensitivity-analysis-for-a-neural-network.html","id":"introduction-1","chapter":"6 Sensitivity analysis for a neural network","heading":"6.1 Introduction","text":"https://beckmw.wordpress.com/tag/nnet/’ve made quite blog posts neural networks diagnostic tools can used ‘demystify’ information contained models. Frankly, ’m kind sick writing neural networks wanted share one last tool ’ve implemented R. ’m strong believer supervised neural networks can used much prediction, common assumption researchers. hope collection posts, including one, shown versatility models develop inference causation. date, ’ve authored posts visualizing neural networks, animating neural networks, determining importance model inputs. post describe function sensitivity analysis neural network. Specifically, describe approach evaluate form relationship response variable explanatory variables used model.general goal sensitivity analysis similar evaluating relative importance explanatory variables, important distinctions. analyses, interested relationships explanatory response variables described model hope neural network explained real-world phenomenon. Using Garson’s algorithm,1 can get idea magnitude sign relationship variables relative . Conversely, sensitivity analysis allows us obtain information form relationship variables rather categorical description, variable x positively strongly related y. example, response variable change relation increasing decreasing values given explanatory variable? linear response, non-linear, uni-modal, response, etc.? Furthermore, form response change given values explanatory variables model? might expect relationship response explanatory variable might differ given context explanatory variables (.e., interaction may present). sensitivity analysis can provide information.posts, ’ve created sensitivity analysis function using ideas people much clever . ’ve simply converted ideas useful form R. Ultimate credit sensitivity analysis goes Sovan Lek (colleagues), developed approach mid-1990s. ‘Lek-profile method’ described briefly Lek et al. 19962 detail Gevrey et al. 2003.3 ’ll provide brief summary since method pretty simple. fact, profile method can extended statistical model specific neural networks, although one methods used evaluate latter. statistical model multiple response variables related multiple explanatory variables, choose one response one explanatory variable. obtain predictions response variable across range values given explanatory variable. explanatory variables held constant given set respective values (e.g., minimum, 20th percentile, maximum). final product set response curves one response variable across range values one explanatory variable, holding explanatory variables constant. implemented R creating matrix values explanatory variables number rows number observations number columns number explanatory variables. explanatory variables held mean (constant value) variable interest sequenced minimum maximum value across range observations. matrix (actually data frame) used predict values response variable fitted model object. repeated different variables.’ll illustrate function using simulated data, ’ve done previous posts. exception ’ll using two response variables instead one. two response variables linear combinations eight explanatory variables, random error components taken normal distribution. relationships variables determined arbitrary set parameters (parms1 parms2). explanatory variables partially correlated taken multivariate normal distribution.","code":"\nrequire(clusterGeneration)\n#> Loading required package: clusterGeneration\n#> Loading required package: MASS\nrequire(nnet)\n#> Loading required package: nnet\n  \n#define number of variables and observations\nset.seed(2)\nnum.vars<-8\nnum.obs<-10000\n  \n#define correlation matrix for explanatory variables \n#define actual parameter values\ncov.mat<-genPositiveDefMat(num.vars,covMethod=c(\"unifcorrmat\"))$Sigma\nrand.vars<-mvrnorm(num.obs,rep(0,num.vars),Sigma=cov.mat)\nparms1<-runif(num.vars,-10,10)\ny1<-rand.vars %*% matrix(parms1) + rnorm(num.obs,sd=20)\nparms2<-runif(num.vars,-10,10)\ny2<-rand.vars %*% matrix(parms2) + rnorm(num.obs,sd=20)\n \n#prep data and create neural network\nrand.vars<-data.frame(rand.vars)\nresp<-apply(cbind(y1,y2),2, function(y) (y-min(y))/(max(y)-min(y)))\nresp<-data.frame(resp)\nnames(resp)<-c('Y1','Y2')\nmod1 <- nnet(rand.vars,resp,size=8,linout=T)\n#> # weights:  90\n#> initial  value 30121.205794 \n#> iter  10 value 130.537462\n#> iter  20 value 57.187090\n#> iter  30 value 47.285919\n#> iter  40 value 42.778564\n#> iter  50 value 39.837784\n#> iter  60 value 36.694632\n#> iter  70 value 35.140948\n#> iter  80 value 34.268819\n#> iter  90 value 33.772282\n#> iter 100 value 33.472654\n#> final  value 33.472654 \n#> stopped after 100 iterations\n#import the function from Github\nlibrary(devtools)\n#> Loading required package: usethis\n\n# source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')\nsource(\"nnet_plot_update.r\")\n \n#plot each model\nplot.nnet(mod1)\n#> Loading required package: scales\n#> Loading required package: reshape"},{"path":"sensitivity-analysis-for-a-neural-network.html","id":"the-lek-profile-function","chapter":"6 Sensitivity analysis for a neural network","heading":"6.2 The Lek profile function","text":"’ve created neural network hopefully describes relationship two response variables eight explanatory variables. sensitivity analysis lets us visualize relationships. Lek profile function can used neural network model workspace. function imported used follows:Fig: Sensitivity analysis two response variables neural network model individual explanatory variables. Splits represent quantile values remaining explanatory variables held constant. function can obtained hereBy default, function runs sensitivity analysis variables. creates busy plot may want look specific variables interest. Maybe want evaluate different quantile values well. options can changed using arguments.Fig: Sensitivity analysis two response variables relation explanatory variables X2 X5 different quantile values remaining variables.function also returns ggplot2 object can modified. may prefer different theme, color, line type, example.","code":"\n# source('https://gist.githubusercontent.com/fawda123/6860630/raw/b8bf4a6c88d6b392b1bfa6ef24759ae98f31877c/lek_fun.r')\nsource(\"lek_fun.r\")\n\nlek.fun(mod1)\n#> Loading required package: ggplot2\nlek.fun(mod1,var.sens=c('X2','X5'),split.vals=seq(0,1,by=0.05))\np1<-lek.fun(mod1)\nclass(p1)\n#> [1] \"gg\"     \"ggplot\"\n# [1] \"gg\"     \"ggplot\"\n \np1 + \n   theme_bw() +\n   scale_colour_brewer(palette=\"PuBu\") +\n   scale_linetype_manual(values=rep('dashed',6)) +\n   scale_size_manual(values=rep(1,6))\n#> Scale for 'linetype' is already present. Adding another scale for 'linetype',\n#> which will replace the existing scale.\n#> Scale for 'size' is already present. Adding another scale for 'size', which\n#> will replace the existing scale."},{"path":"sensitivity-analysis-for-a-neural-network.html","id":"getting-a-dataframe-from-lek","chapter":"6 Sensitivity analysis for a neural network","heading":"6.3 Getting a dataframe from lek","text":"Finally, actual values sensitivity analysis can returned ’d prefer instead. output data frame long form created using melt.list reshape package compatibility ggplot2. six columns indicate values explanatory variables x-axes, names response variables, predicted values response variables, quantiles explanatory variables held constant, names explanatory variables x-axes.","code":"\nhead(lek.fun(mod1,val.out = TRUE))\n#>   Explanatory resp.name Response Splits exp.name\n#> 1       -9.58        Y1    0.466      0       X1\n#> 2       -9.39        Y1    0.466      0       X1\n#> 3       -9.19        Y1    0.467      0       X1\n#> 4       -9.00        Y1    0.467      0       X1\n#> 5       -8.81        Y1    0.468      0       X1\n#> 6       -8.62        Y1    0.468      0       X1"},{"path":"sensitivity-analysis-for-a-neural-network.html","id":"the-lek-function-works-with-lm","chapter":"6 Sensitivity analysis for a neural network","heading":"6.4 The lek function works with lm","text":"mentioned earlier function unique neural networks can work models created R. haven’t done extensive test function, ’m fairly certain work model object predict method (e.g., predict.lm). ’s example using function evaluate multiple linear regression one response variables.function little relevance conventional models like linear regression since wealth diagnostic tools already available (e.g., effects plots, add/drop procedures, outlier tests, etc.). application function neural networks provides insight relationships described models, insights knowledge, obtained using current tools R. post concludes contribution diagnostic tools neural networks R hope useful . spent last year working neural networks opinion utility mixed. see advantages use highly flexible computer-based algorithms, although cases similar conclusions can made using conventional analyses. suggest neural networks used extremely high sample size methods proven inconclusive. Feel free voice opinions suggestions comments.","code":"\nmod2 <-lm(Y1 ~ ., data = cbind(resp[,'Y1', drop = F], rand.vars))\nlek.fun(mod2)"},{"path":"sensitivity-analysis-for-a-neural-network.html","id":"lek-function-works-with-rsnns","chapter":"6 Sensitivity analysis for a neural network","heading":"6.5 lek function works with RSNNS","text":"","code":"\nrequire(clusterGeneration)\nrequire(RSNNS)\n#> Loading required package: RSNNS\n#> Loading required package: Rcpp\nrequire(devtools)\n \n#define number of variables and observations\nset.seed(2)\nnum.vars<-8\nnum.obs<-10000\n \n#define correlation matrix for explanatory variables \n#define actual parameter values\ncov.mat <-genPositiveDefMat(num.vars,covMethod=c(\"unifcorrmat\"))$Sigma\nrand.vars <-mvrnorm(num.obs,rep(0,num.vars),Sigma=cov.mat)\nparms1 <-runif(num.vars,-10,10)\ny1 <-rand.vars %*% matrix(parms1) + rnorm(num.obs,sd=20)\nparms2 <-runif(num.vars,-10,10)\ny2 <-rand.vars %*% matrix(parms2) + rnorm(num.obs,sd=20)\n \n#prep data and create neural network\nrand.vars <- data.frame(rand.vars)\nresp <- apply(cbind(y1,y2),2, function(y) (y-min(y))/(max(y)-min(y)))\nresp <- data.frame(resp)\nnames(resp)<-c('Y1','Y2')\ntibble::as_tibble(rand.vars)\n#> # A tibble: 10,000 x 8\n#>        X1     X2     X3     X4     X5    X6    X7     X8\n#>     <dbl>  <dbl>  <dbl>  <dbl>  <dbl> <dbl> <dbl>  <dbl>\n#> 1  1.61    2.13   2.13   3.97  -1.34   2.00  3.11 -2.55 \n#> 2 -1.25    3.07  -0.325  1.61  -0.484  2.28  2.98 -1.71 \n#> 3 -3.17   -1.29  -1.77  -1.66  -0.549 -3.19  1.07  1.81 \n#> 4 -2.39    3.28  -3.42  -0.160 -1.52   2.67  7.05 -1.14 \n#> 5 -1.55   -0.181 -1.14   2.27  -1.68  -1.67  3.08  0.334\n#> 6  0.0690 -1.54  -2.98   2.84   1.42   1.31  1.82  2.07 \n#> # … with 9,994 more rows\ntibble::as_tibble(resp)\n#> # A tibble: 10,000 x 2\n#>      Y1    Y2\n#>   <dbl> <dbl>\n#> 1 0.461 0.500\n#> 2 0.416 0.509\n#> 3 0.534 0.675\n#> 4 0.548 0.619\n#> 5 0.519 0.659\n#> 6 0.389 0.622\n#> # … with 9,994 more rows\n# create neural network model\nmod2 <- mlp(rand.vars, resp, size = 8, linOut = T)\n \n#import sensitivity analysis function\nsource_url('https://gist.githubusercontent.com/fawda123/6860630/raw/b8bf4a6c88d6b392b1bfa6ef24759ae98f31877c/lek_fun.r')\n#> SHA-1 hash of file is 4a2d33b94a08f46a94518207a4ae7cc412845222\n \n#sensitivity analsyis, note 'exp.in' argument\nlek.fun(mod2, exp.in = rand.vars)"},{"path":"sensitivity-analysis-for-a-neural-network.html","id":"references","chapter":"6 Sensitivity analysis for a neural network","heading":"6.6 References","text":"1 Garson GD. 1991. Interpreting neural network connection weights. Artificial Intelligence Expert. 6:46–51.\n2 Lek S, Delacoste M, Baran P, Dimopoulos , Lauga J, Aulagnier S. 1996. Application neural networks modelling nonlinear relationships Ecology. Ecological Modelling. 90:39-52.\n3 Gevrey M, Dimopoulos , Lek S. 2003. Review comparison methods study contribution variables artificial neural network models. Ecological Modelling. 160:249-264.","code":""},{"path":"data-visualization-for-ml-models.html","id":"data-visualization-for-ml-models","chapter":"7 Data Visualization for ML models","heading":"7 Data Visualization for ML models","text":"","code":""},{"path":"data-visualization-for-ml-models.html","id":"introduction-2","chapter":"7 Data Visualization for ML models","heading":"7.1 Introduction","text":"Source: https://socviz.co/modeling.htmlData visualization generating figures display raw numbers table data. Right beginning, involves summarizing transforming parts data, plotting results. Statistical models central part process. Chapter, begin looking briefly ggplot can use various modeling techniques directly within geoms. see use broom margins libraries tidily extract plot estimates models fit .Histograms, density plots, boxplots, geoms compute either single numbers new variables plotting . saw Section 4.4, calculations done stat_ functions, works hand--hand default geom_ function, vice versa. Moreover, smoothing lines drew almost first plots made, seen stat_ functions can fair amount calculation even model estimation fly. geom_smooth() function can take range method arguments fit LOESS, OLS, robust regression lines, amongst others.geom_smooth() geom_quantile() functions can also instructed use different formulas produce fits. top panel Figure 6.1, access MASS library’s rlm function fit robust regression line. second panel, bs function invoked directly splines library way, fit polynominal curve data. approach directly accessing functions without loading whole library already used several times using functions scales library. geom_quantile() function, meanwhile, like specialized version geom_smooth() can fit quantile regression lines using variety methods. quantiles argument takes vector specifying quantiles fit lines.","code":"\n# load libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(socviz)       # devtools::install_github(\"kjhealy/socviz\")\nlibrary(gapminder)\n# plot two lines\np <- ggplot(data = gapminder,\n            mapping = aes(x = log(gdpPercap), y = lifeExp))\n\np + geom_point(alpha=0.1) +\n    geom_smooth(color = \"tomato\", fill=\"tomato\", method = MASS::rlm) +\n    geom_smooth(color = \"steelblue\", fill=\"steelblue\", method = \"lm\")\n#> `geom_smooth()` using formula 'y ~ x'\n#> `geom_smooth()` using formula 'y ~ x'\n# plot spline\np + geom_point(alpha=0.1) +\n    geom_smooth(color = \"tomato\", method = \"lm\", size = 1.2, \n                formula = y ~ splines::bs(x, 3), se = FALSE)\np + geom_point(alpha=0.1) +\n    geom_quantile(color = \"tomato\", size = 1.2, method = \"rqss\",\n                  lambda = 1, quantiles = c(0.20, 0.5, 0.85))\n#> Smoothing formula not specified. Using: y ~ qss(x, lambda = 1)\n#> Warning in rq.fit.sfn(x, y, tau = tau, rhs = rhs, control = control, ...): tiny diagonals replaced with Inf when calling blkfct"},{"path":"data-visualization-for-ml-models.html","id":"show-several-fits-at-once-with-a-legend","chapter":"7 Data Visualization for ML models","heading":"7.2 Show several fits at once, with a legend","text":"just saw first panel Figure 6.1, plotted OLS robust regression line, can look several fits plot layering new smoothers geom_smooth(). long set color fill aesthetics different values fit, can easily distinguish visually. However, ggplot draw legend guides us fit . smoothers logically connected one another. exist separate layers. comparing several different fits want legend describing ?turns , geom_smooth() can via slightly unusual route mapping color fill aesthetics string describing model fitting, using scale_color_manual() scale_fill_manual() create legend. First use brewer.pal() RColorBrewer library extract three qualitatively different colors larger palette. colors represented hex values. use :: convention use function without loading whole library:create plot three different smoothers, mapping color fill within aes() function name smoother:way cheated little make plot work. now, always mapped aesthetics names variables, strings like “OLS” “Cubic Splines”. Chapter 3, discussed mapping versus setting aesthetics, saw happened tried change color points scatterplot setting “purple” inside aes()function. result points turned red instead, ggplot effect created new variable labeled word “purple”. learned aes() function mapping variables aesthetics.take advantage behavior, creating new single-value variable name models. Ggplot properly construct relevant guide call scale_color_manual() scale_fill_manual(). Remember call two scale functions two mappings. result single plot containing just three smoothers, also appropriate legend guide reader.model-fitting features make ggplot useful exploratory work, make straightforward generate compare model-based trends summaries part process descriptive data visualization. various stat_ functions flexible way add summary estimates various kinds plots. also want , including presenting results models fit .","code":"\nmodel_colors <- RColorBrewer::brewer.pal(3, \"Set1\")\nmodel_colors\n#> [1] \"#E41A1C\" \"#377EB8\" \"#4DAF4A\"\np0 <- ggplot(data = gapminder,\n            mapping = aes(x = log(gdpPercap), y = lifeExp))\n\np1 <- p0 + geom_point(alpha = 0.2) +\n    geom_smooth(method = \"lm\", aes(color = \"OLS\", fill = \"OLS\")) +\n    geom_smooth(method = \"lm\", formula = y ~ splines::bs(x, df = 3),\n                aes(color = \"Cubic Spline\", fill = \"Cubic Spline\")) +\n    geom_smooth(method = \"loess\",\n                aes(color = \"LOESS\", fill = \"LOESS\"))\n\n\np1 + scale_color_manual(name = \"Models\", values = model_colors) +\n    scale_fill_manual(name = \"Models\", values = model_colors) +\n    theme(legend.position = \"top\")\n#> `geom_smooth()` using formula 'y ~ x'\n#> `geom_smooth()` using formula 'y ~ x'"},{"path":"data-visualization-for-ml-models.html","id":"look-inside-model-objects","chapter":"7 Data Visualization for ML models","heading":"7.3 Look inside model objects","text":"Covering details fitting statistical models R beyond scope book. comprehensive, modern introduction topic work way (Gelman & Hill, 2018). (Harrell, 2016) also good many practical connections modeling graphing data. Similarly, (Gelman, 2004) provides detailed discussion use graphics tool model-checking validation. discuss ways take models fit extract information easy work ggplot. goal, always, get however object stored tidy table numbers can plot. classes statistical model R contain information need, special set functions, methods, designed extract .can start learning little output models stored R. Remember, always working objects, objects internal structure consisting named pieces. Sometimes single numbers, sometimes vectors, sometimes lists things like vectors, matrices, formulas.working extensively tibbles data frames. store tables data named columns, perhaps consisting different classes variable, integers, characters, dates, factors. Model objects little complicated .Remember, can use str() function learn internal structure object. example, can get information class (classes) object gapminder , large , components . output str(gapminder) somewhat dense:lot information object whole variable . way, statistical models R internal structure. models complex entities data tables, structure correspondingly complicated. pieces information, kinds information, might want use. information generally stored computable parts model object.can create linear model, ordinary OLS regression, using gapminder data. dataset country-year structure makes OLS specification like wrong one use. never mind now. use lm() function run model, store object called :first argument formula model. lifeExp dependent variable tilde ~ operator used designate left- right-hand sides model (including cases, saw facet_wrap() model just right-hand side.)Let’s look results asking R print summary model.use summary() function , getting simple feed ’s model object. Instead, like function, summary() takes input, performs actions, produces output. case, printed console partly information stored inside model object, partly information summary() function calculated formated display screen. Behind scenes, summary() gets help functions. Objects different classes default methods associated , generic summary() function applied linear model object, function knows pass work specialized function bunch calculations formatting appropriate linear model object. use generic summary() function data frames, summary(gapminder), case different default method applied.Schematic view linear model object.Figure 6.3: Schematic view linear model object.output summary() gives precis model, can’t really analysis directly. example, want plot something model? information necessary make plots inside object, obvious use .take look structure model object str() find lot information . Like complex objects R, organized list components elements. Several elements lists. Figure 6.3 gives schematic view contents linear model object. list items, elements single values, data frames, additional lists simpler items. , remember earlier discussion said objects thought organized like filing system: cabinets contain drawers, drawer may contain may contain pages information, whole documents, groups folders documents inside. alternative analogy, sticking image list, can think master -list project, top-level headings lead contain additional lists tasks different kinds.object created lm contains several different named elements. , like residual degrees freedom model, just single number. Try $df.residual console. Others much larger entities, data frame used fit model, retained default. Try $model, prepared lot stuff printed console. elements computed R stored, coefficients model quantities. can try $coefficients, $residuals, $fitted.values, instance. Others lists (like qr). can see summary() function selecting printing small amount core information, comparison stored model object.Just like tables data saw earlier Section .1.3, output summary() presented way compact efficient terms getting information across, also untidy considered point view manipulation. table coefficients, variable names rows. column names awkward, information (e.g. bottom output) calculated printed , stored model object.","code":"\ngapminder\n#> # A tibble: 1,704 x 6\n#>   country     continent  year lifeExp      pop gdpPercap\n#>   <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n#> 1 Afghanistan Asia       1952    28.8  8425333      779.\n#> 2 Afghanistan Asia       1957    30.3  9240934      821.\n#> 3 Afghanistan Asia       1962    32.0 10267083      853.\n#> 4 Afghanistan Asia       1967    34.0 11537966      836.\n#> 5 Afghanistan Asia       1972    36.1 13079460      740.\n#> 6 Afghanistan Asia       1977    38.4 14880372      786.\n#> # … with 1,698 more rows\nstr(gapminder)\n#> tibble [1,704 × 6] (S3: tbl_df/tbl/data.frame)\n#>  $ country  : Factor w/ 142 levels \"Afghanistan\",..: 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ continent: Factor w/ 5 levels \"Africa\",\"Americas\",..: 3 3 3 3 3 3 3 3 3 3 ...\n#>  $ year     : int [1:1704] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ...\n#>  $ lifeExp  : num [1:1704] 28.8 30.3 32 34 36.1 ...\n#>  $ pop      : int [1:1704] 8425333 9240934 10267083 11537966 13079460 14880372 12881816 13867957 16317921 22227415 ...\n#>  $ gdpPercap: num [1:1704] 779 821 853 836 740 ...\nout <- lm(formula = lifeExp ~ gdpPercap + pop + continent,\n          data = gapminder)\nsummary(out)\n#> \n#> Call:\n#> lm(formula = lifeExp ~ gdpPercap + pop + continent, data = gapminder)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -49.16  -4.49   0.30   5.11  25.17 \n#> \n#> Coefficients:\n#>                   Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)       4.78e+01   3.40e-01  140.82   <2e-16 ***\n#> gdpPercap         4.50e-04   2.35e-05   19.16   <2e-16 ***\n#> pop               6.57e-09   1.98e-09    3.33    9e-04 ***\n#> continentAmericas 1.35e+01   6.00e-01   22.46   <2e-16 ***\n#> continentAsia     8.19e+00   5.71e-01   14.34   <2e-16 ***\n#> continentEurope   1.75e+01   6.25e-01   27.97   <2e-16 ***\n#> continentOceania  1.81e+01   1.78e+00   10.15   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 8.37 on 1697 degrees of freedom\n#> Multiple R-squared:  0.582,  Adjusted R-squared:  0.581 \n#> F-statistic:  394 on 6 and 1697 DF,  p-value: <2e-16"},{"path":"data-visualization-for-ml-models.html","id":"get-model-based-graphics-right","chapter":"7 Data Visualization for ML models","heading":"7.4 Get model-based graphics right","text":"Figures based statistical models face ordinary challenges effective data visualization, . model results usually carry considerable extra burden interpretation necessary background knowledge. complex model, trickier becomes convey information effectively, easier becomes lead one’s audience oneself error. Within social sciences, ability clearly honestly present model-based graphics greatly improved past ten fifteen years. period, become clearer kinds models quite tricky understand, even ones previously seen straightforward elements modeling toolkit (Ai & Norton, 2003; Brambor, Clark, & Golder, 2006).Plotting model estimates closely connected properly estimating models first place. means substitute learning statistics. use graphical methods substitute understanding model used produce . book teach material, can make general points good model-based graphics look like, work examples ggplot additional libraries can make easier get good results.","code":""},{"path":"data-visualization-for-ml-models.html","id":"present-your-findings-in-substantive-terms","chapter":"7 Data Visualization for ML models","heading":"7.4.1 Present your findings in substantive terms","text":"Useful model-based plots show results ways substantively meaningful directly interpretable respect questions analysis trying answer. means showing results context variables analysis held sensible values, means medians. continuous variables, can often useful generate predicted values cover substantively meaningful move across distribution, 25th 75th percentile, rather single-unit increment variable interest. unordered categorical variables, predicted values might presented respect modal category data, particular category theoretical interest. Presenting substantively interpretable findings often also means using (sometimes converting ) scale readers can easily understand. model reports results log-odds, example, converting estimates predicted probabilities make easier interpret. advice quite general. points applies equally well presentation summary results table rather graph. nothing distinctively graphical putting focus substantive meaning findings.","code":""},{"path":"data-visualization-for-ml-models.html","id":"show-your-degree-of-confidence","chapter":"7 Data Visualization for ML models","heading":"7.4.2 Show your degree of confidence","text":"Much applies presenting degree uncertainty confidence results. Model estimates come various measures precision, confidence, credence, significance. Presenting interpreting measures notoriously prone misinterpretation, -interpretation, researchers audiences demand things like confidence intervals p-values statistics can deliver. minimum, decided appropriate measure model fit right assessment confidence, show range present results. family related ggplot geoms allow show range interval defined position x-axis ymin ymax range y-axis. geoms include geom_pointrange() geom_errorbar(), see action shortly. related geom, geom_ribbon() uses arguments draw filled areas, useful plotting ranges y-axis values along continuously varying x-axis.","code":""},{"path":"data-visualization-for-ml-models.html","id":"show-your-data-when-you-can","chapter":"7 Data Visualization for ML models","heading":"7.4.3 Show your data when you can","text":"Plotting results multivariate model generally means one two things. First, can show effect table coefficients associated measures confidence, perhaps organizing coefficients meaningful groups, size predicted association, . Second, can show predicted values variables (rather just model’s coefficients) across range interest. latter approach lets us show original data points wish. way ggplot builds graphics layer layer allows us easily combine model estimates (e.g. regression line associated range) underlying data. effect manually-constructed versions automatically-generated plots producing geom_smooth() since beginning book.","code":""},{"path":"data-visualization-for-ml-models.html","id":"generate-predictions-to-graph","chapter":"7 Data Visualization for ML models","heading":"7.5 Generate predictions to graph","text":"fitted model, , might want get picture estimates produces range particular variable, holding covariates constant sensible values. predict() function generic way using model objects produce kind prediction. R, “generic” functions take inputs pass along specific functions behind scenes, ones suited working particular kind model object . details getting predicted values OLS model, instance, somewhat different getting predictions logistic regression. case can use predict() function, taking care check documentation see form results returned kind model working . Many commonly-used functions R generic way. summary() function, example, works objects many different classes, vectors data frames statistical models, producing appropriate output case way class-specific function background.predict() calculate new values us, needs new data fit model . generate new data frame whose columns names variables model’s original data, rows new values. useful function called expand.grid() help us . give list variables, specifying range values want variable take. expand.grid() generate multiply full range values combinations values give , thus creating new data frame new data need.following bit code, use min() max() get minimum maximum values per capita GDP, create vector one hundred evenly-spaced elements minimum maximum. hold population constant median, let continent take five available values.Now can use predict(). give function new data model, without argument, calculate fitted values every row data frame. specify interval = 'predict' argument, calculate 95% prediction intervals addition point estimate.know , construction, cases pred_df pred_out correspond row row, can bind two data frames together column. method joining merging tables definitely recommended dealing data.end result tidy data frame, containing predicted values model range values specified. Now can plot results. produced full range predicted values, can decide whether use . subset predictions just Europe Africa.use new geom draw area covered prediction intervals: geom_ribbon(). takes x argument like line, ymin ymax argument specified ggplot() aesthetic mapping. defines lower upper limits prediction interval.practice, may use predict() directly often. Instead, might write code using additional libraries encapsulate process producing predictions plots models. especially useful model little complex interpretation coefficients becomes trickier. happens, instance, binary outcome variable need convert results logistic regression predicted probabilities, interaction terms amongst predictions. discuss helper libraries next sections. However, bear mind predict() ability work safely different classes model underpins many libraries. ’s useful see action first hand order understand .","code":"\nmin_gdp <- min(gapminder$gdpPercap)\nmax_gdp <- max(gapminder$gdpPercap)\nmed_pop <- median(gapminder$pop)\n\npred_df <- expand.grid(gdpPercap = (seq(from = min_gdp,\n                                        to = max_gdp,\n                                        length.out = 100)),\n                       pop = med_pop,\n                       continent = c(\"Africa\", \"Americas\",\n                                     \"Asia\", \"Europe\", \"Oceania\"))\n\ndim(pred_df)\n#> [1] 500   3\nhead(pred_df)\n#>   gdpPercap     pop continent\n#> 1       241 7023596    Africa\n#> 2      1385 7023596    Africa\n#> 3      2530 7023596    Africa\n#> 4      3674 7023596    Africa\n#> 5      4818 7023596    Africa\n#> 6      5962 7023596    Africa\npred_out <- predict(object = out,\n                    newdata = pred_df,\n                    interval = \"predict\")\nhead(pred_out)\n#>    fit  lwr  upr\n#> 1 48.0 31.5 64.4\n#> 2 48.5 32.1 64.9\n#> 3 49.0 32.6 65.4\n#> 4 49.5 33.1 65.9\n#> 5 50.0 33.6 66.4\n#> 6 50.5 34.1 67.0\npred_df <- cbind(pred_df, pred_out)\nhead(pred_df)\n#>   gdpPercap     pop continent  fit  lwr  upr\n#> 1       241 7023596    Africa 48.0 31.5 64.4\n#> 2      1385 7023596    Africa 48.5 32.1 64.9\n#> 3      2530 7023596    Africa 49.0 32.6 65.4\n#> 4      3674 7023596    Africa 49.5 33.1 65.9\n#> 5      4818 7023596    Africa 50.0 33.6 66.4\n#> 6      5962 7023596    Africa 50.5 34.1 67.0\np <- ggplot(data = subset(pred_df, continent %in% c(\"Europe\", \"Africa\")),\n            aes(x = gdpPercap,\n                y = fit, ymin = lwr, ymax = upr,\n                color = continent,\n                fill = continent,\n                group = continent))\n\np + geom_point(data = subset(gapminder,\n                             continent %in% c(\"Europe\", \"Africa\")),\n               aes(x = gdpPercap, y = lifeExp,\n                   color = continent),\n               alpha = 0.5,\n               inherit.aes = FALSE) + \n    geom_line() +\n    geom_ribbon(alpha = 0.2, color = FALSE) +\n    scale_x_log10(labels = scales::dollar)"},{"path":"data-visualization-for-ml-models.html","id":"tidy-model-objects-with-broom","chapter":"7 Data Visualization for ML models","heading":"7.6 Tidy model objects with broom","text":"predict method useful, lot things might want model output. use David Robinson’s broom package help us . library functions help us get model results R generates numbers can plot. take model objects turn pieces data frames can use easily ggplot.Broom takes ggplot’s approach tidy data extends model objects R produces. methods can tidily extract three kinds information. First, can see component-level information aspects model , coefficients t-statistics. Second, can obtain observation-level information model’s connection underlying data. includes fitted values residuals observation data. finally can get model-level information summarizes fit whole, F-statistic, model deviance, r-squared. broom function tasks.","code":"\nlibrary(broom)"},{"path":"data-visualization-for-ml-models.html","id":"get-component-level-statistics-with-tidy","chapter":"7 Data Visualization for ML models","heading":"7.6.1 Get component-level statistics with tidy()","text":"tidy() function takes model object returns data frame component-level information. can work make plots familiar way, much easily fishing inside model object extract various terms. example, using default results just returned. convenient display results, pipe object create tidy() function rounds numeric columns data frame two decimal places. doesn’t change anything object , course.now able treat dataframe just like data seen far.can extend clean plot variety ways. example, can tell tidy() calculate confidence intervals estimates, using R’s confint() function.convenience “” operator %nin% available via socviz library. opposite %% selects items first vector characters second. ’ll use drop intercept term table. also want something labels. fitting model categorical variables, R create coefficient names based variable name category name, like continentAmericas. Normally like clean plotting. commonly, just want strip away variable name beginning coefficient label. can use prefix_strip(), convenience function socviz library. tell prefixes drop, using create new column variable out_conf corresponds terms column, nicer labels.Now can use geom_pointrange()make figure displays information confidence variable estimates, opposed just coefficients. boxplots earlier, use reorder() sort names model’s terms estimate variable, thus arranging plot effects largest smallest magnitude.Dotplots kind can compact. vertical axis can often compressed quite bit, loss comprehension. fact, often easier read much less room rows given default square shape.","code":"\nout_comp <- tidy(out)\nout_comp %>% round_df()\n#> # A tibble: 7 x 5\n#>   term              estimate std.error statistic p.value\n#>   <chr>                <dbl>     <dbl>     <dbl>   <dbl>\n#> 1 (Intercept)          47.8      0.34     141.         0\n#> 2 gdpPercap             0        0         19.2        0\n#> 3 pop                   0        0          3.33       0\n#> 4 continentAmericas    13.5      0.6       22.5        0\n#> 5 continentAsia         8.19     0.570     14.3        0\n#> 6 continentEurope      17.5      0.62      28.0        0\n#> # … with 1 more row\np <- ggplot(out_comp, mapping = aes(x = term,\n                                    y = estimate))\n\np + geom_point() + coord_flip() \nout_conf <- tidy(out, conf.int = TRUE)\nout_conf %>% round_df()\n#> # A tibble: 7 x 7\n#>   term              estimate std.error statistic p.value conf.low conf.high\n#>   <chr>                <dbl>     <dbl>     <dbl>   <dbl>    <dbl>     <dbl>\n#> 1 (Intercept)          47.8      0.34     141.         0    47.2      48.5 \n#> 2 gdpPercap             0        0         19.2        0     0         0   \n#> 3 pop                   0        0          3.33       0     0         0   \n#> 4 continentAmericas    13.5      0.6       22.5        0    12.3      14.6 \n#> 5 continentAsia         8.19     0.570     14.3        0     7.07      9.31\n#> 6 continentEurope      17.5      0.62      28.0        0    16.2      18.7 \n#> # … with 1 more row\nout_conf <- subset(out_conf, term %nin% \"(Intercept)\")\nout_conf$nicelabs <- prefix_strip(out_conf$term, \"continent\")\np <- ggplot(out_conf, mapping = aes(x = reorder(nicelabs, estimate),\n                                    y = estimate, ymin = conf.low, ymax = conf.high))\np + geom_pointrange() + coord_flip() + labs(x=\"\", y=\"OLS Estimate\")"},{"path":"data-visualization-for-ml-models.html","id":"get-observation-level-statistics-with-augment","chapter":"7 Data Visualization for ML models","heading":"7.6.2 Get observation-level statistics with augment()","text":"values returned augment() statistics calculated level original observations. , can added data frame model based . Working call augment() return data frame original observations used estimation model, together columns like following:.fitted — fitted values model..se.fit — standard errors fitted values..resid — residuals..hat — diagonal hat matrix..sigma — estimate residual standard deviation corresponding observation dropped model..cooksd — Cook’s distance, common regression diagnostic; .std.resid — standardized residuals.variables named leading dot, example .hat rather hat, . guard accidentally confusing (accidentally overwriting) existing variable data name. columns values return differ slightly depending class model fitted.default, augment() extract available data model object. usually include variables used model , additional ones contained original data frame. Sometimes useful . can add specifying data argument:rows containing missing data dropped fit model, carried augmented dataframe.new columns created augment() can used create standard regression plots. example, can plot residuals versus fitted values. Figure 6.7 suggests, unsurprisingly, country-year data rather structure captured OLS model.","code":"\nout_aug <- augment(out)\nhead(out_aug) %>% round_df()\n#> # A tibble: 6 x 11\n#>   lifeExp gdpPercap    pop continent .fitted .se.fit .resid  .hat .sigma .cooksd\n#>     <dbl>     <dbl>  <dbl> <fct>       <dbl>   <dbl>  <dbl> <dbl>  <dbl>   <dbl>\n#> 1    28.8      779. 8.43e6 Asia         56.4    0.47  -27.6     0   8.34    0.01\n#> 2    30.3      821. 9.24e6 Asia         56.4    0.47  -26.1     0   8.34    0   \n#> 3    32        853. 1.03e7 Asia         56.5    0.47  -24.5     0   8.35    0   \n#> 4    34.0      836. 1.15e7 Asia         56.5    0.47  -22.4     0   8.35    0   \n#> 5    36.1      740. 1.31e7 Asia         56.4    0.47  -20.3     0   8.35    0   \n#> 6    38.4      786. 1.49e7 Asia         56.5    0.47  -18.0     0   8.36    0   \n#> # … with 1 more variable: .std.resid <dbl>\nout_aug <- augment(out, data = gapminder)\nhead(out_aug) %>% round_df()\n#> # A tibble: 6 x 13\n#>   country continent  year lifeExp    pop gdpPercap .fitted .se.fit .resid  .hat\n#>   <fct>   <fct>     <dbl>   <dbl>  <dbl>     <dbl>   <dbl>   <dbl>  <dbl> <dbl>\n#> 1 Afghan… Asia       1952    28.8 8.43e6      779.    56.4    0.47  -27.6     0\n#> 2 Afghan… Asia       1957    30.3 9.24e6      821.    56.4    0.47  -26.1     0\n#> 3 Afghan… Asia       1962    32   1.03e7      853.    56.5    0.47  -24.5     0\n#> 4 Afghan… Asia       1967    34.0 1.15e7      836.    56.5    0.47  -22.4     0\n#> 5 Afghan… Asia       1972    36.1 1.31e7      740.    56.4    0.47  -20.3     0\n#> 6 Afghan… Asia       1977    38.4 1.49e7      786.    56.5    0.47  -18.0     0\n#> # … with 3 more variables: .sigma <dbl>, .cooksd <dbl>, .std.resid <dbl>\np <- ggplot(data = out_aug,\n            mapping = aes(x = .fitted, y = .resid))\np + geom_point()"},{"path":"data-visualization-for-ml-models.html","id":"get-model-level-statistics-with-glance","chapter":"7 Data Visualization for ML models","heading":"7.6.3 Get model-level statistics with glance()","text":"function organizes information typically presented bottom model’s summary() output. , usually just returns table single row . shall see moment, real power broom’s approach way can scale cases grouping subsampling data.Broom able tidy (augment, glance ) wide range model types. functions available classes model. Consult broom’s documentation details available. example, plot created tidied output event-history analysis. First generate Cox proportional hazards model survival data.details fit important , first step Surv() function creates response outcome variable proportional hazards model fitted coxph()function. survfit() function creates survival curve model, much like used predict() generate predicted values earlier. Try summary(out_cph) see model, summary(out_surv) see table predicted values form basis plot. Next tidy out_surv get data frame, plot .","code":"\nglance(out) %>% round_df()\n#> # A tibble: 1 x 11\n#>   r.squared adj.r.squared sigma statistic p.value    df logLik    AIC    BIC\n#>       <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>  <dbl>  <dbl>  <dbl>\n#> 1     0.580         0.580  8.37      394.       0     7 -6034. 12084. 12127.\n#> # … with 2 more variables: deviance <dbl>, df.residual <dbl>\nlibrary(survival)\n\nout_cph <- coxph(Surv(time, status) ~ age + sex, data = lung)\nout_surv <- survfit(out_cph)\n# Figure 6.8: A Kaplan-Meier plot.\nout_tidy <- tidy(out_surv)\n\np <- ggplot(data = out_tidy, mapping = aes(time, estimate))\np + geom_line() +\n    geom_ribbon(mapping = aes(ymin = conf.low, ymax = conf.high), alpha = .2)"},{"path":"data-visualization-for-ml-models.html","id":"grouped-analysis-and-list-columns","chapter":"7 Data Visualization for ML models","heading":"7.7 Grouped analysis and list-columns","text":"Broom makes possible quickly fit models different subsets data get consistent usable tables results end. example, let’s say wanted look gapminder data examining relationship life expectancy GDP continent, year data.gapminder data bottom organized country-years. unit observation rows. wanted, take slice data manually, “countries observed Asia, 1962” “Africa, 2002”. “Europe, 1977”:see relationship life expectancy GDP looked like continent-year group:dplyr broom can every continent-year slice data compact tidy way. start table data, (%>%) group countries continent year using group_by() function. introduced grouping operation Chapter 4. data reorganized first continent, within continent year. take one step nest data make group:Think nest() intensive version group_by() . resulting object tabular form expect (tibble) looks little unusual. first two columns familiar continent year. now also new column, data, contains small table data corresponding continent-year group. list-column, something seen . turns useful bundling together complex objects (structured, case, list tibbles, 33x4 table data) within rows data (remains tabular). “Europe 1977” fit . can look , like, filtering data unnesting list column.List-columns useful can act compact tidy way. particular, can pass functions along row list-column make something happen. example, moment ago ran regression life expectancy logged GDP European countries 1977. can every continent-year combination data. first create convenience function called fit_ols() takes single argument, df (data frame) fits linear model interested . map function list-column rows turn. Recall Chapter 4 mutate creates new variables columns fly within pipeline.map action important idea functional programming. written code , imperative languages can think compact alternative writing … next loops. can course write loops like R. Computationally often less efficient functional alternatives. mapping functions arrays easily integrated sequence data transformations.starting pipeline create new function: convenience function whose job estimate particular OLS model data. Like almost everything R, functions kind object. make new one, use slightly special function() function. (Nerds love sort thing.) little detail creating functions Appendix. see fit_ols() looks like created, type fit_ols without parentheses Console. see , try fit_ols(df = gapminder), summary(fit_ols(gapminder)).Now two list-columns: data, model. latter created mapping fit_ols() function row data. Inside element model linear model continent-year. now sixty OLS fits, one every continent-year grouping. models inside list column much use us . can extract information want keeping things tidy tabular form. clarity run pipeline beginning , time adding new steps.First extract summary statistics model mapping tidy() function broom model list column. unnest result, dropping columns process. Finally, filter Intercept terms, also drop observations Oceania. case Intercepts just convenience. Oceania drop just observations. put results object called out_tidy.now tidy regression output estimate association log GDP per capita life expectancy year, within continents. can plot estimates way takes advantage groupiness.call position_dodge() within geom_pointrange() allows point ranges continent near within years, instead plotted right top one another. faceted results continent, way lets us see differences yearly estimates much easily. technique useful just cases like , also want compare coefficients given different kinds statistical model. sometimes happens ’re interested seeing , say, OLS performs model specification.","code":"\neu77 <- gapminder %>% filter(continent == \"Europe\", year == 1977)\nfit <- lm(lifeExp ~ log(gdpPercap), data = eu77)\nsummary(fit)\n#> \n#> Call:\n#> lm(formula = lifeExp ~ log(gdpPercap), data = eu77)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -7.496 -1.031  0.093  1.176  3.712 \n#> \n#> Coefficients:\n#>                Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)      29.489      7.161    4.12  0.00031 ***\n#> log(gdpPercap)    4.488      0.756    5.94  2.2e-06 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 2.11 on 28 degrees of freedom\n#> Multiple R-squared:  0.557,  Adjusted R-squared:  0.541 \n#> F-statistic: 35.2 on 1 and 28 DF,  p-value: 2.17e-06\nout_le <- gapminder %>%\n    group_by(continent, year) %>%\n    nest()\n\nout_le\n#> # A tibble: 60 x 3\n#> # Groups:   continent, year [60]\n#>   continent  year data             \n#>   <fct>     <int> <list>           \n#> 1 Asia       1952 <tibble [33 × 4]>\n#> 2 Asia       1957 <tibble [33 × 4]>\n#> 3 Asia       1962 <tibble [33 × 4]>\n#> 4 Asia       1967 <tibble [33 × 4]>\n#> 5 Asia       1972 <tibble [33 × 4]>\n#> 6 Asia       1977 <tibble [33 × 4]>\n#> # … with 54 more rows\nout_le %>% filter(continent == \"Europe\" & year == 1977) %>% unnest()\n#> Warning: `cols` is now required.\n#> Please use `cols = c(data)`\n#> # A tibble: 30 x 6\n#> # Groups:   continent, year [5]\n#>   continent  year country                lifeExp     pop gdpPercap\n#>   <fct>     <int> <fct>                    <dbl>   <int>     <dbl>\n#> 1 Europe     1977 Albania                   68.9 2509048     3533.\n#> 2 Europe     1977 Austria                   72.2 7568430    19749.\n#> 3 Europe     1977 Belgium                   72.8 9821800    19118.\n#> 4 Europe     1977 Bosnia and Herzegovina    69.9 4086000     3528.\n#> 5 Europe     1977 Bulgaria                  70.8 8797022     7612.\n#> 6 Europe     1977 Croatia                   70.6 4318673    11305.\n#> # … with 24 more rows\nfit_ols <- function(df) {\n    lm(lifeExp ~ log(gdpPercap), data = df)\n}\n\nout_le <- gapminder %>%\n    group_by(continent, year) %>%\n    nest() %>% \n    mutate(model = map(data, fit_ols)) \n\nout_le\n#> # A tibble: 60 x 4\n#> # Groups:   continent, year [60]\n#>   continent  year data              model \n#>   <fct>     <int> <list>            <list>\n#> 1 Asia       1952 <tibble [33 × 4]> <lm>  \n#> 2 Asia       1957 <tibble [33 × 4]> <lm>  \n#> 3 Asia       1962 <tibble [33 × 4]> <lm>  \n#> 4 Asia       1967 <tibble [33 × 4]> <lm>  \n#> 5 Asia       1972 <tibble [33 × 4]> <lm>  \n#> 6 Asia       1977 <tibble [33 × 4]> <lm>  \n#> # … with 54 more rows\nfit_ols <- function(df) {\n    lm(lifeExp ~ log(gdpPercap), data = df)\n}\n\nout_tidy <- gapminder %>%\n    group_by(continent, year) %>%\n    nest() %>% \n    mutate(model = map(data, fit_ols),\n           tidied = map(model, tidy)) %>%\n    # unnest(tidied, .drop = TRUE) %>%     # .drop is deprecated\n    unnest(tidied) %>%\n    filter(term %nin% \"(Intercept)\" &\n           continent %nin% \"Oceania\") %>% \n    print()\n#> # A tibble: 48 x 9\n#> # Groups:   continent, year [49]\n#>   continent  year data      model  term     estimate std.error statistic p.value\n#>   <fct>     <int> <list>    <list> <chr>       <dbl>     <dbl>     <dbl>   <dbl>\n#> 1 Asia       1952 <tibble … <lm>   log(gdp…     4.16      1.25      3.33 2.28e-3\n#> 2 Asia       1957 <tibble … <lm>   log(gdp…     4.17      1.28      3.26 2.71e-3\n#> 3 Asia       1962 <tibble … <lm>   log(gdp…     4.59      1.24      3.72 7.94e-4\n#> 4 Asia       1967 <tibble … <lm>   log(gdp…     4.50      1.15      3.90 4.77e-4\n#> 5 Asia       1972 <tibble … <lm>   log(gdp…     4.44      1.01      4.41 1.16e-4\n#> 6 Asia       1977 <tibble … <lm>   log(gdp…     4.87      1.03      4.75 4.42e-5\n#> # … with 42 more rows\n# sample with replacement\nout_tidy %>% sample_n(5, replace = TRUE)    # replace to prevent error \n#> # A tibble: 240 x 9\n#> # Groups:   continent, year [49]\n#>   continent  year data       model term     estimate std.error statistic p.value\n#>   <fct>     <int> <list>     <lis> <chr>       <dbl>     <dbl>     <dbl>   <dbl>\n#> 1 Africa     1952 <tibble [… <lm>  log(gdp…     2.34     0.971      2.41  0.0199\n#> 2 Africa     1952 <tibble [… <lm>  log(gdp…     2.34     0.971      2.41  0.0199\n#> 3 Africa     1952 <tibble [… <lm>  log(gdp…     2.34     0.971      2.41  0.0199\n#> 4 Africa     1952 <tibble [… <lm>  log(gdp…     2.34     0.971      2.41  0.0199\n#> 5 Africa     1952 <tibble [… <lm>  log(gdp…     2.34     0.971      2.41  0.0199\n#> 6 Africa     1957 <tibble [… <lm>  log(gdp…     2.69     1.06       2.55  0.0140\n#> # … with 234 more rows\n# Figure 6.9: Yearly estimates of the association between GDP and Life Expectancy, pooled by continent.\np <- ggplot(data = out_tidy,\n            mapping = aes(x = year, y = estimate,\n                          ymin = estimate - 2*std.error,\n                          ymax = estimate + 2*std.error,\n                          group = continent, color = continent))\n\np + geom_pointrange(position = position_dodge(width = 1)) +\n    scale_x_continuous(breaks = unique(gapminder$year)) + \n    theme(legend.position = \"top\") +\n    labs(x = \"Year\", y = \"Estimate\", color = \"Continent\")"},{"path":"data-visualization-for-ml-models.html","id":"plot-marginal-effects","chapter":"7 Data Visualization for ML models","heading":"7.8 Plot marginal effects","text":"earlier discussion predict() obtaining estimates average effect coefficient, net terms model. past decade, estimating plotting partial marginal effects model become increasingly common way presenting accurate interpretively useful predictions. Interest marginal effects plots stimulated realization interpretation terms logistic regression models, particular, trickier seemed—especially interaction terms model (Ai & Norton, 2003). Thomas Leeper’s margins package can make plots us.see action, ’ll take another look General Social Survey data gss_sm, time focusing binary variable, obama.common retrospective questions elections, rather people claim voted Obama consistent vote share received election. coded 1 respondent said voted Barack Obama 2012 presidential election, 0 otherwise. case, mostly convenience , zero code includes answers question, including said voted Mitt Romney, said vote, refused answer, said didn’t know voted . fit logistic regression obama, age, polviews, race, sex predictors. age variable respondent’s age years. sex variable coded “Male” “Female” “Male” reference category. race variable coded “White”, “Black”, “” “White” reference category. polviews measure self-reported scale respondent’s political orientation “Extremely Conservative” “Extremely Liberal”, “Moderate” middle. take polviews create new variable, polviews_m, using relevel() function recode “Moderate” reference category. fit model glm() function, specify interaction race sex.summary reports coefficients information. can now graph data one several ways. Using margins() calculate marginal effects variable:margins library comes several plot methods . wish, point can just try plot(bo_m) see plot average marginal effects, produced general look Stata graphic. plot methods margins library include cplot(), visualizes marginal effects conditional second variable, image(), shows predictions marginal effects filled heatmap contour plot.Alternatively, can take results margins() plot . clean summary little little, convert tibble, use prefix_strip() prefix_replace() tidy labels. want strip polviews_m sex prefixes, (avoid ambiguity “”), adjust race prefix.Now table can plot learned:just interested getting conditional effects particular variable, conveniently can ask plot methods margins library work calculating effects us without drawing plot. Instead, can return results format can easily use ggplot, less need clean , clean-. example, cplot():margins package active development. can much described . vignettes come package provide extensive discussion numerous examples.","code":"\nlibrary(margins)\ngss_sm$polviews_m <- relevel(gss_sm$polviews, ref = \"Moderate\")\n\nout_bo <- glm(obama ~ polviews_m + sex*race,\n              family = \"binomial\", data = gss_sm)\nsummary(out_bo)\n#> \n#> Call:\n#> glm(formula = obama ~ polviews_m + sex * race, family = \"binomial\", \n#>     data = gss_sm)\n#> \n#> Deviance Residuals: \n#>    Min      1Q  Median      3Q     Max  \n#> -2.905  -0.554   0.177   0.542   2.244  \n#> \n#> Coefficients:\n#>                                  Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)                       0.29649    0.13409    2.21   0.0270 *  \n#> polviews_mExtremely Liberal       2.37295    0.52504    4.52  6.2e-06 ***\n#> polviews_mLiberal                 2.60003    0.35667    7.29  3.1e-13 ***\n#> polviews_mSlightly Liberal        1.29317    0.24843    5.21  1.9e-07 ***\n#> polviews_mSlightly Conservative  -1.35528    0.18129   -7.48  7.7e-14 ***\n#> polviews_mConservative           -2.34746    0.20038  -11.71  < 2e-16 ***\n#> polviews_mExtremely Conservative -2.72738    0.38721   -7.04  1.9e-12 ***\n#> sexFemale                         0.25487    0.14537    1.75   0.0796 .  \n#> raceBlack                         3.84953    0.50132    7.68  1.6e-14 ***\n#> raceOther                        -0.00214    0.43576    0.00   0.9961    \n#> sexFemale:raceBlack              -0.19751    0.66007   -0.30   0.7648    \n#> sexFemale:raceOther               1.57483    0.58766    2.68   0.0074 ** \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 2247.9  on 1697  degrees of freedom\n#> Residual deviance: 1345.9  on 1686  degrees of freedom\n#>   (1169 observations deleted due to missingness)\n#> AIC: 1370\n#> \n#> Number of Fisher Scoring iterations: 6\nbo_m <- margins(out_bo)\nsummary(bo_m)\n#>                            factor     AME     SE        z      p   lower\n#>            polviews_mConservative -0.4119 0.0283 -14.5394 0.0000 -0.4674\n#>  polviews_mExtremely Conservative -0.4538 0.0420 -10.7971 0.0000 -0.5361\n#>       polviews_mExtremely Liberal  0.2681 0.0295   9.0996 0.0000  0.2103\n#>                 polviews_mLiberal  0.2768 0.0229  12.0736 0.0000  0.2319\n#>   polviews_mSlightly Conservative -0.2658 0.0330  -8.0596 0.0000 -0.3304\n#>        polviews_mSlightly Liberal  0.1933 0.0303   6.3896 0.0000  0.1340\n#>                         raceBlack  0.4032 0.0173  23.3568 0.0000  0.3694\n#>                         raceOther  0.1247 0.0386   3.2297 0.0012  0.0490\n#>                         sexFemale  0.0443 0.0177   2.5073 0.0122  0.0097\n#>    upper\n#>  -0.3564\n#>  -0.3714\n#>   0.3258\n#>   0.3218\n#>  -0.2011\n#>   0.2526\n#>   0.4371\n#>   0.2005\n#>   0.0789\nbo_gg <- as_tibble(summary(bo_m))\nprefixes <- c(\"polviews_m\", \"sex\")\nbo_gg$factor <- prefix_strip(bo_gg$factor, prefixes)\nbo_gg$factor <- prefix_replace(bo_gg$factor, \"race\", \"Race: \")\n\nbo_gg %>% select(factor, AME, lower, upper) \n#> # A tibble: 9 x 4\n#>   factor                    AME  lower  upper\n#>   <chr>                   <dbl>  <dbl>  <dbl>\n#> 1 Conservative           -0.412 -0.467 -0.356\n#> 2 Extremely Conservative -0.454 -0.536 -0.371\n#> 3 Extremely Liberal       0.268  0.210  0.326\n#> 4 Liberal                 0.277  0.232  0.322\n#> 5 Slightly Conservative  -0.266 -0.330 -0.201\n#> 6 Slightly Liberal        0.193  0.134  0.253\n#> # … with 3 more rows\np <- ggplot(data = bo_gg, aes(x = reorder(factor, AME),\n                              y = AME, ymin = lower, ymax = upper))\n\np + geom_hline(yintercept = 0, color = \"gray80\") +\n    geom_pointrange() + coord_flip() +\n    labs(x = NULL, y = \"Average Marginal Effect\") \npv_cp <- cplot(out_bo, x = \"sex\", draw = FALSE)\n#>    xvals yvals upper lower\n#> 1   Male 0.574 0.638 0.509\n#> 2 Female 0.634 0.689 0.580\n\np <- ggplot(data = pv_cp, aes(x = reorder(xvals, yvals),\n                              y = yvals, ymin = lower, ymax = upper))\n\np + geom_hline(yintercept = 0, color = \"gray80\") +\n    geom_pointrange() + coord_flip() +\n    labs(x = NULL, y = \"Conditional Effect\") "},{"path":"data-visualization-for-ml-models.html","id":"plots-from-complex-surveys","chapter":"7 Data Visualization for ML models","heading":"7.9 Plots from complex surveys","text":"Social scientists often work data collected using complex survey design. Survey instruments may stratified region characteristic, contain replicate weights make comparable reference population, clustered structure, . Chapter 4 learned calculate plot frequency tables categorical variables, using data General Social Survey (GSS). However, want accurate estimates US households GSS, need take survey’s design account, use survey weights provided dataset. Thomas Lumley’s survey library provides comprehensive set tools addressing issues. tools theory behind discussed detail Lumley (2010), overview package provided Lumley (2004). functions survey package straightforward use return results generally tidy form, package predates tidyverse conventions several years. means use survey functions directly dplyr. However, Greg Freedman Ellis written helper package, srvyr, solves problem us, lets us use survey library’s functions within data analysis pipeline familiar way.example, gss_lon data contains small subset measures every wave GSS since inception 1972. also contains several variables describe design survey provide replicate weights observations various years. technical details described GSS documentation. Similar information typically provided complex surveys. use design information calculate weighted estimates distribution educational attainment race, selected survey years 1976 2016.begin, load survey srvyr libraries.Next, take gss_lon dataset use survey tools create new object contains data, , additional information survey’s design:two options set beginning provide information survey library behave. consult Lumley (2010) survey package documentation details. subsequent operations create gss_wt, object one additional column (stratvar), describing yearly sampling strata. use interaction() function . multiplies vstrat variable year variable get vector stratum information year. way GSS codes stratum information. next step, use as_survey_design() function add key pieces information survey design. adds information sampling identifiers (ids), strata (strata), replicate weights (weights). place can take advantage large number specialized functions survey library allow us calculate properly weighted survey means estimate models correct sampling specification. example, can easily calculate distribution education race series years 1976 2016. use survey_mean() :results returned out_grp include standard errors. can also ask survey_mean() calculate confidence intervals us, wish.Grouping group_by() lets us calculate counts means innermost variable, grouped next variable “” “”, case, degree race, proportions degree sum one group race, done separately value year. want marginal frequencies, values combinations race degree sum one within year, first interact variables cross-classifying. group new interacted variable calculation :gives us numbers want returns tidy data frame. interaction() function produces variable labels compound two variables interacted, combination categories separated period, (White.Graduate. However, perhaps like see categories two separate columns, one race one education, . variable labels organized predictable way, can use one convenient functions tidyverse’s tidyr library separate single variable two columns correctly preserving row values. Appropriately, function called separate().call separate() says take racedeg column, split value sees period, reorganize results two columns, race degree. gives us tidy table much like out_grp, marginal frequencies.Reasonable people can disagree best plot small multiple frequency table faceting year, especially measure uncertainty attached. barplot obvious approach single case, many years can become difficult compare bars across panels. especially case standard errors confidence intervals used conjunction bars.Sometimes may preferable show underlying variable categorical, bar chart makes clear, continuous, line graph suggests. trade-favor line graphs bars hard compare across facets. sometimes called “dynamite plot”, looks amazing t-shaped error bars tops columns make look like cartoon dynamite plungers. alternative use line graph join time observations, faceting educational categories instead year. Figure 6.12 shows results GSS data dynamite-plot form, error bars defined twice standard error either direction around point estimate.plot cosmetic details adjustments learn Chapter 8. , encourage peel back plot bottom, one instruction time, see changes. One useful adjustment notice new call scales library adjust labels x-axis. adjustment y-axis familiar, scales::percent convert proportion percentage. x-axis, issue several labels rather long. adjust print one another. scales::wrap_format() function break long labels lines. takes single numerical argument (10) maximum length string can wrapped onto new line.Faceting education instead.Figure 6.13: Faceting education instead.\ngraph like true categorical nature data, showing breakdown groups within year. experiment alternatives. example, might decide better facet degree category instead, put year x-axis within panel. , can use geom_line() show time trend, natural, geom_ribbon() show error range. perhaps better way show data, especially brings time trends within degree category, allows us see similarities differences racial classification time.","code":"\nlibrary(survey)\n#> Loading required package: grid\n#> Loading required package: Matrix\n#> \n#> Attaching package: 'Matrix'\n#> The following objects are masked from 'package:tidyr':\n#> \n#>     expand, pack, unpack\n#> \n#> Attaching package: 'survey'\n#> The following object is masked from 'package:graphics':\n#> \n#>     dotchart\nlibrary(srvyr)\n#> \n#> Attaching package: 'srvyr'\n#> The following object is masked from 'package:stats':\n#> \n#>     filter\noptions(survey.lonely.psu = \"adjust\")\noptions(na.action=\"na.pass\")\n\ngss_wt <- subset(gss_lon, year > 1974) %>%\n    mutate(stratvar = interaction(year, vstrat)) %>%\n    as_survey_design(ids = vpsu,\n                     strata = stratvar,\n                     weights = wtssall,\n                     nest = TRUE)\nout_grp <- gss_wt %>%\n    filter(year %in% seq(1976, 2016, by = 4)) %>%\n    group_by(year, race, degree) %>%\n    summarize(prop = survey_mean(na.rm = TRUE))\n#> Warning: Factor `degree` contains implicit NA, consider using\n#> `forcats::fct_explicit_na`\n#> Warning: Column `year` has different attributes on LHS and RHS of join\n\nout_grp\n#> # A tibble: 162 x 5\n#>    year race  degree            prop  prop_se\n#>   <dbl> <fct> <fct>            <dbl>    <dbl>\n#> 1  1976 White Lt High School  0.328   0.0160 \n#> 2  1976 White High School     0.518   0.0162 \n#> 3  1976 White Junior College  0.0129  0.00298\n#> 4  1976 White Bachelor        0.101   0.00960\n#> 5  1976 White Graduate        0.0393  0.00644\n#> 6  1976 White <NA>           NA      NA      \n#> # … with 156 more rows\nout_mrg <- gss_wt %>%\n    filter(year %in% seq(1976, 2016, by = 4)) %>%\n    mutate(racedeg = interaction(race, degree)) %>%\n    group_by(year, racedeg) %>%\n    summarize(prop = survey_mean(na.rm = TRUE))\n#> Warning: Factor `racedeg` contains implicit NA, consider using\n#> `forcats::fct_explicit_na`\n#> Warning: Column `year` has different attributes on LHS and RHS of join\n\nout_mrg\n#> # A tibble: 155 x 4\n#>    year racedeg                 prop prop_se\n#>   <dbl> <fct>                  <dbl>   <dbl>\n#> 1  1976 White.Lt High School 0.298   0.0146 \n#> 2  1976 Black.Lt High School 0.0471  0.00840\n#> 3  1976 Other.Lt High School 0.00195 0.00138\n#> 4  1976 White.High School    0.471   0.0160 \n#> 5  1976 Black.High School    0.0283  0.00594\n#> 6  1976 Other.High School    0.00325 0.00166\n#> # … with 149 more rows\nout_mrg <- gss_wt %>%\n    filter(year %in% seq(1976, 2016, by = 4)) %>%\n    mutate(racedeg = interaction(race, degree)) %>%\n    group_by(year, racedeg) %>%\n    summarize(prop = survey_mean(na.rm = TRUE)) %>%\n    separate(racedeg, sep = \"\\\\.\", into = c(\"race\", \"degree\"))\n#> Warning: Factor `racedeg` contains implicit NA, consider using\n#> `forcats::fct_explicit_na`\n#> Warning: Column `year` has different attributes on LHS and RHS of join\n\nout_mrg\n#> # A tibble: 155 x 5\n#>    year race  degree            prop prop_se\n#>   <dbl> <chr> <chr>            <dbl>   <dbl>\n#> 1  1976 White Lt High School 0.298   0.0146 \n#> 2  1976 Black Lt High School 0.0471  0.00840\n#> 3  1976 Other Lt High School 0.00195 0.00138\n#> 4  1976 White High School    0.471   0.0160 \n#> 5  1976 Black High School    0.0283  0.00594\n#> 6  1976 Other High School    0.00325 0.00166\n#> # … with 149 more rows\np <- ggplot(data = subset(out_grp, race %nin% \"Other\"),\n            mapping = aes(x = degree, y = prop,\n                          ymin = prop - 2*prop_se,\n                          ymax = prop + 2*prop_se,\n                          fill = race,\n                          color = race,\n                          group = race))\n\ndodge <- position_dodge(width=0.9)\n\np + geom_col(position = dodge, alpha = 0.2) +\n    geom_errorbar(position = dodge, width = 0.2) +\n    scale_x_discrete(labels = scales::wrap_format(10)) +\n    scale_y_continuous(labels = scales::percent) +\n    scale_color_brewer(type = \"qual\", palette = \"Dark2\") +\n    scale_fill_brewer(type = \"qual\", palette = \"Dark2\") +\n    labs(title = \"Educational Attainment by Race\",\n         subtitle = \"GSS 1976-2016\",\n         fill = \"Race\",\n         color = \"Race\",\n         x = NULL, y = \"Percent\") +\n    facet_wrap(~ year, ncol = 2) +\n    theme(legend.position = \"top\")\n#> Warning: Removed 13 rows containing missing values (geom_col).\np <- ggplot(data = subset(out_grp, race %nin% \"Other\"),\n            mapping = aes(x = year, y = prop, ymin = prop - 2*prop_se,\n                          ymax = prop + 2*prop_se, fill = race, color = race,\n                          group = race))\n\np + geom_ribbon(alpha = 0.3, aes(color = NULL)) +\n    geom_line() + \n    facet_wrap(~ degree, ncol = 1) +\n    scale_y_continuous(labels = scales::percent) +\n    scale_color_brewer(type = \"qual\", palette = \"Dark2\") +\n    scale_fill_brewer(type = \"qual\", palette = \"Dark2\") +\n    labs(title = \"Educational Attainment\\nby Race\",\n         subtitle = \"GSS 1976-2016\", fill = \"Race\",\n         color = \"Race\", x = NULL, y = \"Percent\") +\n    theme(legend.position = \"top\")\n#> Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n#> -Inf\n\n#> Warning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n#> -Inf\n#> Warning: Removed 13 row(s) containing missing values (geom_path)."},{"path":"data-visualization-for-ml-models.html","id":"where-to-go-next","chapter":"7 Data Visualization for ML models","heading":"7.10 Where to go next","text":"general, estimate models want plot results, difficult step plotting rather calculating extracting right numbers. Generating predicted values measures confidence uncertainty models requires understand model fitting, function use fit , especially involves interactions, cross-level effects, transformations predictor response scales. details can vary substantially model type model type, also goals particular analysis. unwise approach mechanically. said, several tools exist help work model objects produce default set plots .","code":""},{"path":"data-visualization-for-ml-models.html","id":"default-plots-for-models","chapter":"7 Data Visualization for ML models","heading":"7.10.1 Default plots for models","text":"Just model objects R usually default summary() method, printing overview tailored type model , usually default plot() method, . Figures produced plot() typically generated via ggplot, usually worth exploring . typically make use either R’s base graphics lattice library (Sarkar, 2008). two plotting systems cover book. Default plot methods easy examine. Let’s take look simple OLS model.look R’s default plots model, use plot() function.() statement selects first two four default plots kind model. want easily reproduce base R’s default model graphics using ggplot, ggfortify library worth examining. ways similar broom, tidies output model objects, focuses producing standard plot (group plots) wide variety model types. defining function called autoplot(). idea able use autoplot() output many different kinds model.second option worth looking coefplot library. provides quick way produce good-quality plots point estimates confidence intervals. advantage managing estimation interaction effects occasionally tricky calculations.","code":"\nout <- lm(formula = lifeExp ~ log(gdpPercap) + pop + continent, data = gapminder)\n# Plot not shown\nplot(out, which = c(1,2), ask=FALSE)\nlibrary(coefplot)\nout <- lm(formula = lifeExp ~ log(gdpPercap) + log(pop) + continent, data = gapminder)\n\ncoefplot(out, sort = \"magnitude\", intercept = FALSE)"},{"path":"data-visualization-for-ml-models.html","id":"tools-in-development","chapter":"7 Data Visualization for ML models","heading":"7.10.2 Tools in development","text":"Tidyverse tools modeling model exploration actively developed. broom margins libraries continue get useful. also projects worth paying attention . infer packageinfer.netlify.com early stages can already useful things pipeline-friendly way. can install CRAN install.packages(\"infer\").","code":""},{"path":"data-visualization-for-ml-models.html","id":"extensions-to-ggplot","chapter":"7 Data Visualization for ML models","heading":"7.10.3 Extensions to ggplot","text":"GGally package provides suite functions designed make producing standard somewhat complex plots little easier. instance, can produce generalized pairs plots, useful way quickly examining possible relationships several different variables . sort plot like visual version correlation matrix. shows bivariate plot pairs variables data. relatively straightforward variables continuous measures. Things get complex , often case social sciences, variables categorical otherwise limited range values can take. generalized pairs plot can handle cases. example, Figure ?? shows generalized pairs plot five variables organdata dataset.Multi-panel plots like intrinsically rich information. combined several within-panel types representation, modest number variables, can become quite complex. used less presentation finished work, although possible. often useful tool working researcher quickly investigate aspects dataset. goal pithily summarize single point one already knows, open things exploration.","code":"\nlibrary(GGally)\n\norgandata_sm <- organdata %>%\n    select(donors, pop_dens, pubhealth,\n           roads, consent_law)\n\nggpairs(data = organdata_sm,\n        mapping = aes(color = consent_law),\n        upper = list(continuous = wrap(\"density\"), combo = \"box_no_facet\"),\n        lower = list(continuous = wrap(\"points\"), combo = wrap(\"dot_no_facet\")))"},{"path":"ten-methods-to-assess-variable-importance.html","id":"ten-methods-to-assess-variable-importance","chapter":"8 Ten methods to assess Variable Importance","heading":"8 Ten methods to assess Variable Importance","text":"Datasets: GlaucomaMDatasets: GlaucomaMAlgorithms:\nPartition Trees\nRegularized Random Forest (RRF)\nLasso Regression\nLinear Regression\nRecursive Feature Elimination (RFE)\nGenetic Algorithm\nSimulated Annealing\nAlgorithms:Partition TreesRegularized Random Forest (RRF)Lasso RegressionLinear RegressionRecursive Feature Elimination (RFE)Genetic AlgorithmSimulated AnnealingSource:\nhttps://www.machinelearningplus.com/machine-learning/feature-selection/real-world datasets, fairly common columns \nnothing noise.better getting rid variables memory\nspace occupy, time computational esources going\ncost, especially large datasets.Sometimes, variable makes business sense, \nsure actually helps predicting Y. also need \nconsider fact , feature useful one ML\nalgorithm (say decision tree) may go underrepresented unused \nanother (like regression model).said , still possible variable shows poor\nsigns helping explain response variable (Y), can turn \nsignificantly useful presence (combination ) \npredictors. mean , variable might low\ncorrelation value (~0.2) Y. presence \nvariables, can help explain certain patterns/phenomenon \nvariables can’t explain.cases, can hard make call whether include \nexclude variables.strategies discuss can help fix problems. \n, also help understand particular variable \nimportant much contributing modelAn important caveat. always best variables sound\nbusiness logic backing inclusion variable rely solely \nvariable importance metrics.Alright. Let’s load ‘Glaucoma’ dataset goal \npredict patient Glaucoma based 63 different\nphysiological measurements. can directly run codes download\ndataset .lot interesting examples ahead. Let’s get started.","code":"\n# Load Packages and prepare dataset\nlibrary(DALEX)\n#> Welcome to DALEX (version: 1.2.0).\n#> Find examples and detailed introduction at: https://pbiecek.github.io/ema/\nlibrary(TH.data)\n#> Loading required package: survival\n#> Loading required package: MASS\n#> \n#> Attaching package: 'TH.data'\n#> The following object is masked from 'package:MASS':\n#> \n#>     geyser\nlibrary(caret)\n#> Loading required package: lattice\n#> Loading required package: ggplot2\n#> \n#> Attaching package: 'caret'\n#> The following object is masked from 'package:survival':\n#> \n#>     cluster\nlibrary(tictoc)\n\ndata(\"GlaucomaM\", package = \"TH.data\")\ntrainData <- GlaucomaM\nhead(trainData)\n#>      ag    at    as    an    ai   eag   eat   eas   ean   eai  abrg  abrt  abrs\n#> 2  2.22 0.354 0.580 0.686 0.601 1.267 0.336 0.346 0.255 0.331 0.479 0.260 0.107\n#> 43 2.68 0.475 0.672 0.868 0.667 2.053 0.440 0.520 0.639 0.454 1.090 0.377 0.257\n#> 25 1.98 0.343 0.508 0.624 0.504 1.200 0.299 0.396 0.259 0.246 0.465 0.209 0.112\n#> 65 1.75 0.269 0.476 0.525 0.476 0.612 0.147 0.017 0.044 0.405 0.170 0.062 0.000\n#> 70 2.99 0.599 0.686 1.039 0.667 2.513 0.543 0.607 0.871 0.492 1.800 0.431 0.494\n#> 16 2.92 0.483 0.763 0.901 0.770 2.200 0.462 0.637 0.504 0.597 1.311 0.394 0.365\n#>     abrn  abri    hic   mhcg  mhct   mhcs   mhcn   mhci   phcg   phct   phcs\n#> 2  0.014 0.098  0.214  0.111 0.412  0.036  0.105 -0.022 -0.139  0.242 -0.053\n#> 43 0.212 0.245  0.382  0.140 0.338  0.104  0.080  0.109 -0.015  0.296 -0.015\n#> 25 0.041 0.103  0.195  0.062 0.356  0.045 -0.009 -0.048 -0.149  0.206 -0.092\n#> 65 0.000 0.108 -0.030 -0.015 0.074 -0.084 -0.050  0.035 -0.182 -0.097 -0.125\n#> 70 0.601 0.274  0.383  0.089 0.233  0.145  0.023  0.007 -0.131  0.163  0.055\n#> 16 0.251 0.301  0.442  0.128 0.375  0.049  0.111  0.052 -0.088  0.281 -0.067\n#>      phcn   phci   hvc  vbsg  vbst  vbss  vbsn  vbsi  vasg  vast  vass  vasn\n#> 2   0.010 -0.139 0.613 0.303 0.103 0.088 0.022 0.090 0.062 0.000 0.011 0.032\n#> 43 -0.015  0.036 0.382 0.676 0.181 0.186 0.141 0.169 0.029 0.001 0.007 0.011\n#> 25 -0.081 -0.149 0.557 0.300 0.084 0.088 0.046 0.082 0.036 0.002 0.004 0.016\n#> 65 -0.138 -0.182 0.373 0.048 0.011 0.000 0.000 0.036 0.070 0.005 0.030 0.033\n#> 70 -0.131 -0.115 0.405 0.889 0.151 0.253 0.330 0.155 0.020 0.001 0.004 0.008\n#> 16 -0.062 -0.088 0.507 0.972 0.213 0.316 0.197 0.246 0.043 0.001 0.005 0.028\n#>     vasi  vbrg  vbrt  vbrs  vbrn  vbri  varg  vart  vars  varn  vari   mdg\n#> 2  0.018 0.075 0.039 0.021 0.002 0.014 0.756 0.009 0.209 0.298 0.240 0.705\n#> 43 0.010 0.370 0.127 0.099 0.050 0.093 0.410 0.006 0.105 0.181 0.117 0.898\n#> 25 0.013 0.081 0.034 0.019 0.007 0.021 0.565 0.014 0.132 0.243 0.177 0.687\n#> 65 0.002 0.005 0.001 0.000 0.000 0.004 0.380 0.032 0.147 0.151 0.050 0.207\n#> 70 0.007 0.532 0.103 0.173 0.181 0.075 0.228 0.011 0.026 0.105 0.087 0.721\n#> 16 0.009 0.467 0.136 0.148 0.078 0.104 0.540 0.008 0.133 0.232 0.167 0.927\n#>      mdt   mds   mdn   mdi    tmg    tmt    tms    tmn    tmi    mr   rnf  mdic\n#> 2  0.637 0.738 0.596 0.691 -0.236 -0.018 -0.230 -0.510 -0.158 0.841 0.410 0.137\n#> 43 0.850 0.907 0.771 0.940 -0.211 -0.014 -0.165 -0.317 -0.192 0.924 0.256 0.252\n#> 25 0.643 0.689 0.684 0.700 -0.185 -0.097 -0.235 -0.337 -0.020 0.795 0.378 0.152\n#> 65 0.171 0.022 0.046 0.221 -0.148 -0.035 -0.449 -0.217 -0.091 0.746 0.200 0.027\n#> 70 0.638 0.730 0.730 0.640 -0.052 -0.105  0.084 -0.012 -0.054 0.977 0.193 0.297\n#> 16 0.842 0.953 0.906 0.898 -0.040  0.087  0.018 -0.094 -0.051 0.965 0.339 0.333\n#>      emd    mv  Class\n#> 2  0.239 0.035 normal\n#> 43 0.329 0.022 normal\n#> 25 0.250 0.029 normal\n#> 65 0.078 0.023 normal\n#> 70 0.354 0.034 normal\n#> 16 0.442 0.028 normal"},{"path":"ten-methods-to-assess-variable-importance.html","id":"boruta","chapter":"8 Ten methods to assess Variable Importance","heading":"8.1 1. Boruta","text":"Boruta feature ranking selection algorithm based random\nforests algorithm. advantage Boruta clearly decides\nvariable important helps select variables \nstatistically significant. Besides, can adjust strictness \nalgorithm adjusting \\(p\\) values defaults 0.01 \nmaxRuns.maxRuns number times algorithm run. higher \nmaxRuns selective get picking variables. \ndefault value 100.process deciding feature important , \nfeatures may marked Boruta ‘Tentative’. Sometimes increasing\nmaxRuns can help resolve ‘Tentativeness’ feature.Lets see example based Glaucoma dataset TH.data package\ncreated earlier.boruta function uses formula interface just like predictive\nmodeling functions. first argument boruta() formula\nresponse variable left predictors \nright. placing dot, variables trainData \nClass included model.doTrace argument controls amount output printed \nconsole. Higher value, log details get. save space \nset 0, try setting 1 2 running \ncode.Finally output stored boruta_output.Let’s see boruta_output contains.sure tentative variables selected \ngranted, can choose TentativeRoughFix boruta_output.go. Boruta decided ‘Tentative’ variables \nbehalf. Let’s find importance scores variables.Let’s plot see importances variables.plot reveals importance features.columns green ‘confirmed’ ones red . \ncouple blue bars representing ShadowMax ShadowMin. \nactual features, used boruta algorithm \ndecide variable important .","code":"\n# install.packages('Boruta')\nlibrary(Boruta)\n#> Loading required package: ranger\n# Perform Boruta search\nboruta_output <- Boruta(Class ~ ., data=na.omit(trainData), doTrace=0)  \nnames(boruta_output)\n#>  [1] \"finalDecision\" \"ImpHistory\"    \"pValue\"        \"maxRuns\"      \n#>  [5] \"light\"         \"mcAdj\"         \"timeTaken\"     \"roughfixed\"   \n#>  [9] \"call\"          \"impSource\"\n# Get significant variables including tentatives\nboruta_signif <- getSelectedAttributes(boruta_output, withTentative = TRUE)\nprint(boruta_signif)  \n#>  [1] \"as\"   \"ai\"   \"eas\"  \"ean\"  \"abrg\" \"abrs\" \"abrn\" \"abri\" \"hic\"  \"mhcg\"\n#> [11] \"mhcs\" \"mhcn\" \"mhci\" \"phcg\" \"phcn\" \"phci\" \"hvc\"  \"vbsg\" \"vbss\" \"vbsn\"\n#> [21] \"vbsi\" \"vasg\" \"vass\" \"vasi\" \"vbrg\" \"vbrs\" \"vbrn\" \"vbri\" \"varg\" \"vart\"\n#> [31] \"vars\" \"varn\" \"vari\" \"mdn\"  \"tmg\"  \"tmt\"  \"tms\"  \"tmi\"  \"mr\"   \"rnf\" \n#> [41] \"mdic\" \"emd\"\n# Do a tentative rough fix\nroughFixMod <- TentativeRoughFix(boruta_output)\nboruta_signif <- getSelectedAttributes(roughFixMod)\nprint(boruta_signif)\n#>  [1] \"as\"   \"ai\"   \"ean\"  \"abrg\" \"abrs\" \"abrn\" \"abri\" \"hic\"  \"mhcg\" \"mhcn\"\n#> [11] \"mhci\" \"phcg\" \"phcn\" \"phci\" \"hvc\"  \"vbsn\" \"vbsi\" \"vasg\" \"vass\" \"vasi\"\n#> [21] \"vbrg\" \"vbrs\" \"vbrn\" \"vbri\" \"varg\" \"vart\" \"vars\" \"varn\" \"vari\" \"mdn\" \n#> [31] \"tmg\"  \"tms\"  \"tmi\"  \"mr\"   \"rnf\"  \"mdic\"\n# Variable Importance Scores\nimps <- attStats(roughFixMod)\nimps2 = imps[imps$decision != 'Rejected', c('meanImp', 'decision')]\nhead(imps2[order(-imps2$meanImp), ])  # descending sort\n#>      meanImp  decision\n#> vari   12.37 Confirmed\n#> varg   11.74 Confirmed\n#> vars   10.74 Confirmed\n#> phci    8.34 Confirmed\n#> hic     8.21 Confirmed\n#> varn    7.88 Confirmed\n# Plot variable importance\nplot(boruta_output, cex.axis=.7, las=2, xlab=\"\", main=\"Variable Importance\")  "},{"path":"ten-methods-to-assess-variable-importance.html","id":"variable-importance","chapter":"8 Ten methods to assess Variable Importance","heading":"8.2 2. Variable Importance","text":"Another way look feature selection consider variables \nused various ML algorithms important.Depending machine learning algorithm learns relationship\n\\(X\\)’s $Y$, different machine learning algorithms may\npossibly end using different variables (mostly common vars) \nvarious degrees.mean , variables proved useful tree-based\nalgorithm like rpart, can turn less useful \nregression-based model. variables need equally useful \nalgorithms.find variable importance given ML algo?train() desired model using caret package. , use\nvarImp() determine feature importance.may want try multiple algorithms, get feel \nusefulness features across algos.","code":""},{"path":"ten-methods-to-assess-variable-importance.html","id":"rpart","chapter":"8 Ten methods to assess Variable Importance","heading":"8.3 3. rpart","text":"5 63 features used rpart look closely, \n5 variables used top 6 boruta selected.Let’s one : variable importances Regularized Random\nForest (RRF) algorithm.","code":"\n# Train an rpart model and compute variable importance.\nlibrary(caret)\nset.seed(100)\nrPartMod <- train(Class ~ ., \n                  data=trainData, \n                  method=\"rpart\")\n\nrpartImp <- varImp(rPartMod)\nprint(rpartImp)\n#> rpart variable importance\n#> \n#>   only 20 most important variables shown (out of 62)\n#> \n#>      Overall\n#> varg   100.0\n#> vari    93.2\n#> vars    85.2\n#> varn    76.9\n#> tmi     72.3\n#> mhcn     0.0\n#> as       0.0\n#> phcs     0.0\n#> vbst     0.0\n#> abrt     0.0\n#> vbsg     0.0\n#> eai      0.0\n#> vbrs     0.0\n#> vbsi     0.0\n#> eag      0.0\n#> tmt      0.0\n#> phcn     0.0\n#> vart     0.0\n#> mds      0.0\n#> an       0.0"},{"path":"ten-methods-to-assess-variable-importance.html","id":"regularized-random-forest-rrf","chapter":"8 Ten methods to assess Variable Importance","heading":"8.4 4. Regularized Random Forest (RRF)","text":"topmost important variables pretty much top tier \nBoruta’s selections.algorithms available train() can use \ncompute varImp following:","code":"\ntic()\n# Train an RRF model and compute variable importance.\nset.seed(100)\nrrfMod <- train(Class ~ ., \n                data = trainData, \n                method = \"RRF\")\n#> Registered S3 method overwritten by 'RRF':\n#>   method      from        \n#>   plot.margin randomForest\n\nrrfImp <- varImp(rrfMod, scale=F)\ntoc()\n#> 383.138 sec elapsed\nrrfImp\n#> RRF variable importance\n#> \n#>   only 20 most important variables shown (out of 62)\n#> \n#>      Overall\n#> varg   25.07\n#> vari   18.78\n#> vars    5.29\n#> tmi     4.09\n#> mhcg    3.25\n#> mhci    2.81\n#> hic     2.69\n#> hvc     2.50\n#> mv      2.00\n#> vasg    1.99\n#> phci    1.77\n#> phcn    1.53\n#> phct    1.43\n#> vass    1.37\n#> phcg    1.37\n#> tms     1.32\n#> tmg     1.16\n#> abrs    1.16\n#> tmt     1.13\n#> mdic    1.13\nplot(rrfImp, top = 20, main='Variable Importance')ada, AdaBag, AdaBoost.M1, adaboost, bagEarth, bagEarthGCV, bagFDA, bagFDAGCV, bartMachine, blasso, BstLm, bstSm, C5.0, C5.0Cost, C5.0Rules, C5.0Tree, cforest, chaid, ctree, ctree2, cubist, deepboost, earth, enet, evtree, extraTrees, fda, gamboost, gbm_h2o, gbm, gcvEarth, glmnet_h2o, glmnet, glmStepAIC, J48, JRip, lars, lars2, lasso, LMT, LogitBoost, M5, M5Rules, msaenet, nodeHarvest, OneR, ordinalNet, ORFlog, ORFpls, ORFridge, ORFsvm, pam, parRF, PART, penalized, PenalizedLDA, qrf, ranger, Rborist, relaxo, rf, rFerns, rfRules, rotationForest, rotationForestCp, rpart, rpart1SE, rpart2, rpartCost, rpartScore, rqlasso, rqnc, RRF, RRFglobal, sdwd, smda, sparseLDA, spikeslab, wsrf, xgbLinear, xgbTree."},{"path":"ten-methods-to-assess-variable-importance.html","id":"lasso-regression","chapter":"8 Ten methods to assess Variable Importance","heading":"8.5 5. Lasso Regression","text":"Least Absolute Shrinkage Selection Operator (LASSO) regression \ntype regularization method penalizes L1-norm.basically imposes cost large weights (value \ncoefficients). called L1 regularization, cost added,\nproportional absolute value weight coefficients.result, process shrinking coefficients, eventually\nreduces coefficients certain unwanted features zero.\n, removes unneeded variables altogether.effectively, LASSO regression can considered variable\nselection technique well.Let’s see interpret plot.X axis plot log lambda. means 2\n, lambda value actually 100.numbers top plot show many predictors \nincluded model. position red dots along Y-axis tells\nAUC got include many variables shown top\nx-axis.can also see two dashed vertical lines.first one left points lambda lowest mean\nsquared error. one right point number variables\nhighest deviance within 1 standard deviation.best lambda value stored inside ‘cv.lasso$lambda.min’.output shows variables LASSO considered important. high\npositive low negative implies important variable.","code":"\nlibrary(glmnet)\n#> Loading required package: Matrix\n#> Loaded glmnet 3.0-2\n\n# online data\n# trainData <- read.csv('https://raw.githubusercontent.com/selva86/datasets/master/GlaucomaM.csv')\n\ntrainData <- read.csv(file.path(data_raw_dir, \"glaucoma.csv\"))\n\nx <- as.matrix(trainData[,-63]) # all X vars\ny <- as.double(as.matrix(ifelse(trainData[, 63]=='normal', 0, 1))) # Only Class\n\n# Fit the LASSO model (Lasso: Alpha = 1)\nset.seed(100)\ncv.lasso <- cv.glmnet(x, y, family='binomial', alpha=1, parallel=TRUE, standardize=TRUE, type.measure='auc')\n#> Warning: executing %dopar% sequentially: no parallel backend registered\n\n# Results\nplot(cv.lasso)\n# plot(cv.lasso$glmnet.fit, xvar=\"lambda\", label=TRUE)\ncat('Min Lambda: ', cv.lasso$lambda.min, '\\n 1Sd Lambda: ', cv.lasso$lambda.1se)\n#> Min Lambda:  0.0224 \n#>  1Sd Lambda:  0.144\ndf_coef <- round(as.matrix(coef(cv.lasso, s=cv.lasso$lambda.min)), 2)\n\n# See all contributing variables\ndf_coef[df_coef[, 1] != 0, ]\n#> (Intercept)          as        mhci        phci         hvc        vast \n#>        2.68       -1.59        3.85        5.60       -2.41      -13.90 \n#>        vars        vari         mdn         mdi         tmg         tms \n#>      -20.18       -1.58        0.50        0.99        0.06        2.56 \n#>         tmi \n#>        2.23"},{"path":"ten-methods-to-assess-variable-importance.html","id":"step-wise-forward-and-backward-selection","chapter":"8 Ten methods to assess Variable Importance","heading":"8.6 6. Step wise Forward and Backward Selection","text":"Stepwise regression can used select features Y variable \nnumeric variable. particularly used selecting best linear\nregression models.searches best possible regression model iteratively\nselecting dropping variables arrive model lowest\npossible AIC.can implemented using step() function need \nprovide lower model, base model \nwon’t remove features upper model, full model \npossible features want .case complicated (< 20 vars), lets just simple\nstepwise ‘’ directions.use ozone dataset objective \npredict ozone_reading based weather related observations.data ready. Let’s perform stepwise.selected model 6 features .many features (> 100) training data, \nmight good idea split dataset chunks 10 variables\nY mandatory dataset. Loop chunks\ncollect best features.way variables came important\ntraining data fewer features may show linear reg\nmodel built lots features.Finally, pool shortlisted features (small chunk models),\nrun full stepwise model get final set selected features.can take learning assignment solved within 20\nminutes.","code":"\n# Load data\n# online\n# trainData <- read.csv(\"http://rstatistics.net/wp-content/uploads/2015/09/ozone1.csv\",\n#                      stringsAsFactors=F)\ntrainData <- read.csv(file.path(data_raw_dir, \"ozone1.csv\"))\nprint(head(trainData))\n#>   Month Day_of_month Day_of_week ozone_reading pressure_height Wind_speed\n#> 1     1            1           4             3            5480          8\n#> 2     1            2           5             3            5660          6\n#> 3     1            3           6             3            5710          4\n#> 4     1            4           7             5            5700          3\n#> 5     1            5           1             5            5760          3\n#> 6     1            6           2             6            5720          4\n#>   Humidity Temperature_Sandburg Temperature_ElMonte Inversion_base_height\n#> 1       20                 40.5                39.8                  5000\n#> 2       41                 38.0                46.7                  4109\n#> 3       28                 40.0                49.5                  2693\n#> 4       37                 45.0                52.3                   590\n#> 5       51                 54.0                45.3                  1450\n#> 6       69                 35.0                49.6                  1568\n#>   Pressure_gradient Inversion_temperature Visibility\n#> 1               -15                  30.6        200\n#> 2               -14                  48.0        300\n#> 3               -25                  47.7        250\n#> 4               -24                  55.0        100\n#> 5                25                  57.0         60\n#> 6                15                  53.8         60\n# Step 1: Define base intercept only model\nbase.mod <- lm(ozone_reading ~ 1 , data=trainData)  \n\n# Step 2: Full model with all predictors\nall.mod <- lm(ozone_reading ~ . , data= trainData) \n\n# Step 3: Perform step-wise algorithm. direction='both' implies both forward and backward stepwise\nstepMod <- step(base.mod, scope = list(lower = base.mod, upper = all.mod), direction = \"both\", trace = 0, steps = 1000)  \n\n# Step 4: Get the shortlisted variable.\nshortlistedVars <- names(unlist(stepMod[[1]])) \nshortlistedVars <- shortlistedVars[!shortlistedVars %in% \"(Intercept)\"] # remove intercept\n\n# Show\nprint(shortlistedVars)\n#> [1] \"Temperature_Sandburg\"  \"Humidity\"              \"Temperature_ElMonte\"  \n#> [4] \"Month\"                 \"pressure_height\"       \"Inversion_base_height\""},{"path":"ten-methods-to-assess-variable-importance.html","id":"relative-importance-from-linear-regression","chapter":"8 Ten methods to assess Variable Importance","heading":"8.7 7. Relative Importance from Linear Regression","text":"technique specific linear regression models.Relative importance can used assess variables contributed\nmuch explaining linear model’s R-squared value. , \nsum produced importances, add model’s R-sq\nvalue.essence, directly feature selection method, \nalready provided features go model. \nbuilding model, relaimpo can provide sense important\nfeature contributing R-sq, words, \n‘explaining Y variable’., calculate relative importance?implemented relaimpo package. Basically, build \nlinear regression model pass main argument \ncalc.relimp(). relaimpo multiple options compute relative\nimportance, recommended method use type='lmg', \ndone .Additionally, can use bootstrapping (using boot.relimp) compute\nconfidence intervals produced relative importances.","code":"\n# install.packages('relaimpo')\nlibrary(relaimpo)\n#> Loading required package: boot\n#> \n#> Attaching package: 'boot'\n#> The following object is masked from 'package:lattice':\n#> \n#>     melanoma\n#> The following object is masked from 'package:survival':\n#> \n#>     aml\n#> Loading required package: survey\n#> Loading required package: grid\n#> \n#> Attaching package: 'survey'\n#> The following object is masked from 'package:graphics':\n#> \n#>     dotchart\n#> Loading required package: mitools\n#> This is the global version of package relaimpo.\n#> If you are a non-US user, a version with the interesting additional metric pmvd is available\n#> from Ulrike Groempings web site at prof.beuth-hochschule.de/groemping.\n\n# Build linear regression model\nmodel_formula = ozone_reading ~ Temperature_Sandburg + Humidity + Temperature_ElMonte + Month + pressure_height + Inversion_base_height\nlmMod <- lm(model_formula, data=trainData)\n\n# calculate relative importance\nrelImportance <- calc.relimp(lmMod, type = \"lmg\", rela = F)  \n\n# Sort\ncat('Relative Importances: \\n')\n#> Relative Importances:\nsort(round(relImportance$lmg, 3), decreasing=TRUE)\n#>   Temperature_ElMonte  Temperature_Sandburg       pressure_height \n#>                 0.214                 0.203                 0.104 \n#> Inversion_base_height              Humidity                 Month \n#>                 0.096                 0.086                 0.012\nbootsub <- boot.relimp(ozone_reading ~ Temperature_Sandburg + Humidity + Temperature_ElMonte + Month + pressure_height + Inversion_base_height, data=trainData,\n                       b = 1000, type = 'lmg', rank = TRUE, diff = TRUE)\n\nplot(booteval.relimp(bootsub, level=.95))"},{"path":"ten-methods-to-assess-variable-importance.html","id":"recursive-feature-elimination-rfe","chapter":"8 Ten methods to assess Variable Importance","heading":"8.8 8. Recursive Feature Elimination (RFE)","text":"Recursive feature elimnation (rfe) offers rigorous way determine\nimportant variables even feed ML algo.can implemented using rfe() caret package.rfe() also takes two important parameters.sizesrfeControlSo, sizes rfeControl represent?sizes determines number important features rfe\niterate. , set size 1 5, 10, 15 18.Secondly, rfeControl parameter receives output \nrfeControl(). can set type variable evaluation algorithm\nmust used. , used random forests based rfFuncs. \nmethod='repeatedCV' means repeated k-Fold cross\nvalidation repeats=5.complete, get accuracy kappa model size \nprovided. final selected model subset size marked * \nrightmost selected column., says, Temperature_ElMonte, Pressure_gradient,\nTemperature_Sandburg, Inversion_temperature, Humidity top 5\nvariables order.best model size provided models sizes (subsets) \n10.can see top 10 variables ‘lmProfile$optVariables’\ncreated using rfe function .","code":"\nstr(trainData)\n#> 'data.frame':    366 obs. of  13 variables:\n#>  $ Month                : int  1 1 1 1 1 1 1 1 1 1 ...\n#>  $ Day_of_month         : int  1 2 3 4 5 6 7 8 9 10 ...\n#>  $ Day_of_week          : int  4 5 6 7 1 2 3 4 5 6 ...\n#>  $ ozone_reading        : num  3 3 3 5 5 6 4 4 6 7 ...\n#>  $ pressure_height      : num  5480 5660 5710 5700 5760 5720 5790 5790 5700 5700 ...\n#>  $ Wind_speed           : int  8 6 4 3 3 4 6 3 3 3 ...\n#>  $ Humidity             : num  20 41 28 37 51 ...\n#>  $ Temperature_Sandburg : num  40.5 38 40 45 54 ...\n#>  $ Temperature_ElMonte  : num  39.8 46.7 49.5 52.3 45.3 ...\n#>  $ Inversion_base_height: num  5000 4109 2693 590 1450 ...\n#>  $ Pressure_gradient    : num  -15 -14 -25 -24 25 15 -33 -28 23 -2 ...\n#>  $ Inversion_temperature: num  30.6 48 47.7 55 57 ...\n#>  $ Visibility           : int  200 300 250 100 60 60 100 250 120 120 ...\ntic()\nset.seed(100)\noptions(warn=-1)\n\nsubsets <- c(1:5, 10, 15, 18)\n\nctrl <- rfeControl(functions = rfFuncs,\n                   method = \"repeatedcv\",\n                   repeats = 5,\n                   verbose = FALSE)\n\nlmProfile <- rfe(x=trainData[, c(1:3, 5:13)], y=trainData$ozone_reading,\n                 sizes = subsets,\n                 rfeControl = ctrl)\ntoc()\n#> 95.356 sec elapsed\nlmProfile\n#> \n#> Recursive feature selection\n#> \n#> Outer resampling method: Cross-Validated (10 fold, repeated 5 times) \n#> \n#> Resampling performance over subset size:\n#> \n#>  Variables RMSE Rsquared  MAE RMSESD RsquaredSD MAESD Selected\n#>          1 5.13    0.595 3.92  0.826     0.1275 0.586         \n#>          2 4.03    0.746 3.11  0.542     0.0743 0.416         \n#>          3 3.95    0.756 3.06  0.472     0.0670 0.380         \n#>          4 3.93    0.759 3.01  0.468     0.0683 0.361         \n#>          5 3.90    0.763 2.98  0.467     0.0659 0.350         \n#>         10 3.77    0.782 2.85  0.496     0.0734 0.393        *\n#>         12 3.77    0.781 2.86  0.508     0.0756 0.401         \n#> \n#> The top 5 variables (out of 10):\n#>    Temperature_ElMonte, Pressure_gradient, Temperature_Sandburg, Inversion_temperature, Humidity"},{"path":"ten-methods-to-assess-variable-importance.html","id":"genetic-algorithm","chapter":"8 Ten methods to assess Variable Importance","heading":"8.9 9. Genetic Algorithm","text":"can perform supervised feature selection genetic algorithms\nusing gafs(). quite resource expensive consider\nchoosing number iterations (iters) number \nrepeats gafsControl().‘Month’ ‘Day_of_month’ ‘Wind_speed’ ‘Temperature_ElMonte’\n‘Pressure_gradient’ ‘Visibility’optimal variables according genetic algorithms listed\n. , wouldn’t use just yet , variant \ntuned 3 iterations, quite low. set low\nsave computing time.","code":"\ntic()\n# Define control function\nga_ctrl <- gafsControl(functions = rfGA,  # another option is `caretGA`.\n                        method = \"cv\",\n                        repeats = 3)\n\n# Genetic Algorithm feature selection\nset.seed(100)\nga_obj <- gafs(x=trainData[, c(1:3, 5:13)], \n               y=trainData[, 4], \n               iters = 3,   # normally much higher (100+)\n               gafsControl = ga_ctrl)\ntoc()\n#> 635.279 sec elapsed\nga_obj\n#> \n#> Genetic Algorithm Feature Selection\n#> \n#> 366 samples\n#> 12 predictors\n#> \n#> Maximum generations: 3 \n#> Population per generation: 50 \n#> Crossover probability: 0.8 \n#> Mutation probability: 0.1 \n#> Elitism: 0 \n#> \n#> Internal performance values: RMSE, Rsquared\n#> Subset selection driven to minimize internal RMSE \n#> \n#> External performance values: RMSE, Rsquared, MAE\n#> Best iteration chose by minimizing external RMSE \n#> External resampling method: Cross-Validated (10 fold) \n#> \n#> During resampling:\n#>   * the top 5 selected variables (out of a possible 12):\n#>     Month (100%), Pressure_gradient (100%), Temperature_ElMonte (100%), Humidity (80%), Visibility (80%)\n#>   * on average, 6.8 variables were selected (min = 5, max = 9)\n#> \n#> In the final search using the entire training set:\n#>    * 9 features selected at iteration 2 including:\n#>      Month, Day_of_month, pressure_height, Wind_speed, Humidity ... \n#>    * external performance at this iteration is\n#> \n#>       RMSE   Rsquared        MAE \n#>      3.721      0.788      2.800\n# Optimal variables\nga_obj$optVariables\n#> [1] \"Month\"                 \"Day_of_month\"          \"pressure_height\"      \n#> [4] \"Wind_speed\"            \"Humidity\"              \"Temperature_ElMonte\"  \n#> [7] \"Inversion_base_height\" \"Pressure_gradient\"     \"Inversion_temperature\""},{"path":"ten-methods-to-assess-variable-importance.html","id":"simulated-annealing","chapter":"8 Ten methods to assess Variable Importance","heading":"8.10 10. Simulated Annealing","text":"Simulated annealing global search algorithm allows \nsuboptimal solution accepted hope better solution \nshow eventually.works making small random changes initial solution sees\nperformance improved. change accepted improves, else\ncan still accepted difference performances meet \nacceptance criteria.caret implemented safs() accepts control\nparameter can set using safsControl() function.safsControl similar control functions caret (like \nsaw rfe ga), additionally accepts improve parameter\nnumber iterations wait without improvement\nvalues reset previous iteration.","code":"\ntic()\n# Define control function\nsa_ctrl <- safsControl(functions = rfSA,\n                        method = \"repeatedcv\",\n                        repeats = 3,\n                        improve = 5) # n iterations without improvement before a reset\n\n# Genetic Algorithm feature selection\nset.seed(100)\nsa_obj <- safs(x=trainData[, c(1:3, 5:13)], \n               y=trainData[, 4],\n               safsControl = sa_ctrl)\ntoc()\n#> 108.312 sec elapsed\nsa_obj\n#> \n#> Simulated Annealing Feature Selection\n#> \n#> 366 samples\n#> 12 predictors\n#> \n#> Maximum search iterations: 10 \n#> Restart after 5 iterations without improvement (0.3 restarts on average)\n#> \n#> Internal performance values: RMSE, Rsquared\n#> Subset selection driven to minimize internal RMSE \n#> \n#> External performance values: RMSE, Rsquared, MAE\n#> Best iteration chose by minimizing external RMSE \n#> External resampling method: Cross-Validated (10 fold, repeated 3 times) \n#> \n#> During resampling:\n#>   * the top 5 selected variables (out of a possible 12):\n#>     Temperature_Sandburg (80%), Month (66.7%), Pressure_gradient (66.7%), Temperature_ElMonte (63.3%), Visibility (60%)\n#>   * on average, 6.5 variables were selected (min = 3, max = 11)\n#> \n#> In the final search using the entire training set:\n#>    * 6 features selected at iteration 9 including:\n#>      Day_of_week, pressure_height, Wind_speed, Humidity, Inversion_base_height ... \n#>    * external performance at this iteration is\n#> \n#>       RMSE   Rsquared        MAE \n#>      4.108      0.743      3.111\n# Optimal variables\nprint(sa_obj$optVariables)\n#> [1] \"Day_of_week\"           \"pressure_height\"       \"Wind_speed\"           \n#> [4] \"Humidity\"              \"Inversion_base_height\" \"Pressure_gradient\""},{"path":"ten-methods-to-assess-variable-importance.html","id":"information-value-and-weights-of-evidence","chapter":"8 Ten methods to assess Variable Importance","heading":"8.11 Information Value and Weights of Evidence","text":"Information Value can used judge important given\ncategorical variable explaining binary Y variable. goes\nwell logistic regression classification models can\nmodel binary variables.Let’s try find important categorical variables \npredicting individual earn > 50k adult.csv\ndataset. Just run code import dataset.quantum Information Value means:Less 0.02, predictor useful modeling\n(separating Goods Bads)0.02 0.1, predictor weak relationship. 0.1 \n0.3, predictor medium strength relationship. 0.3 \nhigher, predictor strong relationship. IV.\nWeight Evidence?Weights evidence can useful find important given\ncategorical variable explaining ‘events’ (called ‘Goods’ \ntable.)‘Information Value’ categorical variable can derived\nrespective WOE values.IV=(perc good goods−perc bad bads) *WOEThe ‘WOETable’ given computation detail.total IV variable sum IV’s categories.","code":"\nlibrary(InformationValue)\n#> \n#> Attaching package: 'InformationValue'\n#> The following objects are masked from 'package:caret':\n#> \n#>     confusionMatrix, precision, sensitivity, specificity\n\n# online data\n# inputData <- read.csv(\"http://rstatistics.net/wp-content/uploads/2015/09/adult.csv\")\n\ninputData <- read.csv(file.path(data_raw_dir, \"adult.csv\"))\nprint(head(inputData))\n#>   AGE        WORKCLASS FNLWGT EDUCATION EDUCATIONNUM      MARITALSTATUS\n#> 1  39        State-gov  77516 Bachelors           13      Never-married\n#> 2  50 Self-emp-not-inc  83311 Bachelors           13 Married-civ-spouse\n#> 3  38          Private 215646   HS-grad            9           Divorced\n#> 4  53          Private 234721      11th            7 Married-civ-spouse\n#> 5  28          Private 338409 Bachelors           13 Married-civ-spouse\n#> 6  37          Private 284582   Masters           14 Married-civ-spouse\n#>          OCCUPATION  RELATIONSHIP  RACE    SEX CAPITALGAIN CAPITALLOSS\n#> 1      Adm-clerical Not-in-family White   Male        2174           0\n#> 2   Exec-managerial       Husband White   Male           0           0\n#> 3 Handlers-cleaners Not-in-family White   Male           0           0\n#> 4 Handlers-cleaners       Husband Black   Male           0           0\n#> 5    Prof-specialty          Wife Black Female           0           0\n#> 6   Exec-managerial          Wife White Female           0           0\n#>   HOURSPERWEEK NATIVECOUNTRY ABOVE50K\n#> 1           40 United-States        0\n#> 2           13 United-States        0\n#> 3           40 United-States        0\n#> 4           40 United-States        0\n#> 5           40          Cuba        0\n#> 6           40 United-States        0\n# Choose Categorical Variables to compute Info Value.\ncat_vars <- c (\"WORKCLASS\", \"EDUCATION\", \"MARITALSTATUS\", \"OCCUPATION\", \"RELATIONSHIP\", \"RACE\", \"SEX\", \"NATIVECOUNTRY\")  # get all categorical variables\n\nfactor_vars <- cat_vars\n\n\n# Init Output\ndf_iv <- data.frame(VARS=cat_vars, IV=numeric(length(cat_vars)), STRENGTH=character(length(cat_vars)), stringsAsFactors = F)  # init output dataframe\n\n# Get Information Value for each variable\nfor (factor_var in factor_vars){\n  df_iv[df_iv$VARS == factor_var, \"IV\"] <- InformationValue::IV(X=inputData[, factor_var], Y=inputData$ABOVE50K)\n  df_iv[df_iv$VARS == factor_var, \"STRENGTH\"] <- attr(InformationValue::IV(X=inputData[, factor_var], Y=inputData$ABOVE50K), \"howgood\")\n}\n\n# Sort\ndf_iv <- df_iv[order(-df_iv$IV), ]\n\ndf_iv\n#>            VARS     IV            STRENGTH\n#> 5  RELATIONSHIP 1.5356   Highly Predictive\n#> 3 MARITALSTATUS 1.3388   Highly Predictive\n#> 4    OCCUPATION 0.7762   Highly Predictive\n#> 2     EDUCATION 0.7411   Highly Predictive\n#> 7           SEX 0.3033   Highly Predictive\n#> 1     WORKCLASS 0.1634   Highly Predictive\n#> 8 NATIVECOUNTRY 0.0794 Somewhat Predictive\n#> 6          RACE 0.0693 Somewhat Predictive\nWOETable(X=inputData[, 'WORKCLASS'], Y=inputData$ABOVE50K)\n#>                CAT GOODS  BADS TOTAL   PCT_G    PCT_B    WOE       IV\n#> 1                ?   191  1645  1836 0.02429 0.066545 -1.008 0.042574\n#> 2      Federal-gov   371   589   960 0.04719 0.023827  0.683 0.015964\n#> 3        Local-gov   617  1476  2093 0.07848 0.059709  0.273 0.005131\n#> 4     Never-worked     7     7     7 0.00089 0.000283  1.146 0.000696\n#> 5          Private  4963 17733 22696 0.63126 0.717354 -0.128 0.011006\n#> 6     Self-emp-inc   622   494  1116 0.07911 0.019984  1.376 0.081363\n#> 7 Self-emp-not-inc   724  1817  2541 0.09209 0.073503  0.225 0.004190\n#> 8        State-gov   353   945  1298 0.04490 0.038228  0.161 0.001073\n#> 9      Without-pay    14    14    14 0.00178 0.000566  1.146 0.001391"},{"path":"ten-methods-to-assess-variable-importance.html","id":"dalex-package","chapter":"8 Ten methods to assess Variable Importance","heading":"8.12 DALEX Package","text":"DALEX powerful package explains various things \nvariables used ML model.example, using variable_dropout() function can find \nimportant variable based dropout loss, much\nloss incurred removing variable model.Apart , also single_variable() function gives\nidea model’s output change changing values\none X’s model.also single_prediction() can decompose single model\nprediction understand variable caused effect \npredicting value Y.","code":"\nlibrary(randomForest)\n#> randomForest 4.6-14\n#> Type rfNews() to see new features/changes/bug fixes.\n#> \n#> Attaching package: 'randomForest'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     combine\n#> The following object is masked from 'package:ranger':\n#> \n#>     importance\n#> The following object is masked from 'package:ggplot2':\n#> \n#>     margin\nlibrary(DALEX)\n\n# Load data\n# inputData <- read.csv(\"http://rstatistics.net/wp-content/uploads/2015/09/adult.csv\")\n\ninputData <- read.csv(file.path(data_raw_dir, \"adult.csv\"))\n\n# Train random forest model\nrf_mod <- randomForest(factor(ABOVE50K) ~ ., data = inputData, ntree=100)\nrf_mod\n#> \n#> Call:\n#>  randomForest(formula = factor(ABOVE50K) ~ ., data = inputData,      ntree = 100) \n#>                Type of random forest: classification\n#>                      Number of trees: 100\n#> No. of variables tried at each split: 3\n#> \n#>         OOB estimate of  error rate: 13.6%\n#> Confusion matrix:\n#>       0    1 class.error\n#> 0 23051 1669      0.0675\n#> 1  2754 5087      0.3512\n\n# Variable importance with DALEX\nexplained_rf <- explain(rf_mod, data=inputData, y=inputData$ABOVE50K)\n#> Preparation of a new explainer is initiated\n#>   -> model label       :  randomForest  (  default  )\n#>   -> data              :  32561  rows  15  cols \n#>   -> target variable   :  32561  values \n#>   -> model_info        :  package randomForest , ver. 4.6.14 , task classification (  default  ) \n#>   -> predict function  :  yhat.randomForest  will be used (  default  )\n#>   -> predicted values  :  numerical, min =  0 , mean =  0.237 , max =  1  \n#>   -> residual function :  difference between y and yhat (  default  )\n#>   -> residuals         :  numerical, min =  -0.94 , mean =  0.00374 , max =  0.93  \n#>   A new explainer has been created! \n\n# Get the variable importances\n# varimps = variable_dropout(explained_rf, type='raw')\nvarimps = variable_importance(explained_rf, type='raw')\nprint(varimps)\n#>         variable mean_dropout_loss        label\n#> 1   _full_model_              36.8 randomForest\n#> 2       ABOVE50K              36.8 randomForest\n#> 3           RACE              41.1 randomForest\n#> 4  NATIVECOUNTRY              43.6 randomForest\n#> 5            SEX              46.4 randomForest\n#> 6    CAPITALLOSS              46.9 randomForest\n#> 7      WORKCLASS              55.8 randomForest\n#> 8      EDUCATION              62.1 randomForest\n#> 9   EDUCATIONNUM              63.3 randomForest\n#> 10        FNLWGT              64.0 randomForest\n#> 11  RELATIONSHIP              64.9 randomForest\n#> 12  HOURSPERWEEK              68.6 randomForest\n#> 13   CAPITALGAIN              69.3 randomForest\n#> 14 MARITALSTATUS              73.3 randomForest\n#> 15    OCCUPATION              85.1 randomForest\n#> 16           AGE              85.2 randomForest\n#> 17    _baseline_             299.5 randomForest\nplot(varimps)"},{"path":"ten-methods-to-assess-variable-importance.html","id":"conclusion","chapter":"8 Ten methods to assess Variable Importance","heading":"8.13 Conclusion","text":"Hope find methods useful. turns different methods\nshowed different variables important, least degree \nimportance changed. need conflict, method\ngives different perspective variable can useful\ndepending algorithms learn Y ~ x. cool.find code breaks bugs, report issue just write\n.","code":""},{"path":"employee-attrition-using-feature-importance.html","id":"employee-attrition-using-feature-importance","chapter":"9 Employee Attrition using Feature Importance","heading":"9 Employee Attrition using Feature Importance","text":"Datasets: WA_Fn-UseC_-HR-Employee-Attrition.xlsxAlgorithms:\nAutoML (h2o)\nAutoML (h2o)ML Packages: h2o, LIME, DALEXArticle: https://www.business-science.io/business/2017/09/18/hr_employee_attrition.html\nData: https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/","code":""},{"path":"employee-attrition-using-feature-importance.html","id":"introduction-3","chapter":"9 Employee Attrition using Feature Importance","heading":"9.1 Introduction","text":"","code":""},{"path":"employee-attrition-using-feature-importance.html","id":"employee-attrition-a-major-problem","chapter":"9 Employee Attrition using Feature Importance","heading":"9.1.1 Employee attrition: a major problem","text":"Bill Gates quoted saying,“take away top 20 employees [Microsoft] become mediocre company”.statement cuts core major problem: employee attrition. organization good employees, people true source competitive advantage.Organizations face huge costs resulting employee turnover. costs tangible training expenses time takes employee starts become productive member. However, important costs intangible. Consider ’s lost productive employee quits: new product ideas, great project management, customer relationships.advances machine learning data science, possible predict employee attrition understand key variables influence turnover. ’ll take look two cutting edge techniques:Machine Learning h2o.automl() h2o package: function takes automated machine learning next level testing number advanced algorithms random forests, ensemble methods, deep learning along traditional algorithms logistic regression. main takeaway can now easily achieve predictive performance ball park (cases even better ) commercial algorithms ML/AI software.Machine Learning h2o.automl() h2o package: function takes automated machine learning next level testing number advanced algorithms random forests, ensemble methods, deep learning along traditional algorithms logistic regression. main takeaway can now easily achieve predictive performance ball park (cases even better ) commercial algorithms ML/AI software.Feature Importance lime package: problem advanced machine learning algorithms deep learning ’s near impossible understand algorithm complexity. changed lime package. major advancement lime , recursively analyzing models locally, can extract feature importance repeats globally. means us lime opened door understanding ML models regardless complexity. Now best (typically complex) models can also investigated potentially understood variables features make model tick.Feature Importance lime package: problem advanced machine learning algorithms deep learning ’s near impossible understand algorithm complexity. changed lime package. major advancement lime , recursively analyzing models locally, can extract feature importance repeats globally. means us lime opened door understanding ML models regardless complexity. Now best (typically complex) models can also investigated potentially understood variables features make model tick.","code":""},{"path":"employee-attrition-using-feature-importance.html","id":"employee-attrition-machine-learning-analysis","chapter":"9 Employee Attrition using Feature Importance","heading":"9.1.2 Employee attrition: machine learning analysis","text":"new automated ML tools combined tools uncover critical variables, now capabilities extreme predictive accuracy understandability, previously impossible! ’ll investigate HR Analytic example employee attrition evaluated IBM Watson.","code":""},{"path":"employee-attrition-using-feature-importance.html","id":"where-we-got-the-data","chapter":"9 Employee Attrition using Feature Importance","heading":"9.1.3 Where we got the data","text":"example comes IBM Watson Analytics website. can download data read analysis :Get data used post .\nRead IBM Watson Analytics article .\nsummarize, article makes usage case IBM Watson automated ML platform. article shows using Watson, analyst able detect features led increased probability attrition.","code":""},{"path":"employee-attrition-using-feature-importance.html","id":"automated-machine-learning-what-we-did-with-the-data","chapter":"9 Employee Attrition using Feature Importance","heading":"9.1.4 Automated machine learning (what we did with the data)","text":"example ’ll show can use combination H2O developing complex model high predictive accuracy unseen data can use LIME understand important features related employee attrition.","code":""},{"path":"employee-attrition-using-feature-importance.html","id":"load-packages","chapter":"9 Employee Attrition using Feature Importance","heading":"9.1.5 Load packages","text":"Load following packages.","code":"\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(DALEX)\nlibrary(tidyquant)  # Loads tidyverse and several other pkgs \nlibrary(readxl)     # Super simple excel reader\nlibrary(h2o)        # Professional grade ML pkg\nlibrary(lime)       # Explain complex black-box ML models"},{"path":"employee-attrition-using-feature-importance.html","id":"load-data","chapter":"9 Employee Attrition using Feature Importance","heading":"9.1.6 Load data","text":"Download data . can load data using read_excel(), pointing path local file.Let’s check raw data. ’s 1470 rows (observations) 35 columns (features). “Attrition” column target. ’ll use columns features model.Table 9.1: First 10 rowsThe pre-processing ’ll example change character data types factors. needed H2O. make number numeric data actually categorical factors, tends increase modeling time can little improvement model performance.Let’s take glimpse processed dataset. can see columns. Note target (“Attrition”) first column.","code":"\n# Read excel data\nhr_data_raw <- read_excel(path = file.path(data_raw_dir,\n                                           \"WA_Fn-UseC_-HR-Employee-Attrition.xlsx\"))\n# View first 10 rows\nhr_data_raw[1:10,] %>%\n    knitr::kable(caption = \"First 10 rows\")\nhr_data <- hr_data_raw %>%\n    mutate_if(is.character, as.factor) %>%\n    select(Attrition, everything())\nglimpse(hr_data)\n#> Rows: 1,470\n#> Columns: 35\n#> $ Attrition                <fct> Yes, No, Yes, No, No, No, No, No, No, No, No…\n#> $ Age                      <dbl> 41, 49, 37, 33, 27, 32, 59, 30, 38, 36, 35, …\n#> $ BusinessTravel           <fct> Travel_Rarely, Travel_Frequently, Travel_Rar…\n#> $ DailyRate                <dbl> 1102, 279, 1373, 1392, 591, 1005, 1324, 1358…\n#> $ Department               <fct> Sales, Research & Development, Research & De…\n#> $ DistanceFromHome         <dbl> 1, 8, 2, 3, 2, 2, 3, 24, 23, 27, 16, 15, 26,…\n#> $ Education                <dbl> 2, 1, 2, 4, 1, 2, 3, 1, 3, 3, 3, 2, 1, 2, 3,…\n#> $ EducationField           <fct> Life Sciences, Life Sciences, Other, Life Sc…\n#> $ EmployeeCount            <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n#> $ EmployeeNumber           <dbl> 1, 2, 4, 5, 7, 8, 10, 11, 12, 13, 14, 15, 16…\n#> $ EnvironmentSatisfaction  <dbl> 2, 3, 4, 4, 1, 4, 3, 4, 4, 3, 1, 4, 1, 2, 3,…\n#> $ Gender                   <fct> Female, Male, Male, Female, Male, Male, Fema…\n#> $ HourlyRate               <dbl> 94, 61, 92, 56, 40, 79, 81, 67, 44, 94, 84, …\n#> $ JobInvolvement           <dbl> 3, 2, 2, 3, 3, 3, 4, 3, 2, 3, 4, 2, 3, 3, 2,…\n#> $ JobLevel                 <dbl> 2, 2, 1, 1, 1, 1, 1, 1, 3, 2, 1, 2, 1, 1, 1,…\n#> $ JobRole                  <fct> Sales Executive, Research Scientist, Laborat…\n#> $ JobSatisfaction          <dbl> 4, 2, 3, 3, 2, 4, 1, 3, 3, 3, 2, 3, 3, 4, 3,…\n#> $ MaritalStatus            <fct> Single, Married, Single, Married, Married, S…\n#> $ MonthlyIncome            <dbl> 5993, 5130, 2090, 2909, 3468, 3068, 2670, 26…\n#> $ MonthlyRate              <dbl> 19479, 24907, 2396, 23159, 16632, 11864, 996…\n#> $ NumCompaniesWorked       <dbl> 8, 1, 6, 1, 9, 0, 4, 1, 0, 6, 0, 0, 1, 0, 5,…\n#> $ Over18                   <fct> Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y,…\n#> $ OverTime                 <fct> Yes, No, Yes, Yes, No, No, Yes, No, No, No, …\n#> $ PercentSalaryHike        <dbl> 11, 23, 15, 11, 12, 13, 20, 22, 21, 13, 13, …\n#> $ PerformanceRating        <dbl> 3, 4, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 3, 3, 3,…\n#> $ RelationshipSatisfaction <dbl> 1, 4, 2, 3, 4, 3, 1, 2, 2, 2, 3, 4, 4, 3, 2,…\n#> $ StandardHours            <dbl> 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, …\n#> $ StockOptionLevel         <dbl> 0, 1, 0, 0, 1, 0, 3, 1, 0, 2, 1, 0, 1, 1, 0,…\n#> $ TotalWorkingYears        <dbl> 8, 10, 7, 8, 6, 8, 12, 1, 10, 17, 6, 10, 5, …\n#> $ TrainingTimesLastYear    <dbl> 0, 3, 3, 3, 3, 2, 3, 2, 2, 3, 5, 3, 1, 2, 4,…\n#> $ WorkLifeBalance          <dbl> 1, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 3, 3,…\n#> $ YearsAtCompany           <dbl> 6, 10, 0, 8, 2, 7, 1, 1, 9, 7, 5, 9, 5, 2, 4…\n#> $ YearsInCurrentRole       <dbl> 4, 7, 0, 7, 2, 7, 0, 0, 7, 7, 4, 5, 2, 2, 2,…\n#> $ YearsSinceLastPromotion  <dbl> 0, 1, 0, 3, 2, 3, 0, 0, 1, 7, 0, 0, 4, 1, 0,…\n#> $ YearsWithCurrManager     <dbl> 5, 7, 0, 0, 2, 6, 0, 0, 8, 7, 3, 8, 3, 2, 3,…"},{"path":"employee-attrition-using-feature-importance.html","id":"modeling-employee-attrition","chapter":"9 Employee Attrition using Feature Importance","heading":"9.2 Modeling Employee attrition","text":"going use h2o.automl() function H2O platform model employee attrition.","code":""},{"path":"employee-attrition-using-feature-importance.html","id":"machine-learning-with-h2o","chapter":"9 Employee Attrition using Feature Importance","heading":"9.2.1 Machine Learning with h2o","text":"First, need initialize Java Virtual Machine (JVM) H2O uses locally.Next, change data h2o object package can interpret. also split data training, validation, test sets. preference use 70%, 15%, 15%, respectively.","code":"\n# Initialize H2O JVM\nh2o.init()\n#> \n#> H2O is not running yet, starting it now...\n#> \n#> Note:  In case of errors look at the following log files:\n#>     /tmp/RtmprKhXua/filea825113d6f4/h2o_UnknownUser_started_from_r.out\n#>     /tmp/RtmprKhXua/filea827a89ae0d/h2o_UnknownUser_started_from_r.err\n#> \n#> \n#> Starting H2O JVM and connecting: . Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         1 seconds 375 milliseconds \n#>     H2O cluster timezone:       Etc/UTC \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.30.0.1 \n#>     H2O cluster version age:    7 months and 16 days !!! \n#>     H2O cluster name:           H2O_started_from_R_root_mwl453 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   7.82 GB \n#>     H2O cluster total cores:    8 \n#>     H2O cluster allowed cores:  8 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4 \n#>     R Version:                  R version 3.6.3 (2020-02-29)\nh2o.no_progress() # Turn off output of progress bars\n# Split data into Train/Validation/Test Sets\nhr_data_h2o <- as.h2o(hr_data)\n\nsplit_h2o <- h2o.splitFrame(hr_data_h2o, c(0.7, 0.15), seed = 1234 )\ntrain_h2o <- h2o.assign(split_h2o[[1]], \"train\" ) # 70%\nvalid_h2o <- h2o.assign(split_h2o[[2]], \"valid\" ) # 15%\ntest_h2o  <- h2o.assign(split_h2o[[3]], \"test\" )  # 15%"},{"path":"employee-attrition-using-feature-importance.html","id":"model","chapter":"9 Employee Attrition using Feature Importance","heading":"9.3 Model","text":"Now ready model. ’ll set target feature names. target aim predict (case “Attrition”). features (every column) use model prediction.Now fun begins. run h2o.automl() setting arguments needs run models . information, see h2o.automl documentation.x = x: names feature columns.y = y: name target column.training_frame = train_h2o: training set consisting 70% data.leaderboard_frame = valid_h2o: validation set consisting 15% data. H2O uses ensure model overfit data.max_runtime_secs = 30: supply speed H2O’s modeling. algorithm large number complex models want keep things moving expense accuracy.models stored automl_models_h2o object. However, concerned leader, best model terms accuracy validation set. ’ll extract models object.","code":"\n# Set names for h2o\ny <- \"Attrition\"\nx <- setdiff(names(train_h2o), y)\n# Run the automated machine learning \nautoml_models_h2o <- h2o.automl(\n    x = x, \n    y = y,\n    training_frame    = train_h2o,\n    leaderboard_frame = valid_h2o,\n    max_runtime_secs  = 30\n    )\n# Extract leader model\nautoml_leader <- automl_models_h2o@leader"},{"path":"employee-attrition-using-feature-importance.html","id":"predict","chapter":"9 Employee Attrition using Feature Importance","heading":"9.4 Predict","text":"Now ready predict test set, unseen modeling process. true test performance. use h2o.predict() function make predictions.","code":"\n# Predict on hold-out set, test_h2o\npred_h2o <- h2o.predict(object = automl_leader, newdata = test_h2o)"},{"path":"employee-attrition-using-feature-importance.html","id":"performance","chapter":"9 Employee Attrition using Feature Importance","heading":"9.5 Performance","text":"Now can evaluate leader model. ’ll reformat test set add predictions column actual prediction columns side--side.can use table() function quickly get confusion table results. see leader model wasn’t perfect, decent job identifying employees likely quit. perspective, logistic regression perform nearly well.’ll run binary classification analysis understand model performance.important understand accuracy can misleading: 88% sounds pretty good especially modeling HR data, just pick Attrition = get accuracy 79%. Doesn’t sound great now.make final judgement, let’s dive little deeper precision recall. Precision model predicts yes, often actually yes. Recall (also true positive rate specificity) actual value yes often model correct. Confused yet? Let’s explain terms ’s important HR.HR groups probably prefer incorrectly classify folks looking quit high potential quiting rather classify likely quit risk. ’s important miss risk employees, HR really care recall actual value Attrition = YES often model predicts YES.Recall model 62%. HR context, 62% employees potentially targeted prior quiting. standpoint, organization loses 100 people per year possibly target 62 implementing measures retain.","code":"\n# Prep for performance assessment\ntest_performance <- test_h2o %>%\n    tibble::as_tibble() %>%\n    select(Attrition) %>%\n    add_column(pred = as.vector(pred_h2o$predict)) %>%\n    mutate_if(is.character, as.factor)\ntest_performance\n#> # A tibble: 211 x 2\n#>   Attrition pred \n#>   <fct>     <fct>\n#> 1 No        No   \n#> 2 No        No   \n#> 3 Yes       Yes  \n#> 4 No        No   \n#> 5 No        No   \n#> 6 No        No   \n#> # … with 205 more rows\n# Confusion table counts\nconfusion_matrix <- test_performance %>%\n    table() \nconfusion_matrix\n#>          pred\n#> Attrition  No Yes\n#>       No  169  13\n#>       Yes  15  14\n# Performance analysis\ntn <- confusion_matrix[1]\ntp <- confusion_matrix[4]\nfp <- confusion_matrix[3]\nfn <- confusion_matrix[2]\n\naccuracy <- (tp + tn) / (tp + tn + fp + fn)\nmisclassification_rate <- 1 - accuracy\nrecall <- tp / (tp + fn)\nprecision <- tp / (tp + fp)\nnull_error_rate <- tn / (tp + tn + fp + fn)\n\ntibble(\n    accuracy,\n    misclassification_rate,\n    recall,\n    precision,\n    null_error_rate\n) %>% \n    transpose() \n#> [[1]]\n#> [[1]]$accuracy\n#> [1] 0.867\n#> \n#> [[1]]$misclassification_rate\n#> [1] 0.133\n#> \n#> [[1]]$recall\n#> [1] 0.483\n#> \n#> [[1]]$precision\n#> [1] 0.519\n#> \n#> [[1]]$null_error_rate\n#> [1] 0.801"},{"path":"employee-attrition-using-feature-importance.html","id":"the-lime-package","chapter":"9 Employee Attrition using Feature Importance","heading":"9.6 The lime package","text":"good model capable making accurate predictions unseen data, can tell us causes attrition? Let’s find using LIME.","code":""},{"path":"employee-attrition-using-feature-importance.html","id":"set-up","chapter":"9 Employee Attrition using Feature Importance","heading":"9.6.1 Set up","text":"lime package implements LIME R. One thing note ’s setup ---box work h2o. good news functions can get everything working properly. ’ll need make two custom functions:model_type: Used tell lime type model dealing . classification, regression, survival, etc.model_type: Used tell lime type model dealing . classification, regression, survival, etc.predict_model: Used allow lime perform predictions algorithm can interpret.predict_model: Used allow lime perform predictions algorithm can interpret.first thing need identify class model leader object. class() function.Next create model_type function. ’s input x h2o model. function simply returns “classification”, tells LIME classifying.Now can create predict_model function. trick realize ’s inputs must x model, newdata dataframe object (important), type used can use switch output type. output also little tricky must format probabilities classification (important; shown next). Internally just call h2o.predict() function.Run next script show output looks like test predict_model function. See ’s probabilities classification. must form model_type = “classification”.Now fun part, create explainer using lime() function. Just pass training data set without “Attribution column”. form must data frame, OK since predict_model function switch h2o object. Setmodel = automl_leader leader model, bin_continuous = FALSE. tell algorithm bin continuous variables, may make sense categorical numeric data didn’t change factors.Now run explain() function, returns explanation. can take minute run limit just first ten rows test data set. set n_labels = 1 care explaining single class. Setting n_features = 4 returns top four features critical case. Finally, setting kernel_width = 0.5 allows us increase “model_r2” value shrinking localized evaluation.","code":"\nclass(automl_leader)\n#> [1] \"H2OBinomialModel\"\n#> attr(,\"package\")\n#> [1] \"h2o\"\n# Setup lime::model_type() function for h2o\nmodel_type.H2OBinomialModel <- function(x, ...) {\n    # Function tells lime() what model type we are dealing with\n    # 'classification', 'regression', 'survival', 'clustering', 'multilabel', etc\n    #\n    # x is our h2o model\n    \n    return(\"classification\")\n}\n# Setup lime::predict_model() function for h2o\npredict_model.H2OBinomialModel <- function(x, newdata, type, ...) {\n    # Function performs prediction and returns dataframe with Response\n    #\n    # x is h2o model\n    # newdata is data frame\n    # type is only setup for data frame\n    \n    pred <- h2o.predict(x, as.h2o(newdata))\n    \n    # return probs\n    return(as.data.frame(pred[,-1]))\n    \n}\n# Test our predict_model() function\npredict_model(x = automl_leader, newdata = as.data.frame(test_h2o[,-1]), type = 'raw') %>%\n    tibble::as_tibble()\n#> # A tibble: 211 x 2\n#>       No    Yes\n#>    <dbl>  <dbl>\n#> 1 0.843  0.157 \n#> 2 0.955  0.0447\n#> 3 0.0514 0.949 \n#> 4 0.971  0.0289\n#> 5 0.891  0.109 \n#> 6 0.962  0.0378\n#> # … with 205 more rows\n# Run lime() on training set\nexplainer <- lime::lime(\n    as.data.frame(train_h2o[,-1]), \n    model          = automl_leader, \n    bin_continuous = FALSE)\n#> Warning: Data contains numeric columns with zero variance\n#> Warning: PerformanceRating does not contain enough variance to use quantile\n#> binning. Using standard binning instead.\n# Run explain() on explainer\nexplanation <- lime::explain(\n    as.data.frame(test_h2o[1:10,-1]), \n    explainer    = explainer, \n    n_labels     = 1, \n    n_features   = 4,\n    kernel_width = 0.5)"},{"path":"employee-attrition-using-feature-importance.html","id":"feature-importance-visualization","chapter":"9 Employee Attrition using Feature Importance","heading":"9.7 Feature Importance Visualization","text":"payoff work put using LIME feature importance plot. allows us visualize ten cases (observations) test data. top four features case shown. Note case. green bars mean feature supports model conclusion, red bars contradict. ’ll focus Cases Label = Yes, predicted attrition. can see common theme Case 3 Case 7: Training Time, Job Role, Time among top factors influencing attrition. two cases, can used potentially generalize larger population see next.","code":"\nplot_features(explanation) +\n    labs(title = \"HR Predictive Analytics: LIME Feature Importance Visualization\",\n         subtitle = \"Hold Out (Test) Set, First 10 Cases Shown\")"},{"path":"employee-attrition-using-feature-importance.html","id":"what-features-are-linked-to-employee-attrition","chapter":"9 Employee Attrition using Feature Importance","heading":"9.7.1 What features are linked to employee attrition","text":"Now turn three critical features LIME Feature Importance Plot:Training TimeJob RoleOver TimeWe’ll subset data visualize detect trends.","code":"\n# Focus on critical features of attrition\nattrition_critical_features <- hr_data %>%\n    tibble::as_tibble() %>%\n    select(Attrition, TrainingTimesLastYear, JobRole, OverTime) %>%\n    rowid_to_column(var = \"Case\")\nattrition_critical_features\n#> # A tibble: 1,470 x 5\n#>    Case Attrition TrainingTimesLastYear JobRole               OverTime\n#>   <int> <fct>                     <dbl> <fct>                 <fct>   \n#> 1     1 Yes                           0 Sales Executive       Yes     \n#> 2     2 No                            3 Research Scientist    No      \n#> 3     3 Yes                           3 Laboratory Technician Yes     \n#> 4     4 No                            3 Research Scientist    Yes     \n#> 5     5 No                            3 Laboratory Technician No      \n#> 6     6 No                            2 Laboratory Technician No      \n#> # … with 1,464 more rows"},{"path":"employee-attrition-using-feature-importance.html","id":"training","chapter":"9 Employee Attrition using Feature Importance","heading":"9.7.2 Training","text":"violin plot, employees stay tend large peaks two three trainings per year whereas employees leave tend large peak two trainings per year. suggests employees trainings may less likely leave.","code":"\nggplot(attrition_critical_features, aes(x = Attrition, \n                                        y = TrainingTimesLastYear)) +\n    geom_violin()  +\n    geom_jitter(alpha = 0.25)\nattrition_critical_features %>%\n    ggplot(aes(Attrition, TrainingTimesLastYear)) +\n    geom_jitter(alpha = 0.5, fill = palette_light()[[1]]) +\n    geom_violin(alpha = 0.7, fill = palette_light()[[1]]) +\n    theme_tq() +\n    labs(\n    title = \"Prevalance of Training is Lower in Attrition = Yes\",\n    subtitle = \"Suggests that increased training is related to lower attrition\"\n    )"},{"path":"employee-attrition-using-feature-importance.html","id":"overtime","chapter":"9 Employee Attrition using Feature Importance","heading":"9.7.3 Overtime","text":"plot shows interesting relationship: high proportion employees turnover working time. opposite true employees stay.","code":"\nattrition_critical_features %>% \n    mutate(OverTime = case_when(\n                        OverTime == \"Yes\" ~ 1,\n                        OverTime == \"No\"  ~ 0 )) %>% \n    ggplot(aes(Attrition, OverTime)) +\n    geom_jitter(alpha = 0.5, fill = palette_light()[[1]]) + \n    geom_violin(alpha = 0.7, fill = palette_light()[[1]]) + \n    theme_tq() + \n    labs(\n    title = \"Prevalance of Over Time is Higher in Attrition = Yes\",\n    subtitle = \"Suggests that increased overtime is related to higher attrition\")\nggplot(attrition_critical_features, aes(x = Attrition, \n                                        y = OverTime,\n                                        )) +\n    # geom_violin(aes(y = ..prop.., group = 1)) +\n    geom_jitter(alpha = 0.5)\n    "},{"path":"employee-attrition-using-feature-importance.html","id":"job-role","chapter":"9 Employee Attrition using Feature Importance","heading":"9.7.4 Job Role","text":"Several job roles experiencing turnover. Sales reps highest turnover 40% followed Lab Technician, Human Resources, Sales Executive, Research Scientist. may worthwhile investigate localized issues creating high turnover among groups within organization.","code":"\np <- ggplot(data = subset(attrition_critical_features, Attrition == \"Yes\"),\n            mapping = aes(x = JobRole))\np + geom_bar(mapping = aes(y = ..prop.., group = 1)) + \n    coord_flip()\n\n# geom_bar(mapping = aes(y = ..prop.., group = 1)) \np <- ggplot(data = attrition_critical_features,\n            mapping = aes(x = JobRole))\np + geom_bar(mapping = aes(y = ..prop.., group = 1)) + \n    coord_flip() +\n    facet_wrap(Attrition ~ .)\nattrition_critical_features %>%\n    group_by(JobRole, Attrition) %>% \n    summarize(total = n())\n#> # A tibble: 18 x 3\n#> # Groups:   JobRole [9]\n#>   JobRole                   Attrition total\n#>   <fct>                     <fct>     <int>\n#> 1 Healthcare Representative No          122\n#> 2 Healthcare Representative Yes           9\n#> 3 Human Resources           No           40\n#> 4 Human Resources           Yes          12\n#> 5 Laboratory Technician     No          197\n#> 6 Laboratory Technician     Yes          62\n#> # … with 12 more rows\nattrition_critical_features %>%\n    group_by(JobRole, Attrition) %>% \n    summarize(total = n()) %>% \n    spread(key = Attrition, value = total) %>% \n    mutate(pct_attrition = Yes / (Yes + No))\n#> # A tibble: 9 x 4\n#> # Groups:   JobRole [9]\n#>   JobRole                      No   Yes pct_attrition\n#>   <fct>                     <int> <int>         <dbl>\n#> 1 Healthcare Representative   122     9        0.0687\n#> 2 Human Resources              40    12        0.231 \n#> 3 Laboratory Technician       197    62        0.239 \n#> 4 Manager                      97     5        0.0490\n#> 5 Manufacturing Director      135    10        0.0690\n#> 6 Research Director            78     2        0.025 \n#> # … with 3 more rows\nattrition_critical_features %>%\n    group_by(JobRole, Attrition) %>% \n    summarize(total = n()) %>%\n    spread(key = Attrition, value = total) %>%\n    mutate(pct_attrition = Yes / (Yes + No)) %>%\n    ggplot(aes(x = forcats::fct_reorder(JobRole, pct_attrition), y = pct_attrition)) +\n    geom_bar(stat = \"identity\", alpha = 1, fill = palette_light()[[1]]) +\n    expand_limits(y = c(0, 1)) +\n    coord_flip() +\n    theme_tq() +\n    labs(\n        title = \"Attrition Varies By Job Role\",\n        subtitle = \"Sales Rep, Lab Tech, HR, Sales Exec, and Research Scientist \n        have much higher turnover\",\n        y = \"Attrition Percentage (Yes / Total)\",\n        x = \"JobRole\"\n    )"},{"path":"employee-attrition-using-feature-importance.html","id":"conclusions","chapter":"9 Employee Attrition using Feature Importance","heading":"9.8 Conclusions","text":"’s lot take away article. showed can use predictive analytics develop sophisticated models accurately detect employees risk turnover. autoML algorithm H2O.ai worked well classifying attrition accuracy around 87% unseen / unmodeled data. used LIME breakdown complex ensemble model returned H2O critical features related attrition. Overall, really useful example can see machine learning data science can used business applications.","code":""},{"path":"a-gentle-introduction-to-support-vector-machines.html","id":"a-gentle-introduction-to-support-vector-machines","chapter":"10 A gentle introduction to Support Vector Machines","heading":"10 A gentle introduction to Support Vector Machines","text":"","code":""},{"path":"a-gentle-introduction-to-support-vector-machines.html","id":"introduction-4","chapter":"10 A gentle introduction to Support Vector Machines","heading":"10.1 Introduction","text":"Source: https://eight2late.wordpress.com/2017/02/07/-gentle-introduction--support-vector-machines-using-r/machine learning algorithms involve minimising error measure kind (measure often called objective function loss function). example, error measure linear regression problems famous mean squared error – .e. averaged sum squared differences predicted actual values. Like mean squared error, objective functions depend points training dataset. post, describe support vector machine (SVM) approach focuses instead finding optimal separation boundary datapoints different classifications. ’ll elaborate means next section.’s plan brief. ’ll begin rationale behind SVMs using simple case binary (two class) dataset simple separation boundary (’ll clarify “simple” means minute). Following , ’ll describe can generalised datasets complex boundaries. Finally, ’ll work couple examples R, illustrating principles behind SVMs. line general philosophy “Gentle Introduction Data Science Using R” series, focus developing intuitive understanding algorithm along practical demonstration use toy example.","code":""},{"path":"a-gentle-introduction-to-support-vector-machines.html","id":"the-rationale","chapter":"10 A gentle introduction to Support Vector Machines","heading":"10.2 The rationale","text":"basic idea behind SVMs best illustrated considering simple case: set data points belong one two classes, red blue, illustrated figure 1 . make things simpler still, assumed boundary separating two classes straight line, represented solid green line diagram. technical literature, datasets called linearly separable.linearly separable case, usually fair amount freedom way separating line can drawn. Figure 2 illustrates point: two broken green lines also valid separation boundaries. Indeed, non-zero distance two closest points categories, infinite number possible separation lines. , quite naturally, raises question whether possible choose separation boundary optimal.short answer , yes . One way select boundary line maximises margin, .e. distance separation boundary points closest . optimal boundary illustrated black brace Figure 3. really cool thing criterion location separation boundary depends points closest . means, unlike classification methods, classifier depend points dataset. directed lines boundary closest points either side called support vectors (solid black lines figure 3). direct implication fewer support vectors, better generalizability boundary.\nFigure 10.1: Figure 3\nAlthough sounds great, limited practical value real data sets seldom (ever) linearly separable., can dealing real (.e. non linearly separable) data sets?simple approach tackle small deviations linear separability allow small number points (close boundary) misclassified. number possible misclassifications governed free parameter C, called cost. cost essentially penalty associated making error: higher value C, less likely algorithm misclassify point.approach – called soft margin classification – illustrated Figure 4. Note points wrong side separation boundary. demonstrate soft margin SVMs next section. (Note: risk belabouring obvious, purely linearly separable case discussed previous para simply special case soft margin classifier.)Real life situations much complex dealt using soft margin classifiers. example, shown Figure 5, one widely separated clusters points belong classes. situations, require use multiple (nonlinear) boundaries, can sometimes dealt using clever approach called kernel trick.","code":"\nprint(assets_dir)\n#> [1] \"../assets\"\nlist.files(assets_dir)\n#>  [1] \"linear_regression.jpg\"   \"logistic_regression.jpg\"\n#>  [3] \"neural_networks.jpg\"     \"nn_1h_layer.png\"        \n#>  [5] \"nn_no_hidden_layer.png\"  \"nn_polynomial.png\"      \n#>  [7] \"packages.bib\"            \"softmax_regression.jpg\" \n#>  [9] \"svm-fig-1.png\"           \"svm-fig-2.png\"          \n#> [11] \"svm-fig-3.png\"           \"svm-fig-4.png\"          \n#> [13] \"svm-fig-5.png\""},{"path":"a-gentle-introduction-to-support-vector-machines.html","id":"the-kernel-trick","chapter":"10 A gentle introduction to Support Vector Machines","heading":"10.3 The kernel trick","text":"Recall linearly separable (soft margin) case, SVM algorithm works finding separation boundary maximises margin, distance boundary points closest . distance usual straight line distance boundary closest point(s). called Euclidean distance honour great geometer antiquity. point note process results separation boundary straight line, Figure 5 illustrates, always work. fact cases won’t.can ? answer question, take bit detour…able generalize notion distance way generates nonlinear separation boundaries? turns possible. see , one first understand notion distance can generalized.key properties measure distance must satisfy :mathematical object displays properties akin distance. generalized distances called metrics mathematical space live called metric space. Metrics defined using special mathematical functions designed satisfy conditions. functions known kernels.essence kernel trick lies mapping classification problem metric space problem rendered separable via separation boundary simple new space, complex – – original one. Generally, transformed space higher dimensionality, dimensions (possibly complex) combinations original problem variables. However, necessarily problem practice one doesn’t actually mess around transformations, one just tries different kernels (transformation implicit kernel) sees one job. check simple: simply test predictions resulting using different kernels held subset data (one machine learning algorithm).turns particular function – called radial basis function kernel (RBF kernel) – effective many cases. RBF kernel essentially Gaussian (Normal) function Euclidean distance pairs points variable (see equation 1 ). basic rationale behind RBF kernel creates separation boundaries tends classify points close together (Euclidean sense) original space way. reflected fact kernel decays (.e. drops zero) Euclidean distance points increases.rate kernel decays governed parameter \\(\\gamma\\) – higher value \\(\\gamma\\), rapid decay. serves illustrate RBF kernel extremely flexible….flexibility comes price – danger overfitting large values \\(\\gamma\\) . One choose appropriate values C \\(\\gamma\\) ensure resulting kernel represents best possible balance flexibility accuracy. ’ll discuss done practice later article.Finally, though probably obvious, worth mentioning separation boundaries arbitrary kernels also defined support vectors Figure 3. reiterate point made earlier, means solution fewer support vectors likely robust one many. ? data points defining support vectors ones sensitive noise- therefore fewer, better.many types kernels, pros cons. However, ’ll leave adventurous readers explore . Finally, much detailed….dare say, better… explanation kernel trick, highly recommend article Eric Kim.","code":"Non-negativity – a distance cannot be negative, a point that needs no further explanation I reckon 🙂\nSymmetry – that is, the distance between point A and point B is the same as the distance between point B and point A.\nIdentity– the distance between a point and itself is zero.\nTriangle inequality – that is the sum of distances between point A and B and points B and C must be less than or equal to the distance between A and C (equality holds only if all three points lie along the same line)."},{"path":"a-gentle-introduction-to-support-vector-machines.html","id":"support-vector-machines-in-r","chapter":"10 A gentle introduction to Support Vector Machines","heading":"10.4 Support vector machines in R","text":"demo ’ll use svm interface implemented e1071 R package. interface provides R programmers access comprehensive libsvm library written Chang Lin. ’ll use two toy datasets: famous iris dataset available base R package sonar dataset mlbench package. won’t describe details datasets discussed length documentation linked . However, worth mentioning reasons chose datasets:mentioned earlier, real life dataset linearly separable, iris dataset almost . Consequently, good illustration using linear SVMs. Although one almost never uses practice, illustrated use primarily pedagogical reasons.\nsonar dataset good illustration benefits using RBF kernels cases dataset hard visualise (60 variables case!). general, one almost always use RBF (nonlinear) kernels practice.said, let’s get right . assume R RStudio installed. instructions , look first article series. processing preliminaries – loading libraries, data creating training test datasets much previous articles won’t dwell . completeness, however, ’ll list code can run directly R R studio (complete listing code can found ):","code":""},{"path":"a-gentle-introduction-to-support-vector-machines.html","id":"svm-on-the-iris-dataset","chapter":"10 A gentle introduction to Support Vector Machines","heading":"10.5 SVM on the iris dataset","text":"","code":""},{"path":"a-gentle-introduction-to-support-vector-machines.html","id":"training-and-test-datasets","chapter":"10 A gentle introduction to Support Vector Machines","heading":"10.5.1 Training and test datasets","text":"","code":"\n#load required library\nlibrary(e1071)\n\n#load built-in iris dataset\ndata(iris)\n\n#set seed to ensure reproducible results\nset.seed(42)\n\n#split into training and test sets\niris[, \"train\"] <- ifelse(runif(nrow(iris)) < 0.8, 1, 0)\n\n#separate training and test sets\ntrainset <- iris[iris$train == 1,]\ntestset <- iris[iris$train == 0,]\n\n#get column index of train flag\ntrainColNum <- grep(\"train\", names(trainset))\n\n#remove train flag column from train and test sets\ntrainset <- trainset[,-trainColNum]\ntestset <- testset[,-trainColNum]\n\ndim(trainset)\n#> [1] 115   5\ndim(testset)\n#> [1] 35  5"},{"path":"a-gentle-introduction-to-support-vector-machines.html","id":"building-the-svm-model","chapter":"10 A gentle introduction to Support Vector Machines","heading":"10.5.2 Building the SVM model","text":"output SVM model show 24 support vectors. desired, can examined using SV variable model – .e via svm_model$SV.","code":"\n#get column index of predicted variable in dataset\ntypeColNum <- grep(\"Species\", names(iris))\n\n#build model – linear kernel and C-classification (soft margin) with default cost (C=1)\nsvm_model <- svm(Species~ ., data = trainset, \n                 method = \"C-classification\", \n                 kernel = \"linear\")\nsvm_model\n#> \n#> Call:\n#> svm(formula = Species ~ ., data = trainset, method = \"C-classification\", \n#>     kernel = \"linear\")\n#> \n#> \n#> Parameters:\n#>    SVM-Type:  C-classification \n#>  SVM-Kernel:  linear \n#>        cost:  1 \n#> \n#> Number of Support Vectors:  24"},{"path":"a-gentle-introduction-to-support-vector-machines.html","id":"support-vectors","chapter":"10 A gentle introduction to Support Vector Machines","heading":"10.5.3 Support Vectors","text":"test prediction accuracy indicates linear performs quite well dataset, confirming indeed near linearly separable. check performance class, one can create confusion matrix described post random forests. ’ll leave exercise . Another point used soft-margin classification scheme cost C=1. can experiment explicitly changing value C. , ’ll leave exercise.","code":"\n# support vectors\nsvm_model$SV\n#>     Sepal.Length Sepal.Width Petal.Length Petal.Width\n#> 19       -0.2564      1.7668       -1.323      -1.305\n#> 42       -1.7006     -1.7045       -1.559      -1.305\n#> 45       -0.9785      1.7668       -1.205      -1.171\n#> 53        1.1878      0.1469        0.568       0.309\n#> 55        0.7064     -0.5474        0.390       0.309\n#> 57        0.4657      0.6097        0.450       0.443\n#> 58       -1.2192     -1.4730       -0.378      -0.364\n#> 69        0.3453     -1.9359        0.331       0.309\n#> 71       -0.0157      0.3783        0.509       0.712\n#> 73        0.4657     -1.2416        0.568       0.309\n#> 78        0.9471     -0.0845        0.627       0.578\n#> 84        0.1046     -0.7788        0.686       0.443\n#> 85       -0.6174     -0.0845        0.331       0.309\n#> 86        0.1046      0.8412        0.331       0.443\n#> 99       -0.9785     -1.2416       -0.555      -0.229\n#> 107      -1.2192     -1.2416        0.331       0.578\n#> 111       0.7064      0.3783        0.686       0.981\n#> 117       0.7064     -0.0845        0.922       0.712\n#> 124       0.4657     -0.7788        0.568       0.712\n#> 130       1.5488     -0.0845        1.099       0.443\n#> 138       0.5860      0.1469        0.922       0.712\n#> 139       0.1046     -0.0845        0.509       0.712\n#> 147       0.4657     -1.2416        0.627       0.847\n#> 150      -0.0157     -0.0845        0.686       0.712"},{"path":"a-gentle-introduction-to-support-vector-machines.html","id":"predictions-on-training-model","chapter":"10 A gentle introduction to Support Vector Machines","heading":"10.5.4 Predictions on training model","text":"","code":"\n# training set predictions\npred_train <- predict(svm_model, trainset)\nmean(pred_train == trainset$Species)\n#> [1] 0.983\n# [1] 0.9826087"},{"path":"a-gentle-introduction-to-support-vector-machines.html","id":"predictions-on-test-model","chapter":"10 A gentle introduction to Support Vector Machines","heading":"10.5.5 Predictions on test model","text":"","code":"\n# test set predictions\npred_test <-predict(svm_model, testset)\nmean(pred_test == testset$Species)\n#> [1] 0.914\n# [1] 0.9142857"},{"path":"a-gentle-introduction-to-support-vector-machines.html","id":"confusion-matrix-and-accuracy","chapter":"10 A gentle introduction to Support Vector Machines","heading":"10.5.6 Confusion matrix and Accuracy","text":"","code":"\n# confusion matrix\ncm <- table(pred_test, testset$Species)\ncm\n#>             \n#> pred_test    setosa versicolor virginica\n#>   setosa         18          0         0\n#>   versicolor      0          5         3\n#>   virginica       0          0         9\n# accuracy\nsum(diag(cm)) / sum(cm)\n#> [1] 0.914"},{"path":"a-gentle-introduction-to-support-vector-machines.html","id":"svm-with-radial-basis-function-kernel.-linear","chapter":"10 A gentle introduction to Support Vector Machines","heading":"10.6 SVM with Radial Basis Function kernel. Linear","text":"","code":""},{"path":"a-gentle-introduction-to-support-vector-machines.html","id":"training-and-test-sets","chapter":"10 A gentle introduction to Support Vector Machines","heading":"10.6.1 Training and test sets","text":"","code":"\n#load required library (assuming e1071 is already loaded)\nlibrary(mlbench)\n\n#load Sonar dataset\ndata(Sonar)\n#set seed to ensure reproducible results\nset.seed(42)\n#split into training and test sets\nSonar[, \"train\"] <- ifelse(runif(nrow(Sonar))<0.8,1,0)\n\n#separate training and test sets\ntrainset <- Sonar[Sonar$train==1,]\ntestset <- Sonar[Sonar$train==0,]\n\n#get column index of train flag\ntrainColNum <- grep(\"train\",names(trainset))\n#remove train flag column from train and test sets\ntrainset <- trainset[,-trainColNum]\ntestset <- testset[,-trainColNum]\n\n#get column index of predicted variable in dataset\ntypeColNum <- grep(\"Class\",names(Sonar))"},{"path":"a-gentle-introduction-to-support-vector-machines.html","id":"predictions-on-the-training-model","chapter":"10 A gentle introduction to Support Vector Machines","heading":"10.6.2 Predictions on the training model","text":"","code":"\n#build model – linear kernel and C-classification with default cost (C=1)\nsvm_model <- svm(Class~ ., data=trainset, \n                 method=\"C-classification\", \n                 kernel=\"linear\")\n\n#training set predictions\npred_train <-predict(svm_model,trainset)\nmean(pred_train==trainset$Class)\n#> [1] 0.97"},{"path":"a-gentle-introduction-to-support-vector-machines.html","id":"predictions-on-the-test-model","chapter":"10 A gentle introduction to Support Vector Machines","heading":"10.6.3 Predictions on the test model","text":"’ll leave examine contents model. important point note performance model test set quite dismal compared previous case. simply indicates linear kernel appropriate . Let’s take look happens use RBF kernel default values parameters:","code":"\n#test set predictions\npred_test <-predict(svm_model,testset)\nmean(pred_test==testset$Class)\n#> [1] 0.605"},{"path":"a-gentle-introduction-to-support-vector-machines.html","id":"svm-with-radial-basis-function-kernel.-non-linear","chapter":"10 A gentle introduction to Support Vector Machines","heading":"10.7 SVM with Radial Basis Function kernel. Non-linear","text":"","code":""},{"path":"a-gentle-introduction-to-support-vector-machines.html","id":"predictions-on-training-model-1","chapter":"10 A gentle introduction to Support Vector Machines","heading":"10.7.1 Predictions on training model","text":"","code":"\n#build model: radial kernel, default params\nsvm_model <- svm(Class~ ., data=trainset, \n                 method=\"C-classification\", \n                 kernel=\"radial\")\n# print params\nsvm_model$cost\n#> [1] 1\nsvm_model$gamma\n#> [1] 0.0167\n\n#training set predictions\npred_train <-predict(svm_model,trainset)\nmean(pred_train==trainset$Class)\n#> [1] 0.988"},{"path":"a-gentle-introduction-to-support-vector-machines.html","id":"predictions-on-test-model-1","chapter":"10 A gentle introduction to Support Vector Machines","heading":"10.7.2 Predictions on test model","text":"’s pretty decent improvement linear kernel. Let’s see can better parameter tuning. first invoke tune.svm use parameters gives us call svm:","code":"\n#test set predictions\npred_test <-predict(svm_model,testset)\nmean(pred_test==testset$Class)\n#> [1] 0.767"},{"path":"a-gentle-introduction-to-support-vector-machines.html","id":"tuning-of-parameters","chapter":"10 A gentle introduction to Support Vector Machines","heading":"10.7.3 Tuning of parameters","text":"","code":"\n# find optimal parameters in a specified range\ntune_out <- tune.svm(x = trainset[,-typeColNum], \n                     y = trainset[, typeColNum], \n                     gamma = 10^(-3:3), \n                     cost = c(0.01, 0.1, 1, 10, 100, 1000), \n                     kernel = \"radial\")\n\n#print best values of cost and gamma\ntune_out$best.parameters$cost\n#> [1] 10\ntune_out$best.parameters$gamma\n#> [1] 0.01\n\n#build model\nsvm_model <- svm(Class~ ., data = trainset, \n                 method = \"C-classification\", \n                 kernel = \"radial\", \n                 cost = tune_out$best.parameters$cost, \n                 gamma = tune_out$best.parameters$gamma)"},{"path":"a-gentle-introduction-to-support-vector-machines.html","id":"prediction-on-training-model-with-new-parameters","chapter":"10 A gentle introduction to Support Vector Machines","heading":"10.7.4 Prediction on training model with new parameters","text":"","code":"\n# training set predictions\npred_train <-predict(svm_model,trainset)\nmean(pred_train==trainset$Class)\n#> [1] 1"},{"path":"a-gentle-introduction-to-support-vector-machines.html","id":"prediction-on-test-model-with-new-parameters","chapter":"10 A gentle introduction to Support Vector Machines","heading":"10.7.5 Prediction on test model with new parameters","text":"fairly decent improvement un-optimised case.","code":"\n# test set predictions\npred_test <-predict(svm_model,testset)\nmean(pred_test==testset$Class)\n#> [1] 0.814"},{"path":"a-gentle-introduction-to-support-vector-machines.html","id":"wrapping-up","chapter":"10 A gentle introduction to Support Vector Machines","heading":"10.8 Wrapping up","text":"bring us end introductory exploration SVMs R. recap, distinguishing feature SVMs contrast techniques attempt construct optimal separation boundaries different categories.SVMs quite versatile applied wide variety domains ranging chemistry pattern recognition. best used binary classification scenarios. brings question SVMs preferred binary classification techniques logistic regression. honest response , “depends” – points keep mind choosing two. general point keep mind SVM algorithms tend expensive terms memory computation, issues can start hurt size dataset increases.Given caveats considerations, best way figure whether SVM approach work problem may machine learning practitioners : try !","code":""},{"path":"broad-view-of-svm.html","id":"broad-view-of-svm","chapter":"11 Broad view of SVM","heading":"11 Broad view of SVM","text":"","code":""},{"path":"broad-view-of-svm.html","id":"introduction-5","chapter":"11 Broad view of SVM","heading":"11.1 Introduction","text":"Source: http://uc-r.github.io/svmThe data sets used tutorial (exception Khan) generated using built-R commands. Support Vector Machine methodology sound number dimensions, becomes difficult visualize 2. previously mentioned, SVMs robust number classes, stick 3 duration tutorial.","code":"\n# set pseudorandom number generator\nset.seed(10)\n\n# Attach Packages\nlibrary(tidyverse)    # data manipulation and visualization\n#> ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──\n#> ✔ ggplot2 3.3.0     ✔ purrr   0.3.4\n#> ✔ tibble  3.0.1     ✔ dplyr   0.8.5\n#> ✔ tidyr   1.0.2     ✔ stringr 1.4.0\n#> ✔ readr   1.3.1     ✔ forcats 0.5.0\n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\nlibrary(kernlab)      # SVM methodology\n#> \n#> Attaching package: 'kernlab'\n#> The following object is masked from 'package:purrr':\n#> \n#>     cross\n#> The following object is masked from 'package:ggplot2':\n#> \n#>     alpha\nlibrary(e1071)        # SVM methodology\nlibrary(ISLR)         # contains example data set \"Khan\"\nlibrary(RColorBrewer) # customized coloring of plots"},{"path":"broad-view-of-svm.html","id":"maximal-margin-classifier","chapter":"11 Broad view of SVM","heading":"11.2 Maximal Margin Classifier","text":"classes separable linear boundary, can use Maximal Margin Classifier find classification boundary. visualize example separated data, generate 40 random observations assign two classes. Upon visual inspection, can see infinitely many lines exist split two classes.goal maximal margin classifier identify linear boundary maximizes total distance line closest point class. can use svm() function e1071 package find boundary.plot, points represented “X” support vectors, points directly affect classification line. points marked “o” points, don’t affect calculation line. principle lay foundation support vector machines. plot can generated using kernlab package, following results:kernlab shows little detail e1071, showing color gradient indicates confidently new point classified based features. Just first plot, support vectors marked, case filled-points, classes denoted different shapes.","code":"\n# Construct sample data set - completely separated\nx <- matrix(rnorm(20*2), ncol = 2)\ny <- c(rep(-1,10), rep(1,10))\nx[y==1,] <- x[y==1,] + 3/2\ndat <- data.frame(x=x, y=as.factor(y))\n\n# Plot data\nggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + \n  geom_point(size = 2) +\n  scale_color_manual(values=c(\"#000000\", \"#FF0000\")) +\n  theme(legend.position = \"none\")\n# Fit Support Vector Machine model to data set\nsvmfit <- svm(y~., data = dat, kernel = \"linear\", scale = FALSE)\n# Plot Results\nplot(svmfit, dat)\n# fit model and produce plot\nkernfit <- ksvm(x, y, type = \"C-svc\", kernel = 'vanilladot')\n#>  Setting default kernel parameters\nplot(kernfit, data = x)"},{"path":"broad-view-of-svm.html","id":"support-vector-classifiers","chapter":"11 Broad view of SVM","heading":"11.3 Support Vector Classifiers","text":"convenient maximal marginal classifier understand, real data sets fully separable linear boundary. handle data, must use modified methodology. simulate new data set classes mixed.Whether data separable , svm() command syntax . case data linearly separable, however, cost = argument takes real importance. quantifies penalty associated observation wrong side classification boundary. can plot fit way completely separable case. first use e1071:upping cost misclassification 10 100, can see difference classification line. repeat process plotting SVM using kernlab package:decide costly misclassifications actually ? Instead specifying cost front, can use tune() function e1071 test various costs identify value produces best fitting model.data set, optimal cost (amongst choices provided) calculated 0.1, doesn’t penalize model much misclassified observations. model identified, can construct table predicted classes true classes using predict() command follows:Using support vector classifier, 80% observations correctly classified, matches see plot. wanted test classifier rigorously, split data training testing sets see SVC performed observations used construct model. use training-testing method later tutorial validate SVMs.","code":"\n# Construct sample data set - not completely separated\nx <- matrix(rnorm(20*2), ncol = 2)\ny <- c(rep(-1,10), rep(1,10))\nx[y==1,] <- x[y==1,] + 1\ndat <- data.frame(x=x, y=as.factor(y))\n\n# Plot data set\nggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + \n  geom_point(size = 2) +\n  scale_color_manual(values=c(\"#000000\", \"#FF0000\")) +\n  theme(legend.position = \"none\")\n# Fit Support Vector Machine model to data set\nsvmfit <- svm(y~., data = dat, kernel = \"linear\", cost = 10)\n# Plot Results\nplot(svmfit, dat)\n# Fit Support Vector Machine model to data set\nkernfit <- ksvm(x,y, type = \"C-svc\", kernel = 'vanilladot', C = 100)\n#>  Setting default kernel parameters\n# Plot results\nplot(kernfit, data = x)\n# find optimal cost of misclassification\ntune.out <- tune(svm, y~., data = dat, kernel = \"linear\",\n                 ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))\n# extract the best model\n(bestmod <- tune.out$best.model)\n#> \n#> Call:\n#> best.tune(method = svm, train.x = y ~ ., data = dat, ranges = list(cost = c(0.001, \n#>     0.01, 0.1, 1, 5, 10, 100)), kernel = \"linear\")\n#> \n#> \n#> Parameters:\n#>    SVM-Type:  C-classification \n#>  SVM-Kernel:  linear \n#>        cost:  0.1 \n#> \n#> Number of Support Vectors:  16\n# Create a table of misclassified observations\nypred <- predict(bestmod, dat)\n(misclass <- table(predict = ypred, truth = dat$y))\n#>        truth\n#> predict -1 1\n#>      -1  9 3\n#>      1   1 7"},{"path":"broad-view-of-svm.html","id":"support-vector-machines","chapter":"11 Broad view of SVM","heading":"11.4 Support Vector Machines","text":"Support Vector Classifiers subset group classification structures known Support Vector Machines. Support Vector Machines can construct classification boundaries nonlinear shape. options classification structures using svm() command e1071 package linear, polynomial, radial, sigmoid. demonstrate nonlinear classification boundary, construct new data set.Notice data linearly separable, furthermore, isn’t clustered together single group. two sections class 1 observations cluster class 2 observations . demonstrate power SVMs, ’ll take 100 random observations set use construct boundary. set kernel = “radial” based shape data plot results.procedure can run using kernlab package, far kernel options corresponding function e1071. addition four choices e1071, package allows use hyperbolic tangent, Laplacian, Bessel, Spline, String, ANOVA RBF kernel. fit data, set cost , 1.see , least visually, SVM reasonable job separating two classes. fit model, used cost = 1, mentioned previously, isn’t usually obvious cost produce optimal classification boundary. can use tune() command try several different values cost well several different values \\(\\gamma\\), scaling parameter used fit nonlinear boundaries.model reduces error training data uses cost 1 \\(\\gamma\\)\nvalue 0.5. can now see well SVM performs predicting class 100 testing observations:best-fitting model produces 65% accuracy identifying classes. complicated shape observations, performed reasonably well. can challenge method adding additional classes observations.","code":"\n# construct larger random data set\nx <- matrix(rnorm(200*2), ncol = 2)\nx[1:100,] <- x[1:100,] + 2.5\nx[101:150,] <- x[101:150,] - 2.5\ny <- c(rep(1,150), rep(2,50))\ndat <- data.frame(x=x,y=as.factor(y))\n\n# Plot data\nggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + \n  geom_point(size = 2) +\n  scale_color_manual(values=c(\"#000000\", \"#FF0000\")) +\n  theme(legend.position = \"none\")\n# set pseudorandom number generator\nset.seed(123)\n# sample training data and fit model\ntrain <- base::sample(200,100, replace = FALSE)\nsvmfit <- svm(y~., data = dat[train,], kernel = \"radial\", gamma = 1, cost = 1)\n# plot classifier\nplot(svmfit, dat)\n# Fit radial-based SVM in kernlab\nkernfit <- ksvm(x[train,],y[train], type = \"C-svc\", kernel = 'rbfdot', C = 1, scaled = c())\n# Plot training data\nplot(kernfit, data = x[train,])\n# tune model to find optimal cost, gamma values\ntune.out <- tune(svm, y~., data = dat[train,], kernel = \"radial\",\n                 ranges = list(cost = c(0.1,1,10,100,1000),\n                 gamma = c(0.5,1,2,3,4)))\n# show best model\ntune.out$best.model\n#> \n#> Call:\n#> best.tune(method = svm, train.x = y ~ ., data = dat[train, ], ranges = list(cost = c(0.1, \n#>     1, 10, 100, 1000), gamma = c(0.5, 1, 2, 3, 4)), kernel = \"radial\")\n#> \n#> \n#> Parameters:\n#>    SVM-Type:  C-classification \n#>  SVM-Kernel:  radial \n#>        cost:  1 \n#> \n#> Number of Support Vectors:  30\n# validate model performance\n(valid <- table(true = dat[-train,\"y\"], pred = predict(tune.out$best.model,\n                                             newx = dat[-train,])))\n#>     pred\n#> true  1  2\n#>    1 55 28\n#>    2 12  5\n##     pred\n## true  1  2\n##    1 58 19\n##    2 16  7"},{"path":"broad-view-of-svm.html","id":"svms-for-multiple-classes","chapter":"11 Broad view of SVM","heading":"11.5 SVMs for Multiple Classes","text":"procedure change data sets involve two classes observations. construct data set way previously, now specifying three classes instead two:commands don’t change e1071 package. specify cost tuning parameter\n\\(\\gamma\\)\nfit support vector machine. results interpretation similar two-class classification.can check see well model fit data using predict() command, follows:shown resulting table, 89% training observations correctly classified. However, since didn’t break data training testing sets, didn’t truly validate results.kernlab package, hand, can fit 2 classes, plot results. visualize results ksvm function, take steps listed create grid points, predict value point, plot results:","code":"\n# construct data set\nx <- rbind(x, matrix(rnorm(50*2), ncol = 2))\ny <- c(y, rep(0,50))\nx[y==0,2] <- x[y==0,2] + 2.5\ndat <- data.frame(x=x, y=as.factor(y))\n# plot data set\nggplot(data = dat, aes(x = x.2, y = x.1, color = y, shape = y)) + \n  geom_point(size = 2) +\n  scale_color_manual(values=c(\"#000000\",\"#FF0000\",\"#00BA00\")) +\n  theme(legend.position = \"none\")\n# fit model\nsvmfit <- svm(y~., data = dat, kernel = \"radial\", cost = 10, gamma = 1)\n# plot results\nplot(svmfit, dat)\n#construct table\nypred <- predict(svmfit, dat)\n(misclass <- table(predict = ypred, truth = dat$y))\n#>        truth\n#> predict   0   1   2\n#>       0  38   2   5\n#>       1   7 145   2\n#>       2   5   3  43\n##        truth\n## predict   0   1   2\n##       0  38   2   4\n##       1   8 143   4\n##       2   4   5  42\n# fit and plot\nkernfit <- ksvm(as.matrix(dat[,2:1]),dat$y, type = \"C-svc\", kernel = 'rbfdot', \n                C = 100, scaled = c())\n\n# Create a fine grid of the feature space\nx.1 <- seq(from = min(dat$x.1), to = max(dat$x.1), length = 100)\nx.2 <- seq(from = min(dat$x.2), to = max(dat$x.2), length = 100)\nx.grid <- expand.grid(x.2, x.1)\n\n# Get class predictions over grid\npred <- predict(kernfit, newdata = x.grid)\n\n# Plot the results\ncols <- brewer.pal(3, \"Set1\")\nplot(x.grid, pch = 19, col = adjustcolor(cols[pred], alpha.f = 0.05))\n\nclasses <- matrix(pred, nrow = 100, ncol = 100)\ncontour(x = x.2, y = x.1, z = classes, levels = 1:3, labels = \"\", add = TRUE)\n\npoints(dat[, 2:1], pch = 19, col = cols[predict(kernfit)])"},{"path":"broad-view-of-svm.html","id":"application","chapter":"11 Broad view of SVM","heading":"11.6 Application","text":"Khan data set contains data 83 tissue samples 2308 gene expression measurements sample. split 63 training observations 20 testing observations, four distinct classes set. impossible visualize data, choose simplest classifier (linear) construct model. use svm command e1071 conduct analysis.First , can check well model classifying training observations. usually high, , doesn’t validate model. model doesn’t good job classifying training set, red flag. case, 63 training observations correctly classified.perform validation, can check model performs testing set:model correctly identifies 18 20 testing observations. SVMs boundaries impose difficult interpret higher dimensions, results seem suggest model good classifier gene data.","code":"\n# fit model\ndat <- data.frame(x = Khan$xtrain, y=as.factor(Khan$ytrain))\n(out <- svm(y~., data = dat, kernel = \"linear\", cost=10))\n#> \n#> Call:\n#> svm(formula = y ~ ., data = dat, kernel = \"linear\", cost = 10)\n#> \n#> \n#> Parameters:\n#>    SVM-Type:  C-classification \n#>  SVM-Kernel:  linear \n#>        cost:  10 \n#> \n#> Number of Support Vectors:  58\n# check model performance on training set\ntable(out$fitted, dat$y)\n#>    \n#>      1  2  3  4\n#>   1  8  0  0  0\n#>   2  0 23  0  0\n#>   3  0  0 12  0\n#>   4  0  0  0 20\n# validate model performance\ndat.te <- data.frame(x=Khan$xtest, y=as.factor(Khan$ytest))\npred.te <- predict(out, newdata=dat.te)\ntable(pred.te, dat.te$y)\n#>        \n#> pred.te 1 2 3 4\n#>       1 3 0 0 0\n#>       2 0 6 2 0\n#>       3 0 0 4 0\n#>       4 0 0 0 5"},{"path":"feature-selection-to-enhance-cancer-detection.html","id":"feature-selection-to-enhance-cancer-detection","chapter":"12 Feature Selection to enhance cancer detection","heading":"12 Feature Selection to enhance cancer detection","text":"Datasets: breast-cancer-wisconsin.dataAlgorithms:\nPCA\nRandom Forest\nRecursive Feature Elimination\nGenetic Algorithm\nPCARandom ForestRecursive Feature EliminationGenetic AlgorithmSource: https://shiring.github.io/machine_learning/2017/01/15/rfe_ga_post","code":""},{"path":"feature-selection-to-enhance-cancer-detection.html","id":"read-and-process-the-data","chapter":"12 Feature Selection to enhance cancer detection","heading":"12.1 Read and process the data","text":"","code":"\nbc_data <- read.table(file.path(data_raw_dir, \"breast-cancer-wisconsin.data\"), \n                      header = FALSE, sep = \",\")\n# assign the column names\ncolnames(bc_data) <- c(\"sample_code_number\", \"clump_thickness\", \n                       \"uniformity_of_cell_size\", \"uniformity_of_cell_shape\",\n                       \"marginal_adhesion\", \"single_epithelial_cell_size\", \n                       \"bare_nuclei\", \"bland_chromatin\", \"normal_nucleoli\", \n                       \"mitosis\", \"classes\")\n\n# change classes from numeric to character\nbc_data$classes <- ifelse(bc_data$classes == \"2\", \"benign\",\n                          ifelse(bc_data$classes == \"4\", \"malignant\", NA))\n\n# if query sign make NA\nbc_data[bc_data == \"?\"] <- NA\n\n# how many NAs are in the data\nlength(which(is.na(bc_data)))\n#> [1] 16\nnames(bc_data)\n#>  [1] \"sample_code_number\"          \"clump_thickness\"            \n#>  [3] \"uniformity_of_cell_size\"     \"uniformity_of_cell_shape\"   \n#>  [5] \"marginal_adhesion\"           \"single_epithelial_cell_size\"\n#>  [7] \"bare_nuclei\"                 \"bland_chromatin\"            \n#>  [9] \"normal_nucleoli\"             \"mitosis\"                    \n#> [11] \"classes\""},{"path":"feature-selection-to-enhance-cancer-detection.html","id":"missing-data","chapter":"12 Feature Selection to enhance cancer detection","heading":"12.1.1 Missing data","text":"","code":"\n# impute missing data\nlibrary(mice)\n#> \n#> Attaching package: 'mice'\n#> The following objects are masked from 'package:base':\n#> \n#>     cbind, rbind\n\n# skip these columns: sample_code_number and classes\n# convert to numeric\nbc_data[,2:10] <- apply(bc_data[, 2:10], 2, function(x) as.numeric(as.character(x)))\n\n# impute but mute\ndataset_impute <- mice(bc_data[, 2:10],  print = FALSE)\n\n# bind \"classes\" with the rest. skip \"sample_code_number\"\nbc_data <- cbind(bc_data[, 11, drop = FALSE], \n                 mice::complete(dataset_impute, action =1))\n\nbc_data$classes <- as.factor(bc_data$classes)\n\n# how many benign and malignant cases are there?\nsummary(bc_data$classes)\n#>    benign malignant \n#>       458       241\n# confirm NAs have been removed\nlength(which(is.na(bc_data)))\n#> [1] 0\nstr(bc_data)\n#> 'data.frame':    699 obs. of  10 variables:\n#>  $ classes                    : Factor w/ 2 levels \"benign\",\"malignant\": 1 1 1 1 1 2 1 1 1 1 ...\n#>  $ clump_thickness            : num  5 5 3 6 4 8 1 2 2 4 ...\n#>  $ uniformity_of_cell_size    : num  1 4 1 8 1 10 1 1 1 2 ...\n#>  $ uniformity_of_cell_shape   : num  1 4 1 8 1 10 1 2 1 1 ...\n#>  $ marginal_adhesion          : num  1 5 1 1 3 8 1 1 1 1 ...\n#>  $ single_epithelial_cell_size: num  2 7 2 3 2 7 2 2 2 2 ...\n#>  $ bare_nuclei                : num  1 10 2 4 1 10 10 1 1 1 ...\n#>  $ bland_chromatin            : num  3 3 3 3 3 9 3 3 1 2 ...\n#>  $ normal_nucleoli            : num  1 2 1 7 1 7 1 1 1 1 ...\n#>  $ mitosis                    : num  1 1 1 1 1 1 1 1 5 1 ..."},{"path":"feature-selection-to-enhance-cancer-detection.html","id":"principal-component-analysis-pca","chapter":"12 Feature Selection to enhance cancer detection","heading":"12.2 Principal Component Analysis (PCA)","text":"get idea dimensionality variance datasets, first looking PCA plots samples features. first two principal components (PCs) show two components explain majority variation data.defining custom ggplot2 theme, creating function performs PCA (using pcaGoPromoter package), calculates ellipses data points (ellipse package) produces plot ggplot2. features datasets 2 3 distinct overlap PCA plots, therefore also plotting hierarchical clustering dendrograms.","code":""},{"path":"feature-selection-to-enhance-cancer-detection.html","id":"theme","chapter":"12 Feature Selection to enhance cancer detection","heading":"12.2.0.1 theme","text":"","code":"\n# plotting theme\n\nlibrary(ggplot2)\n\nmy_theme <- function(base_size = 12, base_family = \"sans\"){\n  theme_minimal(base_size = base_size, base_family = base_family) +\n  theme(\n    axis.text = element_text(size = 12),\n    axis.text.x = element_text(angle = 0, vjust = 0.5, hjust = 0.5),\n    axis.title = element_text(size = 14),\n    panel.grid.major = element_line(color = \"grey\"),\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"aliceblue\"),\n    strip.background = element_rect(fill = \"navy\", color = \"navy\", size = 1),\n    strip.text = element_text(face = \"bold\", size = 12, color = \"white\"),\n    legend.position = \"right\",\n    legend.justification = \"top\", \n    legend.background = element_blank(),\n    panel.border = element_rect(color = \"grey\", fill = NA, size = 0.5)\n  )\n}\n\ntheme_set(my_theme())"},{"path":"feature-selection-to-enhance-cancer-detection.html","id":"pca-function","chapter":"12 Feature Selection to enhance cancer detection","heading":"12.2.0.2 PCA function","text":"","code":"\n# function for PCA plotting\nlibrary(pcaGoPromoter)                  # install from BioConductor\n#> Loading required package: ellipse\n#> \n#> Attaching package: 'ellipse'\n#> The following object is masked from 'package:graphics':\n#> \n#>     pairs\n#> Loading required package: Biostrings\n#> Loading required package: BiocGenerics\n#> Loading required package: parallel\n#> \n#> Attaching package: 'BiocGenerics'\n#> The following objects are masked from 'package:parallel':\n#> \n#>     clusterApply, clusterApplyLB, clusterCall, clusterEvalQ,\n#>     clusterExport, clusterMap, parApply, parCapply, parLapply,\n#>     parLapplyLB, parRapply, parSapply, parSapplyLB\n#> The following objects are masked from 'package:mice':\n#> \n#>     cbind, rbind\n#> The following objects are masked from 'package:stats':\n#> \n#>     IQR, mad, sd, var, xtabs\n#> The following objects are masked from 'package:base':\n#> \n#>     anyDuplicated, append, as.data.frame, basename, cbind, colnames,\n#>     dirname, do.call, duplicated, eval, evalq, Filter, Find, get, grep,\n#>     grepl, intersect, is.unsorted, lapply, Map, mapply, match, mget,\n#>     order, paste, pmax, pmax.int, pmin, pmin.int, Position, rank,\n#>     rbind, Reduce, rownames, sapply, setdiff, sort, table, tapply,\n#>     union, unique, unsplit, which, which.max, which.min\n#> Loading required package: S4Vectors\n#> Loading required package: stats4\n#> \n#> Attaching package: 'S4Vectors'\n#> The following object is masked from 'package:base':\n#> \n#>     expand.grid\n#> Loading required package: IRanges\n#> Loading required package: XVector\n#> \n#> Attaching package: 'Biostrings'\n#> The following object is masked from 'package:base':\n#> \n#>     strsplit\nlibrary(ellipse)\n\npca_func <- function(data, groups, title, print_ellipse = TRUE) {\n  \n  # perform pca and extract scores for all principal components: PC1:PC9\n  pcaOutput <- pca(data, printDropped = FALSE, scale = TRUE, center = TRUE)\n  pcaOutput2 <- as.data.frame(pcaOutput$scores)\n  \n  # define groups for plotting. will group the classes\n  pcaOutput2$groups <- groups\n  \n  # when plotting samples calculate ellipses for plotting \n  # (when plotting features, there are no replicates)\n  if (print_ellipse) {\n    # group and summarize by classes: benign, malignant\n    # centroids w/3 columns: groups, PC1, PC2\n    centroids <- aggregate(cbind(PC1, PC2) ~ groups, pcaOutput2, mean)\n    # bind for the two groups (classes)\n    # conf.rgn w/3 columns: groups, PC1, PC2\n    conf.rgn  <- do.call(rbind, lapply(unique(pcaOutput2$groups), function(t)\n      data.frame(groups = as.character(t),\n                 # ellipse data for PC1 and PC2\n                 ellipse(cov(pcaOutput2[pcaOutput2$groups == t, 1:2]),\n                       centre = as.matrix(centroids[centroids$groups == t, 2:3]),\n                       level = 0.95),\n                 stringsAsFactors = FALSE)))\n    \n    plot <- ggplot(data = pcaOutput2, aes(x = PC1, y = PC2, \n                                          group = groups, \n                                          color = groups)) + \n      geom_polygon(data = conf.rgn, aes(fill = groups), alpha = 0.2) + # ellipses\n      geom_point(size = 2, alpha = 0.6) + \n      scale_color_brewer(palette = \"Set1\") +\n      labs(title = title,\n           color = \"\",\n           fill = \"\",\n           x = paste0(\"PC1: \", round(pcaOutput$pov[1], digits = 2) * 100, \n                      \"% variance\"),\n           y = paste0(\"PC2: \", round(pcaOutput$pov[2], digits = 2) * 100, \n                      \"% variance\"))\n    \n  } else {\n    \n    # if < 10 groups (e.g. the predictor classes) have colors from RColorBrewer\n    if (length(unique(pcaOutput2$groups)) <= 10) {\n      \n      plot <- ggplot(data = pcaOutput2, aes(x = PC1, y = PC2, \n                                            group = groups, \n                                            color = groups)) + \n        geom_point(size = 2, alpha = 0.6) + \n        scale_color_brewer(palette = \"Set1\") +\n        labs(title = title,\n             color = \"\",\n             fill = \"\",\n             x = paste0(\"PC1: \", round(pcaOutput$pov[1], digits = 2) * 100, \n                        \"% variance\"),\n             y = paste0(\"PC2: \", round(pcaOutput$pov[2], digits = 2) * 100, \n                        \"% variance\"))\n      \n    } else {\n      # otherwise use the default rainbow colors\n      plot <- ggplot(data = pcaOutput2, aes(x = PC1, y = PC2, \n                                            group = groups, color = groups)) + \n        geom_point(size = 2, alpha = 0.6) + \n        labs(title = title,\n             color = \"\",\n             fill = \"\",\n             x = paste0(\"PC1: \", round(pcaOutput$pov[1], digits = 2) * 100, \n                        \"% variance\"),\n             y = paste0(\"PC2: \", round(pcaOutput$pov[2], digits = 2) * 100, \n                        \"% variance\"))\n    }\n  }\n  \n  return(plot)\n  \n}\nlibrary(gridExtra)\n#> \n#> Attaching package: 'gridExtra'\n#> The following object is masked from 'package:BiocGenerics':\n#> \n#>     combine\nlibrary(grid)\n\n# plot all data. one row is a feature\np1 <- pca_func(data = t(bc_data[, 2:10]), \n               groups = as.character(bc_data$classes), \n               title = \"Breast cancer dataset 1: Samples\")\n\n# plot features only. features as columns\np2 <- pca_func(data = bc_data[, 2:10], \n               groups = as.character(colnames(bc_data[, 2:10])), \n               title = \"Breast cancer dataset 1: Features\", print_ellipse = FALSE)\n\ngrid.arrange(p1, p2, ncol = 2)\nh_1 <- hclust(dist(t(bc_data[, 2:10]), method = \"euclidean\"), method = \"complete\")\nplot(h_1)"},{"path":"feature-selection-to-enhance-cancer-detection.html","id":"density-plots-vs-class","chapter":"12 Feature Selection to enhance cancer detection","heading":"12.2.1 density plots vs class","text":"","code":"\n# density plot showing the feature vs classes\nlibrary(tidyr)\n#> \n#> Attaching package: 'tidyr'\n#> The following object is masked from 'package:S4Vectors':\n#> \n#>     expand\n\n# gather data. from column clump_thickness to mitosis\nbc_data_gather <- bc_data %>%\n  gather(measure, value, clump_thickness:mitosis)\n\nggplot(data = bc_data_gather, aes(x = value, fill = classes, color = classes)) +\n  geom_density(alpha = 0.3, size = 1) +\n  geom_rug() +\n  scale_fill_brewer(palette = \"Set1\") +\n  scale_color_brewer(palette = \"Set1\") +\n  facet_wrap( ~ measure, scales = \"free_y\", ncol = 3)"},{"path":"feature-selection-to-enhance-cancer-detection.html","id":"feature-importance","chapter":"12 Feature Selection to enhance cancer detection","heading":"12.3 Feature importance","text":"get idea feature’s respective importances, ’m running Random Forest models 10 x 10 cross validation using caret package. wanted use feature importance select features modeling, need perform training data instead complete dataset. , want use get acquainted data. defining function estimates feature importance produces plot.","code":"\nlibrary(caret)\n#> Loading required package: lattice\n# library(doParallel) # parallel processing\n# registerDoParallel()\n\n# prepare training scheme\ncontrol <- trainControl(method = \"repeatedcv\", number = 10, repeats = 10)\nfeature_imp <- function(model, title) {\n  # estimate variable importance\n  importance <- varImp(model, scale = TRUE)\n  # prepare dataframes for plotting\n  importance_df_1 <- importance$importance\n  importance_df_1$group <- rownames(importance_df_1)\n  \n  importance_df_2 <- importance_df_1\n  importance_df_2$Overall <- 0\n  importance_df <- rbind(importance_df_1, importance_df_2)\n  \n  plot <- ggplot() +\n    geom_point(data = importance_df_1, aes(x = Overall, \n                                           y = group, \n                                           color = group), size = 2) +\n    geom_path(data = importance_df, aes(x = Overall, \n                                        y = group, \n                                        color = group, \n                                        group = group), size = 1) +\n    theme(legend.position = \"none\") +\n    labs(\n      x = \"Importance\",\n      y = \"\",\n      title = title,\n      subtitle = \"Scaled feature importance\",\n      caption = \"\\nDetermined with Random Forest and\n      repeated cross validation (10 repeats, 10 times)\"\n    )\n  return(plot)\n}\n# train the model\nset.seed(27)\nimp_1 <- train(classes ~ ., data = bc_data, method = \"rf\", \n               preProcess = c(\"scale\", \"center\"), \n               trControl = control)\np1 <- feature_imp(imp_1, title = \"Breast cancer dataset 1\")\np1"},{"path":"feature-selection-to-enhance-cancer-detection.html","id":"feature-selection","chapter":"12 Feature Selection to enhance cancer detection","heading":"12.4 Feature Selection","text":"correlationBy Recursive Feature EliminationBy Genetic Algorithm","code":"\nset.seed(27)\nbc_data_index <- createDataPartition(bc_data$classes, p = 0.7, list = FALSE)\nbc_data_train <- bc_data[bc_data_index, ]\nbc_data_test  <- bc_data[-bc_data_index, ]"},{"path":"feature-selection-to-enhance-cancer-detection.html","id":"correlation","chapter":"12 Feature Selection to enhance cancer detection","heading":"12.4.1 Correlation","text":"Four features removed","code":"\nlibrary(corrplot)\n#> corrplot 0.84 loaded\n\n# calculate correlation matrix\ncorMatMy <- cor(bc_data_train[, -1])\ncorrplot(corMatMy, order = \"hclust\")\n# Apply correlation filter at 0.70,\nhighlyCor <- colnames(bc_data_train[, -1])[findCorrelation(corMatMy, \n                                                           cutoff = 0.7, \n                                                           verbose = TRUE)]\n#> Compare row 2  and column  3 with corr  0.9 \n#>   Means:  0.709 vs 0.595 so flagging column 2 \n#> Compare row 3  and column  7 with corr  0.737 \n#>   Means:  0.674 vs 0.572 so flagging column 3 \n#> All correlations <= 0.7\n# which variables are flagged for removal?\nhighlyCor\n#> [1] \"uniformity_of_cell_size\"  \"uniformity_of_cell_shape\"\n# then we remove these variables\nbc_data_cor <- bc_data_train[, which(!colnames(bc_data_train) %in% highlyCor)]\nnames(bc_data_cor)\n#> [1] \"classes\"                     \"clump_thickness\"            \n#> [3] \"marginal_adhesion\"           \"single_epithelial_cell_size\"\n#> [5] \"bare_nuclei\"                 \"bland_chromatin\"            \n#> [7] \"normal_nucleoli\"             \"mitosis\"\n# confirm features were removed\noutersect <- function(x, y) {\n  sort(c(setdiff(x, y),\n         setdiff(y, x)))\n}\n \noutersect(names(bc_data_cor), names(bc_data_train))\n#> [1] \"uniformity_of_cell_shape\" \"uniformity_of_cell_size\""},{"path":"feature-selection-to-enhance-cancer-detection.html","id":"recursive-feature-elimination-rfe-1","chapter":"12 Feature Selection to enhance cancer detection","heading":"12.4.2 Recursive Feature Elimination (RFE)","text":"features removed RFE","code":"\n# ensure the results are repeatable\nset.seed(7)\n\n# define the control using a random forest selection function with cross validation\ncontrol <- rfeControl(functions = rfFuncs, method = \"cv\", number = 10)\n\n# run the RFE algorithm\nresults_1 <- rfe(x = bc_data_train[, -1], \n                 y = bc_data_train$classes, \n                 sizes = c(1:9), \n                 rfeControl = control)\n\n# chosen features\npredictors(results_1)\n#> [1] \"bare_nuclei\"                 \"clump_thickness\"            \n#> [3] \"normal_nucleoli\"             \"uniformity_of_cell_size\"    \n#> [5] \"uniformity_of_cell_shape\"    \"single_epithelial_cell_size\"\n#> [7] \"bland_chromatin\"             \"marginal_adhesion\"\n# subset the chosen features\nsel_cols <- which(colnames(bc_data_train) %in% predictors(results_1))\nbc_data_rfe <- bc_data_train[, c(1, sel_cols)]\nnames(bc_data_rfe)\n#> [1] \"classes\"                     \"clump_thickness\"            \n#> [3] \"uniformity_of_cell_size\"     \"uniformity_of_cell_shape\"   \n#> [5] \"marginal_adhesion\"           \"single_epithelial_cell_size\"\n#> [7] \"bare_nuclei\"                 \"bland_chromatin\"            \n#> [9] \"normal_nucleoli\"\n# confirm features removed by RFE\noutersect(names(bc_data_rfe), names(bc_data_train))\n#> [1] \"mitosis\""},{"path":"feature-selection-to-enhance-cancer-detection.html","id":"genetic-algorithm-ga","chapter":"12 Feature Selection to enhance cancer detection","heading":"12.4.3 Genetic Algorithm (GA)","text":"Two features removed GA.","code":"\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following object is masked from 'package:gridExtra':\n#> \n#>     combine\n#> The following objects are masked from 'package:Biostrings':\n#> \n#>     collapse, intersect, setdiff, setequal, union\n#> The following object is masked from 'package:XVector':\n#> \n#>     slice\n#> The following objects are masked from 'package:IRanges':\n#> \n#>     collapse, desc, intersect, setdiff, slice, union\n#> The following objects are masked from 'package:S4Vectors':\n#> \n#>     first, intersect, rename, setdiff, setequal, union\n#> The following objects are masked from 'package:BiocGenerics':\n#> \n#>     combine, intersect, setdiff, union\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\n\nga_ctrl <- gafsControl(functions = rfGA, # Assess fitness with RF\n                       method = \"cv\",    # 10 fold cross validation\n                       genParallel = TRUE, # Use parallel programming\n                       allowParallel = TRUE)\n\nlev <- c(\"malignant\", \"benign\")     # Set the levels\n\nset.seed(27)\nmodel_1 <- gafs(x = bc_data_train[, -1], y = bc_data_train$classes,\n                   iters = 10, # generations of algorithm\n                   popSize = 5, # population size for each generation\n                   levels = lev,\n                   gafsControl = ga_ctrl)\n#> \n#> Attaching package: 'recipes'\n#> The following object is masked from 'package:stats':\n#> \n#>     step\nplot(model_1) # Plot mean fitness (AUC) by generation\n# features\nmodel_1$ga$final\n#> [1] \"clump_thickness\"          \"uniformity_of_cell_size\" \n#> [3] \"uniformity_of_cell_shape\" \"marginal_adhesion\"       \n#> [5] \"bare_nuclei\"              \"bland_chromatin\"         \n#> [7] \"normal_nucleoli\"\n# select features\nsel_cols_ga <- which(colnames(bc_data_train) %in% model_1$ga$final)\nbc_data_ga <- bc_data_train[, c(1, sel_cols_ga)]\nnames(bc_data_ga)\n#> [1] \"classes\"                  \"clump_thickness\"         \n#> [3] \"uniformity_of_cell_size\"  \"uniformity_of_cell_shape\"\n#> [5] \"marginal_adhesion\"        \"bare_nuclei\"             \n#> [7] \"bland_chromatin\"          \"normal_nucleoli\"\n# features removed GA\noutersect(names(bc_data_ga), names(bc_data_train))\n#> [1] \"mitosis\"                     \"single_epithelial_cell_size\""},{"path":"feature-selection-to-enhance-cancer-detection.html","id":"model-comparison","chapter":"12 Feature Selection to enhance cancer detection","heading":"12.5 Model comparison","text":"","code":""},{"path":"feature-selection-to-enhance-cancer-detection.html","id":"using-all-features","chapter":"12 Feature Selection to enhance cancer detection","heading":"12.5.1 Using all features","text":"","code":"\nset.seed(27)\nmodel_bc_data_all <- train(classes ~ .,\n                           data = bc_data_train,\n                           method = \"rf\",\n                           preProcess = c(\"scale\", \"center\"),\n                           trControl = trainControl(method = \"repeatedcv\", \n                                                    number = 5, repeats = 10,\n                                                    verboseIter = FALSE))\n# confusion matrix \ncm_all_1 <- confusionMatrix(predict(model_bc_data_all, bc_data_test[, -1]), bc_data_test$classes)\ncm_all_1\n#> Confusion Matrix and Statistics\n#> \n#>            Reference\n#> Prediction  benign malignant\n#>   benign       131         2\n#>   malignant      6        70\n#>                                         \n#>                Accuracy : 0.962         \n#>                  95% CI : (0.926, 0.983)\n#>     No Information Rate : 0.656         \n#>     P-Value [Acc > NIR] : <2e-16        \n#>                                         \n#>                   Kappa : 0.916         \n#>                                         \n#>  Mcnemar's Test P-Value : 0.289         \n#>                                         \n#>             Sensitivity : 0.956         \n#>             Specificity : 0.972         \n#>          Pos Pred Value : 0.985         \n#>          Neg Pred Value : 0.921         \n#>              Prevalence : 0.656         \n#>          Detection Rate : 0.627         \n#>    Detection Prevalence : 0.636         \n#>       Balanced Accuracy : 0.964         \n#>                                         \n#>        'Positive' Class : benign        \n#> "},{"path":"feature-selection-to-enhance-cancer-detection.html","id":"compare-selection-methods","chapter":"12 Feature Selection to enhance cancer detection","heading":"12.5.2 Compare selection methods","text":"4 10 features chosen three methods; biggest overlap seen GA RFE 7 features. RFE GA retained 8 features modeling, compared 5 based correlation method.","code":"\n# compare features selected by the three methods\nlibrary(gplots)\n#> \n#> Attaching package: 'gplots'\n#> The following object is masked from 'package:IRanges':\n#> \n#>     space\n#> The following object is masked from 'package:S4Vectors':\n#> \n#>     space\n#> The following object is masked from 'package:stats':\n#> \n#>     lowess\n\nvenn_list <- list(cor = colnames(bc_data_cor)[-1],\n                  rfe = colnames(bc_data_rfe)[-1],\n                  ga  = colnames(bc_data_ga)[-1])\n\nvenn <- venn(venn_list)\nvenn\n#>     num cor rfe ga\n#> 000   0   0   0  0\n#> 001   0   0   0  1\n#> 010   0   0   1  0\n#> 011   2   0   1  1\n#> 100   1   1   0  0\n#> 101   0   1   0  1\n#> 110   1   1   1  0\n#> 111   5   1   1  1\n#> attr(,\"intersections\")\n#> attr(,\"intersections\")$`cor:rfe:ga`\n#> [1] \"clump_thickness\"   \"marginal_adhesion\" \"bare_nuclei\"      \n#> [4] \"bland_chromatin\"   \"normal_nucleoli\"  \n#> \n#> attr(,\"intersections\")$cor\n#> [1] \"mitosis\"\n#> \n#> attr(,\"intersections\")$`rfe:ga`\n#> [1] \"uniformity_of_cell_size\"  \"uniformity_of_cell_shape\"\n#> \n#> attr(,\"intersections\")$`cor:rfe`\n#> [1] \"single_epithelial_cell_size\"\n#> \n#> attr(,\"class\")\n#> [1] \"venn\""},{"path":"feature-selection-to-enhance-cancer-detection.html","id":"correlation-1","chapter":"12 Feature Selection to enhance cancer detection","heading":"12.5.3 Correlation","text":"","code":"\n# correlation\nset.seed(127)\nmodel_bc_data_cor <- train(classes ~ .,\n                 data = bc_data_cor,\n                 method = \"rf\",\n                 preProcess = c(\"scale\", \"center\"),\n                 trControl = trainControl(method = \"repeatedcv\", number = 5, repeats = 10, verboseIter = FALSE))\ncm_cor_1 <- confusionMatrix(predict(model_bc_data_cor, bc_data_test[, -1]), bc_data_test$classes)\ncm_cor_1\n#> Confusion Matrix and Statistics\n#> \n#>            Reference\n#> Prediction  benign malignant\n#>   benign       130         4\n#>   malignant      7        68\n#>                                         \n#>                Accuracy : 0.947         \n#>                  95% CI : (0.908, 0.973)\n#>     No Information Rate : 0.656         \n#>     P-Value [Acc > NIR] : <2e-16        \n#>                                         \n#>                   Kappa : 0.885         \n#>                                         \n#>  Mcnemar's Test P-Value : 0.546         \n#>                                         \n#>             Sensitivity : 0.949         \n#>             Specificity : 0.944         \n#>          Pos Pred Value : 0.970         \n#>          Neg Pred Value : 0.907         \n#>              Prevalence : 0.656         \n#>          Detection Rate : 0.622         \n#>    Detection Prevalence : 0.641         \n#>       Balanced Accuracy : 0.947         \n#>                                         \n#>        'Positive' Class : benign        \n#> "},{"path":"feature-selection-to-enhance-cancer-detection.html","id":"recursive-feature-elimination","chapter":"12 Feature Selection to enhance cancer detection","heading":"12.5.4 Recursive Feature Elimination","text":"","code":"\nset.seed(127)\nmodel_bc_data_rfe <- train(classes ~ .,\n                           data = bc_data_rfe,\n                           method = \"rf\",\n                           preProcess = c(\"scale\", \"center\"),\n                           trControl = trainControl(method = \"repeatedcv\", \n                                                    number = 5, repeats = 10, \n                                                    verboseIter = FALSE))\ncm_rfe_1 <- confusionMatrix(predict(model_bc_data_rfe, bc_data_test[, -1]), bc_data_test$classes)\ncm_rfe_1\n#> Confusion Matrix and Statistics\n#> \n#>            Reference\n#> Prediction  benign malignant\n#>   benign       130         3\n#>   malignant      7        69\n#>                                         \n#>                Accuracy : 0.952         \n#>                  95% CI : (0.914, 0.977)\n#>     No Information Rate : 0.656         \n#>     P-Value [Acc > NIR] : <2e-16        \n#>                                         \n#>                   Kappa : 0.895         \n#>                                         \n#>  Mcnemar's Test P-Value : 0.343         \n#>                                         \n#>             Sensitivity : 0.949         \n#>             Specificity : 0.958         \n#>          Pos Pred Value : 0.977         \n#>          Neg Pred Value : 0.908         \n#>              Prevalence : 0.656         \n#>          Detection Rate : 0.622         \n#>    Detection Prevalence : 0.636         \n#>       Balanced Accuracy : 0.954         \n#>                                         \n#>        'Positive' Class : benign        \n#> "},{"path":"feature-selection-to-enhance-cancer-detection.html","id":"ga","chapter":"12 Feature Selection to enhance cancer detection","heading":"12.5.5 GA","text":"","code":"\nset.seed(127)\nmodel_bc_data_ga <- train(classes ~ .,\n                           data = bc_data_ga,\n                           method = \"rf\",\n                           preProcess = c(\"scale\", \"center\"),\n                           trControl = trainControl(method = \"repeatedcv\", \n                                                    number = 5, repeats = 10, \n                                                    verboseIter = FALSE))\ncm_ga_1 <- confusionMatrix(predict(model_bc_data_ga, bc_data_test[, -1]), bc_data_test$classes)\ncm_ga_1\n#> Confusion Matrix and Statistics\n#> \n#>            Reference\n#> Prediction  benign malignant\n#>   benign       131         2\n#>   malignant      6        70\n#>                                         \n#>                Accuracy : 0.962         \n#>                  95% CI : (0.926, 0.983)\n#>     No Information Rate : 0.656         \n#>     P-Value [Acc > NIR] : <2e-16        \n#>                                         \n#>                   Kappa : 0.916         \n#>                                         \n#>  Mcnemar's Test P-Value : 0.289         \n#>                                         \n#>             Sensitivity : 0.956         \n#>             Specificity : 0.972         \n#>          Pos Pred Value : 0.985         \n#>          Neg Pred Value : 0.921         \n#>              Prevalence : 0.656         \n#>          Detection Rate : 0.627         \n#>    Detection Prevalence : 0.636         \n#>       Balanced Accuracy : 0.964         \n#>                                         \n#>        'Positive' Class : benign        \n#> "},{"path":"feature-selection-to-enhance-cancer-detection.html","id":"create-comparison-tables","chapter":"12 Feature Selection to enhance cancer detection","heading":"12.6 Create comparison tables","text":"Less accurate: selection features correlationMore accurate: genetic algorithmIncluding features accurate removing features correlation.","code":"\n# take \"overall\" variable only from Confusion Matrix\noverall <- data.frame(dataset = 1, \n           model = rep(c(\"all\", \"cor\", \"rfe\", \"ga\"), 1),\n           rbind(cm_all_1$overall,\n                 cm_cor_1$overall,\n                 cm_rfe_1$overall,\n                 cm_ga_1$overall)\n)\n\n# convert to tidy data\nlibrary(tidyr)\noverall_gather <- overall[, 1:4] %>%     # take the first columns:\n  gather(measure, value, Accuracy:Kappa) # dataset, model, Accuracy and Kappa\n# take \"byClass\" variable only from Confusion Matrix\nbyClass <- data.frame(dataset = 1,\n                      model = rep(c(\"all\", \"cor\", \"rfe\", \"ga\"), 1),\n                      rbind(cm_all_1$byClass,\n                      cm_cor_1$byClass,\n                      cm_rfe_1$byClass,\n                      cm_ga_1$byClass)\n)\n\n# convert to tidy data\nbyClass_gather <- byClass[, c(1:4, 7)] %>%      # select columns: dataset, model\n  gather(measure, value, Sensitivity:Precision) # Sensitiv, Specific, Precis\n# join the two tables\noverall_byClass_gather <- rbind(overall_gather, byClass_gather)\noverall_byClass_gather <- within(\n  overall_byClass_gather, model <- factor(model, \n                                          levels = c(\"all\", \"cor\", \"rfe\", \"ga\")))  \n                                          # convert to factor\n\nggplot(overall_byClass_gather, aes(x = model, y = value, color = measure, \n                                   shape = measure, group = measure)) +\n  geom_point(size = 4, alpha = 0.8) +\n  geom_path(alpha = 0.7) +\n  scale_colour_brewer(palette = \"Set1\") +\n  facet_grid(dataset ~ ., scales = \"free_y\") +\n  labs(\n    x = \"Feature Selection method\",\n    y = \"Value\",\n    color = \"\",\n    shape = \"\",\n    title = \"Comparison of feature selection methods\",\n    subtitle = \"in three breast cancer datasets\",\n    caption = \"\\nBreast Cancer Wisconsin (Diagnostic) Data Sets: 1, 2 & 3\n    Street et al., 1993;\n    all: no feature selection\n    cor: features with correlation > 0.7 removed\n    rfe: Recursive Feature Elimination\n    ga: Genetic Algorithm\"\n  )"},{"path":"feature-selection-to-enhance-cancer-detection.html","id":"notes","chapter":"12 Feature Selection to enhance cancer detection","heading":"12.7 Notes","text":"pcaGoPromoter BioConductor package. dependencies BioGenerics, AnnotationDbi BioStrings, turn require DBI RSQLite packages CRAN. Install first CRAN, move install pcaGoPromoter.","code":""},{"path":"dealing-with-unbalanced-data.html","id":"dealing-with-unbalanced-data","chapter":"13 Dealing with unbalanced data","heading":"13 Dealing with unbalanced data","text":"","code":""},{"path":"dealing-with-unbalanced-data.html","id":"breast-cancer-dataset","chapter":"13 Dealing with unbalanced data","heading":"13.1 Breast cancer dataset","text":"","code":""},{"path":"dealing-with-unbalanced-data.html","id":"introduction-6","chapter":"13 Dealing with unbalanced data","heading":"13.2 Introduction","text":"Source: https://shiring.github.io/machine_learning/2017/04/02/unbalancedIn last post, shared code used produce example analysis go along webinar building meaningful models disease prediction, mentioned advised consider - -sampling unbalanced data sets. focus webinar evaluating model performance, want add additional layer complexity therefore discuss specifically deal unbalanced data.gotten questions regarding , thought worthwhile explain - -sampling techniques detail show can easily implement caret.","code":"\nlibrary(caret)\n#> Loading required package: lattice\n#> Loading required package: ggplot2\nlibrary(mice)\n#> \n#> Attaching package: 'mice'\n#> The following objects are masked from 'package:base':\n#> \n#>     cbind, rbind\nlibrary(ggplot2)"},{"path":"dealing-with-unbalanced-data.html","id":"read-and-process-the-data-1","chapter":"13 Dealing with unbalanced data","heading":"13.3 Read and process the data","text":"","code":"\nbc_data <- read.table(file.path(data_raw_dir, \"breast-cancer-wisconsin.data\"), \n                      header = FALSE, sep = \",\")\n\ncolnames(bc_data) <- c(\"sample_code_number\", \"clump_thickness\", \n                       \"uniformity_of_cell_size\", \"uniformity_of_cell_shape\",\n                       \"marginal_adhesion\", \"single_epithelial_cell_size\", \n                       \"bare_nuclei\", \"bland_chromatin\", \"normal_nucleoli\", \n                       \"mitosis\", \"classes\")\n\nbc_data$classes <- ifelse(bc_data$classes == \"2\", \"benign\",\n                          ifelse(bc_data$classes == \"4\", \"malignant\", NA))\n\nbc_data[bc_data == \"?\"] <- NA\n\n# how many NAs are in the data\nlength(which(is.na(bc_data)))\n#> [1] 16\n# impute missing data\n\n# skip columns: sample_code_number and classes\nbc_data[,2:10] <- apply(bc_data[, 2:10], 2, function(x) as.numeric(as.character(x)))\n\n# impute but stay mute\ndataset_impute <- mice(bc_data[, 2:10],  print = FALSE)\n\n# bind \"classes\" with the rest. skip \"sample_code_number\"\nbc_data <- cbind(bc_data[, 11, drop = FALSE], \n                 mice::complete(dataset_impute, action = 1))\n\nbc_data$classes <- as.factor(bc_data$classes)"},{"path":"dealing-with-unbalanced-data.html","id":"unbalanced-data","chapter":"13 Dealing with unbalanced data","heading":"13.3.1 Unbalanced data","text":"context, unbalanced data refers classification problems unequal instances different classes. unbalanced data actually common general, especially prevalent working disease data usually healthy control samples disease cases. Even extreme unbalance seen fraud detection, e.g. credit card uses okay fraudulent. example used webinar, breast cancer dataset, twice many benign malignant samples.","code":"\n# how many benign and malignant cases are there?\nsummary(bc_data$classes)\n#>    benign malignant \n#>       458       241"},{"path":"dealing-with-unbalanced-data.html","id":"why-is-unbalanced-data-a-problem-in-machine-learning","chapter":"13 Dealing with unbalanced data","heading":"13.3.1.1 Why is unbalanced data a problem in machine learning?","text":"machine learning classification algorithms sensitive unbalance predictor classes. Let’s consider even extreme example breast cancer dataset: assume 10 malignant vs 90 benign samples. machine learning model trained tested dataset now predict “benign” samples still gain high accuracy. unbalanced dataset bias prediction model towards common class!","code":""},{"path":"dealing-with-unbalanced-data.html","id":"how-to-balance-data-for-modeling","chapter":"13 Dealing with unbalanced data","heading":"13.3.1.2 How to balance data for modeling","text":"basic theoretical concepts behind - -sampling simple:-sampling, randomly select subset samples class instances match number samples coming class. example, randomly pick 241 458 benign cases. main disadvantage -sampling lose potentially relevant information left-samples.oversampling, randomly duplicate samples class fewer instances generate additional instances based data , match number samples class. avoid losing information approach, also run risk overfitting model likely get samples training test data, .e. test data longer independent training data. lead overestimation model’s performance generalizability.reality though, simply perform - -sampling training data run model. need account cross-validation perform - -sampling fold independently get honest estimate model performance!","code":""},{"path":"dealing-with-unbalanced-data.html","id":"modeling-the-original-unbalanced-data","chapter":"13 Dealing with unbalanced data","heading":"13.3.1.3 Modeling the original unbalanced data","text":"model used webinar example: randomly divide data training test sets (stratified class) perform Random Forest modeling 10 x 10 repeated cross-validation. Final model performance measured test set.","code":"\nset.seed(42)\nindex <- createDataPartition(bc_data$classes, p = 0.7, list = FALSE)\ntrain_data <- bc_data[index, ]\ntest_data  <- bc_data[-index, ]\nset.seed(42)\nmodel_rf <- caret::train(classes ~ .,\n                         data = train_data,\n                         method = \"rf\",\n                         preProcess = c(\"scale\", \"center\"),\n                         trControl = trainControl(method = \"repeatedcv\", \n                                                  number = 10, \n                                                  repeats = 10, \n                                                  verboseIter = FALSE))\nfinal <- data.frame(actual = test_data$classes,\n                    predict(model_rf, \n                            newdata = test_data, \n                            type = \"prob\"))\n\nfinal$predict <- ifelse(final$benign > 0.5, \"benign\", \"malignant\")\nfinal_predict <- as.factor(final$predict)\ntest_data_classes <- as.factor(test_data$classes)\n\ncm_original <- confusionMatrix(final_predict, test_data_classes)\ncm_original$byClass['Sensitivity']\n#> Sensitivity \n#>       0.978"},{"path":"dealing-with-unbalanced-data.html","id":"under-sampling","chapter":"13 Dealing with unbalanced data","heading":"13.4 Under-sampling","text":"Luckily, caret makes easy incorporate - -sampling techniques cross-validation resampling. can simply add sampling option trainControl choose - (also called -) sampling. rest stays original model.","code":"\nset.seed(42)\nctrl <- trainControl(method = \"repeatedcv\", \n                     number = 10, \n                     repeats = 10, \n                     verboseIter = FALSE,\n                     sampling = \"down\")\n\n\nmodel_rf_under <- caret::train(classes ~ .,\n                         data = train_data,\n                         method = \"rf\",\n                         preProcess = c(\"scale\", \"center\"),\n                         trControl = ctrl)\nfinal_under <- data.frame(actual = test_data$classes,\n                    predict(model_rf_under, \n                            newdata = test_data, \n                            type = \"prob\"))\n\nfinal_under$predict <- ifelse(final_under$benign > 0.5, \"benign\", \"malignant\")\nfinal_under_predict <- as.factor(final_under$predict)\ntest_data_classes <- test_data$classes\n\ncm_under <- confusionMatrix(final_under_predict, test_data_classes)\ncm_under$byClass['Sensitivity']\n#> Sensitivity \n#>       0.978"},{"path":"dealing-with-unbalanced-data.html","id":"oversampling","chapter":"13 Dealing with unbalanced data","heading":"13.5 Oversampling","text":"- (also called -) sampling simply specify sampling = “”.","code":"\nset.seed(42)\nctrl <- trainControl(method = \"repeatedcv\", \n                     number = 10, \n                     repeats = 10, \n                     verboseIter = FALSE,\n                     sampling = \"up\")\n\n\nmodel_rf_over <- caret::train(classes ~ .,\n                         data = train_data,\n                         method = \"rf\",\n                         preProcess = c(\"scale\", \"center\"),\n                         trControl = ctrl)\nfinal_over <- data.frame(actual = test_data$classes,\n                          predict(model_rf_over, \n                                  newdata = test_data, \n                                  type = \"prob\"))\n\nfinal_over$predict <- ifelse(final_over$benign > 0.5, \"benign\", \"malignant\")\nfinal_over_predict <- as.factor(final_over$predict)\ntest_data_classes <- test_data$classes\n\ncm_over <- confusionMatrix(final_over_predict, test_data_classes)\ncm_over$byClass['Sensitivity']\n#> Sensitivity \n#>       0.978"},{"path":"dealing-with-unbalanced-data.html","id":"rose","chapter":"13 Dealing with unbalanced data","heading":"13.5.1 ROSE","text":"Besides - -sampling, hybrid methods combine -sampling generation additional data. Two popular ROSE SMOTE.Nicola Lunardon, Giovanna Menardi Nicola Torelli’s “ROSE: Package Binary Imbalanced Learning” (R Journal, 2014, Vol. 6 Issue 1, p. 79): “ROSE package provides functions deal binary classification problems presence imbalanced classes. Artificial balanced samples generated according smoothed bootstrap approach allow aiding phases estimation accuracy evaluation binary classifier presence rare class. Functions implement traditional remedies class imbalance different metrics evaluate accuracy also provided. estimated holdout, bootstrap, cross-validation methods.”implement way , time choosing sampling = “rose”…","code":"\nset.seed(42)\nctrl <- trainControl(method = \"repeatedcv\", \n                     number = 10, \n                     repeats = 10, \n                     verboseIter = FALSE,\n                     sampling = \"rose\")\n\nmodel_rf_rose <- caret::train(classes ~ .,\n                              data = train_data,\n                              method = \"rf\",\n                              preProcess = c(\"scale\", \"center\"),\n                              trControl = ctrl)\n#> Loaded ROSE 0.0-3\nfinal_rose <- data.frame(actual = test_data$classes,\n                         predict(model_rf_rose, \n                                 newdata = test_data, \n                                 type = \"prob\"))\n\nfinal_rose$predict <- ifelse(final_rose$benign > 0.5, \"benign\", \"malignant\")\ncm_rose <- confusionMatrix(as.factor(final_rose$predict), \n                           as.factor(test_data$classes))\ncm_rose$byClass['Sensitivity']\n#> Sensitivity \n#>       0.985"},{"path":"dealing-with-unbalanced-data.html","id":"smote","chapter":"13 Dealing with unbalanced data","heading":"13.5.2 SMOTE","text":"… choosing sampling = “smote” trainControl settings.Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall W. Philip Kegelmeyer’s “SMOTE: Synthetic Minority -sampling Technique” (Journal Artificial Intelligence Research, 2002, Vol. 16, pp. 321–357): “paper shows combination method -sampling minority (abnormal) class -sampling majority (normal) class can achieve better classifier performance (ROC space) -sampling majority class. paper also shows combination method -sampling minority class -sampling majority class can achieve better classifier performance (ROC space) varying loss ratios Ripper class priors Naive Bayes. method -sampling minority class involves creating synthetic minority class examples.”","code":"\nset.seed(42)\nctrl <- trainControl(method = \"repeatedcv\", \n                     number = 10, \n                     repeats = 10, \n                     verboseIter = FALSE,\n                     sampling = \"smote\")\n\nmodel_rf_smote <- caret::train(classes ~ .,\n                              data = train_data,\n                              method = \"rf\",\n                              preProcess = c(\"scale\", \"center\"),\n                              trControl = ctrl)\n#> Loading required package: grid\n#> Registered S3 method overwritten by 'quantmod':\n#>   method            from\n#>   as.zoo.data.frame zoo\nfinal_smote <- data.frame(actual = test_data$classes,\n                         predict(model_rf_smote, \n                                 newdata = test_data, \n                                 type = \"prob\"))\n\nfinal_smote$predict <- ifelse(final_smote$benign > 0.5, \"benign\", \"malignant\")\ncm_smote <- confusionMatrix(as.factor(final_smote$predict), \n                            as.factor(test_data$classes))\ncm_smote$byClass['Sensitivity']\n#> Sensitivity \n#>       0.978"},{"path":"dealing-with-unbalanced-data.html","id":"predictions","chapter":"13 Dealing with unbalanced data","heading":"13.6 Predictions","text":"Now let’s compare predictions models:small dataset, can already see different techniques can influence model performance. Sensitivity (recall) describes proportion benign cases predicted correctly, specificity describes proportion malignant cases predicted correctly. Precision describes true positives, .e. proportion benign predictions actual benign samples. F1 weighted average precision sensitivity/ recall.","code":"\nmodels <- list(\n                original = model_rf,\n                under = model_rf_under,\n                over = model_rf_over,\n                smote = model_rf_smote,\n                rose = model_rf_rose)\n\nresampling <- resamples(models)\nbwplot(resampling)\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\ncomparison <- data.frame(model = names(models),\n                         Sensitivity = rep(NA, length(models)),\n                         Specificity = rep(NA, length(models)),\n                         Precision   = rep(NA, length(models)),\n                         Recall      = rep(NA, length(models)),\n                         F1          = rep(NA, length(models)))\n\nfor (name in names(models)) {\n    cm_model <- get(paste0(\"cm_\", name))\n    comparison[comparison$model==name, ] <- filter(comparison, model==name) %>%\n    mutate(Sensitivity = cm_model$byClass[\"Sensitivity\"],\n           Specificity = cm_model$byClass[\"Specificity\"],\n           Precision   = cm_model$byClass[\"Precision\"],\n           Recall      = cm_model$byClass[\"Recall\"],\n           F1          = cm_model$byClass[\"F1\"]\n  )\n}    \n\nprint(comparison)\n#>      model Sensitivity Specificity Precision Recall    F1\n#> 1 original       0.978       0.986     0.993  0.978 0.985\n#> 2    under       0.978       1.000     1.000  0.978 0.989\n#> 3     over       0.978       0.986     0.993  0.978 0.985\n#> 4    smote       0.978       0.986     0.993  0.978 0.985\n#> 5     rose       0.985       0.986     0.993  0.985 0.989\nlibrary(tidyr)\ncomparison %>%\n  gather(x, y, Sensitivity:F1) %>%\n  ggplot(aes(x = x, y = y, color = model)) +\n    geom_jitter(width = 0.2, alpha = 0.5, size = 3)"},{"path":"dealing-with-unbalanced-data.html","id":"final-notes","chapter":"13 Dealing with unbalanced data","heading":"13.7 Final notes","text":", four methods improved specificity precision compared original model. -sampling, -sampling ROSE additionally improved precision F1 score.post shows simple example correct unbalance datasets machine learning. advanced instructions potential caveats techniques, check excellent caret documentation.interested machine learning posts, check category listing machine_learning blog.","code":""},{"path":"imputting-missing-values-with-random-forest.html","id":"imputting-missing-values-with-random-forest","chapter":"14 Imputting missing values with Random Forest","heading":"14 Imputting missing values with Random Forest","text":"","code":""},{"path":"imputting-missing-values-with-random-forest.html","id":"flu-prediction.-fluh7n9_china_2013-dataset","chapter":"14 Imputting missing values with Random Forest","heading":"14.1 Flu Prediction. fluH7N9_china_2013 dataset","text":"Source: https://shirinsplayground.netlify.com/2018/04/flu_prediction/Since migrated blog Github Pages blogdown Netlify, wanted start migrating () old posts - use opportunity update make sure code still works.updating first machine learning post 27 Nov 2016: Can predict flu deaths Machine Learning R?. Changes marked bold comments.main changes made :using tidyverse consistently throughout analysisusing tidyverse consistently throughout analysisfocusing comparing multiple imputations mice package, rather comparing different algorithmsfocusing comparing multiple imputations mice package, rather comparing different algorithmsusing purrr, map(), nest() unnest() model predict machine learning algorithm different imputed datasetsusing purrr, map(), nest() unnest() model predict machine learning algorithm different imputed datasetsAmong many nice R packages containing data collections outbreaks package. contains dataset epidemics among data 2013 outbreak influenza H7N9 China analysed Kucharski et al. (2014):. Kucharski, H. Mills, . Pinsent, C. Fraser, M. Van Kerkhove, C. . Donnelly, S. Riley. 2014. Distinguishing reservoir exposure human--human transmission emerging pathogens using case onset data. PLOS Currents Outbreaks. Mar 7, edition 1. doi: 10.1371/currents.outbreaks.e1473d9bfc99d080ca242139a06c455f.. Kucharski, H. Mills, . Pinsent, C. Fraser, M. Van Kerkhove, C. . Donnelly, S. Riley. 2014. Data : Distinguishing reservoir exposure human--human transmission emerging pathogens using case onset data. Dryad Digital Repository. http://dx.doi.org/10.5061/dryad.2g43n.using data example show use Machine Learning algorithms predicting disease outcome.","code":"\nlibrary(outbreaks)\nlibrary(tidyverse)\nlibrary(plyr)\nlibrary(mice)\nlibrary(caret)\nlibrary(purrr)\nlibrary(\"tibble\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")"},{"path":"imputting-missing-values-with-random-forest.html","id":"the-data","chapter":"14 Imputting missing values with Random Forest","heading":"14.2 The data","text":"dataset contains case ID, date onset, date hospitalization, date outcome, gender, age, province course outcome: Death Recovery.","code":""},{"path":"imputting-missing-values-with-random-forest.html","id":"pre-processing","chapter":"14 Imputting missing values with Random Forest","heading":"14.2.1 Pre-processing","text":"Change: variable names (.e. column names) renamed, dots replaced underscores, letters lower case now.Change: using tidyverse notation consistently.First, ’m preprocessing, including:renaming missing data NAadding ID columnsetting column typesgathering date columnschanging factor names dates (make look nicer plots) province (combine provinces cases)’m also adding third gender level unknown genderFor plotting, defining custom ggplot2 theme:use theme visualize data:","code":"\nfrom1 <- c(\"date_of_onset\", \"date_of_hospitalisation\", \"date_of_outcome\")\nto1   <- c(\"date of onset\", \"date of hospitalisation\", \"date of outcome\")\nfrom2 <- c(\"Anhui\", \"Beijing\", \"Fujian\", \"Guangdong\", \"Hebei\", \"Henan\", \n           \"Hunan\", \"Jiangxi\", \"Shandong\", \"Taiwan\")\nto2   <- rep(\"Other\", 10)\n\nfluH7N9_china_2013$age[which(fluH7N9_china_2013$age == \"?\")] <- NA\nfluH7N9_china_2013_gather <- fluH7N9_china_2013 %>%\n  mutate(case_id = paste(\"case\", case_id, sep = \"_\"),\n         age = as.numeric(age)) %>%\n  gather(Group, Date, date_of_onset:date_of_outcome) %>%\n  mutate(Group = as.factor(mapvalues(Group, from = from1, to = to1)),\n         province = mapvalues(province, from = from2, to = to2))\n\nfluH7N9_china_2013 <- as.tibble(fluH7N9_china_2013)\n#> Warning: `as.tibble()` is deprecated as of tibble 2.0.0.\n#> Please use `as_tibble()` instead.\n#> The signature and semantics have changed, see `?as_tibble`.\n#> This warning is displayed once every 8 hours.\n#> Call `lifecycle::last_warnings()` to see where this warning was generated.\nfluH7N9_china_2013_gather <- as.tibble(fluH7N9_china_2013_gather)\nprint(fluH7N9_china_2013)\n#> # A tibble: 136 x 8\n#>   case_id date_of_onset date_of_hospita… date_of_outcome outcome gender age  \n#>   <fct>   <date>        <date>           <date>          <fct>   <fct>  <fct>\n#> 1 1       2013-02-19    NA               2013-03-04      Death   m      87   \n#> 2 2       2013-02-27    2013-03-03       2013-03-10      Death   m      27   \n#> 3 3       2013-03-09    2013-03-19       2013-04-09      Death   f      35   \n#> 4 4       2013-03-19    2013-03-27       NA              <NA>    f      45   \n#> 5 5       2013-03-19    2013-03-30       2013-05-15      Recover f      48   \n#> 6 6       2013-03-21    2013-03-28       2013-04-26      Death   f      32   \n#> # … with 130 more rows, and 1 more variable: province <fct>\nlevels(fluH7N9_china_2013_gather$gender) <- \n  c(levels(fluH7N9_china_2013_gather$gender), \"unknown\")\nfluH7N9_china_2013_gather$gender[is.na(fluH7N9_china_2013_gather$gender)] <- \"unknown\"\nprint(fluH7N9_china_2013_gather)\n#> # A tibble: 408 x 7\n#>   case_id outcome gender   age province Group         Date      \n#>   <chr>   <fct>   <fct>  <dbl> <fct>    <fct>         <date>    \n#> 1 case_1  Death   m         58 Shanghai date of onset 2013-02-19\n#> 2 case_2  Death   m          7 Shanghai date of onset 2013-02-27\n#> 3 case_3  Death   f         11 Other    date of onset 2013-03-09\n#> 4 case_4  <NA>    f         18 Jiangsu  date of onset 2013-03-19\n#> 5 case_5  Recover f         20 Jiangsu  date of onset 2013-03-19\n#> 6 case_6  Death   f          9 Jiangsu  date of onset 2013-03-21\n#> # … with 402 more rows\nmy_theme <- function(base_size = 12, base_family = \"sans\"){\n  theme_minimal(base_size = base_size, base_family = base_family) +\n  theme(\n    axis.text = element_text(size = 12),\n    axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = 0.5),\n    axis.title = element_text(size = 14),\n    panel.grid.major = element_line(color = \"grey\"),\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"aliceblue\"),\n    strip.background = element_rect(fill = \"lightgrey\", color = \"grey\", size = 1),\n    strip.text = element_text(face = \"bold\", size = 12, color = \"black\"),\n    legend.position = \"bottom\",\n    legend.justification = \"top\", \n    legend.box = \"horizontal\",\n    legend.box.background = element_rect(colour = \"grey50\"),\n    legend.background = element_blank(),\n    panel.border = element_rect(color = \"grey\", fill = NA, size = 0.5)\n  )\n}\nggplot(data = fluH7N9_china_2013_gather, aes(x = Date, y = age, fill = outcome)) +\n  stat_density2d(aes(alpha = ..level..), geom = \"polygon\") +\n  geom_jitter(aes(color = outcome, shape = gender), size = 1.5) +\n  geom_rug(aes(color = outcome)) +\n  scale_y_continuous(limits = c(0, 90)) +\n  labs(\n    fill = \"Outcome\",\n    color = \"Outcome\",\n    alpha = \"Level\",\n    shape = \"Gender\",\n    x = \"Date in 2013\",\n    y = \"Age\",\n    title = \"2013 Influenza A H7N9 cases in China\",\n    subtitle = \"Dataset from 'outbreaks' package (Kucharski et al. 2014)\",\n    caption = \"\"\n  ) +\n  facet_grid(Group ~ province) +\n  my_theme() +\n  scale_shape_manual(values = c(15, 16, 17)) +\n  scale_color_brewer(palette=\"Set1\", na.value = \"grey50\") +\n  scale_fill_brewer(palette=\"Set1\")\n#> Warning: Removed 149 rows containing non-finite values (stat_density2d).\n#> Warning: Removed 149 rows containing missing values (geom_point).\nggplot(data = fluH7N9_china_2013_gather, aes(x = Date, y = age, color = outcome)) +\n  geom_point(aes(color = outcome, shape = gender), size = 1.5, alpha = 0.6) +\n  geom_path(aes(group = case_id)) +\n  facet_wrap( ~ province, ncol = 2) +\n  my_theme() +\n  scale_shape_manual(values = c(15, 16, 17)) +\n  scale_color_brewer(palette=\"Set1\", na.value = \"grey50\") +\n  scale_fill_brewer(palette=\"Set1\") +\n  labs(\n    color = \"Outcome\",\n    shape = \"Gender\",\n    x = \"Date in 2013\",\n    y = \"Age\",\n    title = \"2013 Influenza A H7N9 cases in China\",\n    subtitle = \"Dataset from 'outbreaks' package (Kucharski et al. 2014)\",\n    caption = \"\\nTime from onset of flu to outcome.\"\n  )\n#> Warning: Removed 149 rows containing missing values (geom_point).\n#> Warning: Removed 122 row(s) containing missing values (geom_path)."},{"path":"imputting-missing-values-with-random-forest.html","id":"features","chapter":"14 Imputting missing values with Random Forest","heading":"14.3 Features","text":"machine learning-speak features call variables used model training. Using right features dramatically influences accuracy success model.example, keeping age, also generating new features date information converting gender province numerical values.","code":"\ndelta_dates <- function(onset, ref) {\n    d2 = as.Date(as.character(onset), format = \"%Y-%m-%d\")\n    d1 = as.Date(as.character(ref), format = \"%Y-%m-%d\")\n    as.numeric(as.character(gsub(\" days\", \"\", d1 - d2)))\n}\ndataset <- fluH7N9_china_2013 %>%\n  mutate(\n      hospital = as.factor(ifelse(is.na(date_of_hospitalisation), 0, 1)), \n      gender_f = as.factor(ifelse(gender == \"f\", 1, 0)), \n      province_Jiangsu = as.factor(ifelse(province == \"Jiangsu\", 1, 0)), \n      province_Shanghai = as.factor(ifelse(province == \"Shanghai\", 1, 0)), \n      province_Zhejiang = as.factor(ifelse(province == \"Zhejiang\", 1, 0)), \n      province_other = as.factor(ifelse(province == \"Zhejiang\" \n                                           | province == \"Jiangsu\" \n                                           | province == \"Shanghai\", 0, 1)),\n      \n      days_onset_to_outcome = delta_dates(date_of_onset, date_of_outcome),\n      days_onset_to_hospital = delta_dates(date_of_onset, date_of_hospitalisation),\n      age = age,\n      early_onset = as.factor(ifelse(date_of_onset < \n                                     summary(date_of_onset)[[3]], 1, 0)),\n      early_outcome = as.factor(ifelse(date_of_outcome <\n                                       summary(date_of_outcome)[[3]], 1, 0))\n    ) %>% \n  subset(select = -c(2:4, 6, 8))\n# convert tibble to data.frame; tibble causing error\ndataset_df <- as.data.frame(dataset)\nrownames(dataset_df) <- dataset_df$case_id\ndataset_df[, -2] <- as.numeric(as.matrix(dataset_df[, -2]))\ndataset <- dataset_df     # copy to dataset object\nglimpse(dataset)\n#> Rows: 136\n#> Columns: 13\n#> $ case_id                <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,…\n#> $ outcome                <fct> Death, Death, Death, NA, Recover, Death, Death…\n#> $ age                    <dbl> 87, 27, 35, 45, 48, 32, 83, 38, 67, 48, 64, 52…\n#> $ hospital               <dbl> 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0…\n#> $ gender_f               <dbl> 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0…\n#> $ province_Jiangsu       <dbl> 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1…\n#> $ province_Shanghai      <dbl> 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0…\n#> $ province_Zhejiang      <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0…\n#> $ province_other         <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#> $ days_onset_to_outcome  <dbl> 13, 11, 31, NA, 57, 36, 20, 20, NA, 6, 6, 7, 1…\n#> $ days_onset_to_hospital <dbl> NA, 4, 10, 8, 11, 7, 9, 11, 0, 4, 2, NA, 3, NA…\n#> $ early_onset            <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n#> $ early_outcome          <dbl> 1, 1, 1, NA, 0, 1, 1, 1, NA, 1, 1, 1, 1, 1, NA…\nsummary(dataset$outcome)\n#>   Death Recover    NA's \n#>      32      47      57"},{"path":"imputting-missing-values-with-random-forest.html","id":"imputing-missing-values","chapter":"14 Imputting missing values with Random Forest","heading":"14.4 Imputing missing values","text":"using mice package imputing missing valuesNote: Since publishing blogpost learned idea behind using mice compare different imputations see stable , instead picking one imputed set fixed remainder analysis. Therefore, changed focus post little bit: old post compared many different algorithms outcome; updated version showing Random Forest algorithm focus comparing different imputed datasets. ignoring feature importance feature plots nothing changed compared old post.","code":"\n# plot the missing data in a matrix by variables\nmd_pattern <- md.pattern(dataset, rotate.names = TRUE)\n# dataset[, -2] would not work anymore in tibbles\ndataset_impute <- mice(data = dataset[, -2],  print = FALSE)\n#> Warning: Number of logged events: 150"},{"path":"imputting-missing-values-with-random-forest.html","id":"generate-a-dataframe-of-five-imputting-strategies","chapter":"14 Imputting missing values with Random Forest","heading":"14.4.1 Generate a dataframe of five imputting strategies","text":"default, mice() calculates five (m = 5) imputed data setswe can combine one output complete(“long”) functionI want impute missing values outcome column, merge back imputed dataLet’s compare distributions five different imputed datasets:","code":"\n# c(1,2): case_id, outcome\ndatasets_complete <- right_join(dataset[, c(1, 2)], \n                           complete(dataset_impute, \"long\"),\n                           by = \"case_id\") %>% \n  mutate(.imp = as.factor(.imp)) %>%   \n  select(-.id) %>% \n  glimpse()\n#> Rows: 680\n#> Columns: 14\n#> $ case_id                <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,…\n#> $ outcome                <fct> Death, Death, Death, NA, Recover, Death, Death…\n#> $ .imp                   <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n#> $ age                    <dbl> 87, 27, 35, 45, 48, 32, 83, 38, 67, 48, 64, 52…\n#> $ hospital               <dbl> 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0…\n#> $ gender_f               <dbl> 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0…\n#> $ province_Jiangsu       <dbl> 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1…\n#> $ province_Shanghai      <dbl> 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0…\n#> $ province_Zhejiang      <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0…\n#> $ province_other         <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#> $ days_onset_to_outcome  <dbl> 13, 11, 31, 57, 57, 36, 20, 20, 6, 6, 6, 7, 12…\n#> $ days_onset_to_hospital <dbl> 4, 4, 10, 8, 11, 7, 9, 11, 0, 4, 2, 0, 3, 1, 7…\n#> $ early_onset            <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n#> $ early_outcome          <dbl> 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…"},{"path":"imputting-missing-values-with-random-forest.html","id":"plot-effect-of-imputting-on-features","chapter":"14 Imputting missing values with Random Forest","heading":"14.4.2 plot effect of imputting on features","text":"","code":"\ndatasets_complete %>%\n  gather(x, y, age:early_outcome) %>% \n  ggplot(aes(x = y, fill = .imp, color = .imp)) +\n    geom_density(alpha = 0.20) +\n  facet_wrap(~ x, ncol = 3, scales = \"free\") +\n    scale_fill_brewer(palette=\"Set1\", na.value = \"grey50\") +\n    scale_color_brewer(palette=\"Set1\", na.value = \"grey50\") +\n    my_theme()"},{"path":"imputting-missing-values-with-random-forest.html","id":"test-train-and-validation-data-sets","chapter":"14 Imputting missing values with Random Forest","heading":"14.5 Test, train and validation data sets","text":"Now, can go ahead machine learning!dataset contains missing values outcome column; test set used final predictions (see old blog post ).remainder data used modeling. , splitting data 70% training 30% test data.want model imputed dataset separately, using nest() map() functions.","code":"\nlength(which(is.na(datasets_complete$outcome)))\nlength(which(!is.na(datasets_complete$outcome)))\n#> [1] 285\n#> [1] 395\ntrain_index <- which(is.na(datasets_complete$outcome))\ntrain_data <- datasets_complete[-train_index, ]\ntest_data  <- datasets_complete[train_index, -2]       # remove variable outcome\nglimpse(train_data)\n#> Rows: 395\n#> Columns: 14\n#> $ case_id                <dbl> 1, 2, 3, 5, 6, 7, 8, 10, 11, 12, 13, 14, 17, 1…\n#> $ outcome                <fct> Death, Death, Death, Recover, Death, Death, De…\n#> $ .imp                   <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n#> $ age                    <dbl> 87, 27, 35, 48, 32, 83, 38, 48, 64, 52, 67, 4,…\n#> $ hospital               <dbl> 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1…\n#> $ gender_f               <dbl> 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0…\n#> $ province_Jiangsu       <dbl> 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#> $ province_Shanghai      <dbl> 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0…\n#> $ province_Zhejiang      <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0…\n#> $ province_other         <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n#> $ days_onset_to_outcome  <dbl> 13, 11, 31, 57, 36, 20, 20, 6, 6, 7, 12, 10, 1…\n#> $ days_onset_to_hospital <dbl> 4, 4, 10, 11, 7, 9, 11, 4, 2, 0, 3, 1, 6, 4, 5…\n#> $ early_onset            <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n#> $ early_outcome          <dbl> 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1…\n# outcome variable removed\nglimpse(test_data)\n#> Rows: 285\n#> Columns: 13\n#> $ case_id                <dbl> 4, 9, 15, 16, 22, 28, 31, 32, 38, 39, 40, 41, …\n#> $ .imp                   <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n#> $ age                    <dbl> 45, 67, 61, 79, 85, 79, 70, 74, 56, 66, 74, 54…\n#> $ hospital               <dbl> 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0…\n#> $ gender_f               <dbl> 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1…\n#> $ province_Jiangsu       <dbl> 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0…\n#> $ province_Shanghai      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0…\n#> $ province_Zhejiang      <dbl> 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1…\n#> $ province_other         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#> $ days_onset_to_outcome  <dbl> 57, 6, 37, 30, 21, 17, 14, 13, 11, 15, 10, 22,…\n#> $ days_onset_to_hospital <dbl> 8, 0, 7, 11, 4, 6, 4, 6, 4, 0, 5, 6, 7, 3, 6, …\n#> $ early_onset            <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1…\n#> $ early_outcome          <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1…\ntrain_data_nest <- train_data %>%\n  group_by(.imp) %>%\n  nest() %>%\n  print()\n#> # A tibble: 5 x 2\n#> # Groups:   .imp [5]\n#>   .imp  data              \n#>   <fct> <list>            \n#> 1 1     <tibble [79 × 13]>\n#> 2 2     <tibble [79 × 13]>\n#> 3 3     <tibble [79 × 13]>\n#> 4 4     <tibble [79 × 13]>\n#> 5 5     <tibble [79 × 13]>\n# split the training data in validation training and validation test\nset.seed(42)\nval_data <- train_data_nest %>% \n  mutate(val_index = map(data, ~ createDataPartition(.$outcome, \n                                                     p = 0.7, \n                                               list = FALSE)),\n         val_train_data = map2(data, val_index, ~ .x[.y, ]),\n         val_test_data  = map2(data, val_index, ~ .x[-.y, ])) %>% \n  print()\n#> Warning: The `i` argument of ``[`()` can't be a matrix as of tibble 3.0.0.\n#> Convert to a vector.\n#> This warning is displayed once every 8 hours.\n#> Call `lifecycle::last_warnings()` to see where this warning was generated.\n#> # A tibble: 5 x 5\n#> # Groups:   .imp [5]\n#>   .imp  data              val_index          val_train_data    val_test_data    \n#>   <fct> <list>            <list>             <list>            <list>           \n#> 1 1     <tibble [79 × 13… <int[,1] [56 × 1]> <tibble [56 × 13… <tibble [23 × 13…\n#> 2 2     <tibble [79 × 13… <int[,1] [56 × 1]> <tibble [56 × 13… <tibble [23 × 13…\n#> 3 3     <tibble [79 × 13… <int[,1] [56 × 1]> <tibble [56 × 13… <tibble [23 × 13…\n#> 4 4     <tibble [79 × 13… <int[,1] [56 × 1]> <tibble [56 × 13… <tibble [23 × 13…\n#> 5 5     <tibble [79 × 13… <int[,1] [56 × 1]> <tibble [56 × 13… <tibble [23 × 13…"},{"path":"imputting-missing-values-with-random-forest.html","id":"machine-learning-algorithms","chapter":"14 Imputting missing values with Random Forest","heading":"14.6 Machine Learning algorithms","text":"","code":""},{"path":"imputting-missing-values-with-random-forest.html","id":"random-forest","chapter":"14 Imputting missing values with Random Forest","heading":"14.6.1 Random Forest","text":"make code tidier, first defining modeling function parameters want.","code":"\nmodel_function <- function(df) {\n  caret::train(outcome ~ .,\n               data = df,\n               method = \"rf\",\n               preProcess = c(\"scale\", \"center\"),\n               trControl = trainControl(method = \"repeatedcv\", \n                                        number = 5, \n                                        repeats = 3, \n                                        verboseIter = FALSE))\n}"},{"path":"imputting-missing-values-with-random-forest.html","id":"add-model-and-prediction-to-nested-dataframe-and-calculate","chapter":"14 Imputting missing values with Random Forest","heading":"14.6.2 Add model and prediction to nested dataframe and calculate","text":"Next, using nested tibble map() model function, predict outcome calculate confusion matrices.","code":""},{"path":"imputting-missing-values-with-random-forest.html","id":"add-model-list-column","chapter":"14 Imputting missing values with Random Forest","heading":"14.6.2.1 add model list-column","text":"","code":"\nval_data_model <- val_data %>%\n  mutate(model = map(val_train_data, ~ model_function(.x))) %>% \n  select(-val_index) %>% \n  print()\n#> # A tibble: 5 x 5\n#> # Groups:   .imp [5]\n#>   .imp  data               val_train_data     val_test_data      model  \n#>   <fct> <list>             <list>             <list>             <list> \n#> 1 1     <tibble [79 × 13]> <tibble [56 × 13]> <tibble [23 × 13]> <train>\n#> 2 2     <tibble [79 × 13]> <tibble [56 × 13]> <tibble [23 × 13]> <train>\n#> 3 3     <tibble [79 × 13]> <tibble [56 × 13]> <tibble [23 × 13]> <train>\n#> 4 4     <tibble [79 × 13]> <tibble [56 × 13]> <tibble [23 × 13]> <train>\n#> 5 5     <tibble [79 × 13]> <tibble [56 × 13]> <tibble [23 × 13]> <train>"},{"path":"imputting-missing-values-with-random-forest.html","id":"add-prediction-and-confusion-matrix-list-columns","chapter":"14 Imputting missing values with Random Forest","heading":"14.6.2.2 add prediction and confusion matrix list-columns","text":"Finally, nested dataframe 5 rows cases, one per imputting strategy corresponding models prediction results.","code":"\nset.seed(42)\nval_data_model <- val_data_model %>%\n  mutate(\n         predict = map2(model, val_test_data, ~ \n                            data.frame(prediction = predict(.x, .y[, -2]))),\n         predict_prob = map2(model, val_test_data, ~ \n                            data.frame(outcome = .y[, 2], \n                            prediction = predict(.x, .y[, -2], type = \"prob\"))),\n         confusion_matrix = map2(val_test_data, predict, ~ \n                                     confusionMatrix(.x$outcome, .y$prediction)),\n         confusion_matrix_tbl = map(confusion_matrix, ~ as.tibble(.x$table))) %>% \n  print()\n#> # A tibble: 5 x 9\n#> # Groups:   .imp [5]\n#>   .imp  data  val_train_data val_test_data model predict predict_prob\n#>   <fct> <lis> <list>         <list>        <lis> <list>  <list>      \n#> 1 1     <tib… <tibble [56 ×… <tibble [23 … <tra… <df[,1… <df[,3] [23…\n#> 2 2     <tib… <tibble [56 ×… <tibble [23 … <tra… <df[,1… <df[,3] [23…\n#> 3 3     <tib… <tibble [56 ×… <tibble [23 … <tra… <df[,1… <df[,3] [23…\n#> 4 4     <tib… <tibble [56 ×… <tibble [23 … <tra… <df[,1… <df[,3] [23…\n#> 5 5     <tib… <tibble [56 ×… <tibble [23 … <tra… <df[,1… <df[,3] [23…\n#> # … with 2 more variables: confusion_matrix <list>, confusion_matrix_tbl <list>"},{"path":"imputting-missing-values-with-random-forest.html","id":"comparing-accuracy-of-models","chapter":"14 Imputting missing values with Random Forest","heading":"14.7 Comparing accuracy of models","text":"compare different imputations , plotting confusion matrices:prediction probabilities correct wrong predictions:Hope, found example interesting helpful!","code":"\nval_data_model_unnest <- val_data_model %>%\n  unnest(confusion_matrix_tbl) %>%\n  print()\n#> # A tibble: 20 x 11\n#> # Groups:   .imp [5]\n#>   .imp  data  val_train_data val_test_data model predict predict_prob\n#>   <fct> <lis> <list>         <list>        <lis> <list>  <list>      \n#> 1 1     <tib… <tibble [56 ×… <tibble [23 … <tra… <df[,1… <df[,3] [23…\n#> 2 1     <tib… <tibble [56 ×… <tibble [23 … <tra… <df[,1… <df[,3] [23…\n#> 3 1     <tib… <tibble [56 ×… <tibble [23 … <tra… <df[,1… <df[,3] [23…\n#> 4 1     <tib… <tibble [56 ×… <tibble [23 … <tra… <df[,1… <df[,3] [23…\n#> 5 2     <tib… <tibble [56 ×… <tibble [23 … <tra… <df[,1… <df[,3] [23…\n#> 6 2     <tib… <tibble [56 ×… <tibble [23 … <tra… <df[,1… <df[,3] [23…\n#> # … with 14 more rows, and 4 more variables: confusion_matrix <list>,\n#> #   Prediction <chr>, Reference <chr>, n <int>\nval_data_model_unnest %>% \n  ggplot(aes(x = Prediction, y = Reference, fill = n)) +\n    facet_wrap(~ .imp, ncol = 5, scales = \"free\") +\n    geom_tile() +\n    my_theme()\nval_data_model_gather <- val_data_model %>%\n  unnest(predict_prob) %>%\n  gather(x, y, prediction.Death:prediction.Recover) %>%\n  print()\n#> # A tibble: 230 x 11\n#> # Groups:   .imp [5]\n#>   .imp  data  val_train_data val_test_data model predict outcome\n#>   <fct> <lis> <list>         <list>        <lis> <list>  <fct>  \n#> 1 1     <tib… <tibble [56 ×… <tibble [23 … <tra… <df[,1… Death  \n#> 2 1     <tib… <tibble [56 ×… <tibble [23 … <tra… <df[,1… Recover\n#> 3 1     <tib… <tibble [56 ×… <tibble [23 … <tra… <df[,1… Death  \n#> 4 1     <tib… <tibble [56 ×… <tibble [23 … <tra… <df[,1… Death  \n#> 5 1     <tib… <tibble [56 ×… <tibble [23 … <tra… <df[,1… Recover\n#> 6 1     <tib… <tibble [56 ×… <tibble [23 … <tra… <df[,1… Recover\n#> # … with 224 more rows, and 4 more variables: confusion_matrix <list>,\n#> #   confusion_matrix_tbl <list>, x <chr>, y <dbl>\nval_data_model_gather %>% \n  ggplot(aes(x = x, y = y, fill = outcome)) +\n    facet_wrap(~ .imp, ncol = 5) +\n    geom_boxplot() +\n    scale_fill_brewer(palette=\"Set1\", na.value = \"grey50\") +\n    my_theme()\nsessionInfo()\n#> R version 3.6.3 (2020-02-29)\n#> Platform: x86_64-pc-linux-gnu (64-bit)\n#> Running under: Debian GNU/Linux 10 (buster)\n#> \n#> Matrix products: default\n#> BLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.3.5.so\n#> \n#> locale:\n#>  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n#>  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n#>  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=C             \n#>  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n#>  [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n#> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] caret_6.0-86    lattice_0.20-38 mice_3.8.0      plyr_1.8.6     \n#>  [5] forcats_0.5.0   stringr_1.4.0   dplyr_0.8.5     purrr_0.3.4    \n#>  [9] readr_1.3.1     tidyr_1.0.2     tibble_3.0.1    ggplot2_3.3.0  \n#> [13] tidyverse_1.3.0 outbreaks_1.5.0\n#> \n#> loaded via a namespace (and not attached):\n#>  [1] nlme_3.1-144         fs_1.4.1             lubridate_1.7.8     \n#>  [4] RColorBrewer_1.1-2   httr_1.4.1           tools_3.6.3         \n#>  [7] backports_1.1.6      bslib_0.2.2.9000     utf8_1.1.4          \n#> [10] R6_2.4.1             rpart_4.1-15         DBI_1.1.0           \n#> [13] colorspace_1.4-1     nnet_7.3-12          withr_2.2.0         \n#> [16] tidyselect_1.0.0     downlit_0.2.1.9000   compiler_3.6.3      \n#> [19] cli_2.0.2            rvest_0.3.5          xml2_1.3.2          \n#> [22] isoband_0.2.1        labeling_0.3         bookdown_0.21.4     \n#> [25] sass_0.2.0.9005      scales_1.1.0         randomForest_4.6-14 \n#> [28] rappdirs_0.3.1       digest_0.6.25        rmarkdown_2.5.3     \n#> [31] pkgconfig_2.0.3      htmltools_0.5.0.9003 dbplyr_1.4.3        \n#> [34] rlang_0.4.5          readxl_1.3.1         rstudioapi_0.11     \n#> [37] farver_2.0.3         jquerylib_0.1.2      generics_0.0.2      \n#> [40] jsonlite_1.6.1       ModelMetrics_1.2.2.2 magrittr_1.5        \n#> [43] Matrix_1.2-18        Rcpp_1.0.4.6         munsell_0.5.0       \n#> [46] fansi_0.4.1          lifecycle_0.2.0      stringi_1.4.6       \n#> [49] pROC_1.16.2          yaml_2.2.1           MASS_7.3-51.5       \n#> [52] recipes_0.1.10       grid_3.6.3           crayon_1.3.4        \n#> [55] haven_2.2.0          splines_3.6.3        hms_0.5.3           \n#> [58] knitr_1.28           pillar_1.4.3         reshape2_1.4.4      \n#> [61] codetools_0.2-16     stats4_3.6.3         reprex_0.3.0        \n#> [64] glue_1.4.0           evaluate_0.14        data.table_1.12.8   \n#> [67] modelr_0.1.6         vctrs_0.2.4          foreach_1.5.0       \n#> [70] cellranger_1.1.0     gtable_0.3.0         assertthat_0.2.1    \n#> [73] xfun_0.19.4          gower_0.2.1          prodlim_2019.11.13  \n#> [76] broom_0.5.6          e1071_1.7-3          class_7.3-15        \n#> [79] survival_3.1-8       timeDate_3043.102    iterators_1.0.12    \n#> [82] lava_1.6.7           ellipsis_0.3.0       ipred_0.9-9"},{"path":"tuning-of-support-vector-machine-prediction.html","id":"tuning-of-support-vector-machine-prediction","chapter":"15 Tuning of Support Vector Machine prediction","heading":"15 Tuning of Support Vector Machine prediction","text":"Datasets: irisAlgorithms:\nSupport Vector Machines\nSupport Vector Machineshttps://eight2late.wordpress.com/2017/02/07/-gentle-introduction--support-vector-machines-using-r/","code":""},{"path":"tuning-of-support-vector-machine-prediction.html","id":"support-vector-machines-in-r-1","chapter":"15 Tuning of Support Vector Machine prediction","heading":"15.1 Support vector machines in R","text":"demo ’ll use svm interface implemented e1071 R package. interface provides R programmers access comprehensive libsvm library written Chang Lin. ’ll use two toy datasets: famous iris dataset available base R package sonar dataset mlbench package. won’t describe details datasets discussed length documentation linked . However, worth mentioning reasons chose datasets:mentioned earlier, real life dataset linearly separable, iris dataset almost . Consequently, good illustration using linear SVMs. Although one almost never uses practice, illustrated use primarily pedagogical reasons.\nsonar dataset good illustration benefits using RBF kernels cases dataset hard visualise (60 variables case!). general, one almost always use RBF (nonlinear) kernels practice.said, let’s get right . assume R RStudio installed. instructions , look first article series. processing preliminaries – loading libraries, data creating training test datasets much previous articles won’t dwell . completeness, however, ’ll list code can run directly R R studio (complete listing code can found ):","code":""},{"path":"tuning-of-support-vector-machine-prediction.html","id":"svm-on-iris-dataset","chapter":"15 Tuning of Support Vector Machine prediction","heading":"15.2 SVM on iris dataset","text":"","code":""},{"path":"tuning-of-support-vector-machine-prediction.html","id":"training-and-test-datasets-1","chapter":"15 Tuning of Support Vector Machine prediction","heading":"15.2.1 Training and test datasets","text":"","code":"\n#load required library\nlibrary(e1071)\n\n#load built-in iris dataset\ndata(iris)\n\n#set seed to ensure reproducible results\nset.seed(42)\n\n#split into training and test sets\niris[, \"train\"] <- ifelse(runif(nrow(iris)) < 0.8, 1, 0)\n\n#separate training and test sets\ntrainset <- iris[iris$train == 1,]\ntestset <- iris[iris$train == 0,]\n\n#get column index of train flag\ntrainColNum <- grep(\"train\", names(trainset))\n\n#remove train flag column from train and test sets\ntrainset <- trainset[,-trainColNum]\ntestset <- testset[,-trainColNum]\n\ndim(trainset)\n#> [1] 115   5\ndim(testset)\n#> [1] 35  5"},{"path":"tuning-of-support-vector-machine-prediction.html","id":"build-the-svm-model","chapter":"15 Tuning of Support Vector Machine prediction","heading":"15.2.2 Build the SVM model","text":"output SVM model show 24 support vectors. desired, can examined using SV variable model – .e via svm_model$SV.","code":"\n#get column index of predicted variable in dataset\ntypeColNum <- grep(\"Species\", names(iris))\n\n#build model – linear kernel and C-classification (soft margin) with default cost (C=1)\nsvm_model <- svm(Species~ ., data = trainset, \n                 method = \"C-classification\", \n                 kernel = \"linear\")\nsvm_model\n#> \n#> Call:\n#> svm(formula = Species ~ ., data = trainset, method = \"C-classification\", \n#>     kernel = \"linear\")\n#> \n#> \n#> Parameters:\n#>    SVM-Type:  C-classification \n#>  SVM-Kernel:  linear \n#>        cost:  1 \n#> \n#> Number of Support Vectors:  24"},{"path":"tuning-of-support-vector-machine-prediction.html","id":"support-vectors-1","chapter":"15 Tuning of Support Vector Machine prediction","heading":"15.2.3 Support Vectors","text":"test prediction accuracy indicates linear performs quite well dataset, confirming indeed near linearly separable. check performance class, one can create confusion matrix described post random forests. ’ll leave exercise . Another point used soft-margin classification scheme cost C=1. can experiment explicitly changing value C. , ’ll leave exercise.","code":"\n# support vectors\nsvm_model$SV\n#>     Sepal.Length Sepal.Width Petal.Length Petal.Width\n#> 19       -0.2564      1.7668       -1.323      -1.305\n#> 42       -1.7006     -1.7045       -1.559      -1.305\n#> 45       -0.9785      1.7668       -1.205      -1.171\n#> 53        1.1878      0.1469        0.568       0.309\n#> 55        0.7064     -0.5474        0.390       0.309\n#> 57        0.4657      0.6097        0.450       0.443\n#> 58       -1.2192     -1.4730       -0.378      -0.364\n#> 69        0.3453     -1.9359        0.331       0.309\n#> 71       -0.0157      0.3783        0.509       0.712\n#> 73        0.4657     -1.2416        0.568       0.309\n#> 78        0.9471     -0.0845        0.627       0.578\n#> 84        0.1046     -0.7788        0.686       0.443\n#> 85       -0.6174     -0.0845        0.331       0.309\n#> 86        0.1046      0.8412        0.331       0.443\n#> 99       -0.9785     -1.2416       -0.555      -0.229\n#> 107      -1.2192     -1.2416        0.331       0.578\n#> 111       0.7064      0.3783        0.686       0.981\n#> 117       0.7064     -0.0845        0.922       0.712\n#> 124       0.4657     -0.7788        0.568       0.712\n#> 130       1.5488     -0.0845        1.099       0.443\n#> 138       0.5860      0.1469        0.922       0.712\n#> 139       0.1046     -0.0845        0.509       0.712\n#> 147       0.4657     -1.2416        0.627       0.847\n#> 150      -0.0157     -0.0845        0.686       0.712"},{"path":"tuning-of-support-vector-machine-prediction.html","id":"predictions-on-training-model-2","chapter":"15 Tuning of Support Vector Machine prediction","heading":"15.2.4 Predictions on training model","text":"","code":"\n# training set predictions\npred_train <- predict(svm_model, trainset)\nmean(pred_train == trainset$Species)\n#> [1] 0.983\n# [1] 0.9826087"},{"path":"tuning-of-support-vector-machine-prediction.html","id":"predictions-on-test-model-2","chapter":"15 Tuning of Support Vector Machine prediction","heading":"15.2.5 Predictions on test model","text":"","code":"\n# test set predictions\npred_test <-predict(svm_model, testset)\nmean(pred_test == testset$Species)\n#> [1] 0.914\n# [1] 0.9142857"},{"path":"tuning-of-support-vector-machine-prediction.html","id":"confusion-matrix-and-accuracy-1","chapter":"15 Tuning of Support Vector Machine prediction","heading":"15.2.6 Confusion matrix and Accuracy","text":"","code":"\n# confusion matrix\ncm <- table(pred_test, testset$Species)\ncm\n#>             \n#> pred_test    setosa versicolor virginica\n#>   setosa         18          0         0\n#>   versicolor      0          5         3\n#>   virginica       0          0         9\n# accuracy\nsum(diag(cm)) / sum(cm)\n#> [1] 0.914"},{"path":"tuning-of-support-vector-machine-prediction.html","id":"svm-with-radial-basis-function-kernel.-linear-1","chapter":"15 Tuning of Support Vector Machine prediction","heading":"15.3 SVM with Radial Basis Function kernel. Linear","text":"","code":""},{"path":"tuning-of-support-vector-machine-prediction.html","id":"training-and-test-sets-1","chapter":"15 Tuning of Support Vector Machine prediction","heading":"15.3.1 Training and test sets","text":"","code":"\n#load required library (assuming e1071 is already loaded)\nlibrary(mlbench)\n\n#load Sonar dataset\ndata(Sonar)\n#set seed to ensure reproducible results\nset.seed(42)\n#split into training and test sets\nSonar[, \"train\"] <- ifelse(runif(nrow(Sonar))<0.8,1,0)\n\n#separate training and test sets\ntrainset <- Sonar[Sonar$train==1,]\ntestset <- Sonar[Sonar$train==0,]\n\n#get column index of train flag\ntrainColNum <- grep(\"train\",names(trainset))\n#remove train flag column from train and test sets\ntrainset <- trainset[,-trainColNum]\ntestset <- testset[,-trainColNum]\n\n#get column index of predicted variable in dataset\ntypeColNum <- grep(\"Class\",names(Sonar))"},{"path":"tuning-of-support-vector-machine-prediction.html","id":"predictions-on-training-model-3","chapter":"15 Tuning of Support Vector Machine prediction","heading":"15.3.2 Predictions on Training model","text":"","code":"\n#build model – linear kernel and C-classification with default cost (C=1)\nsvm_model <- svm(Class~ ., data=trainset, \n                 method=\"C-classification\", \n                 kernel=\"linear\")\n\n#training set predictions\npred_train <-predict(svm_model,trainset)\nmean(pred_train==trainset$Class)\n#> [1] 0.97"},{"path":"tuning-of-support-vector-machine-prediction.html","id":"predictions-on-test-model-3","chapter":"15 Tuning of Support Vector Machine prediction","heading":"15.3.3 Predictions on test model","text":"’ll leave examine contents model. important point note performance model test set quite dismal compared previous case. simply indicates linear kernel appropriate . Let’s take look happens use RBF kernel default values parameters:","code":"\n#test set predictions\npred_test <-predict(svm_model,testset)\nmean(pred_test==testset$Class)\n#> [1] 0.605"},{"path":"tuning-of-support-vector-machine-prediction.html","id":"svm-with-radial-basis-function-kernel.-non-linear-1","chapter":"15 Tuning of Support Vector Machine prediction","heading":"15.4 SVM with Radial Basis Function kernel. Non-linear","text":"","code":""},{"path":"tuning-of-support-vector-machine-prediction.html","id":"predictions-on-training-model-4","chapter":"15 Tuning of Support Vector Machine prediction","heading":"15.4.1 Predictions on training model","text":"","code":"\n#build model: radial kernel, default params\nsvm_model <- svm(Class~ ., data=trainset, \n                 method=\"C-classification\", \n                 kernel=\"radial\")\n# print params\nsvm_model$cost\n#> [1] 1\nsvm_model$gamma\n#> [1] 0.0167\n\n#training set predictions\npred_train <-predict(svm_model,trainset)\nmean(pred_train==trainset$Class)\n#> [1] 0.988"},{"path":"tuning-of-support-vector-machine-prediction.html","id":"predictions-on-test-model-4","chapter":"15 Tuning of Support Vector Machine prediction","heading":"15.4.2 Predictions on test model","text":"’s pretty decent improvement linear kernel. Let’s see can better parameter tuning. first invoke tune.svm use parameters gives us call svm:","code":"\n#test set predictions\npred_test <-predict(svm_model,testset)\nmean(pred_test==testset$Class)\n#> [1] 0.767"},{"path":"tuning-of-support-vector-machine-prediction.html","id":"tuning-of-parameters-1","chapter":"15 Tuning of Support Vector Machine prediction","heading":"15.4.3 Tuning of parameters","text":"","code":"\n# find optimal parameters in a specified range\ntune_out <- tune.svm(x = trainset[,-typeColNum], \n                     y = trainset[, typeColNum], \n                     gamma = 10^(-3:3), \n                     cost = c(0.01, 0.1, 1, 10, 100, 1000), \n                     kernel = \"radial\")\n\n#print best values of cost and gamma\ntune_out$best.parameters$cost\n#> [1] 10\ntune_out$best.parameters$gamma\n#> [1] 0.01\n\n#build model\nsvm_model <- svm(Class~ ., data = trainset, \n                 method = \"C-classification\", \n                 kernel = \"radial\", \n                 cost = tune_out$best.parameters$cost, \n                 gamma = tune_out$best.parameters$gamma)"},{"path":"tuning-of-support-vector-machine-prediction.html","id":"prediction-on-training-model-with-new-parameters-1","chapter":"15 Tuning of Support Vector Machine prediction","heading":"15.4.4 Prediction on training model with new parameters","text":"","code":"\n# training set predictions\npred_train <-predict(svm_model,trainset)\nmean(pred_train==trainset$Class)\n#> [1] 1"},{"path":"tuning-of-support-vector-machine-prediction.html","id":"prediction-on-test-model-with-new-parameters-1","chapter":"15 Tuning of Support Vector Machine prediction","heading":"15.4.5 Prediction on test model with new parameters","text":"fairly decent improvement un-optimised case.","code":"\n# test set predictions\npred_test <-predict(svm_model,testset)\nmean(pred_test==testset$Class)\n#> [1] 0.814"},{"path":"tuning-of-support-vector-machine-prediction.html","id":"wrapping-up-1","chapter":"15 Tuning of Support Vector Machine prediction","heading":"15.5 Wrapping up","text":"bring us end introductory exploration SVMs R. recap, distinguishing feature SVMs contrast techniques attempt construct optimal separation boundaries different categories.SVMs quite versatile applied wide variety domains ranging chemistry pattern recognition. best used binary classification scenarios. brings question SVMs preferred binary classification techniques logistic regression. honest response , “depends” – points keep mind choosing two. general point keep mind SVM algorithms tend expensive terms memory computation, issues can start hurt size dataset increases.Given caveats considerations, best way figure whether SVM approach work problem may machine learning practitioners : try !","code":""},{"path":"introduction-to-algorithms-for-classification.html","id":"introduction-to-algorithms-for-classification","chapter":"16 Introduction to algorithms for Classification","heading":"16 Introduction to algorithms for Classification","text":"Datasets: PimaIndiansDiabetes","code":""},{"path":"introduction-to-algorithms-for-classification.html","id":"comparison-of-cart-lda-svm-knn-rf","chapter":"16 Introduction to algorithms for Classification","heading":"16.1 Comparison of CART, LDA, SVM, KNN, RF","text":"","code":""},{"path":"introduction-to-algorithms-for-classification.html","id":"introduction-7","chapter":"16 Introduction to algorithms for Classification","heading":"16.2 Introduction","text":"compare following classification algorithms:CARTLDASVMKNNRF","code":""},{"path":"introduction-to-algorithms-for-classification.html","id":"workflow","chapter":"16 Introduction to algorithms for Classification","heading":"16.3 Workflow","text":"Load datasetCreate train datasetTrain modelsCollect resamplesPlot comparisonSummarize p-values","code":"\n# load packages\nlibrary(mlbench)\nlibrary(caret)\n# load the dataset\ndata(PimaIndiansDiabetes)\ndplyr::glimpse(PimaIndiansDiabetes)\n#> Rows: 768\n#> Columns: 9\n#> $ pregnant <dbl> 6, 1, 8, 1, 0, 5, 3, 10, 2, 8, 4, 10, 10, 1, 5, 7, 0, 7, 1, …\n#> $ glucose  <dbl> 148, 85, 183, 89, 137, 116, 78, 115, 197, 125, 110, 168, 139…\n#> $ pressure <dbl> 72, 66, 64, 66, 40, 74, 50, 0, 70, 96, 92, 74, 80, 60, 72, 0…\n#> $ triceps  <dbl> 35, 29, 0, 23, 35, 0, 32, 0, 45, 0, 0, 0, 0, 23, 19, 0, 47, …\n#> $ insulin  <dbl> 0, 0, 0, 94, 168, 0, 88, 0, 543, 0, 0, 0, 0, 846, 175, 0, 23…\n#> $ mass     <dbl> 33.6, 26.6, 23.3, 28.1, 43.1, 25.6, 31.0, 35.3, 30.5, 0.0, 3…\n#> $ pedigree <dbl> 0.627, 0.351, 0.672, 0.167, 2.288, 0.201, 0.248, 0.134, 0.15…\n#> $ age      <dbl> 50, 31, 32, 21, 33, 30, 26, 29, 53, 54, 30, 34, 57, 59, 51, …\n#> $ diabetes <fct> pos, neg, pos, neg, pos, neg, pos, neg, pos, pos, neg, pos, …\ntibble::as_tibble(PimaIndiansDiabetes)\n#> # A tibble: 768 x 9\n#>   pregnant glucose pressure triceps insulin  mass pedigree   age diabetes\n#>      <dbl>   <dbl>    <dbl>   <dbl>   <dbl> <dbl>    <dbl> <dbl> <fct>   \n#> 1        6     148       72      35       0  33.6    0.627    50 pos     \n#> 2        1      85       66      29       0  26.6    0.351    31 neg     \n#> 3        8     183       64       0       0  23.3    0.672    32 pos     \n#> 4        1      89       66      23      94  28.1    0.167    21 neg     \n#> 5        0     137       40      35     168  43.1    2.29     33 pos     \n#> 6        5     116       74       0       0  25.6    0.201    30 neg     \n#> # … with 762 more rows"},{"path":"introduction-to-algorithms-for-classification.html","id":"train-the-models-using-cross-validation","chapter":"16 Introduction to algorithms for Classification","heading":"16.4 Train the models using cross-validation","text":"","code":"\n# prepare training scheme\ntrainControl <- trainControl(method = \"repeatedcv\", \n                             number=10, \n                             repeats=3)\n# CART\nset.seed(7)\nfit.cart <- train(diabetes~., data=PimaIndiansDiabetes, \n                  method = \"rpart\", trControl=trainControl)\n# LDA: Linear Discriminant Analysis\nset.seed(7)\nfit.lda <- train(diabetes~., data=PimaIndiansDiabetes, \n                 method=\"lda\", trControl=trainControl)\n# SVM\nset.seed(7)\nfit.svm <- train(diabetes~., data=PimaIndiansDiabetes, \n                 method=\"svmRadial\", trControl=trainControl)\n# KNN\nset.seed(7)\nfit.knn <- train(diabetes~., data=PimaIndiansDiabetes, \n                 method=\"knn\", trControl=trainControl)\n# Random Forest\nset.seed(7)\nfit.rf <- train(diabetes~., data=PimaIndiansDiabetes, \n                method=\"rf\", trControl=trainControl)\n# collect resamples\nresults <- resamples(list(CART=fit.cart, \n                          LDA=fit.lda, \n                          SVM=fit.svm, \n                          KNN=fit.knn, \n                          RF=fit.rf))"},{"path":"introduction-to-algorithms-for-classification.html","id":"compare-models","chapter":"16 Introduction to algorithms for Classification","heading":"16.5 Compare models","text":"","code":"\n# summarize differences between models\nsummary(results)\n#> \n#> Call:\n#> summary.resamples(object = results)\n#> \n#> Models: CART, LDA, SVM, KNN, RF \n#> Number of resamples: 30 \n#> \n#> Accuracy \n#>       Min. 1st Qu. Median  Mean 3rd Qu.  Max. NA's\n#> CART 0.675   0.727  0.753 0.747   0.766 0.792    0\n#> LDA  0.714   0.751  0.766 0.779   0.800 0.908    0\n#> SVM  0.724   0.751  0.763 0.771   0.792 0.895    0\n#> KNN  0.675   0.704  0.727 0.737   0.766 0.831    0\n#> RF   0.684   0.731  0.760 0.764   0.802 0.842    0\n#> \n#> Kappa \n#>       Min. 1st Qu. Median  Mean 3rd Qu.  Max. NA's\n#> CART 0.276   0.362  0.424 0.415   0.486 0.525    0\n#> LDA  0.301   0.419  0.466 0.486   0.531 0.781    0\n#> SVM  0.339   0.400  0.446 0.462   0.523 0.748    0\n#> KNN  0.255   0.341  0.384 0.398   0.454 0.620    0\n#> RF   0.295   0.378  0.464 0.463   0.545 0.643    0"},{"path":"introduction-to-algorithms-for-classification.html","id":"plot-comparison","chapter":"16 Introduction to algorithms for Classification","heading":"16.6 Plot comparison","text":"","code":"\n# box and whisker plots to compare models\nscales <- list(x=list(relation=\"free\"), y=list(relation=\"free\"))\nbwplot(results, scales=scales)\n# density plots of accuracy\nscales <- list(x=list(relation=\"free\"), y=list(relation=\"free\"))\ndensityplot(results, scales=scales, pch = \"|\")\n# dot plots of accuracy\nscales <- list(x=list(relation=\"free\"), y=list(relation=\"free\"))\ndotplot(results, scales=scales)\n# parallel plots to compare models\nparallelplot(results)\n# pairwise scatter plots of predictions to compare models\nsplom(results)\n# xyplot plots to compare models\nxyplot(results, models=c(\"LDA\", \"SVM\"))\n# difference in model predictions\ndiffs <- diff(results)\n# summarize p-values for pairwise comparisons\nsummary(diffs)\n#> \n#> Call:\n#> summary.diff.resamples(object = diffs)\n#> \n#> p-value adjustment: bonferroni \n#> Upper diagonal: estimates of the difference\n#> Lower diagonal: p-value for H0: difference = 0\n#> \n#> Accuracy \n#>      CART     LDA      SVM      KNN      RF      \n#> CART          -0.03214 -0.02432  0.01002 -0.01688\n#> LDA  0.001186           0.00781  0.04216  0.01525\n#> SVM  0.011640 0.915689           0.03434  0.00744\n#> KNN  1.000000 6.68e-05 0.000294          -0.02690\n#> RF   0.272754 0.449062 1.000000 0.018379         \n#> \n#> Kappa \n#>      CART     LDA       SVM       KNN       RF       \n#> CART          -0.071016 -0.046972  0.016687 -0.047894\n#> LDA  0.000809            0.024044  0.087703  0.023122\n#> SVM  0.025808 0.356273             0.063659 -0.000922\n#> KNN  1.000000 0.000386  0.004082            -0.064581\n#> RF   0.021176 1.000000  1.000000  0.015897"},{"path":"comparing-classification-algorithms.html","id":"comparing-classification-algorithms","chapter":"17 Comparing Classification algorithms","heading":"17 Comparing Classification algorithms","text":"Datasets: irisAlgorithms: LDA, CART, KNN, SVM, RF","code":""},{"path":"comparing-classification-algorithms.html","id":"introduction-8","chapter":"17 Comparing Classification algorithms","heading":"17.1 Introduction","text":"algorithms used:LDACARTKNNSVMRF","code":"\n# load the caret package\nlibrary(caret)\n#> Loading required package: lattice\n#> Loading required package: ggplot2\n# attach the iris dataset to the environment\ndata(iris)\n# rename the dataset\ndataset <- iris"},{"path":"comparing-classification-algorithms.html","id":"workflow-1","chapter":"17 Comparing Classification algorithms","heading":"17.2 Workflow","text":"Load datasetCreate train test datasets, 80/20Inspect datasetVisualize featuresSet train control to10 cross-validationsMetric: accuracyTrain modelsCompare accuracy modelsVisual comparisonMake predictions validation setWe split loaded dataset two, \\(80\\%\\) use train models \\(20\\%\\) hold back validation dataset.","code":"\n# create a list of 80% of the rows in the original dataset we can use for training\nvalidationIndex <- createDataPartition(dataset$Species, p=0.80, list=FALSE)\n# select 20% of the data for validation\nvalidation <- dataset[-validationIndex,]\n\n# use the remaining 80% of data to training and testing the models\ndataset <- dataset[validationIndex,]\n\n# dimensions of dataset\ndim(dataset)\n#> [1] 120   5\n# list types for each attribute\nsapply(dataset, class)\n#> Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n#>    \"numeric\"    \"numeric\"    \"numeric\"    \"numeric\"     \"factor\""},{"path":"comparing-classification-algorithms.html","id":"peek-at-the-dataset","chapter":"17 Comparing Classification algorithms","heading":"17.3 Peek at the dataset","text":"","code":"\n# take a peek at the first 5 rows of the data\nhead(dataset)\n#>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#> 1          5.1         3.5          1.4         0.2  setosa\n#> 2          4.9         3.0          1.4         0.2  setosa\n#> 3          4.7         3.2          1.3         0.2  setosa\n#> 4          4.6         3.1          1.5         0.2  setosa\n#> 5          5.0         3.6          1.4         0.2  setosa\n#> 6          5.4         3.9          1.7         0.4  setosa\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\n\nglimpse(dataset)\n#> Rows: 120\n#> Columns: 5\n#> $ Sepal.Length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4…\n#> $ Sepal.Width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3…\n#> $ Petal.Length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1…\n#> $ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0…\n#> $ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, …\nlibrary(skimr)\nprint(skim(dataset))\n#> ── Data Summary ────────────────────────\n#>                            Values \n#> Name                       dataset\n#> Number of rows             120    \n#> Number of columns          5      \n#> _______________________           \n#> Column type frequency:            \n#>   factor                   1      \n#>   numeric                  4      \n#> ________________________          \n#> Group variables            None   \n#> \n#> ── Variable type: factor ───────────────────────────────────────────────────────\n#>   skim_variable n_missing complete_rate ordered n_unique\n#> 1 Species               0             1 FALSE          3\n#>   top_counts               \n#> 1 set: 40, ver: 40, vir: 40\n#> \n#> ── Variable type: numeric ──────────────────────────────────────────────────────\n#>   skim_variable n_missing complete_rate  mean    sd    p0   p25   p50   p75\n#> 1 Sepal.Length          0             1  5.86 0.837   4.3  5.1   5.8   6.4 \n#> 2 Sepal.Width           0             1  3.06 0.437   2    2.8   3     3.32\n#> 3 Petal.Length          0             1  3.76 1.78    1    1.58  4.35  5.1 \n#> 4 Petal.Width           0             1  1.20 0.759   0.1  0.3   1.3   1.8 \n#>    p100 hist \n#> 1   7.9 ▆▇▇▅▂\n#> 2   4.4 ▁▆▇▂▁\n#> 3   6.9 ▇▁▆▇▂\n#> 4   2.5 ▇▁▆▅▃"},{"path":"comparing-classification-algorithms.html","id":"levels-of-the-class","chapter":"17 Comparing Classification algorithms","heading":"17.4 Levels of the class","text":"","code":"\n# list the levels for the class\nlevels(dataset$Species)\n#> [1] \"setosa\"     \"versicolor\" \"virginica\""},{"path":"comparing-classification-algorithms.html","id":"class-distribution","chapter":"17 Comparing Classification algorithms","heading":"17.5 class distribution","text":"","code":"\n# summarize the class distribution\npercentage <- prop.table(table(dataset$Species)) * 100\ncbind(freq=table(dataset$Species), percentage=percentage)\n#>            freq percentage\n#> setosa       40       33.3\n#> versicolor   40       33.3\n#> virginica    40       33.3"},{"path":"comparing-classification-algorithms.html","id":"visualize-the-dataset","chapter":"17 Comparing Classification algorithms","heading":"17.6 Visualize the dataset","text":"","code":"\n# split input and output\nx <- dataset[,1:4]\ny <- dataset[,5]\n# boxplot for each attribute on one image\npar(mfrow=c(1,4))\nfor(i in 1:4) {\n    boxplot(x[,i], main=names(dataset)[i])\n}\n# barplot for class breakdown\nplot(y)\n# scatter plot matrix\nfeaturePlot(x=x, y=y, plot=\"ellipse\")\n# box and whisker plots for each attribute\nfeaturePlot(x=x, y=y, plot=\"box\")\n# density plots for each attribute by class value\nscales <- list(x=list(relation=\"free\"), y=list(relation=\"free\"))\nfeaturePlot(x=x, y=y, plot=\"density\", scales=scales)"},{"path":"comparing-classification-algorithms.html","id":"evaluate-algorithms","chapter":"17 Comparing Classification algorithms","heading":"17.7 Evaluate algorithms","text":"","code":""},{"path":"comparing-classification-algorithms.html","id":"split-and-metrics","chapter":"17 Comparing Classification algorithms","heading":"17.7.1 split and metrics","text":"","code":"\n# Run algorithms using 10-fold cross-validation\ntrainControl <- trainControl(method=\"cv\", number=10)\nmetric <- \"Accuracy\""},{"path":"comparing-classification-algorithms.html","id":"build-models","chapter":"17 Comparing Classification algorithms","heading":"17.7.2 build models","text":"","code":"\n# LDA\nset.seed(7)\nfit.lda <- train(Species~., data=dataset, method = \"lda\", \n                 metric=metric, trControl=trainControl)\n# CART\nset.seed(7)\nfit.cart <- train(Species~., data=dataset, method = \"rpart\", \n                  metric=metric, trControl=trainControl)\n# KNN\nset.seed(7)\nfit.knn <- train(Species~., data=dataset, method = \"knn\", \n                 metric=metric, trControl=trainControl)\n# SVM\nset.seed(7)\nfit.svm <- train(Species~., data=dataset, method = \"svmRadial\", \n                 metric=metric, trControl=trainControl)\n# Random Forest\nset.seed(7)\nfit.rf <- train(Species~., data=dataset, method = \"rf\", \n                metric=metric, trControl=trainControl)"},{"path":"comparing-classification-algorithms.html","id":"compare","chapter":"17 Comparing Classification algorithms","heading":"17.7.3 compare","text":"","code":"\n#summarize accuracy of models\nresults <- resamples(list(lda  = fit.lda, \n                          cart = fit.cart, \n                          knn  = fit.knn, \n                          svm  = fit.svm, \n                          rf   = fit.rf))\nsummary(results)\n#> \n#> Call:\n#> summary.resamples(object = results)\n#> \n#> Models: lda, cart, knn, svm, rf \n#> Number of resamples: 10 \n#> \n#> Accuracy \n#>       Min. 1st Qu. Median  Mean 3rd Qu. Max. NA's\n#> lda  0.917   1.000      1 0.992       1    1    0\n#> cart 0.917   0.917      1 0.967       1    1    0\n#> knn  0.917   0.938      1 0.975       1    1    0\n#> svm  0.833   0.917      1 0.958       1    1    0\n#> rf   0.917   0.917      1 0.967       1    1    0\n#> \n#> Kappa \n#>       Min. 1st Qu. Median  Mean 3rd Qu. Max. NA's\n#> lda  0.875   1.000      1 0.987       1    1    0\n#> cart 0.875   0.875      1 0.950       1    1    0\n#> knn  0.875   0.906      1 0.962       1    1    0\n#> svm  0.750   0.875      1 0.937       1    1    0\n#> rf   0.875   0.875      1 0.950       1    1    0\n# compare accuracy of models\ndotplot(results)\n# summarize Best Model\nprint(fit.lda)\n#> Linear Discriminant Analysis \n#> \n#> 120 samples\n#>   4 predictor\n#>   3 classes: 'setosa', 'versicolor', 'virginica' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold) \n#> Summary of sample sizes: 108, 108, 108, 108, 108, 108, ... \n#> Resampling results:\n#> \n#>   Accuracy  Kappa\n#>   0.992     0.987"},{"path":"comparing-classification-algorithms.html","id":"make-predictions","chapter":"17 Comparing Classification algorithms","heading":"17.8 Make predictions","text":"","code":"\n# estimate skill of LDA on the validation dataset\npredictions <- predict(fit.lda, validation)\nconfusionMatrix(predictions, validation$Species)\n#> Confusion Matrix and Statistics\n#> \n#>             Reference\n#> Prediction   setosa versicolor virginica\n#>   setosa         10          0         0\n#>   versicolor      0          9         1\n#>   virginica       0          1         9\n#> \n#> Overall Statistics\n#>                                         \n#>                Accuracy : 0.933         \n#>                  95% CI : (0.779, 0.992)\n#>     No Information Rate : 0.333         \n#>     P-Value [Acc > NIR] : 8.75e-12      \n#>                                         \n#>                   Kappa : 0.9           \n#>                                         \n#>  Mcnemar's Test P-Value : NA            \n#> \n#> Statistics by Class:\n#> \n#>                      Class: setosa Class: versicolor Class: virginica\n#> Sensitivity                  1.000             0.900            0.900\n#> Specificity                  1.000             0.950            0.950\n#> Pos Pred Value               1.000             0.900            0.900\n#> Neg Pred Value               1.000             0.950            0.950\n#> Prevalence                   0.333             0.333            0.333\n#> Detection Rate               0.333             0.300            0.300\n#> Detection Prevalence         0.333             0.333            0.333\n#> Balanced Accuracy            1.000             0.925            0.925"},{"path":"who-buys-social-network-ads.html","id":"who-buys-social-network-ads","chapter":"18 Who buys Social Network ads","heading":"18 Who buys Social Network ads","text":"Datasets: Social_Network_Ads.csvDatasets: Social_Network_Ads.csvAlgorithms:\nSupport Vector Machines\nAlgorithms:Support Vector Machines","code":""},{"path":"who-buys-social-network-ads.html","id":"classification-with-svm","chapter":"18 Who buys Social Network ads","heading":"18.1 Classification with SVM","text":"","code":""},{"path":"who-buys-social-network-ads.html","id":"introduction-9","chapter":"18 Who buys Social Network ads","heading":"18.2 Introduction","text":"Source: https://www.geeksforgeeks.org/classifying-data-using-support-vector-machinessvms--r/","code":""},{"path":"who-buys-social-network-ads.html","id":"data-operations","chapter":"18 Who buys Social Network ads","heading":"18.3 Data Operations","text":"","code":""},{"path":"who-buys-social-network-ads.html","id":"load-libraries","chapter":"18 Who buys Social Network ads","heading":"18.3.1 Load libraries","text":"","code":"\n# load packages\nlibrary(dplyr)\nlibrary(caTools) \nlibrary(e1071) \nlibrary(ElemStatLearn) "},{"path":"who-buys-social-network-ads.html","id":"importing-the-dataset","chapter":"18 Who buys Social Network ads","heading":"18.3.2 Importing the dataset","text":"","code":"\n# Importing the dataset \ndataset = read.csv(file.path(data_raw_dir, 'Social_Network_Ads.csv')) \ndplyr::glimpse(dataset)\n#> Rows: 400\n#> Columns: 5\n#> $ User.ID         <int> 15624510, 15810944, 15668575, 15603246, 15804002, 157…\n#> $ Gender          <fct> Male, Male, Female, Female, Male, Male, Female, Femal…\n#> $ Age             <int> 19, 35, 26, 27, 19, 27, 27, 32, 25, 35, 26, 26, 20, 3…\n#> $ EstimatedSalary <int> 19000, 20000, 43000, 57000, 76000, 58000, 84000, 1500…\n#> $ Purchased       <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,…\ntibble::as_tibble(dataset)\n#> # A tibble: 400 x 5\n#>    User.ID Gender   Age EstimatedSalary Purchased\n#>      <int> <fct>  <int>           <int>     <int>\n#> 1 15624510 Male      19           19000         0\n#> 2 15810944 Male      35           20000         0\n#> 3 15668575 Female    26           43000         0\n#> 4 15603246 Female    27           57000         0\n#> 5 15804002 Male      19           76000         0\n#> 6 15728773 Male      27           58000         0\n#> # … with 394 more rows\n# Taking columns 3-5 \ndataset = dataset[3:5]\ntibble::as_tibble(dataset)\n#> # A tibble: 400 x 3\n#>     Age EstimatedSalary Purchased\n#>   <int>           <int>     <int>\n#> 1    19           19000         0\n#> 2    35           20000         0\n#> 3    26           43000         0\n#> 4    27           57000         0\n#> 5    19           76000         0\n#> 6    27           58000         0\n#> # … with 394 more rows"},{"path":"who-buys-social-network-ads.html","id":"encoding-the-target-feature-as-factor","chapter":"18 Who buys Social Network ads","heading":"18.3.3 Encoding the target feature as factor","text":"","code":"\n# Encoding the target feature as factor \ndataset$Purchased = factor(dataset$Purchased, levels = c(0, 1)) \nstr(dataset)\n#> 'data.frame':    400 obs. of  3 variables:\n#>  $ Age            : int  19 35 26 27 19 27 27 32 25 35 ...\n#>  $ EstimatedSalary: int  19000 20000 43000 57000 76000 58000 84000 150000 33000 65000 ...\n#>  $ Purchased      : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 2 1 1 ..."},{"path":"who-buys-social-network-ads.html","id":"training-and-test-datasets-2","chapter":"18 Who buys Social Network ads","heading":"18.3.4 Training and test datasets","text":"","code":"\n# Splitting the dataset into the Training set and Test set \nset.seed(123) \nsplit = sample.split(dataset$Purchased, SplitRatio = 0.75) \n  \ntraining_set = subset(dataset, split == TRUE) \ntest_set = subset(dataset, split == FALSE) \ndim(training_set)\n#> [1] 300   3\ndim(test_set)\n#> [1] 100   3"},{"path":"who-buys-social-network-ads.html","id":"feature-scaling","chapter":"18 Who buys Social Network ads","heading":"18.3.5 Feature Scaling","text":"","code":"\n# Feature Scaling \ntraining_set[-3] = scale(training_set[-3]) \ntest_set[-3] = scale(test_set[-3]) "},{"path":"who-buys-social-network-ads.html","id":"fitting-svm-to-the-training-set","chapter":"18 Who buys Social Network ads","heading":"18.3.6 Fitting SVM to the Training set","text":"","code":"\n# Fitting SVM to the Training set \nclassifier = svm(formula = Purchased ~ ., \n                 data = training_set, \n                 type = 'C-classification', \n                 kernel = 'linear') \nclassifier\n#> \n#> Call:\n#> svm(formula = Purchased ~ ., data = training_set, type = \"C-classification\", \n#>     kernel = \"linear\")\n#> \n#> \n#> Parameters:\n#>    SVM-Type:  C-classification \n#>  SVM-Kernel:  linear \n#>        cost:  1 \n#> \n#> Number of Support Vectors:  116\nsummary(classifier)\n#> \n#> Call:\n#> svm(formula = Purchased ~ ., data = training_set, type = \"C-classification\", \n#>     kernel = \"linear\")\n#> \n#> \n#> Parameters:\n#>    SVM-Type:  C-classification \n#>  SVM-Kernel:  linear \n#>        cost:  1 \n#> \n#> Number of Support Vectors:  116\n#> \n#>  ( 58 58 )\n#> \n#> \n#> Number of Classes:  2 \n#> \n#> Levels: \n#>  0 1"},{"path":"who-buys-social-network-ads.html","id":"predicting-the-on-the-test-dataset","chapter":"18 Who buys Social Network ads","heading":"18.3.7 Predicting the on the test dataset","text":"","code":"\n# Predicting the Test set results \ny_pred = predict(classifier, newdata = test_set[-3]) \ny_pred\n#>   2   4   5   9  12  18  19  20  22  29  32  34  35  38  45  46  48  52  66  69 \n#>   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n#>  74  75  82  84  85  86  87  89 103 104 107 108 109 117 124 126 127 131 134 139 \n#>   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0 \n#> 148 154 156 159 162 163 170 175 176 193 199 200 208 213 224 226 228 229 230 234 \n#>   0   0   0   0   0   0   0   0   0   0   0   0   1   1   1   0   1   0   1   1 \n#> 236 237 239 241 255 264 265 266 273 274 281 286 292 299 302 305 307 310 316 324 \n#>   1   0   1   1   1   0   1   1   1   1   1   0   1   1   1   0   1   0   0   0 \n#> 326 332 339 341 343 347 353 363 364 367 368 369 372 373 380 383 389 392 395 400 \n#>   0   1   0   1   0   1   1   0   1   1   1   0   1   0   1   1   0   0   0   0 \n#> Levels: 0 1"},{"path":"who-buys-social-network-ads.html","id":"confusion-matrix","chapter":"18 Who buys Social Network ads","heading":"18.3.7.1 Confusion Matrix","text":"","code":"\n# Making the Confusion Matrix \ncm = table(test_set[, 3], y_pred) \ncm\n#>    y_pred\n#>      0  1\n#>   0 57  7\n#>   1 13 23\nxtable::xtable(cm)\n#> % latex table generated in R 3.6.3 by xtable 1.8-4 package\n#> % Fri Nov 20 02:57:46 2020\n#> \\begin{table}[ht]\n#> \\centering\n#> \\begin{tabular}{rrr}\n#>   \\hline\n#>  & 0 & 1 \\\\ \n#>   \\hline\n#> 0 &  57 &   7 \\\\ \n#>   1 &  13 &  23 \\\\ \n#>    \\hline\n#> \\end{tabular}\n#> \\end{table}"},{"path":"who-buys-social-network-ads.html","id":"end","chapter":"18 Who buys Social Network ads","heading":"18.4 End","text":"","code":""},{"path":"who-buys-social-network-ads.html","id":"plotting-the-training-dataset","chapter":"18 Who buys Social Network ads","heading":"18.4.1 Plotting the training dataset","text":"","code":"\n# installing library ElemStatLearn \n# library(ElemStatLearn) \n  \n# Plotting the training data set results \nset = training_set \nX1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01) \nX2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01) \n  \ngrid_set = expand.grid(X1, X2) \ncolnames(grid_set) = c('Age', 'EstimatedSalary') \ny_grid = predict(classifier, newdata = grid_set) \n  \nplot(set[, -3], \n     main = 'SVM (Training set)', \n     xlab = 'Age', ylab = 'Estimated Salary', \n     xlim = range(X1), ylim = range(X2)) \n  \ncontour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE) \n  \npoints(grid_set, pch = '.', col = ifelse(y_grid == 1, 'coral1', 'aquamarine')) \n  \npoints(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3')) "},{"path":"who-buys-social-network-ads.html","id":"plotting-the-test-dataset","chapter":"18 Who buys Social Network ads","heading":"18.4.2 Plotting the test dataset","text":"","code":"\nset = test_set \nX1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01) \nX2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01) \n  \ngrid_set = expand.grid(X1, X2) \ncolnames(grid_set) = c('Age', 'EstimatedSalary') \ny_grid = predict(classifier, newdata = grid_set) \n  \nplot(set[, -3], main = 'SVM (Test set)', \n     xlab = 'Age', ylab = 'Estimated Salary', \n     xlim = range(X1), ylim = range(X2)) \n  \ncontour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE) \n  \npoints(grid_set, pch = '.', col = ifelse(y_grid == 1, 'coral1', 'aquamarine')) \n  \npoints(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3')) "},{"path":"predicting-ozone-levels.html","id":"predicting-ozone-levels","chapter":"19 Predicting Ozone levels","heading":"19 Predicting Ozone levels","text":"Datasets: OzoneAlgorithms:\nSVM\nPartition Tree\nSVMPartition Treehttps://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdf","code":"\nlibrary(e1071)\nlibrary(rpart)\n\ndata(Ozone, package=\"mlbench\")\n## split data into a train and test set\nindex <- 1:nrow(Ozone)\ntestindex <- sample(index, trunc(length(index)/3))\ntestset <- na.omit(Ozone[testindex,-3])\ntrainset <- na.omit(Ozone[-testindex,-3])\n## svm\nsvm.model <- svm(V4 ~ ., data = trainset, cost = 1000, gamma = 0.0001)\nsvm.pred  <- predict(svm.model, testset[,-3])\ncrossprod(svm.pred - testset[,3]) / length(testindex)\n#>      [,1]\n#> [1,] 10.7\n## rpart\nrpart.model <- rpart(V4 ~ ., data = trainset)\nrpart.pred  <- predict(rpart.model, testset[,-3])\ncrossprod(rpart.pred - testset[,3]) / length(testindex)\n#>      [,1]\n#> [1,] 11.9"},{"path":"building-a-naive-bayes-classifier.html","id":"building-a-naive-bayes-classifier","chapter":"20 Building a Naive Bayes Classifier","heading":"20 Building a Naive Bayes Classifier","text":"Datasets: iris.csvAlgorithms:\nNaive Bayes\nNaive Bayeshttps://www.machinelearningplus.com/predictive-modeling/-naive-bayes-algorithm-works--example--full-code/","code":""},{"path":"building-a-naive-bayes-classifier.html","id":"building-a-naive-bayes-classifier-in-r","chapter":"20 Building a Naive Bayes Classifier","heading":"20.1 8. Building a Naive Bayes Classifier in R","text":"Understanding Naive Bayes (slightly) tricky part. Implementing fairly straightforward.R, Naive Bayes classifier implemented packages e1071, klaR bnlearn. Python, implemented scikit-learn.sake demonstration, let’s use standard iris dataset predict Species flower using 4 different features: Sepal.Length, Sepal.Width, Petal.Length, Petal.WidthThe training data now contained training test data test dataframe. Lets load klaR package build naive bayes model.Lets see confusion matrix.","code":"\n# Import Data\ntraining <- read.csv('https://raw.githubusercontent.com/selva86/datasets/master/iris_train.csv')\ntest <- read.csv('https://raw.githubusercontent.com/selva86/datasets/master/iris_test.csv')\n# Using klaR for Naive Bayes\nlibrary(klaR)\n#> Loading required package: MASS\nnb_mod <- NaiveBayes(Species ~ ., data=training)\npred <- predict(nb_mod, test)\n# Confusion Matrix\ntab <- table(pred$class, test$Species)\ncaret::confusionMatrix(tab)  \n#> Confusion Matrix and Statistics\n#> \n#>             \n#>              setosa versicolor virginica\n#>   setosa         15          0         0\n#>   versicolor      0         11         0\n#>   virginica       0          4        15\n#> \n#> Overall Statistics\n#>                                         \n#>                Accuracy : 0.911         \n#>                  95% CI : (0.788, 0.975)\n#>     No Information Rate : 0.333         \n#>     P-Value [Acc > NIR] : 8.47e-16      \n#>                                         \n#>                   Kappa : 0.867         \n#>                                         \n#>  Mcnemar's Test P-Value : NA            \n#> \n#> Statistics by Class:\n#> \n#>                      Class: setosa Class: versicolor Class: virginica\n#> Sensitivity                  1.000             0.733            1.000\n#> Specificity                  1.000             1.000            0.867\n#> Pos Pred Value               1.000             1.000            0.789\n#> Neg Pred Value               1.000             0.882            1.000\n#> Prevalence                   0.333             0.333            0.333\n#> Detection Rate               0.333             0.244            0.333\n#> Detection Prevalence         0.333             0.244            0.422\n#> Balanced Accuracy            1.000             0.867            0.933\n# Plot density of each feature using nb_mod\nopar = par(mfrow=c(2, 2), mar=c(4,0,0,0))\nplot(nb_mod, main=\"\")  \npar(opar)\n# Plot the Confusion Matrix\nlibrary(ggplot2)\ntest$pred <- pred$class\nggplot(test, aes(Species, pred, color = Species)) +\n  geom_jitter(width = 0.2, height = 0.1, size=2) +\n  labs(title=\"Confusion Matrix\", \n       subtitle=\"Predicted vs. Observed from Iris dataset\", \n       y=\"Predicted\", \n       x=\"Truth\",\n       caption=\"machinelearningplus.com\")"},{"path":"linear-and-non-linear-algorithms-for-classification.html","id":"linear-and-non-linear-algorithms-for-classification","chapter":"21 Linear and Non-Linear Algorithms for Classification","heading":"21 Linear and Non-Linear Algorithms for Classification","text":"Datasets: BreastCancerAlgorithms: LG, LDA, GLMNET, KNN, CARTM NB, SVM","code":""},{"path":"linear-and-non-linear-algorithms-for-classification.html","id":"introduction-10","chapter":"21 Linear and Non-Linear Algorithms for Classification","heading":"21.1 Introduction","text":"classification problem apply algorithms:Linear\nLG (logistic regression)\nLDA (linear discriminant analysis)\nGLMNET (Regularized logistic regression)\nLinearLG (logistic regression)LDA (linear discriminant analysis)GLMNET (Regularized logistic regression)Non-linear\nKNN (k-Nearest Neighbors)\nCART (Classification Regression Trees)\nNB (Naive Bayes)\nSVM (Support Vector Machines)\nNon-linearKNN (k-Nearest Neighbors)CART (Classification Regression Trees)NB (Naive Bayes)SVM (Support Vector Machines)","code":"\n# load packages\nlibrary(mlbench)\nlibrary(caret)\n#> Loading required package: lattice\n#> Loading required package: ggplot2\nlibrary(tictoc)\n\n# Load data\ndata(BreastCancer)"},{"path":"linear-and-non-linear-algorithms-for-classification.html","id":"workflow-2","chapter":"21 Linear and Non-Linear Algorithms for Classification","heading":"21.2 Workflow","text":"Load datasetCreate train validation datasets, 80/20Inspect dataset:dimensionclass variablesskimrClean featuresConvert character numericFrequency table classremove NAsVisualize featureshistograms (loop variables)density plots (loop)boxplot (loop)Pairwise jittered plotBarplots features (loop)Train -isSet train control \n10 cross-validations\n3 repetitions\nMetric: Accuracy\nSet train control to10 cross-validations3 repetitionsMetric: AccuracyTrain modelsTrain modelsNumeric comparison model resultsNumeric comparison model resultsVisual comparison\ndot plot\nVisual comparisondot plotTrain data transformationdata transformatiom\nBoxCox\ndata transformatiomBoxCoxTrain modelsTrain modelsNumeric comparisonNumeric comparisonVisual comparison\ndot plot\nVisual comparisondot plotTune best model: SVMSet train control \n10 cross-validations\n3 repetitions\nMetric: Accuracy\nSet train control to10 cross-validations3 repetitionsMetric: AccuracyTrain models\nRadial SVM\nSigma vector\n.C\nBoxCox\nTrain modelsRadial SVMSigma vector.CBoxCoxEvaluate tuning parametersEvaluate tuning parametersTune best model: KNNSet train control \n10 cross-validations\n3 repetitions\nMetric: Accuracy\nSet train control to10 cross-validations3 repetitionsMetric: AccuracyTrain models\n.k\nBoxCox\nTrain models.kBoxCoxEvaluate tuning parameters\nScatter plot 10, Ensembling\nEvaluate tuning parametersScatter plot 10, EnsemblingSelect algorithms\nBagged CART\nRandom Forest\nStochastic Gradient Boosting\nC5.0\nSelect algorithmsBagged CARTRandom ForestStochastic Gradient BoostingC5.0Numeric comparison\nresamples\nsummary\nNumeric comparisonresamplessummaryVisual comparison\ndot plot\nVisual comparisondot plotFinalize modelBack transformation\npreProcess\npredict\nBack transformationpreProcesspredictApply model validation setPrepare validation setPrepare validation setTransform datasetTransform datasetMake prediction\nknn3Train\nMake predictionknn3TrainCalculate accuracy\nConfusion Matrix\nCalculate accuracyConfusion Matrix","code":""},{"path":"linear-and-non-linear-algorithms-for-classification.html","id":"inspect-the-dataset","chapter":"21 Linear and Non-Linear Algorithms for Classification","heading":"21.3 Inspect the dataset","text":"can see besides Id, attributes factors. makes sense. think modeling may useful work data numbers factors. Factors might make things easier decision tree algorithms (). Given ordinal relationship levels can expose structure algorithms better work directly integer numbers.","code":"\ndplyr::glimpse(BreastCancer)\n#> Rows: 699\n#> Columns: 11\n#> $ Id              <chr> \"1000025\", \"1002945\", \"1015425\", \"1016277\", \"1017023\"…\n#> $ Cl.thickness    <ord> 5, 5, 3, 6, 4, 8, 1, 2, 2, 4, 1, 2, 5, 1, 8, 7, 4, 4,…\n#> $ Cell.size       <ord> 1, 4, 1, 8, 1, 10, 1, 1, 1, 2, 1, 1, 3, 1, 7, 4, 1, 1…\n#> $ Cell.shape      <ord> 1, 4, 1, 8, 1, 10, 1, 2, 1, 1, 1, 1, 3, 1, 5, 6, 1, 1…\n#> $ Marg.adhesion   <ord> 1, 5, 1, 1, 3, 8, 1, 1, 1, 1, 1, 1, 3, 1, 10, 4, 1, 1…\n#> $ Epith.c.size    <ord> 2, 7, 2, 3, 2, 7, 2, 2, 2, 2, 1, 2, 2, 2, 7, 6, 2, 2,…\n#> $ Bare.nuclei     <fct> 1, 10, 2, 4, 1, 10, 10, 1, 1, 1, 1, 1, 3, 3, 9, 1, 1,…\n#> $ Bl.cromatin     <fct> 3, 3, 3, 3, 3, 9, 3, 3, 1, 2, 3, 2, 4, 3, 5, 4, 2, 3,…\n#> $ Normal.nucleoli <fct> 1, 2, 1, 7, 1, 7, 1, 1, 1, 1, 1, 1, 4, 1, 5, 3, 1, 1,…\n#> $ Mitoses         <fct> 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 4, 1, 1, 1,…\n#> $ Class           <fct> benign, benign, benign, benign, benign, malignant, be…\ntibble::as_tibble(BreastCancer)\n#> # A tibble: 699 x 11\n#>   Id    Cl.thickness Cell.size Cell.shape Marg.adhesion Epith.c.size Bare.nuclei\n#>   <chr> <ord>        <ord>     <ord>      <ord>         <ord>        <fct>      \n#> 1 1000… 5            1         1          1             2            1          \n#> 2 1002… 5            4         4          5             7            10         \n#> 3 1015… 3            1         1          1             2            2          \n#> 4 1016… 6            8         8          1             3            4          \n#> 5 1017… 4            1         1          3             2            1          \n#> 6 1017… 8            10        10         8             7            10         \n#> # … with 693 more rows, and 4 more variables: Bl.cromatin <fct>,\n#> #   Normal.nucleoli <fct>, Mitoses <fct>, Class <fct>\n# Split out validation dataset\n# create a list of 80% of the rows in the original dataset we can use for training\nset.seed(7)\nvalidationIndex <- createDataPartition(BreastCancer$Class, \n                                       p=0.80, \n                                       list=FALSE)\n\n# select 20% of the data for validation\nvalidation <- BreastCancer[-validationIndex,]\n# use the remaining 80% of data to training and testing the models\ndataset <- BreastCancer[validationIndex,]\n# dimensions of dataset\ndim(validation)\n#> [1] 139  11\ndim(dataset)\n#> [1] 560  11\n# peek\nhead(dataset, n=20)\n#>         Id Cl.thickness Cell.size Cell.shape Marg.adhesion Epith.c.size\n#> 1  1000025            5         1          1             1            2\n#> 2  1002945            5         4          4             5            7\n#> 3  1015425            3         1          1             1            2\n#> 5  1017023            4         1          1             3            2\n#> 6  1017122            8        10         10             8            7\n#> 7  1018099            1         1          1             1            2\n#> 8  1018561            2         1          2             1            2\n#> 9  1033078            2         1          1             1            2\n#> 10 1033078            4         2          1             1            2\n#> 11 1035283            1         1          1             1            1\n#> 13 1041801            5         3          3             3            2\n#> 14 1043999            1         1          1             1            2\n#> 15 1044572            8         7          5            10            7\n#> 16 1047630            7         4          6             4            6\n#> 18 1049815            4         1          1             1            2\n#> 19 1050670           10         7          7             6            4\n#> 21 1054590            7         3          2            10            5\n#> 22 1054593           10         5          5             3            6\n#> 23 1056784            3         1          1             1            2\n#> 24 1057013            8         4          5             1            2\n#>    Bare.nuclei Bl.cromatin Normal.nucleoli Mitoses     Class\n#> 1            1           3               1       1    benign\n#> 2           10           3               2       1    benign\n#> 3            2           3               1       1    benign\n#> 5            1           3               1       1    benign\n#> 6           10           9               7       1 malignant\n#> 7           10           3               1       1    benign\n#> 8            1           3               1       1    benign\n#> 9            1           1               1       5    benign\n#> 10           1           2               1       1    benign\n#> 11           1           3               1       1    benign\n#> 13           3           4               4       1 malignant\n#> 14           3           3               1       1    benign\n#> 15           9           5               5       4 malignant\n#> 16           1           4               3       1 malignant\n#> 18           1           3               1       1    benign\n#> 19          10           4               1       2 malignant\n#> 21          10           5               4       4 malignant\n#> 22           7           7              10       1 malignant\n#> 23           1           2               1       1    benign\n#> 24        <NA>           7               3       1 malignant\nlibrary(skimr)\nprint(skim(dataset))\n#> ── Data Summary ────────────────────────\n#>                            Values \n#> Name                       dataset\n#> Number of rows             560    \n#> Number of columns          11     \n#> _______________________           \n#> Column type frequency:            \n#>   character                1      \n#>   factor                   10     \n#> ________________________          \n#> Group variables            None   \n#> \n#> ── Variable type: character ────────────────────────────────────────────────────\n#>   skim_variable n_missing complete_rate   min   max empty n_unique whitespace\n#> 1 Id                    0             1     5     8     0      523          0\n#> \n#> ── Variable type: factor ───────────────────────────────────────────────────────\n#>    skim_variable   n_missing complete_rate ordered n_unique\n#>  1 Cl.thickness            0         1     TRUE          10\n#>  2 Cell.size               0         1     TRUE          10\n#>  3 Cell.shape              0         1     TRUE          10\n#>  4 Marg.adhesion           0         1     TRUE          10\n#>  5 Epith.c.size            0         1     TRUE          10\n#>  6 Bare.nuclei            12         0.979 FALSE         10\n#>  7 Bl.cromatin             0         1     FALSE         10\n#>  8 Normal.nucleoli         0         1     FALSE         10\n#>  9 Mitoses                 0         1     FALSE          9\n#> 10 Class                   0         1     FALSE          2\n#>    top_counts                   \n#>  1 1: 113, 3: 94, 5: 94, 4: 62  \n#>  2 1: 307, 10: 55, 3: 43, 2: 36 \n#>  3 1: 281, 10: 48, 3: 47, 2: 46 \n#>  4 1: 331, 2: 44, 10: 44, 3: 42 \n#>  5 2: 304, 3: 60, 4: 40, 1: 38  \n#>  6 1: 327, 10: 102, 2: 28, 5: 24\n#>  7 3: 136, 2: 134, 1: 118, 7: 64\n#>  8 1: 347, 10: 50, 3: 36, 2: 29 \n#>  9 1: 466, 3: 29, 2: 23, 4: 10  \n#> 10 ben: 367, mal: 193\n# types\nsapply(dataset, class)\n#> $Id\n#> [1] \"character\"\n#> \n#> $Cl.thickness\n#> [1] \"ordered\" \"factor\" \n#> \n#> $Cell.size\n#> [1] \"ordered\" \"factor\" \n#> \n#> $Cell.shape\n#> [1] \"ordered\" \"factor\" \n#> \n#> $Marg.adhesion\n#> [1] \"ordered\" \"factor\" \n#> \n#> $Epith.c.size\n#> [1] \"ordered\" \"factor\" \n#> \n#> $Bare.nuclei\n#> [1] \"factor\"\n#> \n#> $Bl.cromatin\n#> [1] \"factor\"\n#> \n#> $Normal.nucleoli\n#> [1] \"factor\"\n#> \n#> $Mitoses\n#> [1] \"factor\"\n#> \n#> $Class\n#> [1] \"factor\""},{"path":"linear-and-non-linear-algorithms-for-classification.html","id":"clean-up","chapter":"21 Linear and Non-Linear Algorithms for Classification","heading":"21.4 clean up","text":"can see 13 NA values Bare.nuclei attribute. suggests may need remove records (impute values) NA values analysis modeling techniques.","code":"\n# Remove redundant variable Id\ndataset <- dataset[,-1]\n# convert input values to numeric\nfor(i in 1:9) {\n    dataset[,i] <- as.numeric(as.character(dataset[,i]))\n}\n# summary\nsummary(dataset)\n#>   Cl.thickness     Cell.size       Cell.shape    Marg.adhesion  \n#>  Min.   : 1.00   Min.   : 1.00   Min.   : 1.00   Min.   : 1.00  \n#>  1st Qu.: 2.00   1st Qu.: 1.00   1st Qu.: 1.00   1st Qu.: 1.00  \n#>  Median : 4.00   Median : 1.00   Median : 1.00   Median : 1.00  \n#>  Mean   : 4.44   Mean   : 3.14   Mean   : 3.21   Mean   : 2.79  \n#>  3rd Qu.: 6.00   3rd Qu.: 5.00   3rd Qu.: 5.00   3rd Qu.: 4.00  \n#>  Max.   :10.00   Max.   :10.00   Max.   :10.00   Max.   :10.00  \n#>                                                                 \n#>   Epith.c.size    Bare.nuclei     Bl.cromatin    Normal.nucleoli\n#>  Min.   : 1.00   Min.   : 1.00   Min.   : 1.00   Min.   : 1.00  \n#>  1st Qu.: 2.00   1st Qu.: 1.00   1st Qu.: 2.00   1st Qu.: 1.00  \n#>  Median : 2.00   Median : 1.00   Median : 3.00   Median : 1.00  \n#>  Mean   : 3.23   Mean   : 3.48   Mean   : 3.45   Mean   : 2.94  \n#>  3rd Qu.: 4.00   3rd Qu.: 5.25   3rd Qu.: 5.00   3rd Qu.: 4.00  \n#>  Max.   :10.00   Max.   :10.00   Max.   :10.00   Max.   :10.00  \n#>                  NA's   :12                                     \n#>     Mitoses            Class    \n#>  Min.   : 1.00   benign   :367  \n#>  1st Qu.: 1.00   malignant:193  \n#>  Median : 1.00                  \n#>  Mean   : 1.59                  \n#>  3rd Qu.: 1.00                  \n#>  Max.   :10.00                  \n#> \nprint(skim(dataset))\n#> ── Data Summary ────────────────────────\n#>                            Values \n#> Name                       dataset\n#> Number of rows             560    \n#> Number of columns          10     \n#> _______________________           \n#> Column type frequency:            \n#>   factor                   1      \n#>   numeric                  9      \n#> ________________________          \n#> Group variables            None   \n#> \n#> ── Variable type: factor ───────────────────────────────────────────────────────\n#>   skim_variable n_missing complete_rate ordered n_unique top_counts        \n#> 1 Class                 0             1 FALSE          2 ben: 367, mal: 193\n#> \n#> ── Variable type: numeric ──────────────────────────────────────────────────────\n#>   skim_variable   n_missing complete_rate  mean    sd    p0   p25   p50   p75\n#> 1 Cl.thickness            0         1      4.44  2.83     1     2     4  6   \n#> 2 Cell.size               0         1      3.14  3.07     1     1     1  5   \n#> 3 Cell.shape              0         1      3.21  2.97     1     1     1  5   \n#> 4 Marg.adhesion           0         1      2.79  2.85     1     1     1  4   \n#> 5 Epith.c.size            0         1      3.23  2.22     1     2     2  4   \n#> 6 Bare.nuclei            12         0.979  3.48  3.62     1     1     1  5.25\n#> 7 Bl.cromatin             0         1      3.45  2.43     1     2     3  5   \n#> 8 Normal.nucleoli         0         1      2.94  3.08     1     1     1  4   \n#> 9 Mitoses                 0         1      1.59  1.70     1     1     1  1   \n#>    p100 hist \n#> 1    10 ▇▇▆▃▃\n#> 2    10 ▇▂▁▁▂\n#> 3    10 ▇▂▁▁▁\n#> 4    10 ▇▂▁▁▁\n#> 5    10 ▇▂▂▁▁\n#> 6    10 ▇▁▁▁▂\n#> 7    10 ▇▅▁▂▁\n#> 8    10 ▇▁▁▁▂\n#> 9    10 ▇▁▁▁▁"},{"path":"linear-and-non-linear-algorithms-for-classification.html","id":"analyze-the-class-variable","chapter":"21 Linear and Non-Linear Algorithms for Classification","heading":"21.5 Analyze the class variable","text":"indeed 65% 35% split benign-malignant class values imbalanced, much need thinking rebalancing dataset, least yet.","code":"\n# class distribution\ncbind(freq = table(dataset$Class), \n      percentage = prop.table(table(dataset$Class))*100)\n#>           freq percentage\n#> benign     367       65.5\n#> malignant  193       34.5"},{"path":"linear-and-non-linear-algorithms-for-classification.html","id":"remove-nas","chapter":"21 Linear and Non-Linear Algorithms for Classification","heading":"21.5.1 remove NAs","text":"can see modest high correlation attributes. example cell shape cell size 0.90 correlation.","code":"\n# summarize correlations between input variables\ncomplete_cases <- complete.cases(dataset)\ncor(dataset[complete_cases,1:9])\n#>                 Cl.thickness Cell.size Cell.shape Marg.adhesion Epith.c.size\n#> Cl.thickness           1.000     0.665      0.667         0.486        0.543\n#> Cell.size              0.665     1.000      0.904         0.722        0.773\n#> Cell.shape             0.667     0.904      1.000         0.694        0.739\n#> Marg.adhesion          0.486     0.722      0.694         1.000        0.643\n#> Epith.c.size           0.543     0.773      0.739         0.643        1.000\n#> Bare.nuclei            0.598     0.700      0.721         0.669        0.614\n#> Bl.cromatin            0.565     0.752      0.739         0.692        0.628\n#> Normal.nucleoli        0.570     0.737      0.741         0.644        0.642\n#> Mitoses                0.347     0.453      0.432         0.419        0.485\n#>                 Bare.nuclei Bl.cromatin Normal.nucleoli Mitoses\n#> Cl.thickness          0.598       0.565           0.570   0.347\n#> Cell.size             0.700       0.752           0.737   0.453\n#> Cell.shape            0.721       0.739           0.741   0.432\n#> Marg.adhesion         0.669       0.692           0.644   0.419\n#> Epith.c.size          0.614       0.628           0.642   0.485\n#> Bare.nuclei           1.000       0.685           0.605   0.351\n#> Bl.cromatin           0.685       1.000           0.692   0.356\n#> Normal.nucleoli       0.605       0.692           1.000   0.432\n#> Mitoses               0.351       0.356           0.432   1.000"},{"path":"linear-and-non-linear-algorithms-for-classification.html","id":"unimodal-visualization","chapter":"21 Linear and Non-Linear Algorithms for Classification","heading":"21.6 Unimodal visualization","text":"can see almost distributions exponential bimodal shape .plots add support initial ideas. can see bimodal distributions (two bumps) exponential-looking distributions.","code":"\n# histograms each attribute\npar(mfrow=c(3,3))\nfor(i in 1:9) {\n    hist(dataset[,i], main=names(dataset)[i])\n}\n# density plot for each attribute\npar(mfrow=c(3,3))\ncomplete_cases <- complete.cases(dataset)\nfor(i in 1:9) {\n    plot(density(dataset[complete_cases,i]), main=names(dataset)[i])\n}\n# boxplots for each attribute\npar(mfrow=c(3,3))\nfor(i in 1:9) {\n    boxplot(dataset[,i], main=names(dataset)[i])\n}"},{"path":"linear-and-non-linear-algorithms-for-classification.html","id":"multimodal-visualization","chapter":"21 Linear and Non-Linear Algorithms for Classification","heading":"21.7 Multimodal visualization","text":"can see black (benign) part clustered around bottom-right corner (smaller values) red (malignant) place.","code":"\n# scatter plot matrix\njittered_x <- sapply(dataset[,1:9], jitter)\npairs(jittered_x, names(dataset[,1:9]), col=dataset$Class)\n# bar plots of each variable by class\npar(mfrow=c(3,3))\nfor(i in 1:9) {\n    barplot(table(dataset$Class,dataset[,i]), main=names(dataset)[i], \n            legend.text=unique(dataset$Class))\n}"},{"path":"linear-and-non-linear-algorithms-for-classification.html","id":"algorithms-evaluation","chapter":"21 Linear and Non-Linear Algorithms for Classification","heading":"21.8 Algorithms Evaluation","text":"Linear Algorithms: Logistic Regression (LG), Linear Discriminate Analysis (LDA) Regularized Logistic Regression (GLMNET).Linear Algorithms: Logistic Regression (LG), Linear Discriminate Analysis (LDA) Regularized Logistic Regression (GLMNET).Nonlinear Algorithms: k-Nearest Neighbors (KNN), Classiﬁcation Regression Trees (CART), Naive Bayes (NB) Support Vector Machines Radial Basis Functions (SVM).Nonlinear Algorithms: k-Nearest Neighbors (KNN), Classiﬁcation Regression Trees (CART), Naive Bayes (NB) Support Vector Machines Radial Basis Functions (SVM).simplicity, use Accuracy Kappa metrics. Given medical test, gone Area ROC Curve (AUC) looked sensitivity speciﬁcity select best algorithms.can see good accuracy across board. algorithms mean accuracy 90%, well baseline 65% just predicted benign. problem learnable. can see KNN (97.08%) logistic regression (NB 96.2% GLMNET 96.4%) highest accuracy problem.","code":"\n# 10-fold cross-validation with 3 repeats\ntrainControl <- trainControl(method = \"repeatedcv\", number=10, repeats=3)\nmetric <- \"Accuracy\"\ntic()\n# LG\nset.seed(7)\nfit.glm <- train(Class~., data=dataset, method=\"glm\", metric=metric,\n                 trControl=trainControl, na.action=na.omit)\n# LDA\nset.seed(7)\nfit.lda <- train(Class~., data=dataset, method=\"lda\", metric=metric,\n                 trControl=trainControl, na.action=na.omit)\n# GLMNET\nset.seed(7)\nfit.glmnet <- train(Class~., data=dataset, method=\"glmnet\", metric=metric,\n                    trControl=trainControl, na.action=na.omit)\n# KNN\nset.seed(7)\nfit.knn <- train(Class~., data=dataset, method=\"knn\", metric=metric, \n                 trControl=trainControl, na.action=na.omit)\n# CART\nset.seed(7)\nfit.cart <- train(Class~., data=dataset, method=\"rpart\", metric=metric, \n                  trControl=trainControl, na.action=na.omit)\n# Naive Bayes\nset.seed(7)\nfit.nb <- train(Class~., data=dataset, method=\"nb\", metric=metric, \n                trControl=trainControl, na.action=na.omit)\n# SVM\nset.seed(7)\nfit.svm <- train(Class~., data=dataset, method=\"svmRadial\", metric=metric, \n                 trControl=trainControl, na.action=na.omit)\n\n# Compare algorithms\nresults <- resamples(list(LG     = fit.glm, \n                          LDA    = fit.lda, \n                          GLMNET = fit.glmnet, \n                          KNN    = fit.knn, \n                          CART   = fit.cart, \n                          NB     = fit.nb, \n                          SVM    = fit.svm))\ntoc()\n#> 12.796 sec elapsed\nsummary(results)\n#> \n#> Call:\n#> summary.resamples(object = results)\n#> \n#> Models: LG, LDA, GLMNET, KNN, CART, NB, SVM \n#> Number of resamples: 30 \n#> \n#> Accuracy \n#>         Min. 1st Qu. Median  Mean 3rd Qu. Max. NA's\n#> LG     0.909   0.945  0.964 0.968   0.995    1    0\n#> LDA    0.907   0.945  0.964 0.963   0.982    1    0\n#> GLMNET 0.927   0.964  0.964 0.973   0.995    1    0\n#> KNN    0.927   0.964  0.982 0.976   1.000    1    0\n#> CART   0.833   0.927  0.945 0.943   0.964    1    0\n#> NB     0.927   0.963  0.981 0.970   0.982    1    0\n#> SVM    0.907   0.945  0.964 0.965   0.982    1    0\n#> \n#> Kappa \n#>         Min. 1st Qu. Median  Mean 3rd Qu. Max. NA's\n#> LG     0.806   0.880  0.921 0.930   0.990    1    0\n#> LDA    0.786   0.880  0.918 0.917   0.959    1    0\n#> GLMNET 0.843   0.918  0.922 0.940   0.990    1    0\n#> KNN    0.843   0.920  0.959 0.947   1.000    1    0\n#> CART   0.630   0.840  0.879 0.876   0.920    1    0\n#> NB     0.835   0.918  0.959 0.934   0.960    1    0\n#> SVM    0.804   0.883  0.922 0.926   0.960    1    0\ndotplot(results)"},{"path":"linear-and-non-linear-algorithms-for-classification.html","id":"data-transform","chapter":"21 Linear and Non-Linear Algorithms for Classification","heading":"21.9 Data transform","text":"know skewed distributions. transform methods can use adjust normalize distributions. favorite positive input attributes (case) Box-Cox transform.can see accuracy previous best algorithm KNN elevated 97.14%. new ranking, showing SVM accurate mean accuracy 97.20%.","code":"\n# 10-fold cross-validation with 3 repeats\ntrainControl <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\nmetric <- \"Accuracy\"\n\n# LG\nset.seed(7)\nfit.glm <- train(Class~., data=dataset, method=\"glm\", metric=metric, \n                 preProc=c(\"BoxCox\"), \n                 trControl=trainControl, na.action=na.omit)\n# LDA\nset.seed(7)\nfit.lda <- train(Class~., data=dataset, method=\"lda\", metric=metric,\n                 preProc=c(\"BoxCox\"), \n                 trControl=trainControl, na.action=na.omit)\n# GLMNET\nset.seed(7)\nfit.glmnet <- train(Class~., data=dataset, method=\"glmnet\", metric=metric, \n                    preProc=c(\"BoxCox\"), \n                    trControl=trainControl, \n                    na.action=na.omit)\n# KNN\nset.seed(7)\nfit.knn <- train(Class~., data=dataset, method=\"knn\", metric=metric, \n                 preProc=c(\"BoxCox\"), \n                 trControl=trainControl, na.action=na.omit)\n# CART\nset.seed(7)\nfit.cart <- train(Class~., data=dataset, method=\"rpart\", metric=metric, \n                  preProc=c(\"BoxCox\"), \n                  trControl=trainControl, \n                  na.action=na.omit)\n# Naive Bayes\nset.seed(7)\nfit.nb <- train(Class~., data=dataset, method=\"nb\", metric=metric, \n                preProc=c(\"BoxCox\"), trControl=trainControl, na.action=na.omit)\n#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\n#> observation 1\n#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\n#> observation 24\n#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\n#> observation 28\n#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\n#> observation 20\n#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\n#> observation 11\n#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\n#> observation 18\n#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\n#> observation 54\n#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\n#> observation 3\n#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\n#> observation 23\n#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\n#> observation 21\n#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\n#> observation 27\n#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\n#> observation 53\n#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\n#> observation 12\n#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\n#> observation 9\n#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\n#> observation 2\n#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\n#> observation 17\n#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\n#> observation 9\n#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\n#> observation 55\n#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\n#> observation 23\n#> Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with\n#> observation 32\n# SVM\nset.seed(7)\nfit.svm <- train(Class~., data=dataset, method=\"svmRadial\", metric=metric, \n                 preProc=c(\"BoxCox\"), \n                 trControl=trainControl, na.action=na.omit)\n\n# Compare algorithms\ntransformResults <- resamples(list(LG     = fit.glm, \n                                  LDA    = fit.lda, \n                                  GLMNET = fit.glmnet, \n                                  KNN    = fit.knn, \n                                  CART   = fit.cart, \n                                  NB     = fit.nb, \n                                  SVM    = fit.svm))\nsummary(transformResults)\n#> \n#> Call:\n#> summary.resamples(object = transformResults)\n#> \n#> Models: LG, LDA, GLMNET, KNN, CART, NB, SVM \n#> Number of resamples: 30 \n#> \n#> Accuracy \n#>         Min. 1st Qu. Median  Mean 3rd Qu. Max. NA's\n#> LG     0.909   0.963  0.982 0.973   0.996    1    0\n#> LDA    0.927   0.964  0.981 0.974   0.982    1    0\n#> GLMNET 0.944   0.964  0.982 0.980   1.000    1    0\n#> KNN    0.909   0.964  0.981 0.976   0.982    1    0\n#> CART   0.833   0.927  0.945 0.943   0.964    1    0\n#> NB     0.927   0.964  0.982 0.978   1.000    1    0\n#> SVM    0.927   0.964  0.982 0.980   1.000    1    0\n#> \n#> Kappa \n#>         Min. 1st Qu. Median  Mean 3rd Qu. Max. NA's\n#> LG     0.806   0.919  0.959 0.941   0.990    1    0\n#> LDA    0.847   0.921  0.959 0.945   0.961    1    0\n#> GLMNET 0.878   0.922  0.960 0.957   1.000    1    0\n#> KNN    0.806   0.922  0.959 0.949   0.961    1    0\n#> CART   0.630   0.840  0.879 0.876   0.920    1    0\n#> NB     0.843   0.922  0.960 0.953   1.000    1    0\n#> SVM    0.847   0.922  0.960 0.957   1.000    1    0\ndotplot(transformResults)"},{"path":"linear-and-non-linear-algorithms-for-classification.html","id":"tuning-svm","chapter":"21 Linear and Non-Linear Algorithms for Classification","heading":"21.10 Tuning SVM","text":"can see made little difference results. accurate model score 97.31% (previously rounded score 97.20%) using sigma = 0.1 C = 1. tune , don’t expect payoﬀ.","code":"\n# 10-fold cross-validation with 3 repeats\ntrainControl <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\nmetric <- \"Accuracy\"\nset.seed(7)\n\ngrid <- expand.grid(.sigma = c(0.025, 0.05, 0.1, 0.15), \n                    .C = seq(1, 10, by=1))\n\nfit.svm <- train(Class~., data=dataset, method=\"svmRadial\", metric=metric, \n                 tuneGrid=grid, \n                 preProc=c(\"BoxCox\"), trControl=trainControl, \n                 na.action=na.omit)\nprint(fit.svm)\n#> Support Vector Machines with Radial Basis Function Kernel \n#> \n#> 560 samples\n#>   9 predictor\n#>   2 classes: 'benign', 'malignant' \n#> \n#> Pre-processing: Box-Cox transformation (9) \n#> Resampling: Cross-Validated (10 fold, repeated 3 times) \n#> Summary of sample sizes: 493, 492, 493, 493, 493, 494, ... \n#> Resampling results across tuning parameters:\n#> \n#>   sigma  C   Accuracy  Kappa\n#>   0.025   1  0.979     0.956\n#>   0.025   2  0.979     0.954\n#>   0.025   3  0.979     0.956\n#>   0.025   4  0.977     0.950\n#>   0.025   5  0.977     0.950\n#>   0.025   6  0.978     0.951\n#>   0.025   7  0.979     0.954\n#>   0.025   8  0.979     0.956\n#>   0.025   9  0.979     0.956\n#>   0.025  10  0.979     0.956\n#>   0.050   1  0.980     0.957\n#>   0.050   2  0.980     0.957\n#>   0.050   3  0.979     0.956\n#>   0.050   4  0.980     0.957\n#>   0.050   5  0.980     0.957\n#>   0.050   6  0.980     0.957\n#>   0.050   7  0.979     0.956\n#>   0.050   8  0.979     0.954\n#>   0.050   9  0.979     0.954\n#>   0.050  10  0.979     0.954\n#>   0.100   1  0.980     0.957\n#>   0.100   2  0.980     0.957\n#>   0.100   3  0.979     0.956\n#>   0.100   4  0.978     0.953\n#>   0.100   5  0.978     0.952\n#>   0.100   6  0.975     0.946\n#>   0.100   7  0.976     0.948\n#>   0.100   8  0.976     0.948\n#>   0.100   9  0.976     0.948\n#>   0.100  10  0.975     0.946\n#>   0.150   1  0.980     0.957\n#>   0.150   2  0.978     0.953\n#>   0.150   3  0.978     0.953\n#>   0.150   4  0.976     0.949\n#>   0.150   5  0.976     0.948\n#>   0.150   6  0.975     0.946\n#>   0.150   7  0.974     0.944\n#>   0.150   8  0.972     0.939\n#>   0.150   9  0.970     0.934\n#>   0.150  10  0.968     0.930\n#> \n#> Accuracy was used to select the optimal model using the largest value.\n#> The final values used for the model were sigma = 0.15 and C = 1.\nplot(fit.svm)"},{"path":"linear-and-non-linear-algorithms-for-classification.html","id":"tuning-knn","chapter":"21 Linear and Non-Linear Algorithms for Classification","heading":"21.11 Tuning KNN","text":"can see tuning made little difference, settling value k = 7 accuracy 97.19%. higher previous 97.14%, similar (perhaps identical!) result achieved tuned SVM.","code":"\n# 10-fold cross-validation with 3 repeats\ntrainControl <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\nmetric <- \"Accuracy\"\nset.seed(7)\n\ngrid <- expand.grid(.k = seq(1,20, by=1))\nfit.knn <- train(Class~., data=dataset, method=\"knn\", metric=metric, \n                 tuneGrid=grid, \n                 preProc=c(\"BoxCox\"), trControl=trainControl, \n                 na.action=na.omit)\nprint(fit.knn)\n#> k-Nearest Neighbors \n#> \n#> 560 samples\n#>   9 predictor\n#>   2 classes: 'benign', 'malignant' \n#> \n#> Pre-processing: Box-Cox transformation (9) \n#> Resampling: Cross-Validated (10 fold, repeated 3 times) \n#> Summary of sample sizes: 493, 492, 493, 493, 493, 494, ... \n#> Resampling results across tuning parameters:\n#> \n#>   k   Accuracy  Kappa\n#>    1  0.958     0.908\n#>    2  0.960     0.912\n#>    3  0.968     0.931\n#>    4  0.970     0.935\n#>    5  0.972     0.939\n#>    6  0.973     0.942\n#>    7  0.976     0.949\n#>    8  0.975     0.946\n#>    9  0.976     0.947\n#>   10  0.976     0.949\n#>   11  0.976     0.949\n#>   12  0.976     0.947\n#>   13  0.974     0.945\n#>   14  0.975     0.946\n#>   15  0.976     0.947\n#>   16  0.975     0.946\n#>   17  0.976     0.947\n#>   18  0.976     0.947\n#>   19  0.978     0.951\n#>   20  0.977     0.950\n#> \n#> Accuracy was used to select the optimal model using the largest value.\n#> The final value used for the model was k = 19.\nplot(fit.knn)"},{"path":"linear-and-non-linear-algorithms-for-classification.html","id":"ensemble","chapter":"21 Linear and Non-Linear Algorithms for Classification","heading":"21.12 Ensemble","text":"see Random Forest accurate score 97.26%. similar tuned models . spend time tuning parameters Random Forest (e.g. increasing number trees) ensemble methods, don’t expect see better accuracy scores random statistical fluctuations.","code":"\n# 10-fold cross-validation with 3 repeats\ntrainControl <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\nmetric <- \"Accuracy\"\n\n# Bagged CART\nset.seed(7)\nfit.treebag <- train(Class~., data=dataset, method=\"treebag\", \n                     metric=metric, \n                     trControl=trainControl, na.action=na.omit)\n\n# Random Forest\nset.seed(7)\nfit.rf <- train(Class~., data=dataset, method=\"rf\", \n                metric=metric, preProc=c(\"BoxCox\"), \n                trControl=trainControl, na.action=na.omit)\n\n# Stochastic Gradient Boosting\nset.seed(7)\nfit.gbm <- train(Class~., data=dataset, method=\"gbm\", \n                 metric=metric, preProc=c(\"BoxCox\"), \n                 trControl=trainControl, verbose=FALSE, na.action=na.omit)\n\n# C5.0\nset.seed(7)\nfit.c50 <- train(Class~., data=dataset, method=\"C5.0\", \n                 metric=metric, preProc=c(\"BoxCox\"), \n                 trControl=trainControl, na.action=na.omit)\n\n# Compare results\nensembleResults <- resamples(list(BAG = fit.treebag, \n                                  RF  = fit.rf, \n                                  GBM = fit.gbm, \n                                  C50 = fit.c50))\nsummary(ensembleResults)\n#> \n#> Call:\n#> summary.resamples(object = ensembleResults)\n#> \n#> Models: BAG, RF, GBM, C50 \n#> Number of resamples: 30 \n#> \n#> Accuracy \n#>      Min. 1st Qu. Median  Mean 3rd Qu. Max. NA's\n#> BAG 0.907   0.963  0.982 0.973   0.982    1    0\n#> RF  0.926   0.981  0.982 0.979   0.995    1    0\n#> GBM 0.929   0.963  0.981 0.973   0.995    1    0\n#> C50 0.907   0.964  0.982 0.972   0.982    1    0\n#> \n#> Kappa \n#>      Min. 1st Qu. Median  Mean 3rd Qu. Max. NA's\n#> BAG 0.804   0.920  0.959 0.942    0.96    1    0\n#> RF  0.841   0.959  0.960 0.955    0.99    1    0\n#> GBM 0.844   0.919  0.959 0.941    0.99    1    0\n#> C50 0.795   0.918  0.959 0.939    0.96    1    0\ndotplot(ensembleResults)"},{"path":"linear-and-non-linear-algorithms-for-classification.html","id":"finalize-model","chapter":"21 Linear and Non-Linear Algorithms for Classification","heading":"21.13 Finalize model","text":"now need finalize model, really means choose model like use. simplicity probably select KNN method, expense memory required store training dataset. SVM good choice trade-oﬀ space time complexity. probably select Random Forest algorithm given complexity model. seems overkill dataset, lots trees little benefit Accuracy.Let’s go KNN algorithm. really simple, need store model. need capture parameters Box-Cox transform though. also need prepare data removing unused Id attribute converting inputs numeric format.implementation KNN (knn3()) belongs caret package support missing values. remove rows missing values training dataset well validation dataset. code shows preparation pre-processing parameters using training dataset.","code":"\n# prepare parameters for data transform\nset.seed(7)\n\ndatasetNoMissing <- dataset[complete.cases(dataset),]\nx <- datasetNoMissing[,1:9]\n\n# transform\npreprocessParams <- preProcess(x, method=c(\"BoxCox\"))\nx <- predict(preprocessParams, x)"},{"path":"linear-and-non-linear-algorithms-for-classification.html","id":"prepare-the-validation-set","chapter":"21 Linear and Non-Linear Algorithms for Classification","heading":"21.14 Prepare the validation set","text":"Next need prepare validation dataset making prediction. must:Remove Id attribute.Remove rows missing data.Convert input attributes numeric.Apply Box-Cox transform input attributes using parameters prepared training dataset.can see accuracy final model validation dataset 99.26%. optimistic 136 rows, show accurate standalone model use unclassified data.","code":"\n# prepare the validation dataset\nset.seed(7)\n\n# remove id column\nvalidation <- validation[,-1]\n\n# remove missing values (not allowed in this implementation of knn)\nvalidation <- validation[complete.cases(validation),]\n\n# convert to numeric\nfor(i in 1:9) {\n    validation[,i] <- as.numeric(as.character(validation[,i]))\n}\n\n# transform the validation dataset\nvalidationX <- predict(preprocessParams, validation[,1:9])\n# make predictions\nset.seed(7)\n# knn3Train(train, test, cl, k = 1, l = 0, prob = TRUE, use.all = TRUE)\n# k: number of neighbours considered.\npredictions <- knn3Train(x, validationX, datasetNoMissing$Class, \n                         k = 9, \n                         prob = FALSE)\n\n# convert \nconfusionMatrix(as.factor(predictions), validation$Class)\n#> Confusion Matrix and Statistics\n#> \n#>            Reference\n#> Prediction  benign malignant\n#>   benign        83         1\n#>   malignant      4        47\n#>                                         \n#>                Accuracy : 0.963         \n#>                  95% CI : (0.916, 0.988)\n#>     No Information Rate : 0.644         \n#>     P-Value [Acc > NIR] : <2e-16        \n#>                                         \n#>                   Kappa : 0.92          \n#>                                         \n#>  Mcnemar's Test P-Value : 0.371         \n#>                                         \n#>             Sensitivity : 0.954         \n#>             Specificity : 0.979         \n#>          Pos Pred Value : 0.988         \n#>          Neg Pred Value : 0.922         \n#>              Prevalence : 0.644         \n#>          Detection Rate : 0.615         \n#>    Detection Prevalence : 0.622         \n#>       Balanced Accuracy : 0.967         \n#>                                         \n#>        'Positive' Class : benign        \n#> "},{"path":"detect-mines-vs-rocks-with-random-forest.html","id":"detect-mines-vs-rocks-with-random-forest","chapter":"22 Detect mines vs rocks with Random Forest","heading":"22 Detect mines vs rocks with Random Forest","text":"Datasets: SonarAlgorithms:\nRandom Forest\nRandom Forest","code":""},{"path":"detect-mines-vs-rocks-with-random-forest.html","id":"introduction-11","chapter":"22 Detect mines vs rocks with Random Forest","heading":"22.1 Introduction","text":"mtry: Number variables randomly sampled candidates split.ntree: Number trees grow.","code":""},{"path":"detect-mines-vs-rocks-with-random-forest.html","id":"load-libraries-1","chapter":"22 Detect mines vs rocks with Random Forest","heading":"22.2 Load libraries","text":"","code":"\n# load packages\nlibrary(caret)\nlibrary(mlbench)\nlibrary(randomForest)\nlibrary(tictoc)\n\n# load dataset\ndata(Sonar)\nset.seed(7)"},{"path":"detect-mines-vs-rocks-with-random-forest.html","id":"explore-data","chapter":"22 Detect mines vs rocks with Random Forest","heading":"22.3 Explore data","text":"Accuracy: 85.26% mtry=2","code":"\ndplyr::glimpse(Sonar)\n#> Rows: 208\n#> Columns: 61\n#> $ V1    <dbl> 0.0200, 0.0453, 0.0262, 0.0100, 0.0762, 0.0286, 0.0317, 0.0519,…\n#> $ V2    <dbl> 0.0371, 0.0523, 0.0582, 0.0171, 0.0666, 0.0453, 0.0956, 0.0548,…\n#> $ V3    <dbl> 0.0428, 0.0843, 0.1099, 0.0623, 0.0481, 0.0277, 0.1321, 0.0842,…\n#> $ V4    <dbl> 0.0207, 0.0689, 0.1083, 0.0205, 0.0394, 0.0174, 0.1408, 0.0319,…\n#> $ V5    <dbl> 0.0954, 0.1183, 0.0974, 0.0205, 0.0590, 0.0384, 0.1674, 0.1158,…\n#> $ V6    <dbl> 0.0986, 0.2583, 0.2280, 0.0368, 0.0649, 0.0990, 0.1710, 0.0922,…\n#> $ V7    <dbl> 0.1539, 0.2156, 0.2431, 0.1098, 0.1209, 0.1201, 0.0731, 0.1027,…\n#> $ V8    <dbl> 0.1601, 0.3481, 0.3771, 0.1276, 0.2467, 0.1833, 0.1401, 0.0613,…\n#> $ V9    <dbl> 0.3109, 0.3337, 0.5598, 0.0598, 0.3564, 0.2105, 0.2083, 0.1465,…\n#> $ V10   <dbl> 0.2111, 0.2872, 0.6194, 0.1264, 0.4459, 0.3039, 0.3513, 0.2838,…\n#> $ V11   <dbl> 0.1609, 0.4918, 0.6333, 0.0881, 0.4152, 0.2988, 0.1786, 0.2802,…\n#> $ V12   <dbl> 0.1582, 0.6552, 0.7060, 0.1992, 0.3952, 0.4250, 0.0658, 0.3086,…\n#> $ V13   <dbl> 0.2238, 0.6919, 0.5544, 0.0184, 0.4256, 0.6343, 0.0513, 0.2657,…\n#> $ V14   <dbl> 0.0645, 0.7797, 0.5320, 0.2261, 0.4135, 0.8198, 0.3752, 0.3801,…\n#> $ V15   <dbl> 0.0660, 0.7464, 0.6479, 0.1729, 0.4528, 1.0000, 0.5419, 0.5626,…\n#> $ V16   <dbl> 0.2273, 0.9444, 0.6931, 0.2131, 0.5326, 0.9988, 0.5440, 0.4376,…\n#> $ V17   <dbl> 0.3100, 1.0000, 0.6759, 0.0693, 0.7306, 0.9508, 0.5150, 0.2617,…\n#> $ V18   <dbl> 0.300, 0.887, 0.755, 0.228, 0.619, 0.902, 0.426, 0.120, 0.380, …\n#> $ V19   <dbl> 0.508, 0.802, 0.893, 0.406, 0.203, 0.723, 0.202, 0.668, 0.740, …\n#> $ V20   <dbl> 0.4797, 0.7818, 0.8619, 0.3973, 0.4636, 0.5122, 0.4233, 0.9402,…\n#> $ V21   <dbl> 0.578, 0.521, 0.797, 0.274, 0.415, 0.207, 0.772, 0.783, 0.980, …\n#> $ V22   <dbl> 0.507, 0.405, 0.674, 0.369, 0.429, 0.399, 0.974, 0.535, 0.889, …\n#> $ V23   <dbl> 0.433, 0.396, 0.429, 0.556, 0.573, 0.589, 0.939, 0.681, 0.671, …\n#> $ V24   <dbl> 0.555, 0.391, 0.365, 0.485, 0.540, 0.287, 0.556, 0.917, 0.429, …\n#> $ V25   <dbl> 0.671, 0.325, 0.533, 0.314, 0.316, 0.204, 0.527, 0.761, 0.337, …\n#> $ V26   <dbl> 0.641, 0.320, 0.241, 0.533, 0.229, 0.578, 0.683, 0.822, 0.737, …\n#> $ V27   <dbl> 0.7104, 0.3271, 0.5070, 0.5256, 0.6995, 0.5389, 0.5713, 0.8872,…\n#> $ V28   <dbl> 0.8080, 0.2767, 0.8533, 0.2520, 1.0000, 0.3750, 0.5429, 0.6091,…\n#> $ V29   <dbl> 0.6791, 0.4423, 0.6036, 0.2090, 0.7262, 0.3411, 0.2177, 0.2967,…\n#> $ V30   <dbl> 0.3857, 0.2028, 0.8514, 0.3559, 0.4724, 0.5067, 0.2149, 0.1103,…\n#> $ V31   <dbl> 0.131, 0.379, 0.851, 0.626, 0.510, 0.558, 0.581, 0.132, 0.301, …\n#> $ V32   <dbl> 0.2604, 0.2947, 0.5045, 0.7340, 0.5459, 0.4778, 0.6323, 0.0624,…\n#> $ V33   <dbl> 0.512, 0.198, 0.186, 0.612, 0.288, 0.330, 0.296, 0.099, 0.317, …\n#> $ V34   <dbl> 0.7547, 0.2341, 0.2709, 0.3497, 0.0981, 0.2198, 0.1873, 0.4006,…\n#> $ V35   <dbl> 0.8537, 0.1306, 0.4232, 0.3953, 0.1951, 0.1407, 0.2969, 0.3666,…\n#> $ V36   <dbl> 0.851, 0.418, 0.304, 0.301, 0.418, 0.286, 0.516, 0.105, 0.219, …\n#> $ V37   <dbl> 0.669, 0.384, 0.612, 0.541, 0.460, 0.381, 0.615, 0.192, 0.246, …\n#> $ V38   <dbl> 0.6097, 0.1057, 0.6756, 0.8814, 0.3217, 0.4158, 0.4283, 0.3930,…\n#> $ V39   <dbl> 0.4943, 0.1840, 0.5375, 0.9857, 0.2828, 0.4054, 0.5479, 0.4288,…\n#> $ V40   <dbl> 0.2744, 0.1970, 0.4719, 0.9167, 0.2430, 0.3296, 0.6133, 0.2546,…\n#> $ V41   <dbl> 0.0510, 0.1674, 0.4647, 0.6121, 0.1979, 0.2707, 0.5017, 0.1151,…\n#> $ V42   <dbl> 0.2834, 0.0583, 0.2587, 0.5006, 0.2444, 0.2650, 0.2377, 0.2196,…\n#> $ V43   <dbl> 0.2825, 0.1401, 0.2129, 0.3210, 0.1847, 0.0723, 0.1957, 0.1879,…\n#> $ V44   <dbl> 0.4256, 0.1628, 0.2222, 0.3202, 0.0841, 0.1238, 0.1749, 0.1437,…\n#> $ V45   <dbl> 0.2641, 0.0621, 0.2111, 0.4295, 0.0692, 0.1192, 0.1304, 0.2146,…\n#> $ V46   <dbl> 0.1386, 0.0203, 0.0176, 0.3654, 0.0528, 0.1089, 0.0597, 0.2360,…\n#> $ V47   <dbl> 0.1051, 0.0530, 0.1348, 0.2655, 0.0357, 0.0623, 0.1124, 0.1125,…\n#> $ V48   <dbl> 0.1343, 0.0742, 0.0744, 0.1576, 0.0085, 0.0494, 0.1047, 0.0254,…\n#> $ V49   <dbl> 0.0383, 0.0409, 0.0130, 0.0681, 0.0230, 0.0264, 0.0507, 0.0285,…\n#> $ V50   <dbl> 0.0324, 0.0061, 0.0106, 0.0294, 0.0046, 0.0081, 0.0159, 0.0178,…\n#> $ V51   <dbl> 0.0232, 0.0125, 0.0033, 0.0241, 0.0156, 0.0104, 0.0195, 0.0052,…\n#> $ V52   <dbl> 0.0027, 0.0084, 0.0232, 0.0121, 0.0031, 0.0045, 0.0201, 0.0081,…\n#> $ V53   <dbl> 0.0065, 0.0089, 0.0166, 0.0036, 0.0054, 0.0014, 0.0248, 0.0120,…\n#> $ V54   <dbl> 0.0159, 0.0048, 0.0095, 0.0150, 0.0105, 0.0038, 0.0131, 0.0045,…\n#> $ V55   <dbl> 0.0072, 0.0094, 0.0180, 0.0085, 0.0110, 0.0013, 0.0070, 0.0121,…\n#> $ V56   <dbl> 0.0167, 0.0191, 0.0244, 0.0073, 0.0015, 0.0089, 0.0138, 0.0097,…\n#> $ V57   <dbl> 0.0180, 0.0140, 0.0316, 0.0050, 0.0072, 0.0057, 0.0092, 0.0085,…\n#> $ V58   <dbl> 0.0084, 0.0049, 0.0164, 0.0044, 0.0048, 0.0027, 0.0143, 0.0047,…\n#> $ V59   <dbl> 0.0090, 0.0052, 0.0095, 0.0040, 0.0107, 0.0051, 0.0036, 0.0048,…\n#> $ V60   <dbl> 0.0032, 0.0044, 0.0078, 0.0117, 0.0094, 0.0062, 0.0103, 0.0053,…\n#> $ Class <fct> R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, R, …\ntibble::as_tibble(Sonar)\n#> # A tibble: 208 x 61\n#>       V1     V2     V3     V4     V5     V6    V7    V8     V9   V10    V11\n#>    <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl> <dbl> <dbl>  <dbl> <dbl>  <dbl>\n#> 1 0.02   0.0371 0.0428 0.0207 0.0954 0.0986 0.154 0.160 0.311  0.211 0.161 \n#> 2 0.0453 0.0523 0.0843 0.0689 0.118  0.258  0.216 0.348 0.334  0.287 0.492 \n#> 3 0.0262 0.0582 0.110  0.108  0.0974 0.228  0.243 0.377 0.560  0.619 0.633 \n#> 4 0.01   0.0171 0.0623 0.0205 0.0205 0.0368 0.110 0.128 0.0598 0.126 0.0881\n#> 5 0.0762 0.0666 0.0481 0.0394 0.059  0.0649 0.121 0.247 0.356  0.446 0.415 \n#> 6 0.0286 0.0453 0.0277 0.0174 0.0384 0.099  0.120 0.183 0.210  0.304 0.299 \n#> # … with 202 more rows, and 50 more variables: V12 <dbl>, V13 <dbl>, V14 <dbl>,\n#> #   V15 <dbl>, V16 <dbl>, V17 <dbl>, V18 <dbl>, V19 <dbl>, V20 <dbl>,\n#> #   V21 <dbl>, V22 <dbl>, V23 <dbl>, V24 <dbl>, V25 <dbl>, V26 <dbl>,\n#> #   V27 <dbl>, V28 <dbl>, V29 <dbl>, V30 <dbl>, V31 <dbl>, V32 <dbl>,\n#> #   V33 <dbl>, V34 <dbl>, V35 <dbl>, V36 <dbl>, V37 <dbl>, V38 <dbl>,\n#> #   V39 <dbl>, V40 <dbl>, V41 <dbl>, V42 <dbl>, V43 <dbl>, V44 <dbl>,\n#> #   V45 <dbl>, V46 <dbl>, V47 <dbl>, V48 <dbl>, V49 <dbl>, V50 <dbl>,\n#> #   V51 <dbl>, V52 <dbl>, V53 <dbl>, V54 <dbl>, V55 <dbl>, V56 <dbl>,\n#> #   V57 <dbl>, V58 <dbl>, V59 <dbl>, V60 <dbl>, Class <fct>\n# create 80%/20% for training and validation datasets\nvalidationIndex <- createDataPartition(Sonar$Class, p=0.80, list=FALSE)\nvalidation <- Sonar[-validationIndex,]\ntraining   <- Sonar[validationIndex,]\ntic()\n# train a model and summarize model\nset.seed(7)\ntrainControl <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\nfit.rf <- train(Class~., data=training, \n                method = \"rf\", \n                metric = \"Accuracy\", \n                trControl = trainControl, \n                ntree = 2000)\ntoc()\n#> 67.56 sec elapsed\nprint(fit.rf)\n#> Random Forest \n#> \n#> 167 samples\n#>  60 predictor\n#>   2 classes: 'M', 'R' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold, repeated 3 times) \n#> Summary of sample sizes: 150, 150, 150, 151, 151, 150, ... \n#> Resampling results across tuning parameters:\n#> \n#>   mtry  Accuracy  Kappa\n#>    2    0.845     0.682\n#>   31    0.828     0.651\n#>   60    0.808     0.611\n#> \n#> Accuracy was used to select the optimal model using the largest value.\n#> The final value used for the model was mtry = 2.\nprint(fit.rf$finalModel)\n#> \n#> Call:\n#>  randomForest(x = x, y = y, ntree = 2000, mtry = param$mtry) \n#>                Type of random forest: classification\n#>                      Number of trees: 2000\n#> No. of variables tried at each split: 2\n#> \n#>         OOB estimate of  error rate: 14.4%\n#> Confusion matrix:\n#>    M  R class.error\n#> M 84  5      0.0562\n#> R 19 59      0.2436"},{"path":"detect-mines-vs-rocks-with-random-forest.html","id":"apply-tuning-parameters-for-final-model","chapter":"22 Detect mines vs rocks with Random Forest","heading":"22.4 Apply tuning parameters for final model","text":"Accuracy: 82.93%","code":"\n# create standalone model using all training data\nset.seed(7)\nfinalModel <- randomForest(Class~., training, mtry=2, ntree=2000)\n\n# make a predictions on \"new data\" using the final model\nfinalPredictions <- predict(finalModel, validation[,1:60])\nconfusionMatrix(finalPredictions, validation$Class)\n#> Confusion Matrix and Statistics\n#> \n#>           Reference\n#> Prediction  M  R\n#>          M 20  4\n#>          R  2 15\n#>                                         \n#>                Accuracy : 0.854         \n#>                  95% CI : (0.708, 0.944)\n#>     No Information Rate : 0.537         \n#>     P-Value [Acc > NIR] : 1.88e-05      \n#>                                         \n#>                   Kappa : 0.704         \n#>                                         \n#>  Mcnemar's Test P-Value : 0.683         \n#>                                         \n#>             Sensitivity : 0.909         \n#>             Specificity : 0.789         \n#>          Pos Pred Value : 0.833         \n#>          Neg Pred Value : 0.882         \n#>              Prevalence : 0.537         \n#>          Detection Rate : 0.488         \n#>    Detection Prevalence : 0.585         \n#>       Balanced Accuracy : 0.849         \n#>                                         \n#>        'Positive' Class : M             \n#> "},{"path":"detect-mines-vs-rocks-with-random-forest.html","id":"save-model","chapter":"22 Detect mines vs rocks with Random Forest","heading":"22.5 Save model","text":"","code":"\n# save the model to disk\nsaveRDS(finalModel, file.path(model_out_dir, \"sonar-finalModel.rds\"))"},{"path":"detect-mines-vs-rocks-with-random-forest.html","id":"use-the-saved-model","chapter":"22 Detect mines vs rocks with Random Forest","heading":"22.6 Use the saved model","text":"","code":"\n# load the model\nsuperModel <- readRDS(file.path(model_out_dir, \"sonar-finalModel.rds\"))\nprint(superModel)\n#> \n#> Call:\n#>  randomForest(formula = Class ~ ., data = training, mtry = 2,      ntree = 2000) \n#>                Type of random forest: classification\n#>                      Number of trees: 2000\n#> No. of variables tried at each split: 2\n#> \n#>         OOB estimate of  error rate: 16.2%\n#> Confusion matrix:\n#>    M  R class.error\n#> M 81  8      0.0899\n#> R 19 59      0.2436"},{"path":"detect-mines-vs-rocks-with-random-forest.html","id":"make-prediction-with-new-data","chapter":"22 Detect mines vs rocks with Random Forest","heading":"22.7 Make prediction with new data","text":"","code":"\n# make a predictions on \"new data\" using the final model\nfinalPredictions <- predict(superModel, validation[,1:60])\nconfusionMatrix(finalPredictions, validation$Class)\n#> Confusion Matrix and Statistics\n#> \n#>           Reference\n#> Prediction  M  R\n#>          M 20  4\n#>          R  2 15\n#>                                         \n#>                Accuracy : 0.854         \n#>                  95% CI : (0.708, 0.944)\n#>     No Information Rate : 0.537         \n#>     P-Value [Acc > NIR] : 1.88e-05      \n#>                                         \n#>                   Kappa : 0.704         \n#>                                         \n#>  Mcnemar's Test P-Value : 0.683         \n#>                                         \n#>             Sensitivity : 0.909         \n#>             Specificity : 0.789         \n#>          Pos Pred Value : 0.833         \n#>          Neg Pred Value : 0.882         \n#>              Prevalence : 0.537         \n#>          Detection Rate : 0.488         \n#>    Detection Prevalence : 0.585         \n#>       Balanced Accuracy : 0.849         \n#>                                         \n#>        'Positive' Class : M             \n#> "},{"path":"predicting-the-type-of-glass.html","id":"predicting-the-type-of-glass","chapter":"23 Predicting the type of glass","heading":"23 Predicting the type of glass","text":"Datasets: GlassAlgorithms:\nSVM\nPartitioning Tree\nSVMPartitioning Treehttps://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdfIn example, use glass data UCI Repository Machine\nLearning Databases classification. task predict type glass\nbasis chemical analysis. start splitting data train \ntest set:SVM partitioning tree (via rpart()), fit model \ntry predict test set values:(dependent variable, Type, column number 10. cost general penalizing\nparameter C-classification gamma radial basis function-specific\nkernel parameter.)cross-tabulation true versus predicted values yields:","code":"\nlibrary(caret)\n#> Loading required package: lattice\n#> Loading required package: ggplot2\nlibrary(e1071)\nlibrary(rpart)\n\ndata(Glass, package=\"mlbench\")\nstr(Glass)\n#> 'data.frame':    214 obs. of  10 variables:\n#>  $ RI  : num  1.52 1.52 1.52 1.52 1.52 ...\n#>  $ Na  : num  13.6 13.9 13.5 13.2 13.3 ...\n#>  $ Mg  : num  4.49 3.6 3.55 3.69 3.62 3.61 3.6 3.61 3.58 3.6 ...\n#>  $ Al  : num  1.1 1.36 1.54 1.29 1.24 1.62 1.14 1.05 1.37 1.36 ...\n#>  $ Si  : num  71.8 72.7 73 72.6 73.1 ...\n#>  $ K   : num  0.06 0.48 0.39 0.57 0.55 0.64 0.58 0.57 0.56 0.57 ...\n#>  $ Ca  : num  8.75 7.83 7.78 8.22 8.07 8.07 8.17 8.24 8.3 8.4 ...\n#>  $ Ba  : num  0 0 0 0 0 0 0 0 0 0 ...\n#>  $ Fe  : num  0 0 0 0 0 0.26 0 0 0 0.11 ...\n#>  $ Type: Factor w/ 6 levels \"1\",\"2\",\"3\",\"5\",..: 1 1 1 1 1 1 1 1 1 1 ...\n## split data into a train and test set\nindex <- 1:nrow(Glass)\ntestindex <- sample(index, trunc(length(index)/3))\ntestset  <- Glass[testindex,]\ntrainset <- Glass[-testindex,]\n## svm\nsvm.model <- svm(Type ~ ., data = trainset, cost = 100, gamma = 1)\nsvm.pred  <- predict(svm.model, testset[,-10])\n## rpart\nrpart.model <- rpart(Type ~ ., data = trainset)\nrpart.pred <- predict(rpart.model, testset[,-10], type = \"class\")\n## compute svm confusion matrix\ntable(pred = svm.pred, true = testset[,10])\n#>     true\n#> pred  1  2  3  5  6  7\n#>    1 20  3  3  0  0  0\n#>    2  6 13  5  4  2  4\n#>    3  2  1  0  0  0  0\n#>    5  0  0  0  1  0  0\n#>    6  0  0  0  0  0  0\n#>    7  0  0  0  0  0  7\n## compute rpart confusion matrix\ntable(pred = rpart.pred, true = testset[,10])\n#>     true\n#> pred  1  2  3  5  6  7\n#>    1 22  0  3  0  0  0\n#>    2  5 12  4  0  0  0\n#>    3  0  2  1  0  0  0\n#>    5  0  2  0  5  2  1\n#>    6  0  0  0  0  0  0\n#>    7  1  1  0  0  0 10"},{"path":"predicting-the-type-of-glass.html","id":"comparison-test-sets","chapter":"23 Predicting the type of glass","heading":"23.0.1 Comparison test sets","text":"","code":"\nconfusionMatrix(svm.pred, testset$Type)\n#> Confusion Matrix and Statistics\n#> \n#>           Reference\n#> Prediction  1  2  3  5  6  7\n#>          1 20  3  3  0  0  0\n#>          2  6 13  5  4  2  4\n#>          3  2  1  0  0  0  0\n#>          5  0  0  0  1  0  0\n#>          6  0  0  0  0  0  0\n#>          7  0  0  0  0  0  7\n#> \n#> Overall Statistics\n#>                                         \n#>                Accuracy : 0.577         \n#>                  95% CI : (0.454, 0.694)\n#>     No Information Rate : 0.394         \n#>     P-Value [Acc > NIR] : 0.00137       \n#>                                         \n#>                   Kappa : 0.413         \n#>                                         \n#>  Mcnemar's Test P-Value : NA            \n#> \n#> Statistics by Class:\n#> \n#>                      Class: 1 Class: 2 Class: 3 Class: 5 Class: 6 Class: 7\n#> Sensitivity             0.714    0.765   0.0000   0.2000   0.0000   0.6364\n#> Specificity             0.860    0.611   0.9524   1.0000   1.0000   1.0000\n#> Pos Pred Value          0.769    0.382   0.0000   1.0000      NaN   1.0000\n#> Neg Pred Value          0.822    0.892   0.8824   0.9429   0.9718   0.9375\n#> Prevalence              0.394    0.239   0.1127   0.0704   0.0282   0.1549\n#> Detection Rate          0.282    0.183   0.0000   0.0141   0.0000   0.0986\n#> Detection Prevalence    0.366    0.479   0.0423   0.0141   0.0000   0.0986\n#> Balanced Accuracy       0.787    0.688   0.4762   0.6000   0.5000   0.8182\nconfusionMatrix(rpart.pred, testset$Type)\n#> Confusion Matrix and Statistics\n#> \n#>           Reference\n#> Prediction  1  2  3  5  6  7\n#>          1 22  0  3  0  0  0\n#>          2  5 12  4  0  0  0\n#>          3  0  2  1  0  0  0\n#>          5  0  2  0  5  2  1\n#>          6  0  0  0  0  0  0\n#>          7  1  1  0  0  0 10\n#> \n#> Overall Statistics\n#>                                         \n#>                Accuracy : 0.704         \n#>                  95% CI : (0.584, 0.807)\n#>     No Information Rate : 0.394         \n#>     P-Value [Acc > NIR] : 1.23e-07      \n#>                                         \n#>                   Kappa : 0.605         \n#>                                         \n#>  Mcnemar's Test P-Value : NA            \n#> \n#> Statistics by Class:\n#> \n#>                      Class: 1 Class: 2 Class: 3 Class: 5 Class: 6 Class: 7\n#> Sensitivity             0.786    0.706   0.1250   1.0000   0.0000    0.909\n#> Specificity             0.930    0.833   0.9683   0.9242   1.0000    0.967\n#> Pos Pred Value          0.880    0.571   0.3333   0.5000      NaN    0.833\n#> Neg Pred Value          0.870    0.900   0.8971   1.0000   0.9718    0.983\n#> Prevalence              0.394    0.239   0.1127   0.0704   0.0282    0.155\n#> Detection Rate          0.310    0.169   0.0141   0.0704   0.0000    0.141\n#> Detection Prevalence    0.352    0.296   0.0423   0.1408   0.0000    0.169\n#> Balanced Accuracy       0.858    0.770   0.5466   0.9621   0.5000    0.938"},{"path":"predicting-the-type-of-glass.html","id":"comparison-with-resamples","chapter":"23 Predicting the type of glass","heading":"23.0.2 Comparison with resamples","text":"Finally, compare performance two methods computing \nrespective accuracy rates kappa indices (computed classAgreement()\nalso contained package e1071). Table 1, summarize results\n10 replications—Support Vector Machines show better results.","code":"\nset.seed(1234567)\n\n# SVM\nfit.svm <- train(Type ~., data = trainset, \n                 method = \"svmRadial\")\n\n# Random Forest\nfit.rpart <- train(Type ~., data = trainset, \n                method=\"rpart\")\n\n# collect resamples\nresults <- resamples(list(svm = fit.svm, \n                          rpart  = fit.rpart))\n\nsummary(results)\n#> \n#> Call:\n#> summary.resamples(object = results)\n#> \n#> Models: svm, rpart \n#> Number of resamples: 25 \n#> \n#> Accuracy \n#>        Min. 1st Qu. Median  Mean 3rd Qu.  Max. NA's\n#> svm   0.510   0.565  0.600 0.599   0.625 0.704    0\n#> rpart 0.462   0.519  0.554 0.558   0.600 0.660    0\n#> \n#> Kappa \n#>        Min. 1st Qu. Median  Mean 3rd Qu.  Max. NA's\n#> svm   0.267   0.376  0.406 0.410   0.446 0.559    0\n#> rpart 0.135   0.299  0.358 0.363   0.443 0.545    0"},{"path":"naive-bayes-for-sms-spam.html","id":"naive-bayes-for-sms-spam","chapter":"24 Naive Bayes for SMS spam","heading":"24 Naive Bayes for SMS spam","text":"Datasets: sms_spam.csvAlgorithms:\nNaive Bayes\nNaive BayesDataset: https://github.com/stedy/Machine-Learning--R-datasets/blob/master/sms_spam.csvInstructions: Machine Learning R. Page 104.","code":"\nlibrary(tictoc)\nsms_raw <- read.csv(file.path(data_raw_dir, \"sms_spam.csv\"), stringsAsFactors = FALSE)\nstr(sms_raw)\n#> 'data.frame':    5574 obs. of  2 variables:\n#>  $ type: chr  \"ham\" \"ham\" \"spam\" \"ham\" ...\n#>  $ text: chr  \"Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\" \"Ok lar... Joking wif u oni...\" \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(\"| __truncated__ \"U dun say so early hor... U c already then say...\" ..."},{"path":"naive-bayes-for-sms-spam.html","id":"convert-type-to-a-factor","chapter":"24 Naive Bayes for SMS spam","heading":"24.0.1 convert type to a factor","text":"many email type ham spam:Create corpus:Let’s see couple documents:","code":"\nsms_raw$type <- factor(sms_raw$type)\nstr(sms_raw$type)\n#>  Factor w/ 2 levels \"ham\",\"spam\": 1 1 2 1 1 2 1 1 2 2 ...\ntable(sms_raw$type)\n#> \n#>  ham spam \n#> 4827  747\nlibrary(tm)\n#> Loading required package: NLP\n\nsms_corpus <- VCorpus(VectorSource(sms_raw$text))\nprint(sms_corpus)\n#> <<VCorpus>>\n#> Metadata:  corpus specific: 0, document level (indexed): 0\n#> Content:  documents: 5574\ninspect(sms_corpus[1:2])\n#> <<VCorpus>>\n#> Metadata:  corpus specific: 0, document level (indexed): 0\n#> Content:  documents: 2\n#> \n#> [[1]]\n#> <<PlainTextDocument>>\n#> Metadata:  7\n#> Content:  chars: 111\n#> \n#> [[2]]\n#> <<PlainTextDocument>>\n#> Metadata:  7\n#> Content:  chars: 29\n# show some text\nas.character(sms_corpus[[1]])\n#> [1] \"Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\"\n# show three documents\nlapply(sms_corpus[1:3], as.character)\n#> $`1`\n#> [1] \"Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\"\n#> \n#> $`2`\n#> [1] \"Ok lar... Joking wif u oni...\"\n#> \n#> $`3`\n#> [1] \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\""},{"path":"naive-bayes-for-sms-spam.html","id":"some-conversion","chapter":"24 Naive Bayes for SMS spam","heading":"24.1 Some conversion","text":"transformations availableStemming:Show ’ve got far","code":"\n# convert to lowercase\nsms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))\nas.character(sms_corpus[[1]])\n#> [1] \"Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\"\n# converted to lowercase\nas.character(sms_corpus_clean[[1]])\n#> [1] \"go until jurong point, crazy.. available only in bugis n great world la e buffet... cine there got amore wat...\"\n# remove numbers\nsms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)\n# what transformations are available\ngetTransformations()\n#> [1] \"removeNumbers\"     \"removePunctuation\" \"removeWords\"      \n#> [4] \"stemDocument\"      \"stripWhitespace\"\n# remove stop words\nsms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords())\n# remove punctuation\nsms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation)\nlibrary(SnowballC)\nwordStem(c(\"learn\", \"learned\", \"learning\", \"learns\"))\n#> [1] \"learn\" \"learn\" \"learn\" \"learn\"\n# stemming corpus\nsms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)\n# remove white spaces\nsms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)\n# show what we've got so far\nlapply(sms_corpus[1:3], as.character)\n#> $`1`\n#> [1] \"Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\"\n#> \n#> $`2`\n#> [1] \"Ok lar... Joking wif u oni...\"\n#> \n#> $`3`\n#> [1] \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"\n\nlapply(sms_corpus_clean[1:3], as.character)\n#> $`1`\n#> [1] \"go jurong point crazi avail bugi n great world la e buffet cine got amor wat\"\n#> \n#> $`2`\n#> [1] \"ok lar joke wif u oni\"\n#> \n#> $`3`\n#> [1] \"free entri wkli comp win fa cup final tkts st may text fa receiv entri questionstd txt ratetc appli s\""},{"path":"naive-bayes-for-sms-spam.html","id":"convert-to-document-term-matrix-dtm","chapter":"24 Naive Bayes for SMS spam","heading":"24.2 Convert to Document Term Matrix (dtm","text":")","code":"\nsms_dtm <- DocumentTermMatrix(sms_corpus_clean)\nsms_dtm\n#> <<DocumentTermMatrix (documents: 5574, terms: 6592)>>\n#> Non-/sparse entries: 42608/36701200\n#> Sparsity           : 100%\n#> Maximal term length: 40\n#> Weighting          : term frequency (tf)"},{"path":"naive-bayes-for-sms-spam.html","id":"split-in-training-and-test-datasets","chapter":"24 Naive Bayes for SMS spam","heading":"24.3 split in training and test datasets","text":"","code":"\nsms_dtm_train <- sms_dtm[1:4169, ]\nsms_dtm_test  <- sms_dtm[4170:5559, ]"},{"path":"naive-bayes-for-sms-spam.html","id":"separate-the-labels","chapter":"24 Naive Bayes for SMS spam","heading":"24.3.1 separate the labels","text":"","code":"\nsms_train_labels <- sms_raw[1:4169, ]$type\nsms_test_labels  <- sms_raw[4170:5559, ]$type\nprop.table(table(sms_train_labels))\n#> sms_train_labels\n#>   ham  spam \n#> 0.865 0.135\nprop.table(table(sms_test_labels))\n#> sms_test_labels\n#>  ham spam \n#> 0.87 0.13\n# convert dtm to matrix\nsms_mat_train <- as.matrix(t(sms_dtm_train))\ndtm.rs <- sort(rowSums(sms_mat_train), decreasing=TRUE)\n\n# dataframe with word-frequency\ndtm.df <- data.frame(word = names(dtm.rs), freq = as.integer(dtm.rs),\n                     stringsAsFactors = FALSE)"},{"path":"naive-bayes-for-sms-spam.html","id":"plot-wordcloud","chapter":"24 Naive Bayes for SMS spam","heading":"24.4 plot wordcloud","text":"Words related spamWords related ham","code":"\nlibrary(wordcloud)\n#> Loading required package: RColorBrewer\nwordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)\nspam <- subset(sms_raw, type == \"spam\")\nham  <- subset(sms_raw, type == \"ham\")\nwordcloud(spam$text, max.words = 40, scale = c(3, 0.5))\n#> Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation): transformation\n#> drops documents\n#> Warning in tm_map.SimpleCorpus(corpus, function(x) tm::removeWords(x,\n#> tm::stopwords())): transformation drops documents\nwordcloud(ham$text, max.words = 40, scale = c(3, 0.5))\n#> Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation): transformation\n#> drops documents\n#> Warning in tm_map.SimpleCorpus(corpus, function(x) tm::removeWords(x,\n#> tm::stopwords())): transformation drops documents"},{"path":"naive-bayes-for-sms-spam.html","id":"limit-frequent-words","chapter":"24 Naive Bayes for SMS spam","heading":"24.5 Limit Frequent words","text":"","code":"\n# words that appear at least in 5 messages\nsms_freq_words <- findFreqTerms(sms_dtm_train, 6)\nstr(sms_freq_words)\n#>  chr [1:997] \"abiola\" \"abl\" \"abt\" \"accept\" \"access\" \"account\" \"across\" ..."},{"path":"naive-bayes-for-sms-spam.html","id":"get-only-frequent-words","chapter":"24 Naive Bayes for SMS spam","heading":"24.5.1 get only frequent words","text":"","code":"\nsms_dtm_freq_train<- sms_dtm_train[ , sms_freq_words]\nsms_dtm_freq_test <- sms_dtm_test[ , sms_freq_words]"},{"path":"naive-bayes-for-sms-spam.html","id":"function-to-change-value-to-yesno","chapter":"24 Naive Bayes for SMS spam","heading":"24.5.2 function to change value to Yes/No","text":"Misclassified:\n20+9 (frequency = 5)\n25+7 (freq=4)\n23+7 (freq=3)\n25+8 (freq=2)\n21+7 (freq=6)Decreasing minimum word frequency doesn’t make model better.","code":"\nconvert_counts <- function(x) {\n    x <- ifelse(x > 0, \"Yes\", \"No\")\n  }\n# change from number to Yes/No\n# also the result returns a matrix\nsms_train <- apply(sms_dtm_freq_train, MARGIN = 2,\n                                       convert_counts)\nsms_test  <- apply(sms_dtm_freq_test, MARGIN = 2,\n                                      convert_counts)\n# matrix of\n# 4169 documents as rows\n# 1159 terms as columns\ndim(sms_train)\n#> [1] 4169  997\nlength(sms_train_labels)\n#> [1] 4169\n# this is how the matrix looks\nsms_train[1:10, 10:15]\n#>     Terms\n#> Docs add  address admir advanc aft  afternoon\n#>   1  \"No\" \"No\"    \"No\"  \"No\"   \"No\" \"No\"     \n#>   2  \"No\" \"No\"    \"No\"  \"No\"   \"No\" \"No\"     \n#>   3  \"No\" \"No\"    \"No\"  \"No\"   \"No\" \"No\"     \n#>   4  \"No\" \"No\"    \"No\"  \"No\"   \"No\" \"No\"     \n#>   5  \"No\" \"No\"    \"No\"  \"No\"   \"No\" \"No\"     \n#>   6  \"No\" \"No\"    \"No\"  \"No\"   \"No\" \"No\"     \n#>   7  \"No\" \"No\"    \"No\"  \"No\"   \"No\" \"No\"     \n#>   8  \"No\" \"No\"    \"No\"  \"No\"   \"No\" \"No\"     \n#>   9  \"No\" \"No\"    \"No\"  \"No\"   \"No\" \"No\"     \n#>   10 \"No\" \"No\"    \"No\"  \"No\"   \"No\" \"No\"\nlibrary(e1071)\nsms_classifier <- naiveBayes(sms_train, sms_train_labels)\ntic()\nsms_test_pred <- predict(sms_classifier, sms_test)\ntoc()\n#> 20.184 sec elapsed\nlibrary(gmodels)\nCrossTable(sms_test_pred, sms_test_labels,\n    prop.chisq = FALSE, prop.t = FALSE,\n    dnn = c('predicted', 'actual'))\n#> \n#>  \n#>    Cell Contents\n#> |-------------------------|\n#> |                       N |\n#> |           N / Row Total |\n#> |           N / Col Total |\n#> |-------------------------|\n#> \n#>  \n#> Total Observations in Table:  1390 \n#> \n#>  \n#>              | actual \n#>    predicted |       ham |      spam | Row Total | \n#> -------------|-----------|-----------|-----------|\n#>          ham |      1202 |        21 |      1223 | \n#>              |     0.983 |     0.017 |     0.880 | \n#>              |     0.994 |     0.116 |           | \n#> -------------|-----------|-----------|-----------|\n#>         spam |         7 |       160 |       167 | \n#>              |     0.042 |     0.958 |     0.120 | \n#>              |     0.006 |     0.884 |           | \n#> -------------|-----------|-----------|-----------|\n#> Column Total |      1209 |       181 |      1390 | \n#>              |     0.870 |     0.130 |           | \n#> -------------|-----------|-----------|-----------|\n#> \n#> "},{"path":"naive-bayes-for-sms-spam.html","id":"improve-model-performance","chapter":"24 Naive Bayes for SMS spam","heading":"24.6 Improve model performance","text":"Misclassified: 28+7","code":"\nsms_classifier2 <- naiveBayes(sms_train, sms_train_labels, \n                              laplace = 1)\ntic()\nsms_test_pred2 <- predict(sms_classifier2, sms_test)\ntoc()\n#> 20.305 sec elapsed\nCrossTable(sms_test_pred2, sms_test_labels,\n    prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,\n    dnn = c('predicted', 'actual'))\n#> \n#>  \n#>    Cell Contents\n#> |-------------------------|\n#> |                       N |\n#> |           N / Col Total |\n#> |-------------------------|\n#> \n#>  \n#> Total Observations in Table:  1390 \n#> \n#>  \n#>              | actual \n#>    predicted |       ham |      spam | Row Total | \n#> -------------|-----------|-----------|-----------|\n#>          ham |      1203 |        28 |      1231 | \n#>              |     0.995 |     0.155 |           | \n#> -------------|-----------|-----------|-----------|\n#>         spam |         6 |       153 |       159 | \n#>              |     0.005 |     0.845 |           | \n#> -------------|-----------|-----------|-----------|\n#> Column Total |      1209 |       181 |      1390 | \n#>              |     0.870 |     0.130 |           | \n#> -------------|-----------|-----------|-----------|\n#> \n#> "},{"path":"vehicles-classiification-with-decision-trees.html","id":"vehicles-classiification-with-decision-trees","chapter":"25 Vehicles classiification with Decision Trees","heading":"25 Vehicles classiification with Decision Trees","text":"Datasets: Vehicle (mlbench)Algorithms:\nDecision Trees\nDecision TreesInstructions: book “Applied Predictive Modeling Techniques”, Lewis, N.D.","code":""},{"path":"vehicles-classiification-with-decision-trees.html","id":"load-packages-1","chapter":"25 Vehicles classiification with Decision Trees","heading":"25.1 Load packages","text":"","code":"\nlibrary(tree)\nlibrary(mlbench)\n\ndata(Vehicle)\nstr(Vehicle)\n#> 'data.frame':    846 obs. of  19 variables:\n#>  $ Comp        : num  95 91 104 93 85 107 97 90 86 93 ...\n#>  $ Circ        : num  48 41 50 41 44 57 43 43 34 44 ...\n#>  $ D.Circ      : num  83 84 106 82 70 106 73 66 62 98 ...\n#>  $ Rad.Ra      : num  178 141 209 159 205 172 173 157 140 197 ...\n#>  $ Pr.Axis.Ra  : num  72 57 66 63 103 50 65 65 61 62 ...\n#>  $ Max.L.Ra    : num  10 9 10 9 52 6 6 9 7 11 ...\n#>  $ Scat.Ra     : num  162 149 207 144 149 255 153 137 122 183 ...\n#>  $ Elong       : num  42 45 32 46 45 26 42 48 54 36 ...\n#>  $ Pr.Axis.Rect: num  20 19 23 19 19 28 19 18 17 22 ...\n#>  $ Max.L.Rect  : num  159 143 158 143 144 169 143 146 127 146 ...\n#>  $ Sc.Var.Maxis: num  176 170 223 160 241 280 176 162 141 202 ...\n#>  $ Sc.Var.maxis: num  379 330 635 309 325 957 361 281 223 505 ...\n#>  $ Ra.Gyr      : num  184 158 220 127 188 264 172 164 112 152 ...\n#>  $ Skew.Maxis  : num  70 72 73 63 127 85 66 67 64 64 ...\n#>  $ Skew.maxis  : num  6 9 14 6 9 5 13 3 2 4 ...\n#>  $ Kurt.maxis  : num  16 14 9 10 11 9 1 3 14 14 ...\n#>  $ Kurt.Maxis  : num  187 189 188 199 180 181 200 193 200 195 ...\n#>  $ Holl.Ra     : num  197 199 196 207 183 183 204 202 208 204 ...\n#>  $ Class       : Factor w/ 4 levels \"bus\",\"opel\",\"saab\",..: 4 4 3 4 1 1 1 4 4 3 ...\nsummary(Vehicle[1])\n#>       Comp      \n#>  Min.   : 73.0  \n#>  1st Qu.: 87.0  \n#>  Median : 93.0  \n#>  Mean   : 93.7  \n#>  3rd Qu.:100.0  \n#>  Max.   :119.0\nsummary(Vehicle[2])\n#>       Circ     \n#>  Min.   :33.0  \n#>  1st Qu.:40.0  \n#>  Median :44.0  \n#>  Mean   :44.9  \n#>  3rd Qu.:49.0  \n#>  Max.   :59.0\nattributes(Vehicle$Class)\n#> $levels\n#> [1] \"bus\"  \"opel\" \"saab\" \"van\" \n#> \n#> $class\n#> [1] \"factor\""},{"path":"vehicles-classiification-with-decision-trees.html","id":"prepare-data","chapter":"25 Vehicles classiification with Decision Trees","heading":"25.2 Prepare data","text":"","code":"\nset.seed(107)\nN = nrow(Vehicle)\ntrain <- sample(1:N, 500, FALSE)\n# training and test sets\ntrainset <- Vehicle[train,]\ntestset  <- Vehicle[-train,]"},{"path":"vehicles-classiification-with-decision-trees.html","id":"estimate-the-decision-tree","chapter":"25 Vehicles classiification with Decision Trees","heading":"25.3 Estimate the decision tree","text":"use deviance splitting criteria, common alternative use\nsplit=“gini”.branch tree (root) see order:\n1. branch number (e.g. case 1,2,14 15);\n2. split (e.g. Elong < 41.5);\n3. number samples going along split (e.g. 229);\n4. deviance associated split (e.g. 489.1);\n5. predicted class (e.g. opel);\n6. associated probabilities (e.g. ( 0.222707 0.410480 0.366812 0.000000\n);\n7. terminal node (leaf), symbol \"*\".Notice summary(fit) shows:\n1. type tree, case Classification tree;\n2. formula used fit tree;\n3. variables used fit tree;\n4. number terminal nodes case 15;\n5. residual mean deviance - 0.9381;\n6. misclassification error rate 0.232 23.2%.","code":"\nfit <- tree(Class ~., data = trainset, split = \"deviance\")\nfit\n#> node), split, n, deviance, yval, (yprob)\n#>       * denotes terminal node\n#> \n#>   1) root 500 1000 opel ( 0 0 0 0 )  \n#>     2) Elong < 41.5 215  500 saab ( 0 0 0 0 )  \n#>       4) Max.L.Ra < 7.5 51   50 bus ( 1 0 0 0 )  \n#>         8) Comp < 93.5 12   20 bus ( 0 0 0 0 )  \n#>          16) Pr.Axis.Ra < 67.5 7    8 saab ( 0 0 1 0 ) *\n#>          17) Pr.Axis.Ra > 67.5 5    0 bus ( 1 0 0 0 ) *\n#>         9) Comp > 93.5 39    9 bus ( 1 0 0 0 ) *\n#>       5) Max.L.Ra > 7.5 164  200 opel ( 0 1 0 0 )  \n#>        10) Sc.Var.maxis < 723 149  200 saab ( 0 0 1 0 )  \n#>          20) Comp < 109.5 137  200 opel ( 0 1 0 0 ) *\n#>          21) Comp > 109.5 12    0 saab ( 0 0 1 0 ) *\n#>        11) Sc.Var.maxis > 723 15    7 opel ( 0 1 0 0 ) *\n#>     3) Elong > 41.5 285  700 van ( 0 0 0 0 )  \n#>       6) Sc.Var.maxis < 305.5 116  200 van ( 0 0 0 1 )  \n#>        12) Max.L.Rect < 128.5 40   90 saab ( 0 0 0 0 )  \n#>          24) Scat.Ra < 120.5 15   30 van ( 0 0 0 1 ) *\n#>          25) Scat.Ra > 120.5 25   30 saab ( 0 0 1 0 ) *\n#>        13) Max.L.Rect > 128.5 76   90 van ( 0 0 0 1 )  \n#>          26) Max.L.Rect < 138.5 38   60 van ( 0 0 0 1 )  \n#>            52) Circ < 37.5 17   10 van ( 0 0 0 1 ) *\n#>            53) Circ > 37.5 21   40 opel ( 0 0 0 0 ) *\n#>          27) Max.L.Rect > 138.5 38   20 van ( 0 0 0 1 ) *\n#>       7) Sc.Var.maxis > 305.5 169  400 bus ( 0 0 0 0 )  \n#>        14) Max.L.Ra < 8.5 116  200 bus ( 1 0 0 0 )  \n#>          28) D.Circ < 76.5 97  100 bus ( 1 0 0 0 )  \n#>            56) Skew.maxis < 10.5 87   70 bus ( 1 0 0 0 )  \n#>             112) Max.L.Rect < 134.5 12   20 bus ( 0 0 0 0 ) *\n#>             113) Max.L.Rect > 134.5 75   20 bus ( 1 0 0 0 ) *\n#>            57) Skew.maxis > 10.5 10   20 opel ( 0 0 0 0 ) *\n#>          29) D.Circ > 76.5 19   30 opel ( 0 1 0 0 ) *\n#>        15) Max.L.Ra > 8.5 53   20 van ( 0 0 0 1 ) *\n# fit <- tree(Class ~., data = Vehicle[train,], split =\"deviance\")\n# fit\nsummary(fit)\n#> \n#> Classification tree:\n#> tree(formula = Class ~ ., data = trainset, split = \"deviance\")\n#> Variables actually used in tree construction:\n#>  [1] \"Elong\"        \"Max.L.Ra\"     \"Comp\"         \"Pr.Axis.Ra\"   \"Sc.Var.maxis\"\n#>  [6] \"Max.L.Rect\"   \"Scat.Ra\"      \"Circ\"         \"D.Circ\"       \"Skew.maxis\"  \n#> Number of terminal nodes:  16 \n#> Residual mean deviance:  0.943 = 456 / 484 \n#> Misclassification error rate: 0.252 = 126 / 500\nplot(fit); text(fit)"},{"path":"vehicles-classiification-with-decision-trees.html","id":"assess-model","chapter":"25 Vehicles classiification with Decision Trees","heading":"25.4 Assess model","text":"Unfortunately, classification trees tendency overfit data. One\napproach reduce risk use cross-validation. hold \nsample fit model note level tree gives best results\n(using deviance misclassification rate). hold different\nsample repeat. can carried using cv.tree() function.\nuse leave-one-cross-validation using misclassification rate \ndeviance (FUN=prune.misclass, followed FUN=prune.tree).results plotted side side Figure 1.2. jagged lines\nshows minimum deviance / misclassification occurred \ncross-validated tree. Since cross validated misclassification deviance\nreach minimum close number branches original\nfitted tree little gained pruning tree","code":"\nfitM.cv <- cv.tree(fit, K=346, FUN = prune.misclass)\nfitP.cv <- cv.tree(fit, K=346, FUN = prune.tree)\npar(mfrow = c(1, 2))\nplot(fitM.cv)\nplot(fitP.cv)"},{"path":"vehicles-classiification-with-decision-trees.html","id":"make-predictions-1","chapter":"25 Vehicles classiification with Decision Trees","heading":"25.5 Make predictions","text":"use validation data set fitted decision tree predict vehicle\nclasses; display confusion matrix calculate error rate \nfitted tree. Overall, model error rate 32%.","code":"\ntestLabels <- Vehicle$Class[-train]\ntestLabels\n#>   [1] van  bus  bus  van  van  bus  bus  saab opel bus  van  saab van  saab saab\n#>  [16] van  saab opel van  saab saab saab bus  bus  saab opel bus  opel bus  opel\n#>  [31] van  opel opel saab saab bus  bus  bus  van  van  saab opel bus  opel van \n#>  [46] opel saab bus  van  bus  opel van  saab bus  opel bus  opel opel van  bus \n#>  [61] van  saab opel bus  van  saab opel opel saab saab saab opel bus  van  bus \n#>  [76] opel bus  saab bus  bus  bus  opel opel van  saab bus  bus  bus  van  saab\n#>  [91] opel van  van  bus  bus  opel bus  opel saab opel bus  opel bus  saab van \n#> [106] van  saab saab bus  van  opel van  saab opel saab saab van  van  van  van \n#> [121] bus  bus  opel bus  bus  van  saab bus  opel bus  bus  bus  bus  opel van \n#> [136] saab saab bus  opel van  bus  saab bus  van  bus  opel van  saab opel saab\n#> [151] opel van  saab van  saab opel bus  van  bus  saab saab opel opel bus  bus \n#> [166] opel van  van  bus  van  van  saab bus  saab opel saab opel bus  bus  bus \n#> [181] saab bus  opel opel saab saab saab van  van  opel opel van  van  opel bus \n#> [196] saab bus  van  opel opel bus  bus  bus  opel saab opel van  bus  opel opel\n#> [211] saab opel bus  opel opel opel van  opel van  saab saab van  saab saab saab\n#> [226] saab van  van  van  saab bus  van  van  bus  saab opel saab saab opel saab\n#> [241] saab saab saab van  saab opel bus  saab bus  opel opel opel saab bus  van \n#> [256] opel saab opel bus  bus  saab van  opel bus  saab van  opel saab saab saab\n#> [271] saab van  opel bus  bus  bus  opel saab saab saab van  saab bus  opel saab\n#> [286] van  opel bus  saab saab opel opel van  saab bus  opel bus  van  van  opel\n#> [301] bus  bus  saab bus  van  saab bus  van  saab van  opel bus  bus  opel saab\n#> [316] opel bus  bus  saab van  saab saab bus  opel opel opel bus  saab bus  van \n#> [331] bus  van  saab opel saab van  opel opel van  bus  saab saab van  saab opel\n#> [346] saab\n#> Levels: bus opel saab van\n# Confusion Matrix\npred <- predict(fit, newdata = testset)\n# find column whih has the maximum of all rows \npred.class <- colnames(pred)[max.col(pred, ties.method = c(\"random\"))]\ncm <- table(testLabels, pred.class, \n      dnn = c(\"Observed Class\", \"Predicted Class\"))\ncm\n#>               Predicted Class\n#> Observed Class bus opel saab van\n#>           bus   85    1    1   5\n#>           opel   3   70   10   2\n#>           saab   7   67   14   7\n#>           van    1    4    5  64\n# Sensitivity\nsum(diag(cm)) / sum(cm)\n#> [1] 0.673\n# pred <- predict(fit, newdata = Vehicle[-train,])\n# pred.class <- colnames(pred)[max.col(pred, ties.method = c(\"random\"))]\n# table(Vehicle$Class[-train], pred.class, \n#       dnn = c(\"Observed Class\", \"Predicted Class\"))\nerror_rate = (1 - sum(pred.class == testset) / nrow(testset))\nround(error_rate, 3)\n#> [1] 0.327\n# error_rate = (1 - sum(pred.class == Vehicle$Class[-train])/346)\n# round(error_rate,3)"},{"path":"applying-naive-bayes-on-the-titanic-case.html","id":"applying-naive-bayes-on-the-titanic-case","chapter":"26 Applying Naive-Bayes on the Titanic case","heading":"26 Applying Naive-Bayes on the Titanic case","text":"Datasets: TitanicAlgorithms:\nNaive Bayes\nNaive BayesThe Titanic dataset R table 2200 passengers summarised according four factors – economic status ranging 1st class, 2nd class, 3rd class crew; gender either male female; Age category either Child Adult whether type passenger survived. combination Age, Gender, Class Survived status, table gives number passengers fall combination. use Naive Bayes Technique classify passengers check well performs.see 32 observations represent possible combinations Class, Sex, Age Survived frequency. Since summarised, table suitable modelling purposes. need expand table individual rows. Let’s create repeating sequence rows based frequencies tableThe data now ready Naive Bayes process. Let’s fit modelThe model creates conditional probability feature separately. also -priori probabilities indicates distribution data. Let’s calculate perform data.results! able classify 1364 1490 “” cases correctly 349 711 “Yes” cases correctly. means ability Naive Bayes algorithm predict “” cases 91.5% falls 49% “Yes” cases resulting overall accuracy 77.8%","code":"\n#Getting started with Naive Bayes\n#Install the package\n#install.packages(“e1071”)\n#Loading the library\nlibrary(e1071)\n\n#Next load the Titanic dataset\ndata(\"Titanic\")\n#Save into a data frame and view it\nTitanic_df = as.data.frame(Titanic)\n#Creating data from table\nrepeating_sequence=rep.int(seq_len(nrow(Titanic_df)), Titanic_df$Freq) #This will repeat each combination equal to the frequency of each combination\n\n# Create the dataset by row repetition created\nTitanic_dataset=Titanic_df[repeating_sequence,]\n\n# We no longer need the frequency, drop the feature\nTitanic_dataset$Freq=NULL\n# Fitting the Naive Bayes model\nNaive_Bayes_Model=naiveBayes(Survived ~., data=Titanic_dataset)\n\n# What does the model say? Print the model summary\nNaive_Bayes_Model\n#> \n#> Naive Bayes Classifier for Discrete Predictors\n#> \n#> Call:\n#> naiveBayes.default(x = X, y = Y, laplace = laplace)\n#> \n#> A-priori probabilities:\n#> Y\n#>    No   Yes \n#> 0.677 0.323 \n#> \n#> Conditional probabilities:\n#>      Class\n#> Y        1st    2nd    3rd   Crew\n#>   No  0.0819 0.1121 0.3544 0.4517\n#>   Yes 0.2855 0.1660 0.2504 0.2982\n#> \n#>      Sex\n#> Y       Male Female\n#>   No  0.9154 0.0846\n#>   Yes 0.5162 0.4838\n#> \n#>      Age\n#> Y      Child  Adult\n#>   No  0.0349 0.9651\n#>   Yes 0.0802 0.9198\n# Prediction on the dataset\nNB_Predictions=predict(Naive_Bayes_Model,Titanic_dataset)\n# Confusion matrix to check accuracy\ntable(NB_Predictions,Titanic_dataset$Survived)\n#>               \n#> NB_Predictions   No  Yes\n#>            No  1364  362\n#>            Yes  126  349"},{"path":"applying-naive-bayes-on-the-titanic-case.html","id":"can-we-do-any-better","chapter":"26 Applying Naive-Bayes on the Titanic case","heading":"26.1 Can we Do any Better?","text":"Naive Bayes parametric algorithm implies perform differently different runs long data remains . , however, learn another implementation Naive Bayes algorithm using ‘mlr’ package. Assuming session going readers, install load package start fitting modelThe mlr package consists lot models works creating tasks learners trained. Let’s create classification task using titanic dataset fit model naive bayes algorithm.summary model printed e3071 package stored learner model. Let’s print compareThe -priori probabilities conditional probabilities model similar one calculated e3071 package expected. means predictions also .see, predictions exactly . way improve features data. Perhaps, features exact age, size family, number parents ship siblings may arrive better model using Naive Bayes. essence, Naive Bayes advantage strong foundation build robust. know ‘caret’ package also consists Naive Bayes function also give us predictions probability.","code":"\n# Getting started with Naive Bayes in mlr\n# install.packages(“mlr”)\n# Loading the library\nlibrary(mlr)\n#> Loading required package: ParamHelpers\n#> 'mlr' is in maintenance mode since July 2019. Future development\n#> efforts will go into its successor 'mlr3' (<https://mlr3.mlr-org.com>).\n#> \n#> Attaching package: 'mlr'\n#> The following object is masked from 'package:e1071':\n#> \n#>     impute\n# Create a classification task for learning on Titanic Dataset and specify the target feature\ntask = makeClassifTask(data = Titanic_dataset, target = \"Survived\")\n\n# Initialize the Naive Bayes classifier\nselected_model = makeLearner(\"classif.naiveBayes\")\n\n# Train the model\nNB_mlr = train(selected_model, task)\n# Read the model learned  \nNB_mlr$learner.model\n#> \n#> Naive Bayes Classifier for Discrete Predictors\n#> \n#> Call:\n#> naiveBayes.default(x = X, y = Y, laplace = laplace)\n#> \n#> A-priori probabilities:\n#> Y\n#>    No   Yes \n#> 0.677 0.323 \n#> \n#> Conditional probabilities:\n#>      Class\n#> Y        1st    2nd    3rd   Crew\n#>   No  0.0819 0.1121 0.3544 0.4517\n#>   Yes 0.2855 0.1660 0.2504 0.2982\n#> \n#>      Sex\n#> Y       Male Female\n#>   No  0.9154 0.0846\n#>   Yes 0.5162 0.4838\n#> \n#>      Age\n#> Y      Child  Adult\n#>   No  0.0349 0.9651\n#>   Yes 0.0802 0.9198\n# Predict on the dataset without passing the target feature\npredictions_mlr = as.data.frame(predict(NB_mlr, newdata = Titanic_dataset[,1:3]))\n\n## Confusion matrix to check accuracy\ntable(predictions_mlr[,1],Titanic_dataset$Survived)\n#>      \n#>         No  Yes\n#>   No  1364  362\n#>   Yes  126  349"},{"path":"classification-on-bad-loans.html","id":"classification-on-bad-loans","chapter":"27 Classification on bad loans","heading":"27 Classification on bad loans","text":"Datasets: loan.csvAlgorithms:\n1. Generalized Linear Model (GLM)\n2. Random Forest (RF)\n3. Gradient Boosting Machine (GBM)\n4. Deep Learning (DL)\n5. Naive Bayes (NB)","code":""},{"path":"classification-on-bad-loans.html","id":"comparison-of-glm-rf-gbm-dl-nb","chapter":"27 Classification on bad loans","heading":"27.1 Comparison of GLM, RF, GBM, DL, NB","text":"Source: https://github.com/h2oai/h2o-tutorials/blob/master/h2o-open-tour-2016/chicago/intro--h2o.R","code":""},{"path":"classification-on-bad-loans.html","id":"introduction-12","chapter":"27 Classification on bad loans","heading":"27.2 Introduction","text":"Introductory H2O Machine Learning Tutorial\nPrepared H2O Open Chicago 2016: http://open.h2o.ai/chicago.html","code":""},{"path":"classification-on-bad-loans.html","id":"install-and-download-h2o","chapter":"27 Classification on bad loans","heading":"27.2.1 Install and download h2o","text":"First step download & install h2o R library\nlatest version available clicking R tab : http://h2o-release.s3.amazonaws.com/h2o/latest_stable.htmlLoad H2O library start H2O cluster locally machine:","code":"\n# Load the H2O library and start up the H2O cluster locally on your machine\nlibrary(h2o)\nh2o.init(nthreads = -1, #Number of threads -1 means use all cores on your machine\n         max_mem_size = \"8G\")  #max mem size is the maximum memory to allocate to H2O\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         16 minutes 46 seconds \n#>     H2O cluster timezone:       Etc/UTC \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.30.0.1 \n#>     H2O cluster version age:    7 months and 16 days !!! \n#>     H2O cluster name:           H2O_started_from_R_root_mwl453 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   7.81 GB \n#>     H2O cluster total cores:    8 \n#>     H2O cluster allowed cores:  8 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4 \n#>     R Version:                  R version 3.6.3 (2020-02-29)"},{"path":"classification-on-bad-loans.html","id":"load-the-dataset","chapter":"27 Classification on bad loans","heading":"27.2.2 Load the dataset","text":"Next import cleaned version Lending Club “Bad Loans” dataset purpose predict whether loan bad (repaid lender). response column, bad_loan, \\(1\\) loan bad, \\(0\\) otherwiseImport data\nloan_csv <- \"/Volumes/H2OTOUR/loan.csv\"\nAlternatively, can import data directly URL","code":"# modify this for your machine\nloan_csv <- \"https://raw.githubusercontent.com/h2oai/app-consumer-loan/master/data/loan.csv\"\ndata <- h2o.importFile(loan_csv)  # 163,987 rows x 15 columns\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |======================================================================| 100%\ndim(data)\n#> [1] 163987     15\n# [1] 163987     15\nurl <- \"https://raw.githubusercontent.com/h2oai/app-consumer-loan/master/data/loan.csv\"\nloans <- read.csv(url)\nwrite.csv(loans, file = file.path(data_raw_dir, \"loan.csv\"))"},{"path":"classification-on-bad-loans.html","id":"feature-engineering","chapter":"27 Classification on bad loans","heading":"27.2.3 Feature Engineering","text":"Since want train binary classification model, must ensure response coded factor response \\(0/1\\), H2O assume ’s numeric, means H2O train regression model instead","code":"\ndata$bad_loan <- as.factor(data$bad_loan)  #encode the binary response as a factor\nh2o.levels(data$bad_loan)  #optional: after encoding, this shows the two factor levels, '0' and '1'\n#> [1] \"0\" \"1\"\n# [1] \"0\" \"1\""},{"path":"classification-on-bad-loans.html","id":"partition-the-data","chapter":"27 Classification on bad loans","heading":"27.2.4 Partition the data","text":"Partition data training, validation test sets:Take look size partition. Notice h2o.splitFrame uses approximate splitting, exact splitting, (efficiency), , exactly 70%, 15% 15% total rows.","code":"\n# Partition the data into training, validation and test sets\nsplits <- h2o.splitFrame(data = data, \n                         ratios = c(0.7, 0.15),  #partition data into 70%, 15%, 15% chunks\n                         seed = 1)  #setting a seed will guarantee reproducibility\ntrain <- splits[[1]]\nvalid <- splits[[2]]\ntest <- splits[[3]]\nnrow(train)  # 114908\n#> [1] 114908\nnrow(valid) # 24498\n#> [1] 24498\nnrow(test)  # 24581\n#> [1] 24581"},{"path":"classification-on-bad-loans.html","id":"identify-response-and-predictor-variables","chapter":"27 Classification on bad loans","heading":"27.2.5 Identify response and predictor variables","text":"","code":"\n# Identify response and predictor variables\ny <- \"bad_loan\"\nx <- setdiff(names(data), c(y, \"int_rate\"))  # remove the interest rate column because it's correlated with the outcome\nprint(x)\n#>  [1] \"loan_amnt\"             \"term\"                  \"emp_length\"           \n#>  [4] \"home_ownership\"        \"annual_inc\"            \"purpose\"              \n#>  [7] \"addr_state\"            \"dti\"                   \"delinq_2yrs\"          \n#> [10] \"revol_util\"            \"total_acc\"             \"longest_credit_length\"\n#> [13] \"verification_status\"\n# [1] \"loan_amnt\"             \"term\"                 \n# [3] \"emp_length\"            \"home_ownership\"       \n# [5] \"annual_inc\"            \"verification_status\"  \n# [7] \"purpose\"               \"addr_state\"           \n# [9] \"dti\"                   \"delinq_2yrs\"          \n# [11] \"revol_util\"            \"total_acc\"            \n# [13] \"longest_credit_length\""},{"path":"classification-on-bad-loans.html","id":"algorithms","chapter":"27 Classification on bad loans","heading":"27.3 Algorithms","text":"Now prepared data, can train models. start training single model H2O supervised algos:Generalized Linear Model (GLM)Random Forest (RF)Gradient Boosting Machine (GBM)Deep Learning (DL)Naive Bayes (NB)","code":""},{"path":"classification-on-bad-loans.html","id":"glm","chapter":"27 Classification on bad loans","heading":"27.4 GLM","text":"Let’s start basic binomial Generalized Linear Model. default, h2o.glm uses regularized, elastic net model.Next, automatic tuning passing validation frame setting lambda_search = True. Since training GLM regularization, try find right amount regularization (avoid overfitting). model parameter, lambda, controls amount regularization GLM model can find optimal value lambda automatically setting lambda_search = TRUE, passing validation frame (used evaluate model performance using particular value lambda).Let’s compare performance two GLMs:Instead printing entire model performance metrics object, probably easier print just metric interested comparing. Retrieve test set AUC:Compare test AUC, training AUC, validation AUC:","code":"glm_fit1 <- h2o.glm(x = x, \n                    y = y, \n                    training_frame = train,\n                    model_id = \"glm_fit1\",\n                    family = \"binomial\")  #similar to R's glm, h2o.glm has the family argument\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%glm_fit2 <- h2o.glm(x = x, \n                    y = y, \n                    training_frame = train,\n                    model_id = \"glm_fit2\",\n                    validation_frame = valid,\n                    family = \"binomial\",\n                    lambda_search = TRUE)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |======================================================================| 100%\n# Let's compare the performance of the two GLMs\nglm_perf1 <- h2o.performance(model = glm_fit1,\n                             newdata = test)\nglm_perf2 <- h2o.performance(model = glm_fit2,\n                             newdata = test)\n\n# Print model performance\nglm_perf1\n#> H2OBinomialMetrics: glm\n#> \n#> MSE:  0.142\n#> RMSE:  0.377\n#> LogLoss:  0.451\n#> Mean Per-Class Error:  0.37\n#> AUC:  0.677\n#> AUCPR:  0.327\n#> Gini:  0.355\n#> R^2:  0.0639\n#> Residual Deviance:  22176\n#> AIC:  22280\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>            0    1    Error         Rate\n#> 0      13647 6344 0.317343  =6344/19991\n#> 1       1939 2651 0.422440   =1939/4590\n#> Totals 15586 8995 0.336968  =8283/24581\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold        value idx\n#> 1                       max f1  0.193323     0.390283 222\n#> 2                       max f2  0.118690     0.556685 308\n#> 3                 max f0point5  0.276272     0.354086 146\n#> 4                 max accuracy  0.494244     0.814410  29\n#> 5                max precision  0.744500     1.000000   0\n#> 6                   max recall  0.001225     1.000000 399\n#> 7              max specificity  0.744500     1.000000   0\n#> 8             max absolute_mcc  0.198334     0.210606 216\n#> 9   max min_per_class_accuracy  0.180070     0.627783 236\n#> 10 max mean_per_class_accuracy  0.193323     0.630109 222\n#> 11                     max tns  0.744500 19991.000000   0\n#> 12                     max fns  0.744500  4589.000000   0\n#> 13                     max fps  0.001225 19991.000000 399\n#> 14                     max tps  0.001225  4590.000000 399\n#> 15                     max tnr  0.744500     1.000000   0\n#> 16                     max fnr  0.744500     0.999782   0\n#> 17                     max fpr  0.001225     1.000000 399\n#> 18                     max tpr  0.001225     1.000000 399\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\nglm_perf2\n#> H2OBinomialMetrics: glm\n#> \n#> MSE:  0.142\n#> RMSE:  0.377\n#> LogLoss:  0.451\n#> Mean Per-Class Error:  0.372\n#> AUC:  0.677\n#> AUCPR:  0.327\n#> Gini:  0.354\n#> R^2:  0.0635\n#> Residual Deviance:  22186\n#> AIC:  22282\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>            0    1    Error         Rate\n#> 0      13699 6292 0.314742  =6292/19991\n#> 1       1968 2622 0.428758   =1968/4590\n#> Totals 15667 8914 0.336032  =8260/24581\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold        value idx\n#> 1                       max f1  0.194171     0.388329 215\n#> 2                       max f2  0.119200     0.555998 306\n#> 3                 max f0point5  0.256488     0.351893 152\n#> 4                 max accuracy  0.474001     0.814654  32\n#> 5                max precision  0.736186     1.000000   0\n#> 6                   max recall  0.001255     1.000000 399\n#> 7              max specificity  0.736186     1.000000   0\n#> 8             max absolute_mcc  0.198114     0.208337 211\n#> 9   max min_per_class_accuracy  0.180131     0.625181 230\n#> 10 max mean_per_class_accuracy  0.194171     0.628250 215\n#> 11                     max tns  0.736186 19991.000000   0\n#> 12                     max fns  0.736186  4589.000000   0\n#> 13                     max fps  0.001255 19991.000000 399\n#> 14                     max tps  0.001255  4590.000000 399\n#> 15                     max tnr  0.736186     1.000000   0\n#> 16                     max fnr  0.736186     0.999782   0\n#> 17                     max fpr  0.001255     1.000000 399\n#> 18                     max tpr  0.001255     1.000000 399\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\nh2o.auc(glm_perf1)  #0.677449084114\n#> [1] 0.677\nh2o.auc(glm_perf2)  #0.677675858276\n#> [1] 0.677\n# Compare test AUC to the training AUC and validation AUC\nh2o.auc(glm_fit2, train = TRUE)  #0.674306164325 \n#> [1] 0.673\nh2o.auc(glm_fit2, valid = TRUE)  #0.675512216705\n#> [1] 0.675\nglm_fit2@model$validation_metrics  #0.675512216705\n#> H2OBinomialMetrics: glm\n#> ** Reported on validation data. **\n#> \n#> MSE:  0.142\n#> RMSE:  0.377\n#> LogLoss:  0.451\n#> Mean Per-Class Error:  0.37\n#> AUC:  0.675\n#> AUCPR:  0.316\n#> Gini:  0.351\n#> R^2:  0.0597\n#> Residual Deviance:  22101\n#> AIC:  22197\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>            0    1    Error         Rate\n#> 0      13591 6365 0.318952  =6365/19956\n#> 1       1916 2626 0.421841   =1916/4542\n#> Totals 15507 8991 0.338028  =8281/24498\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold        value idx\n#> 1                       max f1  0.193519     0.388088 217\n#> 2                       max f2  0.116672     0.555206 309\n#> 3                 max f0point5  0.288405     0.343386 132\n#> 4                 max accuracy  0.487882     0.815250  29\n#> 5                max precision  0.576333     0.681818   9\n#> 6                   max recall  0.004789     1.000000 398\n#> 7              max specificity  0.715719     0.999950   0\n#> 8             max absolute_mcc  0.195417     0.209494 215\n#> 9   max min_per_class_accuracy  0.180760     0.627731 231\n#> 10 max mean_per_class_accuracy  0.192639     0.629672 218\n#> 11                     max tns  0.715719 19955.000000   0\n#> 12                     max fns  0.715719  4542.000000   0\n#> 13                     max fps  0.001048 19956.000000 399\n#> 14                     max tps  0.004789  4542.000000 398\n#> 15                     max tnr  0.715719     0.999950   0\n#> 16                     max fnr  0.715719     1.000000   0\n#> 17                     max fpr  0.001048     1.000000 399\n#> 18                     max tpr  0.004789     1.000000 398\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`"},{"path":"classification-on-bad-loans.html","id":"random-forest-1","chapter":"27 Classification on bad loans","heading":"27.5 Random Forest","text":"H2O’s Random Forest (RF) implements distributed version standard Random Forest algorithm variable importance measures. First train basic Random Forest model default parameters. Random Forest model infer response distribution response encoding. seed required reproducibility.Next, increase number trees used forest setting ntrees = 100. default number trees H2O Random Forest 50, RF twice big default. Usually increasing number trees RF increase performance well. Unlike Gradient Boosting Machines (GBMs), Random Forests fairly resistant (although free ) overfitting. See GBM example additional guidance preventing overfitting using H2O’s early stopping functionality.Let’s compare performance two RFs:","code":"rf_fit1 <- h2o.randomForest(x = x,\n                            y = y,\n                            training_frame = train,\n                            model_id = \"rf_fit1\",\n                            seed = 1)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |======================================================================| 100%rf_fit2 <- h2o.randomForest(x = x,\n                            y = y,\n                            training_frame = train,\n                            model_id = \"rf_fit2\",\n                            #validation_frame = valid,  #only used if stopping_rounds > 0\n                            ntrees = 100,\n                            seed = 1)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |======================================================================| 100%\n# Let's compare the performance of the two RFs\nrf_perf1 <- h2o.performance(model = rf_fit1,\n                            newdata = test)\nrf_perf2 <- h2o.performance(model = rf_fit2,\n                            newdata = test)\n\n# Print model performance\nrf_perf1\n#> H2OBinomialMetrics: drf\n#> \n#> MSE:  0.144\n#> RMSE:  0.379\n#> LogLoss:  0.459\n#> Mean Per-Class Error:  0.379\n#> AUC:  0.663\n#> AUCPR:  0.312\n#> Gini:  0.327\n#> R^2:  0.0519\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>            0    1    Error         Rate\n#> 0      12842 7149 0.357611  =7149/19991\n#> 1       1839 2751 0.400654   =1839/4590\n#> Totals 14681 9900 0.365648  =8988/24581\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold        value idx\n#> 1                       max f1  0.193397     0.379710 229\n#> 2                       max f2  0.077797     0.547982 344\n#> 3                 max f0point5  0.277524     0.344251 158\n#> 4                 max accuracy  0.543639     0.813799  29\n#> 5                max precision  0.817454     1.000000   0\n#> 6                   max recall  0.001181     1.000000 399\n#> 7              max specificity  0.817454     1.000000   0\n#> 8             max absolute_mcc  0.252480     0.195090 178\n#> 9   max min_per_class_accuracy  0.186549     0.619826 235\n#> 10 max mean_per_class_accuracy  0.192603     0.620890 230\n#> 11                     max tns  0.817454 19991.000000   0\n#> 12                     max fns  0.817454  4589.000000   0\n#> 13                     max fps  0.001181 19991.000000 399\n#> 14                     max tps  0.001181  4590.000000 399\n#> 15                     max tnr  0.817454     1.000000   0\n#> 16                     max fnr  0.817454     0.999782   0\n#> 17                     max fpr  0.001181     1.000000 399\n#> 18                     max tpr  0.001181     1.000000 399\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\nrf_perf2\n#> H2OBinomialMetrics: drf\n#> \n#> MSE:  0.143\n#> RMSE:  0.378\n#> LogLoss:  0.454\n#> Mean Per-Class Error:  0.377\n#> AUC:  0.669\n#> AUCPR:  0.32\n#> Gini:  0.339\n#> R^2:  0.0584\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>            0    1    Error         Rate\n#> 0      13172 6819 0.341103  =6819/19991\n#> 1       1891 2699 0.411983   =1891/4590\n#> Totals 15063 9518 0.354339  =8710/24581\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold        value idx\n#> 1                       max f1  0.196407     0.382620 225\n#> 2                       max f2  0.092270     0.549691 331\n#> 3                 max f0point5  0.291396     0.349342 144\n#> 4                 max accuracy  0.555908     0.813840  20\n#> 5                max precision  0.651522     0.785714   5\n#> 6                   max recall  0.004212     1.000000 398\n#> 7              max specificity  0.711667     0.999950   0\n#> 8             max absolute_mcc  0.229251     0.204236 194\n#> 9   max min_per_class_accuracy  0.184594     0.619829 235\n#> 10 max mean_per_class_accuracy  0.196407     0.623457 225\n#> 11                     max tns  0.711667 19990.000000   0\n#> 12                     max fns  0.711667  4590.000000   0\n#> 13                     max fps  0.002687 19991.000000 399\n#> 14                     max tps  0.004212  4590.000000 398\n#> 15                     max tnr  0.711667     0.999950   0\n#> 16                     max fnr  0.711667     1.000000   0\n#> 17                     max fpr  0.002687     1.000000 399\n#> 18                     max tpr  0.004212     1.000000 398\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n\n# Retrieve test set AUC:\nh2o.auc(rf_perf1)  # 0.662266990734\n#> [1] 0.663\nh2o.auc(rf_perf2)  # 0.66525468051\n#> [1] 0.669"},{"path":"classification-on-bad-loans.html","id":"cross-validate-performance","chapter":"27 Classification on bad loans","heading":"27.5.1 Cross-validate performance","text":"Rather using held-test set evaluate model performance, user may wish estimate model performance using cross-validation. Using RF algorithm (default model parameters) example, demonstrate perform k-fold cross-validation using H2O. custom code loops required; simply specify number desired folds nfolds argument. Since going use test set , can use original (full) dataset, called data rather subsampled train dataset. Note take approximately \\(k\\) (nfolds) times longer training single RF model, since train \\(k\\) models cross-validation process (trained \\(n(k-1)/k\\) rows), addition final model trained full training_frame dataset \\(n\\) rows.evaluate cross-validated AUC, following:","code":"rf_fit3 <- h2o.randomForest(x = x,\n                            y = y,\n                            training_frame = train,\n                            model_id = \"rf_fit3\",\n                            seed = 1,\n                            nfolds = 5)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |======================================================================| 100%\n# To evaluate the cross-validated AUC, do the following:\nh2o.auc(rf_fit3, xval = TRUE)  # 0.661201482614\n#> [1] 0.659"},{"path":"classification-on-bad-loans.html","id":"gradient-boosting-machine-gbm","chapter":"27 Classification on bad loans","heading":"27.6 Gradient Boosting Machine (GBM)","text":"H2O’s Gradient Boosting Machine (GBM) offers Stochastic GBM, can\nincrease performance quite bit compared original GBM implementation.Now train basic GBM model. GBM model infer response distribution response encoding specified explicitly distribution argument. seed required reproducibility.Next, increase number trees used GBM setting ntrees=500. default number trees H2O GBM 50, GBM trained using ten times default. Increasing number trees GBM one way increase performance model, however, careful overfit model training data using many trees. automatically find optimal number trees, must use H2O’s early stopping functionality. example , however, following example .set ntrees = 500, however, time use early stopping order prevent overfitting (many trees). H2O’s algorithms early stopping available, however early stopping enabled default (exception Deep Learning).several parameters used control early stopping. three common algorithms : stopping_rounds, stopping_metric stopping_tolerance. stopping metric metric ’d like measure performance, choose AUC . score_tree_interval parameter specific Random Forest model GBM. Setting score_tree_interval = 5 score model every five trees. parameters set specify model stop training three scoring intervals AUC increased \\(0.0005\\). Since specified validation frame, stopping tolerance computed validation AUC rather training AUC.Let’s compare performance two GBMs:examine scoring history, use scoring_history method trained model. score_tree_interval specified, score various intervals, can see h2o.scoreHistory() . However, regular 5-tree intervals used h2o.scoreHistory(). gbm_fit2 trained using training set (validation set), scoring history calculated training set performance metrics .early stopping used, see training stopped \\(105\\) trees instead full \\(500\\). Since used validation set gbm_fit3, training validation performance metrics stored scoring history object. Take look validation AUC observe correct stopping tolerance enforced.Look scoring history third GBM model:","code":"gbm_fit1 <- h2o.gbm(x = x,\n                    y = y,\n                    training_frame = train,\n                    model_id = \"gbm_fit1\",\n                    seed = 1)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |======================================================================| 100%gbm_fit2 <- h2o.gbm(x = x,\n                    y = y,\n                    training_frame = train,\n                    model_id = \"gbm_fit2\",\n                    #validation_frame = valid,  #only used if stopping_rounds > 0\n                    ntrees = 500,\n                    seed = 1)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |======================================================================| 100%gbm_fit3 <- h2o.gbm(x = x,\n                    y = y,\n                    training_frame = train,\n                    model_id = \"gbm_fit3\",\n                    validation_frame = valid,  #only used if stopping_rounds > 0\n                    ntrees = 500,\n                    score_tree_interval = 5,      #used for early stopping\n                    stopping_rounds = 3,          #used for early stopping\n                    stopping_metric = \"AUC\",      #used for early stopping\n                    stopping_tolerance = 0.0005,  #used for early stopping\n                    seed = 1)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |======================================================================| 100%\n# Let's compare the performance of the two GBMs\ngbm_perf1 <- h2o.performance(model = gbm_fit1,\n                             newdata = test)\ngbm_perf2 <- h2o.performance(model = gbm_fit2,\n                             newdata = test)\ngbm_perf3 <- h2o.performance(model = gbm_fit3,\n                             newdata = test)\n\n# Print model performance\ngbm_perf1\n#> H2OBinomialMetrics: gbm\n#> \n#> MSE:  0.141\n#> RMSE:  0.376\n#> LogLoss:  0.448\n#> Mean Per-Class Error:  0.367\n#> AUC:  0.684\n#> AUCPR:  0.332\n#> Gini:  0.368\n#> R^2:  0.0684\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>            0     1    Error         Rate\n#> 0      12143  7848 0.392577  =7848/19991\n#> 1       1571  3019 0.342266   =1571/4590\n#> Totals 13714 10867 0.383182  =9419/24581\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold        value idx\n#> 1                       max f1  0.171139     0.390632 251\n#> 2                       max f2  0.108885     0.560103 328\n#> 3                 max f0point5  0.285149     0.356430 146\n#> 4                 max accuracy  0.510077     0.814410  27\n#> 5                max precision  0.601699     0.636364   8\n#> 6                   max recall  0.037543     1.000000 398\n#> 7              max specificity  0.719189     0.999950   0\n#> 8             max absolute_mcc  0.220471     0.215401 199\n#> 9   max min_per_class_accuracy  0.176689     0.628540 244\n#> 10 max mean_per_class_accuracy  0.170225     0.632624 252\n#> 11                     max tns  0.719189 19990.000000   0\n#> 12                     max fns  0.719189  4590.000000   0\n#> 13                     max fps  0.033483 19991.000000 399\n#> 14                     max tps  0.037543  4590.000000 398\n#> 15                     max tnr  0.719189     0.999950   0\n#> 16                     max fnr  0.719189     1.000000   0\n#> 17                     max fpr  0.033483     1.000000 399\n#> 18                     max tpr  0.037543     1.000000 398\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\ngbm_perf2\n#> H2OBinomialMetrics: gbm\n#> \n#> MSE:  0.142\n#> RMSE:  0.376\n#> LogLoss:  0.449\n#> Mean Per-Class Error:  0.367\n#> AUC:  0.684\n#> AUCPR:  0.329\n#> Gini:  0.368\n#> R^2:  0.0673\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>            0    1    Error         Rate\n#> 0      13661 6330 0.316642  =6330/19991\n#> 1       1918 2672 0.417865   =1918/4590\n#> Totals 15579 9002 0.335544  =8248/24581\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold        value idx\n#> 1                       max f1  0.189615     0.393172 234\n#> 2                       max f2  0.096969     0.558918 332\n#> 3                 max f0point5  0.278776     0.359560 160\n#> 4                 max accuracy  0.521287     0.814287  38\n#> 5                max precision  0.901295     1.000000   0\n#> 6                   max recall  0.018504     1.000000 398\n#> 7              max specificity  0.901295     1.000000   0\n#> 8             max absolute_mcc  0.231089     0.217255 196\n#> 9   max min_per_class_accuracy  0.174914     0.630834 249\n#> 10 max mean_per_class_accuracy  0.156944     0.633792 267\n#> 11                     max tns  0.901295 19991.000000   0\n#> 12                     max fns  0.901295  4589.000000   0\n#> 13                     max fps  0.010794 19991.000000 399\n#> 14                     max tps  0.018504  4590.000000 398\n#> 15                     max tnr  0.901295     1.000000   0\n#> 16                     max fnr  0.901295     0.999782   0\n#> 17                     max fpr  0.010794     1.000000 399\n#> 18                     max tpr  0.018504     1.000000 398\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\ngbm_perf3\n#> H2OBinomialMetrics: gbm\n#> \n#> MSE:  0.141\n#> RMSE:  0.376\n#> LogLoss:  0.448\n#> Mean Per-Class Error:  0.367\n#> AUC:  0.684\n#> AUCPR:  0.331\n#> Gini:  0.369\n#> R^2:  0.0683\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>            0    1    Error         Rate\n#> 0      13652 6339 0.317093  =6339/19991\n#> 1       1917 2673 0.417647   =1917/4590\n#> Totals 15569 9012 0.335869  =8256/24581\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold        value idx\n#> 1                       max f1  0.189870     0.393030 234\n#> 2                       max f2  0.109837     0.559580 321\n#> 3                 max f0point5  0.294571     0.356508 148\n#> 4                 max accuracy  0.510869     0.814572  40\n#> 5                max precision  0.797620     1.000000   0\n#> 6                   max recall  0.026373     1.000000 397\n#> 7              max specificity  0.797620     1.000000   0\n#> 8             max absolute_mcc  0.231530     0.218255 197\n#> 9   max min_per_class_accuracy  0.176566     0.631808 248\n#> 10 max mean_per_class_accuracy  0.175468     0.633400 249\n#> 11                     max tns  0.797620 19991.000000   0\n#> 12                     max fns  0.797620  4589.000000   0\n#> 13                     max fps  0.016648 19991.000000 399\n#> 14                     max tps  0.026373  4590.000000 397\n#> 15                     max tnr  0.797620     1.000000   0\n#> 16                     max fnr  0.797620     0.999782   0\n#> 17                     max fpr  0.016648     1.000000 399\n#> 18                     max tpr  0.026373     1.000000 397\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n\n# Retreive test set AUC\nh2o.auc(gbm_perf1)  # 0.682765594191\n#> [1] 0.684\nh2o.auc(gbm_perf2)  # 0.671854616713\n#> [1] 0.684\nh2o.auc(gbm_perf3)  # 0.68309902855\n#> [1] 0.684\nh2o.scoreHistory(gbm_fit2)\n#> Scoring History: \n#>             timestamp   duration number_of_trees training_rmse training_logloss\n#> 1 2020-11-20 03:05:10  0.001 sec               0       0.38563          0.47403\n#> 2 2020-11-20 03:05:10  0.067 sec               1       0.38370          0.46913\n#> 3 2020-11-20 03:05:11  0.103 sec               2       0.38206          0.46512\n#> 4 2020-11-20 03:05:11  0.142 sec               3       0.38069          0.46184\n#> 5 2020-11-20 03:05:11  0.188 sec               4       0.37954          0.45912\n#>   training_auc training_pr_auc training_lift training_classification_error\n#> 1      0.50000         0.18175       1.00000                       0.81825\n#> 2      0.65779         0.30158       2.68330                       0.40069\n#> 3      0.66583         0.31082       2.79399                       0.33325\n#> 4      0.66851         0.31576       2.97100                       0.34475\n#> 5      0.67011         0.31825       2.97544                       0.33180\n#> \n#> ---\n#>              timestamp   duration number_of_trees training_rmse\n#> 52 2020-11-20 03:05:14  3.745 sec              51       0.36691\n#> 53 2020-11-20 03:05:14  3.832 sec              52       0.36680\n#> 54 2020-11-20 03:05:14  3.916 sec              53       0.36668\n#> 55 2020-11-20 03:05:14  4.005 sec              54       0.36659\n#> 56 2020-11-20 03:05:18  8.006 sec             309       0.36129\n#> 57 2020-11-20 03:05:20  9.762 sec             500       0.36129\n#>    training_logloss training_auc training_pr_auc training_lift\n#> 52          0.42985      0.71600         0.37580       3.67930\n#> 53          0.42960      0.71647         0.37657       3.66973\n#> 54          0.42934      0.71695         0.37739       3.70322\n#> 55          0.42915      0.71733         0.37780       3.71279\n#> 56          0.41770      0.74024         0.41870       4.25823\n#> 57          0.41770      0.74024         0.41870       4.25823\n#>    training_classification_error\n#> 52                       0.29229\n#> 53                       0.29704\n#> 54                       0.29613\n#> 55                       0.30007\n#> 56                       0.25719\n#> 57                       0.25719\nh2o.scoreHistory(gbm_fit3)\n#> Scoring History: \n#>              timestamp   duration number_of_trees training_rmse\n#> 1  2020-11-20 03:05:21  0.001 sec               0       0.38563\n#> 2  2020-11-20 03:05:21  0.188 sec               5       0.37853\n#> 3  2020-11-20 03:05:21  0.389 sec              10       0.37508\n#> 4  2020-11-20 03:05:21  0.629 sec              15       0.37307\n#> 5  2020-11-20 03:05:22  0.848 sec              20       0.37152\n#> 6  2020-11-20 03:05:22  1.062 sec              25       0.37041\n#> 7  2020-11-20 03:05:22  1.282 sec              30       0.36947\n#> 8  2020-11-20 03:05:22  1.524 sec              35       0.36877\n#> 9  2020-11-20 03:05:23  1.750 sec              40       0.36808\n#> 10 2020-11-20 03:05:23  1.967 sec              45       0.36748\n#> 11 2020-11-20 03:05:23  2.187 sec              50       0.36699\n#> 12 2020-11-20 03:05:23  2.403 sec              55       0.36651\n#> 13 2020-11-20 03:05:23  2.637 sec              60       0.36607\n#> 14 2020-11-20 03:05:24  2.851 sec              65       0.36574\n#> 15 2020-11-20 03:05:24  3.064 sec              70       0.36531\n#> 16 2020-11-20 03:05:24  3.354 sec              75       0.36503\n#> 17 2020-11-20 03:05:24  3.685 sec              80       0.36460\n#> 18 2020-11-20 03:05:25  3.989 sec              85       0.36426\n#> 19 2020-11-20 03:05:25  4.245 sec              90       0.36394\n#> 20 2020-11-20 03:05:25  4.474 sec              95       0.36362\n#>    training_logloss training_auc training_pr_auc training_lift\n#> 1           0.47403      0.50000         0.18175       1.00000\n#> 2           0.45676      0.67362         0.32214       3.04518\n#> 3           0.44884      0.68117         0.33334       3.19128\n#> 4           0.44424      0.68708         0.34115       3.30132\n#> 5           0.44060      0.69499         0.34944       3.46878\n#> 6           0.43800      0.69983         0.35483       3.47356\n#> 7           0.43578      0.70425         0.36018       3.48792\n#> 8           0.43410      0.70747         0.36382       3.53576\n#> 9           0.43252      0.71082         0.36841       3.61710\n#> 10          0.43116      0.71346         0.37187       3.66495\n#> 11          0.43002      0.71567         0.37501       3.66016\n#> 12          0.42899      0.71764         0.37855       3.70801\n#> 13          0.42801      0.71952         0.38161       3.73193\n#> 14          0.42726      0.72093         0.38415       3.78934\n#> 15          0.42632      0.72277         0.38738       3.82762\n#> 16          0.42572      0.72396         0.38959       3.85154\n#> 17          0.42477      0.72595         0.39290       3.95202\n#> 18          0.42407      0.72713         0.39553       3.99508\n#> 19          0.42338      0.72852         0.39823       4.02857\n#> 20          0.42272      0.72984         0.40073       4.02378\n#>    training_classification_error validation_rmse validation_logloss\n#> 1                        0.81825         0.38864            0.47953\n#> 2                        0.32117         0.38233            0.46398\n#> 3                        0.32202         0.37958            0.45742\n#> 4                        0.32027         0.37828            0.45428\n#> 5                        0.33371         0.37739            0.45210\n#> 6                        0.32537         0.37676            0.45053\n#> 7                        0.29722         0.37636            0.44949\n#> 8                        0.29544         0.37604            0.44866\n#> 9                        0.28871         0.37587            0.44818\n#> 10                       0.30181         0.37574            0.44781\n#> 11                       0.30144         0.37560            0.44744\n#> 12                       0.29464         0.37552            0.44718\n#> 13                       0.30343         0.37547            0.44703\n#> 14                       0.28692         0.37543            0.44694\n#> 15                       0.28579         0.37536            0.44676\n#> 16                       0.26903         0.37536            0.44673\n#> 17                       0.28476         0.37534            0.44664\n#> 18                       0.26950         0.37537            0.44671\n#> 19                       0.27036         0.37538            0.44671\n#> 20                       0.26573         0.37540            0.44673\n#>    validation_auc validation_pr_auc validation_lift\n#> 1         0.50000           0.18540         1.00000\n#> 2         0.66168           0.30468         2.75098\n#> 3         0.66766           0.30982         2.68582\n#> 4         0.67061           0.31310         2.70784\n#> 5         0.67427           0.31780         2.79590\n#> 6         0.67684           0.32143         2.99403\n#> 7         0.67865           0.32334         2.97202\n#> 8         0.68006           0.32479         3.03806\n#> 9         0.68113           0.32495         2.90597\n#> 10        0.68183           0.32551         2.88396\n#> 11        0.68252           0.32661         2.86194\n#> 12        0.68326           0.32629         2.86194\n#> 13        0.68354           0.32687         2.88396\n#> 14        0.68363           0.32706         2.83993\n#> 15        0.68405           0.32755         2.97202\n#> 16        0.68423           0.32748         2.88396\n#> 17        0.68453           0.32738         2.86194\n#> 18        0.68432           0.32727         2.83993\n#> 19        0.68434           0.32719         2.95000\n#> 20        0.68436           0.32679         2.92799\n#>    validation_classification_error\n#> 1                          0.81460\n#> 2                          0.35387\n#> 3                          0.35285\n#> 4                          0.39028\n#> 5                          0.36770\n#> 6                          0.35240\n#> 7                          0.34848\n#> 8                          0.34386\n#> 9                          0.34807\n#> 10                         0.38681\n#> 11                         0.33774\n#> 12                         0.34215\n#> 13                         0.34431\n#> 14                         0.34146\n#> 15                         0.34329\n#> 16                         0.33872\n#> 17                         0.34276\n#> 18                         0.34256\n#> 19                         0.34289\n#> 20                         0.35097\n# Look at scoring history for third GBM model\nplot(gbm_fit3, \n     timestep = \"number_of_trees\", \n     metric = \"AUC\")\n\nplot(gbm_fit3, \n     timestep = \"number_of_trees\", \n     metric = \"logloss\")"},{"path":"classification-on-bad-loans.html","id":"deep-learning","chapter":"27 Classification on bad loans","heading":"27.7 Deep Learning","text":"H2O’s Deep Learning (DL) algorithm multilayer feed-forward artificial neural network. can also used train autoencoder. example train standard supervised prediction model.","code":""},{"path":"classification-on-bad-loans.html","id":"train-a-default-dl","chapter":"27 Classification on bad loans","heading":"27.7.1 Train a default DL","text":"First, train basic DL model default parameters. DL model infer response distribution response encoding specified explicitly distribution argument. H2O’s DL reproducible run single core; , example, performance metrics may vary slightly see machine. H2O’s DL, early stopping enabled default, , , use training set default stopping parameters perform early stopping.","code":"dl_fit1 <- h2o.deeplearning(x = x,\n                            y = y,\n                            training_frame = train,\n                            model_id = \"dl_fit1\",\n                            seed = 1)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |==============================                                        |  44%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |======================================================================| 100%"},{"path":"classification-on-bad-loans.html","id":"train-a-dl-with-new-architecture-and-more-epochs.","chapter":"27 Classification on bad loans","heading":"27.7.2 Train a DL with new architecture and more epochs.","text":"Next increase number epochs used GBM setting epochs=20 (default \\(10\\)). Increasing number epochs deep neural net may increase performance model, however, careful overfit model training data. automatically find optimal number epochs, must use H2O’s early stopping functionality. Unlike rest H2O algorithms, H2O’s DL use early stopping default, comparison first turn early stopping. next example setting stopping_rounds=0.","code":"dl_fit2 <- h2o.deeplearning(x = x,\n                            y = y,\n                            training_frame = train,\n                            model_id = \"dl_fit2\",\n                            #validation_frame = valid,  #only used if stopping_rounds > 0\n                            epochs = 20,\n                            hidden= c(10,10),\n                            stopping_rounds = 0,  # disable early stopping\n                            seed = 1)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |======================================================================| 100%"},{"path":"classification-on-bad-loans.html","id":"train-a-dl-with-early-stopping","chapter":"27 Classification on bad loans","heading":"27.7.3 Train a DL with early stopping","text":"example use model parameters dl_fit2. time, turn early stopping specify stopping criterion. also pass validation set, recommended early stopping.Let’s compare performance three DL models:Look scoring history third DL model:","code":"dl_fit3 <- h2o.deeplearning(x = x,\n                            y = y,\n                            training_frame = train,\n                            model_id = \"dl_fit3\",\n                            validation_frame = valid,  #in DL, early stopping is on by default\n                            epochs = 20,\n                            hidden = c(10,10),\n                            score_interval = 1,           #used for early stopping\n                            stopping_rounds = 3,          #used for early stopping\n                            stopping_metric = \"AUC\",      #used for early stopping\n                            stopping_tolerance = 0.0005,  #used for early stopping\n                            seed = 1)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |======================================================================| 100%\n# Let's compare the performance of the three DL models\ndl_perf1 <- h2o.performance(model = dl_fit1,\n                            newdata = test)\ndl_perf2 <- h2o.performance(model = dl_fit2,\n                            newdata = test)\ndl_perf3 <- h2o.performance(model = dl_fit3,\n                            newdata = test)\n\n# Print model performance\ndl_perf1\n#> H2OBinomialMetrics: deeplearning\n#> \n#> MSE:  0.142\n#> RMSE:  0.377\n#> LogLoss:  0.451\n#> Mean Per-Class Error:  0.37\n#> AUC:  0.679\n#> AUCPR:  0.328\n#> Gini:  0.359\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>            0    1    Error         Rate\n#> 0      12888 7103 0.355310  =7103/19991\n#> 1       1762 2828 0.383878   =1762/4590\n#> Totals 14650 9931 0.360644  =8865/24581\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold        value idx\n#> 1                       max f1  0.211040     0.389505 198\n#> 2                       max f2  0.118115     0.557856 300\n#> 3                 max f0point5  0.294134     0.353336 119\n#> 4                 max accuracy  0.426635     0.814328  28\n#> 5                max precision  0.757588     1.000000   0\n#> 6                   max recall  0.003595     1.000000 398\n#> 7              max specificity  0.757588     1.000000   0\n#> 8             max absolute_mcc  0.261776     0.213795 149\n#> 9   max min_per_class_accuracy  0.205829     0.628083 203\n#> 10 max mean_per_class_accuracy  0.195771     0.631132 213\n#> 11                     max tns  0.757588 19991.000000   0\n#> 12                     max fns  0.757588  4589.000000   0\n#> 13                     max fps  0.000252 19991.000000 399\n#> 14                     max tps  0.003595  4590.000000 398\n#> 15                     max tnr  0.757588     1.000000   0\n#> 16                     max fnr  0.757588     0.999782   0\n#> 17                     max fpr  0.000252     1.000000 399\n#> 18                     max tpr  0.003595     1.000000 398\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\ndl_perf2\n#> H2OBinomialMetrics: deeplearning\n#> \n#> MSE:  0.143\n#> RMSE:  0.379\n#> LogLoss:  0.457\n#> Mean Per-Class Error:  0.369\n#> AUC:  0.68\n#> AUCPR:  0.33\n#> Gini:  0.36\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>            0     1    Error         Rate\n#> 0      12313  7678 0.384073  =7678/19991\n#> 1       1626  2964 0.354248   =1626/4590\n#> Totals 13939 10642 0.378504  =9304/24581\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold        value idx\n#> 1                       max f1  0.133564     0.389181 254\n#> 2                       max f2  0.087649     0.557511 309\n#> 3                 max f0point5  0.280283     0.352892 132\n#> 4                 max accuracy  0.488492     0.814491  27\n#> 5                max precision  0.623709     1.000000   0\n#> 6                   max recall  0.001493     1.000000 399\n#> 7              max specificity  0.623709     1.000000   0\n#> 8             max absolute_mcc  0.153747     0.208223 233\n#> 9   max min_per_class_accuracy  0.136508     0.628683 251\n#> 10 max mean_per_class_accuracy  0.133564     0.630839 254\n#> 11                     max tns  0.623709 19991.000000   0\n#> 12                     max fns  0.623709  4589.000000   0\n#> 13                     max fps  0.001493 19991.000000 399\n#> 14                     max tps  0.001493  4590.000000 399\n#> 15                     max tnr  0.623709     1.000000   0\n#> 16                     max fnr  0.623709     0.999782   0\n#> 17                     max fpr  0.001493     1.000000 399\n#> 18                     max tpr  0.001493     1.000000 399\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\ndl_perf3\n#> H2OBinomialMetrics: deeplearning\n#> \n#> MSE:  0.142\n#> RMSE:  0.377\n#> LogLoss:  0.451\n#> Mean Per-Class Error:  0.369\n#> AUC:  0.68\n#> AUCPR:  0.327\n#> Gini:  0.36\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>            0    1    Error         Rate\n#> 0      13132 6859 0.343104  =6859/19991\n#> 1       1813 2777 0.394989   =1813/4590\n#> Totals 14945 9636 0.352793  =8672/24581\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold        value idx\n#> 1                       max f1  0.182153     0.390412 224\n#> 2                       max f2  0.097224     0.556291 318\n#> 3                 max f0point5  0.278982     0.354307 139\n#> 4                 max accuracy  0.450146     0.814125  35\n#> 5                max precision  0.705370     1.000000   0\n#> 6                   max recall  0.013292     1.000000 396\n#> 7              max specificity  0.705370     1.000000   0\n#> 8             max absolute_mcc  0.217746     0.211019 191\n#> 9   max min_per_class_accuracy  0.173911     0.629283 232\n#> 10 max mean_per_class_accuracy  0.168476     0.631535 238\n#> 11                     max tns  0.705370 19991.000000   0\n#> 12                     max fns  0.705370  4589.000000   0\n#> 13                     max fps  0.000604 19991.000000 399\n#> 14                     max tps  0.013292  4590.000000 396\n#> 15                     max tnr  0.705370     1.000000   0\n#> 16                     max fnr  0.705370     0.999782   0\n#> 17                     max fpr  0.000604     1.000000 399\n#> 18                     max tpr  0.013292     1.000000 396\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n\n# Retreive test set AUC\nh2o.auc(dl_perf1)  # 0.6774335\n#> [1] 0.679\nh2o.auc(dl_perf2)  # 0.678446\n#> [1] 0.68\nh2o.auc(dl_perf3)  # 0.6770498\n#> [1] 0.68\n# Look at scoring history for third DL model\nplot(dl_fit3, \n     timestep = \"epochs\", \n     metric = \"AUC\")"},{"path":"classification-on-bad-loans.html","id":"scoring-history","chapter":"27 Classification on bad loans","heading":"27.7.4 Scoring history","text":"","code":"\n# Scoring history\nh2o.scoreHistory(dl_fit3)\n#> Scoring History: \n#>             timestamp   duration training_speed   epochs iterations\n#> 1 2020-11-20 03:06:01  0.000 sec             NA  0.00000          0\n#> 2 2020-11-20 03:06:02  0.341 sec 378757 obs/sec  0.87019          1\n#> 3 2020-11-20 03:06:03  1.468 sec 522731 obs/sec  6.09129          7\n#> 4 2020-11-20 03:06:04  2.511 sec 604003 obs/sec 12.18435         14\n#> 5 2020-11-20 03:06:05  3.531 sec 643064 obs/sec 18.27765         21\n#> 6 2020-11-20 03:06:05  3.872 sec 648153 obs/sec 20.01859         23\n#>          samples training_rmse training_logloss training_r2 training_auc\n#> 1       0.000000            NA               NA          NA           NA\n#> 2   99992.000000       0.38346          0.47371     0.02709      0.65638\n#> 3  699938.000000       0.37796          0.45399     0.05481      0.66989\n#> 4 1400079.000000       0.37944          0.45825     0.04738      0.67205\n#> 5 2100248.000000       0.37775          0.45199     0.05585      0.67398\n#> 6 2300296.000000       0.37660          0.44962     0.06163      0.67869\n#>   training_pr_auc training_lift training_classification_error validation_rmse\n#> 1              NA            NA                            NA              NA\n#> 2         0.30142       2.39491                       0.38157         0.38352\n#> 3         0.31362       2.44934                       0.37065         0.37689\n#> 4         0.31780       2.99364                       0.40807         0.37917\n#> 5         0.31481       2.77592                       0.36236         0.37790\n#> 6         0.31984       2.72149                       0.33950         0.37681\n#>   validation_logloss validation_r2 validation_auc validation_pr_auc\n#> 1                 NA            NA             NA                NA\n#> 2            0.47329       0.02608        0.65949           0.30023\n#> 3            0.45187       0.05950        0.67625           0.32000\n#> 4            0.45815       0.04804        0.67313           0.31331\n#> 5            0.45270       0.05445        0.67269           0.31116\n#> 6            0.45063       0.05987        0.67640           0.31478\n#>   validation_lift validation_classification_error\n#> 1              NA                              NA\n#> 2         2.59776                         0.37309\n#> 3         2.75187                         0.33386\n#> 4         2.79590                         0.38664\n#> 5         2.55373                         0.36166\n#> 6         2.64179                         0.34995\n# Scoring History: \n#   timestamp   duration  training_speed   epochs\n# 1 2016-05-03 10:33:29  0.000 sec                  0.00000\n# 2 2016-05-03 10:33:29  0.347 sec 424697 rows/sec  0.86851\n# 3 2016-05-03 10:33:30  1.356 sec 601925 rows/sec  6.09185\n# 4 2016-05-03 10:33:31  2.348 sec 717617 rows/sec 13.05168\n# 5 2016-05-03 10:33:32  3.281 sec 777538 rows/sec 20.00783\n# 6 2016-05-03 10:33:32  3.345 sec 777275 rows/sec 20.00783\n# iterations        samples training_MSE training_r2\n# 1          0       0.000000                         \n# 2          1   99804.000000      0.14402     0.03691\n# 3          7  700039.000000      0.14157     0.05333\n# 4         15 1499821.000000      0.14033     0.06159\n# 5         23 2299180.000000      0.14079     0.05853\n# 6         23 2299180.000000      0.14157     0.05333\n# training_logloss training_AUC training_lift\n# 1                                            \n# 2          0.45930      0.66685       2.20727\n# 3          0.45220      0.68133       2.59354\n# 4          0.44710      0.67993       2.70390\n# 5          0.45100      0.68192       2.81426\n# 6          0.45220      0.68133       2.59354\n# training_classification_error validation_MSE validation_r2\n# 1                                                           \n# 2                       0.36145        0.14682       0.03426\n# 3                       0.33647        0.14500       0.04619\n# 4                       0.37126        0.14411       0.05204\n# 5                       0.32868        0.14474       0.04793\n# 6                       0.33647        0.14500       0.04619\n# validation_logloss validation_AUC validation_lift\n# 1                                                  \n# 2            0.46692        0.66582         2.53209\n# 3            0.46256        0.67354         2.64124\n# 4            0.45789        0.66986         2.44478\n# 5            0.46292        0.67117         2.70672\n# 6            0.46256        0.67354         2.64124\n# validation_classification_error\n# 1                                \n# 2                         0.37197\n# 3                         0.34716\n# 4                         0.34385\n# 5                         0.36544\n# 6                         0.34716"},{"path":"classification-on-bad-loans.html","id":"naive-bayes-model","chapter":"27 Classification on bad loans","heading":"27.8 Naive Bayes model","text":"Naive Bayes (NB) algorithm usually beat algorithm like Random Forest GBM, however still popular algorithm, especially text domain (input text encoded “Bag Words”, example). Naive Bayes algorithm binary multiclass classification problems , regression. Therefore, response\nmust factor instead numeric.","code":"# First we will train a basic NB model with default parameters. \nnb_fit1 <- h2o.naiveBayes(x = x,\n                          y = y,\n                          training_frame = train,\n                          model_id = \"nb_fit1\")\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |======================================================================| 100%"},{"path":"classification-on-bad-loans.html","id":"train-a-nb-model-with-laplace-smoothing","chapter":"27 Classification on bad loans","heading":"27.8.1 Train a NB model with Laplace Smoothing","text":"One tunable model parameters Naive Bayes algorithm amount Laplace smoothing. H2O Naive Bayes model use Laplace smoothing default.","code":"nb_fit2 <- h2o.naiveBayes(x = x,\n                          y = y,\n                          training_frame = train,\n                          model_id = \"nb_fit2\",\n                          laplace = 6)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |======================================================================| 100%\n\n# Let's compare the performance of the two NB models\nnb_perf1 <- h2o.performance(model = nb_fit1,\n                            newdata = test)\nnb_perf2 <- h2o.performance(model = nb_fit2,\n                            newdata = test)\n\n# Print model performance\nnb_perf1\n#> H2OBinomialMetrics: naivebayes\n#> \n#> MSE:  0.15\n#> RMSE:  0.387\n#> LogLoss:  0.489\n#> Mean Per-Class Error:  0.39\n#> AUC:  0.651\n#> AUCPR:  0.297\n#> Gini:  0.303\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>            0    1    Error         Rate\n#> 0      13184 6807 0.340503  =6807/19991\n#> 1       2021 2569 0.440305   =2021/4590\n#> Totals 15205 9376 0.359139  =8828/24581\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold        value idx\n#> 1                       max f1  0.225948     0.367893 235\n#> 2                       max f2  0.090634     0.545538 346\n#> 3                 max f0point5  0.340471     0.335807 165\n#> 4                 max accuracy  0.999554     0.812945   0\n#> 5                max precision  0.559607     0.428747  69\n#> 6                   max recall  0.000196     1.000000 399\n#> 7              max specificity  0.999554     0.999550   0\n#> 8             max absolute_mcc  0.287231     0.188641 195\n#> 9   max min_per_class_accuracy  0.207470     0.604672 250\n#> 10 max mean_per_class_accuracy  0.225948     0.609596 235\n#> 11                     max tns  0.999554 19982.000000   0\n#> 12                     max fns  0.999554  4589.000000   0\n#> 13                     max fps  0.000196 19991.000000 399\n#> 14                     max tps  0.000196  4590.000000 399\n#> 15                     max tnr  0.999554     0.999550   0\n#> 16                     max fnr  0.999554     0.999782   0\n#> 17                     max fpr  0.000196     1.000000 399\n#> 18                     max tpr  0.000196     1.000000 399\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\nnb_perf2\n#> H2OBinomialMetrics: naivebayes\n#> \n#> MSE:  0.15\n#> RMSE:  0.387\n#> LogLoss:  0.489\n#> Mean Per-Class Error:  0.39\n#> AUC:  0.651\n#> AUCPR:  0.297\n#> Gini:  0.303\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>            0    1    Error         Rate\n#> 0      14002 5989 0.299585  =5989/19991\n#> 1       2207 2383 0.480828   =2207/4590\n#> Totals 16209 8372 0.333428  =8196/24581\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold        value idx\n#> 1                       max f1  0.242206     0.367690 222\n#> 2                       max f2  0.088529     0.545649 347\n#> 3                 max f0point5  0.362995     0.336012 152\n#> 4                 max accuracy  0.999564     0.812945   0\n#> 5                max precision  0.574610     0.428775  63\n#> 6                   max recall  0.000207     1.000000 399\n#> 7              max specificity  0.999564     0.999550   0\n#> 8             max absolute_mcc  0.286635     0.189479 192\n#> 9   max min_per_class_accuracy  0.207609     0.604357 248\n#> 10 max mean_per_class_accuracy  0.247906     0.609878 218\n#> 11                     max tns  0.999564 19982.000000   0\n#> 12                     max fns  0.999564  4589.000000   0\n#> 13                     max fps  0.000207 19991.000000 399\n#> 14                     max tps  0.000207  4590.000000 399\n#> 15                     max tnr  0.999564     0.999550   0\n#> 16                     max fnr  0.999564     0.999782   0\n#> 17                     max fpr  0.000207     1.000000 399\n#> 18                     max tpr  0.000207     1.000000 399\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n\n# Retreive test set AUC\nh2o.auc(nb_perf1)  # 0.6488014\n#> [1] 0.651\nh2o.auc(nb_perf2)  # 0.6490678\n#> [1] 0.651"},{"path":"predicting-flu-outcome-comparing-eight-classification-algorithms.html","id":"predicting-flu-outcome-comparing-eight-classification-algorithms","chapter":"28 Predicting Flu outcome comparing eight classification algorithms","heading":"28 Predicting Flu outcome comparing eight classification algorithms","text":"Datasets: fluH7N9_china_2013Algorithms: RF, GLMNET, KNN, PDA, LDA, NSC, C5, PLS","code":""},{"path":"predicting-flu-outcome-comparing-eight-classification-algorithms.html","id":"introduction-13","chapter":"28 Predicting Flu outcome comparing eight classification algorithms","heading":"28.1 Introduction","text":"Among many nice R packages containing data collections outbreaks package. contains datsets epidemics among data 2013 outbreak influenza H7N9 China analysed Kucharski et al. (2014):. Kucharski, H. Mills, . Pinsent, C. Fraser, M. Van Kerkhove, C. . Donnelly, S. Riley. 2014. Distinguishing reservoir exposure human--human transmission emerging pathogens using case onset data. PLOS Currents Outbreaks. Mar 7, edition 1. doi: 10.1371/currents.outbreaks.e1473d9bfc99d080ca242139a06c455f.. Kucharski, H. Mills, . Pinsent, C. Fraser, M. Van Kerkhove, C. . Donnelly, S. Riley. 2014. Data : Distinguishing reservoir exposure human--human transmission emerging pathogens using case onset data. Dryad Digital Repository. http://dx.doi.org/10.5061/dryad.2g43n.","code":""},{"path":"predicting-flu-outcome-comparing-eight-classification-algorithms.html","id":"algorithms-1","chapter":"28 Predicting Flu outcome comparing eight classification algorithms","heading":"28.1.1 Algorithms","text":"compare classification algorithms:Random ForestGLM netk-Nearest NeighborsPenalized Discriminant AnalysisStabilized Linear Discriminant AnalysisNearest Shrunken CentroidsSingle C5.0 TreePartial Least Squares","code":""},{"path":"predicting-flu-outcome-comparing-eight-classification-algorithms.html","id":"workflow-3","chapter":"28 Predicting Flu outcome comparing eight classification algorithms","heading":"28.1.2 Workflow","text":"Load datasetData wranglingMany dates one variable: gatherConvert group factor: factorChange dates descriptions: dplyr::mapvaluesSet several provinces : mapvaluesConvert gender factorConvert province factorFeatures visualizationArea density plot Month vs Age Province, date group, outcome, genderNumber flu cases gender, provinceAge density plot flu cases outcomeDays passed outset province, age, gender, outcomeFeature engineeringGenerate new featuresGender numericProvinces binary classifiersOutcome binary classifierImpute missing data: miceSplit dataset training, validation test setsFeature importancewith Decision Treeswith Random ForestWrangling datasetPlot Importance vs VariableImpact DatasetsDensity plot training, validation test datasetsFeatures vs outcome age, days onset hospital, day onset outcomeTrain models training validation datasetNumeric visual Comparison models\nAccuracy\nKappa\nNumeric visual Comparison modelsAccuracyKappaCompare predictions training validation set\nCreate dataset results\nNew variable outcome\nCompare predictions training validation setCreate dataset resultsNew variable outcomePredicting unknown outcomes training dataNumeric visual Comparison models\nAccuracy\nKappa\nNumeric visual Comparison modelsAccuracyKappaCompare predictions training validation set\nCreate dataset results\nNew variable outcome\nCompare predictions training validation setCreate dataset resultsNew variable outcomeCalculate predicted outcomeCalculate predicted outcomeSave CSV fileSave CSV fileCalculate recovery casesCalculate recovery casesSummarize outcomeSummarize outcomePlot month vs log2-ratio recovery vs death, gender, age, date groupPlot month vs log2-ratio recovery vs death, gender, age, date group","code":""},{"path":"predicting-flu-outcome-comparing-eight-classification-algorithms.html","id":"can-we-predict-flu-outcome-with-machine-learning-in-r","chapter":"28 Predicting Flu outcome comparing eight classification algorithms","heading":"28.1.3 Can we predict flu outcome with Machine Learning in R?","text":"using data example show use Machine Learning algorithms predicting disease outcome., selected extracted features raw data, including age, days onset outcome, gender, whether patients hospitalized, etc. Missing values imputed different model algorithms used predict outcome (death recovery). prediction accuracy, sensitivity specificity. thus prepared dataset divided training testing subsets. test subset contained cases unknown outcome. applied models test data, split training data validation subsets.tested modeling algorithms similarly successful predicting outcomes validation data. decide final classifications, compared predictions models defined outcome “Death” “Recovery” function models, whereas classifications low prediction probability flagged “uncertain”. Accounting uncertainty led 100% correct classification validation test set.training cases unknown outcome classified based algorithms. 57 unknown cases, 14 classified “Recovery”, 10 “Death” 33 uncertain.Part 2, looking extreme gradient boosting performs dataset.Disclaimer: expert Machine Learning. Everything know, taught last months. , see mistakes tips tricks improvement, please don’t hesitate let know! Thanks. :-)","code":""},{"path":"predicting-flu-outcome-comparing-eight-classification-algorithms.html","id":"the-data-1","chapter":"28 Predicting Flu outcome comparing eight classification algorithms","heading":"28.2 The data","text":"dataset contains case ID, date onset, date hospitalization, date outcome, gender, age, province course outcome: Death Recovery. can already see couple missing values data, deal later.start preparing data Machine Learning, want get idea distribution data points different variables plotting.provinces handful cases, combining category “” keep Jiangsu, Shanghai Zhejian separate provinces.plot shows dates onset, hospitalization outcome (known) data point. Outcome marked color age shown y-axis. Gender marked point shape.density distribution date age cases seems indicate older people died frequently Jiangsu Zhejiang province Shanghai provinces.look distribution points along time axis, suggests might positive correlation likelihood death early onset early outcome.also want know many cases gender province compare genders’ age distribution.dataset, male female cases correspondingly, see deaths, recoveries unknown outcomes men women. potentially problem later modeling inherent likelihoods outcome directly comparable sexes.unknown outcomes recorded Zhejiang. Similarly gender, don’t equal distribution data points across provinces either.look age distribution obvious people died tended slightly older recovered. density curve unknown outcomes similar death recovery, suggesting among people might deaths recoveries.lastly, want plot many days passed onset, hospitalization outcome case.plot shows many missing values dates, hard draw general conclusion.","code":"\noptions(width = 1000)\n\nif (!require(\"outbreaks\")) install.packages(\"outbreaks\")\n#> Loading required package: outbreaks\nlibrary(outbreaks)\n\nfluH7N9_china_2013_backup <- fluH7N9_china_2013\n\nfluH7N9_china_2013$age[which(fluH7N9_china_2013$age == \"?\")] <- NA\nfluH7N9_china_2013$case_ID <- paste(\"case\", \n                                    fluH7N9_china_2013$case_id, \n                                    sep = \"_\")\nhead(fluH7N9_china_2013)\n#>   case_id date_of_onset date_of_hospitalisation date_of_outcome outcome gender age province case_ID\n#> 1       1    2013-02-19                    <NA>      2013-03-04   Death      m  87 Shanghai  case_1\n#> 2       2    2013-02-27              2013-03-03      2013-03-10   Death      m  27 Shanghai  case_2\n#> 3       3    2013-03-09              2013-03-19      2013-04-09   Death      f  35    Anhui  case_3\n#> 4       4    2013-03-19              2013-03-27            <NA>    <NA>      f  45  Jiangsu  case_4\n#> 5       5    2013-03-19              2013-03-30      2013-05-15 Recover      f  48  Jiangsu  case_5\n#> 6       6    2013-03-21              2013-03-28      2013-04-26   Death      f  32  Jiangsu  case_6\n# make tidy data, clean up and simplify\nlibrary(tidyr)\n\n# put different type of dates in one variable (Date)\nfluH7N9_china_2013_gather <- fluH7N9_china_2013 %>%\n  gather(Group, Date, date_of_onset:date_of_outcome)\n\n# convert Group to factor\nfluH7N9_china_2013_gather$Group <- factor(fluH7N9_china_2013_gather$Group, \n                                          levels = c(\"date_of_onset\", \n                                                     \"date_of_hospitalisation\", \n                                                     \"date_of_outcome\"))\n\n# change the dates description with plyr::mapvalues\nlibrary(plyr)\nfrom <- c(\"date_of_onset\", \"date_of_hospitalisation\", \"date_of_outcome\")\nto <- c(\"Date of onset\", \"Date of hospitalisation\", \"Date of outcome\")\nfluH7N9_china_2013_gather$Group <- mapvalues(fluH7N9_china_2013_gather$Group, \n                                             from = from, to = to)\n\n# change additional provinces to Other\nfrom <- c(\"Anhui\", \"Beijing\", \"Fujian\", \"Guangdong\", \"Hebei\", \"Henan\", \n          \"Hunan\", \"Jiangxi\", \"Shandong\", \"Taiwan\")\nto <- rep(\"Other\", 10)\nfluH7N9_china_2013_gather$province <- \n  mapvalues(fluH7N9_china_2013_gather$province, \n            from = from, to = to)\n\n# convert gender to factor\nlevels(fluH7N9_china_2013_gather$gender) <- \n  c(levels(fluH7N9_china_2013_gather$gender), \"unknown\")\n\n# replace NA in gender by \"unknown\nis_na <- is.na(fluH7N9_china_2013_gather$gender)\nfluH7N9_china_2013_gather$gender[is_na] <- \"unknown\"\n\n# convert province to factor\nfluH7N9_china_2013_gather$province <- factor(fluH7N9_china_2013_gather$province, \n                                             levels = c(\"Jiangsu\",  \"Shanghai\", \n                                                        \"Zhejiang\", \"Other\"))\nlibrary(ggplot2)\nmy_theme <- function(base_size = 12, base_family = \"sans\"){\n  theme_minimal(base_size = base_size, base_family = base_family) +\n  theme(\n    axis.text = element_text(size = 12),\n    axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = 0.5),\n    axis.title = element_text(size = 14),\n    panel.grid.major = element_line(color = \"grey\"),\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"aliceblue\"),\n    strip.background = element_rect(fill = \"lightgrey\", color = \"grey\", \n                                    size = 1),\n    strip.text = element_text(face = \"bold\", size = 12, color = \"black\"),\n    legend.position = \"bottom\",\n    legend.justification = \"top\", \n    legend.box = \"horizontal\",\n    legend.box.background = element_rect(colour = \"grey50\"),\n    legend.background = element_blank(),\n    panel.border = element_rect(color = \"grey\", fill = NA, size = 0.5)\n  )\n}\nggplot(data = fluH7N9_china_2013_gather, \n       aes(x = Date, y = as.numeric(age), fill = outcome)) +\n  stat_density2d(aes(alpha = ..level..), geom = \"polygon\") +\n  geom_jitter(aes(color = outcome, shape = gender), size = 1.5) +\n  geom_rug(aes(color = outcome)) +\n  labs(\n    fill = \"Outcome\",\n    color = \"Outcome\",\n    alpha = \"Level\",\n    shape = \"Gender\",\n    x = \"Date in 2013\",\n    y = \"Age\",\n    title = \"2013 Influenza A H7N9 cases in China\",\n    subtitle = \"Dataset from 'outbreaks' package (Kucharski et al. 2014)\",\n    caption = \"\"\n  ) +\n  facet_grid(Group ~ province) +\n  my_theme() +\n  scale_shape_manual(values = c(15, 16, 17)) +\n  scale_color_brewer(palette=\"Set1\", na.value = \"grey50\") +\n  scale_fill_brewer(palette=\"Set1\")\n#> Warning: Removed 149 rows containing non-finite values (stat_density2d).\n#> Warning: Removed 149 rows containing missing values (geom_point).\n# more tidy data. first remove age\nfluH7N9_china_2013_gather_2 <- fluH7N9_china_2013_gather[, -4] %>%\n  gather(group_2, value, gender:province)\n#> Warning: attributes are not identical across measure variables;\n#> they will be dropped\n\n# change descriptions\nfrom <- c(\"m\", \"f\", \"unknown\", \"Other\")\nto <- c(\"Male\", \"Female\", \"Unknown gender\", \"Other province\")\nfluH7N9_china_2013_gather_2$value <- mapvalues(fluH7N9_china_2013_gather_2$value, \n                                               from = from, to = to)\n# convert to factor\nfluH7N9_china_2013_gather_2$value <- factor(fluH7N9_china_2013_gather_2$value, \n                                            levels = c(\"Female\", \"Male\", \n                                                       \"Unknown gender\", \n                                                       \"Jiangsu\", \"Shanghai\", \n                                                       \"Zhejiang\", \"Other \n                                                       province\"))\n\np1 <- ggplot(data = fluH7N9_china_2013_gather_2, aes(x = value, \n                                                     fill = outcome, \n                                                     color = outcome)) +\n  geom_bar(position = \"dodge\", alpha = 0.7, size = 1) +\n  my_theme() +\n  scale_fill_brewer(palette=\"Set1\", na.value = \"grey50\") +\n  scale_color_brewer(palette=\"Set1\", na.value = \"grey50\") +\n  labs(\n    color = \"\",\n    fill = \"\",\n    x = \"\",\n    y = \"Count\",\n    title = \"2013 Influenza A H7N9 cases in China\",\n    subtitle = \"Gender and Province numbers of flu cases\",\n    caption = \"\"\n  )\n\np2 <- ggplot(data = fluH7N9_china_2013_gather, aes(x = as.numeric(age), \n                                                   fill = outcome, \n                                                   color = outcome)) +\n  geom_density(alpha = 0.3, size = 1) +\n  geom_rug() +\n  scale_color_brewer(palette=\"Set1\", na.value = \"grey50\") +\n  scale_fill_brewer(palette=\"Set1\", na.value = \"grey50\") +\n  my_theme() +\n  labs(\n    color = \"\",\n    fill = \"\",\n    x = \"Age\",\n    y = \"Density\",\n    title = \"\",\n    subtitle = \"Age distribution of flu cases\",\n    caption = \"\"\n  )\n\nlibrary(gridExtra)\nlibrary(grid)\n\ngrid.arrange(p1, p2, ncol = 2)\n#> Warning: Removed 6 rows containing non-finite values (stat_density).\nggplot(data = fluH7N9_china_2013_gather, aes(x = Date, y = as.numeric(age), \n                                             color = outcome)) +\n  geom_point(aes(color = outcome, shape = gender), size = 1.5, alpha = 0.6) +\n  geom_path(aes(group = case_ID)) +\n  facet_wrap( ~ province, ncol = 2) +\n  my_theme() +\n  scale_shape_manual(values = c(15, 16, 17)) +\n  scale_color_brewer(palette=\"Set1\", na.value = \"grey50\") +\n  scale_fill_brewer(palette=\"Set1\") +\n  labs(\n    color = \"Outcome\",\n    shape = \"Gender\",\n    x = \"Date in 2013\",\n    y = \"Age\",\n    title = \"2013 Influenza A H7N9 cases in China\",\n    subtitle = \"Dataset from 'outbreaks' package (Kucharski et al. 2014)\",\n    caption = \"\\nTime from onset of flu to outcome.\"\n  )\n#> Warning: Removed 149 rows containing missing values (geom_point).\n#> Warning: Removed 122 row(s) containing missing values (geom_path)."},{"path":"predicting-flu-outcome-comparing-eight-classification-algorithms.html","id":"features-1","chapter":"28 Predicting Flu outcome comparing eight classification algorithms","heading":"28.3 Features","text":"Machine Learning-speak features variables used model training. Using right features dramatically influences accuracy model. don’t many features, keeping age , also generating new features:date information calculating days onset outcome onset hospitalizationfrom date information calculating days onset outcome onset hospitalizationI converting gender numeric values 1 female 0 maleI converting gender numeric values 1 female 0 malesimilarly, converting provinces binary classifiers (yes == 1, == 0) Shanghai, Zhejiang, Jiangsu provincessimilarly, converting provinces binary classifiers (yes == 1, == 0) Shanghai, Zhejiang, Jiangsu provincesthe binary classification given whether case hospitalised, whether early onset early outcome (earlier median date)binary classification given whether case hospitalised, whether early onset early outcome (earlier median date)","code":"\n# convert gender, provinces to discrete and numeric values\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following object is masked from 'package:gridExtra':\n#> \n#>     combine\n#> The following objects are masked from 'package:plyr':\n#> \n#>     arrange, count, desc, failwith, id, mutate, rename, summarise, summarize\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\n\ndataset <- fluH7N9_china_2013 %>%\n  mutate(hospital = as.factor(ifelse(is.na(date_of_hospitalisation), 0, 1)),\n         gender_f = as.factor(ifelse(gender == \"f\", 1, 0)),\n         province_Jiangsu = as.factor(ifelse(province == \"Jiangsu\", 1, 0)),\n         province_Shanghai = as.factor(ifelse(province == \"Shanghai\", 1, 0)),\n         province_Zhejiang = as.factor(ifelse(province == \"Zhejiang\", 1, 0)),\n         province_other = as.factor(ifelse(province == \"Zhejiang\" | \n                                             province == \"Jiangsu\" | \n                                             province == \"Shanghai\", 0, 1)),\n         days_onset_to_outcome = as.numeric(as.character(gsub(\" days\", \"\",\n                as.Date(as.character(date_of_outcome), format = \"%Y-%m-%d\") - \n                as.Date(as.character(date_of_onset), format = \"%Y-%m-%d\")))),\n         \n         days_onset_to_hospital = as.numeric(as.character(gsub(\" days\", \"\",\n            as.Date(as.character(date_of_hospitalisation), format = \"%Y-%m-%d\") - \n            as.Date(as.character(date_of_onset), format = \"%Y-%m-%d\")))),\n         age = as.numeric(as.character(age)),\n         early_onset = as.factor(ifelse(date_of_onset < \n                        summary(date_of_onset)[[3]], 1, 0)),\n         early_outcome = as.factor(ifelse(date_of_outcome < \n                        summary(date_of_outcome)[[3]], 1, 0))) %>%\n  subset(select = -c(2:4, 6, 8))\n  # print\n\nrownames(dataset) <- dataset$case_ID\ndataset <- dataset[, -c(1,4)]\nhead(dataset)\n#>        outcome age hospital gender_f province_Jiangsu province_Shanghai province_Zhejiang province_other days_onset_to_outcome days_onset_to_hospital early_onset early_outcome\n#> case_1   Death  87        0        0                0                 1                 0              0                    13                     NA           1             1\n#> case_2   Death  27        1        0                0                 1                 0              0                    11                      4           1             1\n#> case_3   Death  35        1        1                0                 0                 0              1                    31                     10           1             1\n#> case_4    <NA>  45        1        1                1                 0                 0              0                    NA                      8           1          <NA>\n#> case_5 Recover  48        1        1                1                 0                 0              0                    57                     11           1             0\n#> case_6   Death  32        1        1                1                 0                 0              0                    36                      7           1             1\nsummary(dataset$outcome)\n#>   Death Recover    NA's \n#>      32      47      57"},{"path":"predicting-flu-outcome-comparing-eight-classification-algorithms.html","id":"imputing-missing-values-1","chapter":"28 Predicting Flu outcome comparing eight classification algorithms","heading":"28.3.1 Imputing missing values","text":"https://www.r-bloggers.com/imputing-missing-data--r-mice-package/looking dataset created modeling, obvious quite missing values.missing values outcome column want predict rest either remove entire row data impute missing information. decided try latter mice package.","code":"\nlibrary(mice)\n#> \n#> Attaching package: 'mice'\n#> The following objects are masked from 'package:base':\n#> \n#>     cbind, rbind\n\ndataset_impute  <- mice(dataset[, -1],  print = FALSE)  # remove outcome\n#> Warning: Number of logged events: 150\nimpute_complete <- mice::complete(dataset_impute, 1) # return 1st imputed dataset\nrownames(impute_complete) <- row.names(dataset)\nimpute_complete\n#>          age hospital gender_f province_Jiangsu province_Shanghai province_Zhejiang province_other days_onset_to_outcome days_onset_to_hospital early_onset early_outcome\n#> case_1    87        0        0                0                 1                 0              0                    13                      6           1             1\n#> case_2    27        1        0                0                 1                 0              0                    11                      4           1             1\n#> case_3    35        1        1                0                 0                 0              1                    31                     10           1             1\n#> case_4    45        1        1                1                 0                 0              0                    14                      8           1             1\n#> case_5    48        1        1                1                 0                 0              0                    57                     11           1             0\n#> case_6    32        1        1                1                 0                 0              0                    36                      7           1             1\n#> case_7    83        1        0                1                 0                 0              0                    20                      9           1             1\n#> case_8    38        1        0                0                 0                 1              0                    20                     11           1             1\n#> case_9    67        1        0                0                 0                 1              0                     7                      0           1             1\n#> case_10   48        1        0                0                 1                 0              0                     6                      4           1             1\n#> case_11   64        1        0                0                 0                 1              0                     6                      2           1             1\n#> case_12   52        0        1                0                 1                 0              0                     7                      7           1             1\n#> case_13   67        1        1                0                 1                 0              0                    12                      3           1             1\n#> case_14    4        0        0                0                 1                 0              0                    10                      5           1             1\n#> case_15   61        0        1                1                 0                 0              0                    32                      4           1             0\n#> case_16   79        0        0                1                 0                 0              0                    38                      4           1             0\n#> case_17   74        1        0                0                 1                 0              0                    14                      6           1             1\n#> case_18   66        1        0                0                 1                 0              0                    20                      4           1             1\n#> case_19   59        1        0                0                 1                 0              0                    67                      5           1             0\n#> case_20   55        1        0                0                 0                 0              1                    22                      4           1             1\n#> case_21   67        1        0                0                 1                 0              0                    23                      1           1             1\n#> case_22   85        1        0                1                 0                 0              0                    31                      4           1             0\n#> case_23   25        1        1                1                 0                 0              0                    46                      0           1             0\n#> case_24   64        0        0                0                 1                 0              0                     6                      5           1             1\n#> case_25   62        1        0                0                 1                 0              0                    35                      0           1             0\n#> case_26   77        1        0                0                 1                 0              0                    11                      4           1             1\n#> case_27   51        1        1                0                 0                 1              0                    37                     27           1             1\n#> case_28   79        0        0                0                 0                 1              0                    32                      0           1             0\n#> case_29   76        1        1                0                 1                 0              0                    32                      4           1             0\n#> case_30   81        0        1                0                 1                 0              0                    23                      0           1             0\n#> case_31   70        0        0                1                 0                 0              0                    32                      4           1             0\n#> case_32   74        0        0                1                 0                 0              0                    38                      4           1             0\n#> case_33   65        0        0                0                 0                 1              0                    24                      8           1             0\n#> case_34   74        1        0                0                 1                 0              0                    11                      5           1             1\n#> case_35   83        1        1                0                 1                 0              0                    38                      5           1             0\n#> case_36   68        0        0                0                 1                 0              0                    17                      4           1             1\n#> case_37   31        0        0                1                 0                 0              0                    45                      7           1             0\n#> case_38   56        0        0                1                 0                 0              0                    23                      7           1             0\n#> case_39   66        1        0                0                 0                 1              0                    13                      0           0             1\n#> case_40   74        1        0                0                 0                 1              0                     6                      5           1             1\n#> case_41   54        1        1                0                 0                 1              0                    23                      6           1             1\n#> case_42   53        1        0                0                 1                 0              0                    11                      7           1             1\n#> case_43   86        1        0                0                 1                 0              0                    18                      3           1             1\n#> case_44    7        1        1                0                 0                 0              1                     6                      0           0             1\n#> case_45   56        1        0                0                 1                 0              0                    86                      3           1             0\n#> case_46   77        0        1                1                 0                 0              0                     9                      0           1             1\n#> case_47   72        0        0                1                 0                 0              0                    32                      5           1             0\n#> case_48   65        1        0                0                 0                 1              0                    20                      6           1             1\n#> case_49   38        1        0                0                 0                 1              0                    28                      5           1             0\n#> case_50   34        1        0                0                 0                 0              1                    17                      3           1             0\n#> case_51   65        1        0                0                 0                 0              1                    15                      2           0             1\n#> case_52   64        0        1                0                 0                 1              0                    17                      7           1             1\n#> case_53   62        0        1                0                 0                 1              0                    22                      7           1             1\n#> case_54   75        0        0                0                 0                 1              0                    37                      7           1             0\n#> case_55   79        0        0                0                 0                 1              0                    18                      7           0             0\n#> case_56   73        1        0                0                 1                 0              0                    11                      6           1             1\n#> case_57   54        1        0                0                 1                 0              0                    13                      4           0             1\n#> case_58   78        1        0                0                 1                 0              0                    31                      4           1             0\n#> case_59   50        0        0                1                 0                 0              0                    37                      7           1             0\n#> case_60   26        0        0                1                 0                 0              0                    18                      4           0             1\n#> case_61   60        0        0                1                 0                 0              0                    10                      4           1             1\n#> case_62   68        0        1                0                 0                 1              0                    11                      5           1             1\n#> case_63   60        0        0                0                 0                 0              1                    26                      3           0             0\n#> case_64   56        0        0                1                 0                 0              0                    16                      4           0             0\n#> case_65   21        0        1                1                 0                 0              0                    37                      4           1             0\n#> case_66   72        0        0                1                 0                 0              0                    32                      3           0             0\n#> case_67   56        0        0                0                 0                 1              0                    31                      3           0             0\n#> case_68   57        0        0                0                 0                 1              0                     6                     10           0             1\n#> case_69   62        0        0                0                 0                 1              0                     8                      7           0             1\n#> case_70   58        0        1                0                 0                 1              0                    10                     11           0             1\n#> case_71   72        0        1                0                 0                 1              0                    13                      6           0             0\n#> case_72   47        1        0                0                 1                 0              0                    17                      5           0             0\n#> case_73   69        0        0                0                 1                 0              0                     7                      4           0             1\n#> case_74   54        0        0                0                 1                 0              0                    17                      6           1             0\n#> case_75   83        0        0                0                 1                 0              0                     6                      4           1             1\n#> case_76   55        0        0                0                 1                 0              0                     6                      7           1             1\n#> case_77    2        0        0                0                 1                 0              0                    13                      7           1             0\n#> case_78   89        1        0                0                 1                 0              0                    17                      4           0             0\n#> case_79   37        0        1                0                 0                 1              0                    16                      0           0             0\n#> case_80   74        0        0                0                 0                 1              0                    22                      3           0             0\n#> case_81   86        0        0                0                 0                 1              0                    10                      6           0             1\n#> case_82   41        0        0                0                 0                 1              0                     8                     10           0             1\n#> case_83   38        0        0                0                 0                 0              1                    23                      4           0             0\n#> case_84   26        0        1                1                 0                 0              0                    13                      1           1             1\n#> case_85   80        1        1                0                 1                 0              0                    13                      7           0             1\n#> case_86   54        0        1                0                 0                 1              0                    10                      3           0             1\n#> case_87   69        0        0                0                 0                 1              0                     8                      3           0             1\n#> case_88    4        0        0                0                 0                 0              1                    11                      0           0             0\n#> case_89   54        1        0                1                 0                 0              0                    36                      6           0             0\n#> case_90   43        1        0                0                 0                 1              0                     9                      3           0             1\n#> case_91   48        1        0                0                 0                 1              0                    16                      6           0             0\n#> case_92   66        1        1                0                 0                 1              0                    13                      7           0             1\n#> case_93   56        0        0                0                 0                 1              0                    17                      0           0             0\n#> case_94   35        0        1                0                 0                 1              0                    13                      6           0             0\n#> case_95   37        0        0                0                 0                 1              0                     7                      4           1             1\n#> case_96   43        0        0                1                 0                 0              0                    37                      4           1             0\n#> case_97   75        1        1                0                 1                 0              0                    22                      5           0             0\n#> case_98   76        0        0                0                 0                 1              0                    13                      3           0             1\n#> case_99   68        0        1                0                 0                 1              0                     6                      7           0             1\n#> case_100  58        0        0                0                 0                 1              0                    20                      7           0             0\n#> case_101  79        0        1                0                 0                 1              0                    13                      3           0             1\n#> case_102  81        0        0                0                 0                 1              0                    32                      6           0             0\n#> case_103  68        1        0                1                 0                 0              0                    37                      6           0             0\n#> case_104  54        0        1                0                 0                 1              0                    20                      7           0             0\n#> case_105  32        0        0                0                 0                 1              0                    22                      6           0             0\n#> case_106  36        1        0                0                 0                 0              1                    30                      6           0             0\n#> case_107  91        0        0                0                 0                 0              1                    21                     10           0             0\n#> case_108  84        0        0                0                 0                 1              0                    13                      3           0             0\n#> case_109  62        0        0                0                 0                 1              0                     6                      0           0             1\n#> case_110  53        1        0                0                 0                 0              1                     9                      4           0             0\n#> case_111  56        0        0                0                 0                 0              1                    22                      5           0             0\n#> case_112  69        0        0                0                 0                 0              1                    10                      1           0             0\n#> case_113  60        0        1                0                 0                 1              0                    13                      3           0             0\n#> case_114  50        0        1                0                 0                 1              0                    10                      6           0             1\n#> case_115  38        0        0                0                 0                 1              0                    16                      5           0             0\n#> case_116  65        1        0                0                 0                 0              1                    21                      5           0             0\n#> case_117  76        0        1                0                 0                 0              1                    28                      5           0             0\n#> case_118  49        0        0                1                 0                 0              0                    18                      5           0             1\n#> case_119  36        0        0                1                 0                 0              0                    26                      4           0             0\n#> case_120  60        0        0                1                 0                 0              0                    31                      7           1             0\n#> case_121  64        1        1                0                 0                 0              1                    30                      5           0             0\n#> case_122  38        0        0                0                 0                 1              0                     2                      3           0             1\n#> case_123  54        1        0                0                 0                 0              1                    16                      7           0             0\n#> case_124  80        0        0                0                 0                 0              1                    17                      7           0             0\n#> case_125  31        0        1                0                 0                 0              1                    20                      0           0             0\n#> case_126  80        1        0                0                 0                 0              1                    24                     10           0             0\n#> case_127   4        0        0                0                 0                 0              1                     9                      4           0             0\n#> case_128  58        1        0                0                 0                 0              1                    22                      7           0             0\n#> case_129  69        1        0                0                 0                 0              1                    28                      4           0             0\n#> case_130  69        1        0                0                 0                 0              1                    30                      1           0             0\n#> case_131   9        1        0                0                 0                 0              1                    11                      1           0             0\n#> case_132  79        1        1                0                 0                 0              1                    18                      0           0             1\n#> case_133   6        1        0                0                 0                 0              1                     2                      0           0             0\n#> case_134  15        1        0                1                 0                 0              0                     7                      1           0             0\n#> case_135  61        1        1                0                 0                 0              1                    32                      3           0             0\n#> case_136  51        1        1                0                 0                 0              1                     8                      1           0             1\ndataset_complete <- merge(dataset[, 1, drop = FALSE], \n                          # mice::complete(dataset_impute, 1), \n                          impute_complete,\n                          by = \"row.names\", all = TRUE)\nrownames(dataset_complete) <- dataset_complete$Row.names\ndataset_complete <- dataset_complete[, -1]\ndataset_complete\n#>          outcome age hospital gender_f province_Jiangsu province_Shanghai province_Zhejiang province_other days_onset_to_outcome days_onset_to_hospital early_onset early_outcome\n#> case_1     Death  87        0        0                0                 1                 0              0                    13                      6           1             1\n#> case_10    Death  48        1        0                0                 1                 0              0                     6                      4           1             1\n#> case_100    <NA>  58        0        0                0                 0                 1              0                    20                      7           0             0\n#> case_101    <NA>  79        0        1                0                 0                 1              0                    13                      3           0             1\n#> case_102    <NA>  81        0        0                0                 0                 1              0                    32                      6           0             0\n#> case_103    <NA>  68        1        0                1                 0                 0              0                    37                      6           0             0\n#> case_104    <NA>  54        0        1                0                 0                 1              0                    20                      7           0             0\n#> case_105    <NA>  32        0        0                0                 0                 1              0                    22                      6           0             0\n#> case_106 Recover  36        1        0                0                 0                 0              1                    30                      6           0             0\n#> case_107   Death  91        0        0                0                 0                 0              1                    21                     10           0             0\n#> case_108    <NA>  84        0        0                0                 0                 1              0                    13                      3           0             0\n#> case_109    <NA>  62        0        0                0                 0                 1              0                     6                      0           0             1\n#> case_11    Death  64        1        0                0                 0                 1              0                     6                      2           1             1\n#> case_110    <NA>  53        1        0                0                 0                 0              1                     9                      4           0             0\n#> case_111   Death  56        0        0                0                 0                 0              1                    22                      5           0             0\n#> case_112    <NA>  69        0        0                0                 0                 0              1                    10                      1           0             0\n#> case_113    <NA>  60        0        1                0                 0                 1              0                    13                      3           0             0\n#> case_114    <NA>  50        0        1                0                 0                 1              0                    10                      6           0             1\n#> case_115    <NA>  38        0        0                0                 0                 1              0                    16                      5           0             0\n#> case_116 Recover  65        1        0                0                 0                 0              1                    21                      5           0             0\n#> case_117 Recover  76        0        1                0                 0                 0              1                    28                      5           0             0\n#> case_118    <NA>  49        0        0                1                 0                 0              0                    18                      5           0             1\n#> case_119 Recover  36        0        0                1                 0                 0              0                    26                      4           0             0\n#> case_12    Death  52        0        1                0                 1                 0              0                     7                      7           1             1\n#> case_120    <NA>  60        0        0                1                 0                 0              0                    31                      7           1             0\n#> case_121   Death  64        1        1                0                 0                 0              1                    30                      5           0             0\n#> case_122    <NA>  38        0        0                0                 0                 1              0                     2                      3           0             1\n#> case_123   Death  54        1        0                0                 0                 0              1                    16                      7           0             0\n#> case_124 Recover  80        0        0                0                 0                 0              1                    17                      7           0             0\n#> case_125 Recover  31        0        1                0                 0                 0              1                    20                      0           0             0\n#> case_126    <NA>  80        1        0                0                 0                 0              1                    24                     10           0             0\n#> case_127 Recover   4        0        0                0                 0                 0              1                     9                      4           0             0\n#> case_128 Recover  58        1        0                0                 0                 0              1                    22                      7           0             0\n#> case_129 Recover  69        1        0                0                 0                 0              1                    28                      4           0             0\n#> case_13    Death  67        1        1                0                 1                 0              0                    12                      3           1             1\n#> case_130    <NA>  69        1        0                0                 0                 0              1                    30                      1           0             0\n#> case_131 Recover   9        1        0                0                 0                 0              1                    11                      1           0             0\n#> case_132    <NA>  79        1        1                0                 0                 0              1                    18                      0           0             1\n#> case_133 Recover   6        1        0                0                 0                 0              1                     2                      0           0             0\n#> case_134 Recover  15        1        0                1                 0                 0              0                     7                      1           0             0\n#> case_135   Death  61        1        1                0                 0                 0              1                    32                      3           0             0\n#> case_136    <NA>  51        1        1                0                 0                 0              1                     8                      1           0             1\n#> case_14  Recover   4        0        0                0                 1                 0              0                    10                      5           1             1\n#> case_15     <NA>  61        0        1                1                 0                 0              0                    32                      4           1             0\n#> case_16     <NA>  79        0        0                1                 0                 0              0                    38                      4           1             0\n#> case_17    Death  74        1        0                0                 1                 0              0                    14                      6           1             1\n#> case_18  Recover  66        1        0                0                 1                 0              0                    20                      4           1             1\n#> case_19    Death  59        1        0                0                 1                 0              0                    67                      5           1             0\n#> case_2     Death  27        1        0                0                 1                 0              0                    11                      4           1             1\n#> case_20  Recover  55        1        0                0                 0                 0              1                    22                      4           1             1\n#> case_21  Recover  67        1        0                0                 1                 0              0                    23                      1           1             1\n#> case_22     <NA>  85        1        0                1                 0                 0              0                    31                      4           1             0\n#> case_23  Recover  25        1        1                1                 0                 0              0                    46                      0           1             0\n#> case_24    Death  64        0        0                0                 1                 0              0                     6                      5           1             1\n#> case_25  Recover  62        1        0                0                 1                 0              0                    35                      0           1             0\n#> case_26    Death  77        1        0                0                 1                 0              0                    11                      4           1             1\n#> case_27  Recover  51        1        1                0                 0                 1              0                    37                     27           1             1\n#> case_28     <NA>  79        0        0                0                 0                 1              0                    32                      0           1             0\n#> case_29  Recover  76        1        1                0                 1                 0              0                    32                      4           1             0\n#> case_3     Death  35        1        1                0                 0                 0              1                    31                     10           1             1\n#> case_30  Recover  81        0        1                0                 1                 0              0                    23                      0           1             0\n#> case_31     <NA>  70        0        0                1                 0                 0              0                    32                      4           1             0\n#> case_32     <NA>  74        0        0                1                 0                 0              0                    38                      4           1             0\n#> case_33  Recover  65        0        0                0                 0                 1              0                    24                      8           1             0\n#> case_34    Death  74        1        0                0                 1                 0              0                    11                      5           1             1\n#> case_35    Death  83        1        1                0                 1                 0              0                    38                      5           1             0\n#> case_36  Recover  68        0        0                0                 1                 0              0                    17                      4           1             1\n#> case_37  Recover  31        0        0                1                 0                 0              0                    45                      7           1             0\n#> case_38     <NA>  56        0        0                1                 0                 0              0                    23                      7           1             0\n#> case_39     <NA>  66        1        0                0                 0                 1              0                    13                      0           0             1\n#> case_4      <NA>  45        1        1                1                 0                 0              0                    14                      8           1             1\n#> case_40     <NA>  74        1        0                0                 0                 1              0                     6                      5           1             1\n#> case_41     <NA>  54        1        1                0                 0                 1              0                    23                      6           1             1\n#> case_42     <NA>  53        1        0                0                 1                 0              0                    11                      7           1             1\n#> case_43    Death  86        1        0                0                 1                 0              0                    18                      3           1             1\n#> case_44  Recover   7        1        1                0                 0                 0              1                     6                      0           0             1\n#> case_45    Death  56        1        0                0                 1                 0              0                    86                      3           1             0\n#> case_46    Death  77        0        1                1                 0                 0              0                     9                      0           1             1\n#> case_47     <NA>  72        0        0                1                 0                 0              0                    32                      5           1             0\n#> case_48     <NA>  65        1        0                0                 0                 1              0                    20                      6           1             1\n#> case_49  Recover  38        1        0                0                 0                 1              0                    28                      5           1             0\n#> case_5   Recover  48        1        1                1                 0                 0              0                    57                     11           1             0\n#> case_50  Recover  34        1        0                0                 0                 0              1                    17                      3           1             0\n#> case_51  Recover  65        1        0                0                 0                 0              1                    15                      2           0             1\n#> case_52     <NA>  64        0        1                0                 0                 1              0                    17                      7           1             1\n#> case_53    Death  62        0        1                0                 0                 1              0                    22                      7           1             1\n#> case_54     <NA>  75        0        0                0                 0                 1              0                    37                      7           1             0\n#> case_55  Recover  79        0        0                0                 0                 1              0                    18                      7           0             0\n#> case_56     <NA>  73        1        0                0                 1                 0              0                    11                      6           1             1\n#> case_57  Recover  54        1        0                0                 1                 0              0                    13                      4           0             1\n#> case_58  Recover  78        1        0                0                 1                 0              0                    31                      4           1             0\n#> case_59  Recover  50        0        0                1                 0                 0              0                    37                      7           1             0\n#> case_6     Death  32        1        1                1                 0                 0              0                    36                      7           1             1\n#> case_60  Recover  26        0        0                1                 0                 0              0                    18                      4           0             1\n#> case_61    Death  60        0        0                1                 0                 0              0                    10                      4           1             1\n#> case_62     <NA>  68        0        1                0                 0                 1              0                    11                      5           1             1\n#> case_63     <NA>  60        0        0                0                 0                 0              1                    26                      3           0             0\n#> case_64  Recover  56        0        0                1                 0                 0              0                    16                      4           0             0\n#> case_65  Recover  21        0        1                1                 0                 0              0                    37                      4           1             0\n#> case_66     <NA>  72        0        0                1                 0                 0              0                    32                      3           0             0\n#> case_67     <NA>  56        0        0                0                 0                 1              0                    31                      3           0             0\n#> case_68     <NA>  57        0        0                0                 0                 1              0                     6                     10           0             1\n#> case_69     <NA>  62        0        0                0                 0                 1              0                     8                      7           0             1\n#> case_7     Death  83        1        0                1                 0                 0              0                    20                      9           1             1\n#> case_70     <NA>  58        0        1                0                 0                 1              0                    10                     11           0             1\n#> case_71     <NA>  72        0        1                0                 0                 1              0                    13                      6           0             0\n#> case_72  Recover  47        1        0                0                 1                 0              0                    17                      5           0             0\n#> case_73  Recover  69        0        0                0                 1                 0              0                     7                      4           0             1\n#> case_74  Recover  54        0        0                0                 1                 0              0                    17                      6           1             0\n#> case_75    Death  83        0        0                0                 1                 0              0                     6                      4           1             1\n#> case_76    Death  55        0        0                0                 1                 0              0                     6                      7           1             1\n#> case_77  Recover   2        0        0                0                 1                 0              0                    13                      7           1             0\n#> case_78    Death  89        1        0                0                 1                 0              0                    17                      4           0             0\n#> case_79  Recover  37        0        1                0                 0                 1              0                    16                      0           0             0\n#> case_8     Death  38        1        0                0                 0                 1              0                    20                     11           1             1\n#> case_80     <NA>  74        0        0                0                 0                 1              0                    22                      3           0             0\n#> case_81    Death  86        0        0                0                 0                 1              0                    10                      6           0             1\n#> case_82  Recover  41        0        0                0                 0                 1              0                     8                     10           0             1\n#> case_83  Recover  38        0        0                0                 0                 0              1                    23                      4           0             0\n#> case_84     <NA>  26        0        1                1                 0                 0              0                    13                      1           1             1\n#> case_85     <NA>  80        1        1                0                 1                 0              0                    13                      7           0             1\n#> case_86     <NA>  54        0        1                0                 0                 1              0                    10                      3           0             1\n#> case_87    Death  69        0        0                0                 0                 1              0                     8                      3           0             1\n#> case_88     <NA>   4        0        0                0                 0                 0              1                    11                      0           0             0\n#> case_89  Recover  54        1        0                1                 0                 0              0                    36                      6           0             0\n#> case_9      <NA>  67        1        0                0                 0                 1              0                     7                      0           1             1\n#> case_90     <NA>  43        1        0                0                 0                 1              0                     9                      3           0             1\n#> case_91  Recover  48        1        0                0                 0                 1              0                    16                      6           0             0\n#> case_92     <NA>  66        1        1                0                 0                 1              0                    13                      7           0             1\n#> case_93     <NA>  56        0        0                0                 0                 1              0                    17                      0           0             0\n#> case_94  Recover  35        0        1                0                 0                 1              0                    13                      6           0             0\n#> case_95     <NA>  37        0        0                0                 0                 1              0                     7                      4           1             1\n#> case_96     <NA>  43        0        0                1                 0                 0              0                    37                      4           1             0\n#> case_97  Recover  75        1        1                0                 1                 0              0                    22                      5           0             0\n#> case_98    Death  76        0        0                0                 0                 1              0                    13                      3           0             1\n#> case_99     <NA>  68        0        1                0                 0                 1              0                     6                      7           0             1\ncat(\"NAs before imput: \", sum(is.na(dataset)), \"\\n\")\n#> NAs before imput:  277\nsummary(dataset$outcome)\n#>   Death Recover    NA's \n#>      32      47      57\ncat(\"NAs after imput: \", sum(is.na(dataset_complete)), \"\\n\")\n#> NAs after imput:  57\nsummary(dataset_complete$outcome)\n#>   Death Recover    NA's \n#>      32      47      57"},{"path":"predicting-flu-outcome-comparing-eight-classification-algorithms.html","id":"test-train-and-validation-data-sets-1","chapter":"28 Predicting Flu outcome comparing eight classification algorithms","heading":"28.4 Test, train and validation data sets","text":"building model, separating imputed data frame training test data. Test data 57 cases unknown outcome.training data divided validation models: 70% training data kept model building remaining 30% used model testing.using caret package modeling:","code":"\ntrain_index <- which(is.na(dataset_complete$outcome))\ntrain_data <- dataset_complete[-train_index, ]     # 79x12\ntest_data  <- dataset_complete[train_index, -1]    # 57x11\n\nlibrary(caret)\n#> Loading required package: lattice\nset.seed(27)\nval_index <- createDataPartition(train_data$outcome, p = 0.7, list=FALSE)\nval_train_data <- train_data[val_index, ]\nval_test_data  <- train_data[-val_index, ]\nval_train_X <- val_train_data[,-1]\nval_test_X <- val_test_data[,-1]"},{"path":"predicting-flu-outcome-comparing-eight-classification-algorithms.html","id":"decision-trees","chapter":"28 Predicting Flu outcome comparing eight classification algorithms","heading":"28.4.1 Decision trees","text":"get idea feature contributes prediction outcome, first built decision tree based training data using rpart rattle.randomly generated decision tree shows cases early outcome likely die 68 older, also early onset sick fewer 13 days. person among first cases younger 52, good chance recovering, 82 older, likely die flu.","code":"\nlibrary(rpart)\nlibrary(rattle)\n#> Rattle: A free graphical interface for data science with R.\n#> Version 5.3.0 Copyright (c) 2006-2018 Togaware Pty Ltd.\n#> Type 'rattle()' to shake, rattle, and roll your data.\nlibrary(rpart.plot)\nlibrary(RColorBrewer)\n\nset.seed(27)\nfit <- rpart(outcome ~ ., data = train_data, method = \"class\", \n             control = rpart.control(xval = 10, minbucket = 2, cp = 0), \n             parms = list(split = \"information\"))\n\nfancyRpartPlot(fit)"},{"path":"predicting-flu-outcome-comparing-eight-classification-algorithms.html","id":"feature-importance-1","chapter":"28 Predicting Flu outcome comparing eight classification algorithms","heading":"28.4.2 Feature Importance","text":"features created equally important model. decision tree already gave idea features might important also want estimate feature importance using Random Forest approach repeated cross validation.tells age important determining factor predicting disease outcome, followed days onset outcome, early outcome days onset hospitalization.","code":"\n# prepare training scheme\ncontrol <- trainControl(method = \"repeatedcv\", number = 10, repeats = 10)\n\n# train the model\nset.seed(27)\nmodel <- train(outcome ~ ., data = train_data, method = \"rf\", \n               preProcess = NULL, trControl = control)\n\n# estimate variable importance\nimportance <- varImp(model, scale=TRUE)\nfrom <- c(\"age\", \"hospital1\", \"gender_f1\", \"province_Jiangsu1\", \n          \"province_Shanghai1\", \"province_Zhejiang1\", \"province_other1\", \n          \"days_onset_to_outcome\", \"days_onset_to_hospital\", \"early_onset1\", \n          \"early_outcome1\" )\n\nto <- c(\"Age\", \"Hospital\", \"Female\", \"Jiangsu\", \"Shanghai\", \"Zhejiang\",\n        \"Other province\", \"Days onset to outcome\", \"Days onset to hospital\", \n        \"Early onset\", \"Early outcome\" )\n\nimportance_df_1 <- importance$importance\nimportance_df_1$group <- rownames(importance_df_1)\n\nimportance_df_1$group <- mapvalues(importance_df_1$group, \n                                           from = from, \n                                           to = to)\nf = importance_df_1[order(importance_df_1$Overall, decreasing = FALSE), \"group\"]\n\nimportance_df_2 <- importance_df_1\nimportance_df_2$Overall <- 0\n\nimportance_df <- rbind(importance_df_1, importance_df_2)\n\n# setting factor levels\nimportance_df <- within(importance_df, group <- factor(group, levels = f))\nimportance_df_1 <- within(importance_df_1, group <- factor(group, levels = f))\n\nggplot() +\n  geom_point(data = importance_df_1, aes(x = Overall, y = group, \n                                         color = group), size = 2) +\n  geom_path(data = importance_df, aes(x = Overall, y = group, color = group, \n                                      group = group), size = 1) +\n  scale_color_manual(values = rep(brewer.pal(1, \"Set1\")[1], 11)) +\n  my_theme() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 0, vjust = 0.5, hjust = 0.5)) +\n  labs(\n    x = \"Importance\",\n    y = \"\",\n    title = \"2013 Influenza A H7N9 cases in China\",\n    subtitle = \"Scaled feature importance\",\n    caption = \"\\nDetermined with Random Forest and\n    repeated cross validation (10 repeats, 10 times)\"\n  )\n#> Warning in brewer.pal(1, \"Set1\"): minimal value for n is 3, returning requested palette with 3 different levels"},{"path":"predicting-flu-outcome-comparing-eight-classification-algorithms.html","id":"feature-plot","chapter":"28 Predicting Flu outcome comparing eight classification algorithms","heading":"28.4.3 Feature Plot","text":"start actually building models, want check whether distribution feature values comparable training, validation test datasets.","code":"\n# tidy dataframe of 11 variables for plotting features for all datasets\n# dataset_complete: 136x12. test + val_train + val_test\ndataset_complete_gather <- dataset_complete %>%\n  mutate(set = ifelse(rownames(dataset_complete) %in% rownames(test_data), \n                      \"Test Data\", \n               ifelse(rownames(dataset_complete) %in% rownames(val_train_data), \n                      \"Validation Train Data\",\n               ifelse(rownames(dataset_complete) %in% rownames(val_test_data), \n                      \"Validation Test Data\", \"NA\"))),\n         case_ID = rownames(.)) %>%\n  gather(group, value, age:early_outcome)\n#> Warning: attributes are not identical across measure variables;\n#> they will be dropped\n\n# map values in group to more readable\nfrom <- c(\"age\", \"hospital\", \"gender_f\", \"province_Jiangsu\", \"province_Shanghai\", \n          \"province_Zhejiang\", \"province_other\", \"days_onset_to_outcome\", \n          \"days_onset_to_hospital\", \"early_onset\",  \"early_outcome\" )\nto <- c(\"Age\", \"Hospital\", \"Female\", \"Jiangsu\", \"Shanghai\", \"Zhejiang\", \n        \"Other province\", \"Days onset to outcome\", \"Days onset to hospital\", \n        \"Early onset\", \"Early outcome\" )\ndataset_complete_gather$group <- mapvalues(dataset_complete_gather$group, \n                      from = from, \n                      to = to)"},{"path":"predicting-flu-outcome-comparing-eight-classification-algorithms.html","id":"plot-distribution-of-features-in-each-dataset","chapter":"28 Predicting Flu outcome comparing eight classification algorithms","heading":"28.4.4 Plot distribution of features in each dataset","text":"","code":"\n# plot features all datasets\nggplot(data = dataset_complete_gather, \n       aes(x = as.numeric(value), fill = outcome, color = outcome)) +\n  geom_density(alpha = 0.2) +\n  geom_rug() +\n  scale_color_brewer(palette=\"Set1\", na.value = \"grey50\") +\n  scale_fill_brewer(palette=\"Set1\", na.value = \"grey50\") +\n  my_theme() +\n  facet_wrap(set ~ group, ncol = 11, scales = \"free\") +\n  labs(\n    x = \"Value\",\n    y = \"Density\",\n    title = \"2013 Influenza A H7N9 cases in China\",\n    subtitle = \"Features for classifying outcome\",\n    caption = \"\\nDensity distribution of all features used for \n    classification of flu outcome.\"\n  )"},{"path":"predicting-flu-outcome-comparing-eight-classification-algorithms.html","id":"plot-3-features-vs-outcome-all-datasets","chapter":"28 Predicting Flu outcome comparing eight classification algorithms","heading":"28.4.5 Plot 3 features vs outcome, all datasets","text":"Luckily, distributions looks reasonably similar validation test data, except outliers.","code":"\n# plot three groups vs outcome\nggplot(subset(dataset_complete_gather, \n              group == \"Age\" | \n              group == \"Days onset to hospital\" | \n              group == \"Days onset to outcome\"), \n       aes(x=outcome, y=as.numeric(value), fill=set)) + \n  geom_boxplot() +\n  my_theme() +\n  scale_fill_brewer(palette=\"Set1\", type = \"div \") +\n  facet_wrap( ~ group, ncol = 3, scales = \"free\") +\n  labs(\n    fill = \"\",\n    x = \"Outcome\",\n    y = \"Value\",\n    title = \"2013 Influenza A H7N9 cases in China\",\n    subtitle = \"Features for classifying outcome\",\n    caption = \"\\nBoxplot of the features age, days from onset to \n    hospitalisation and days from onset to outcome.\"\n  )"},{"path":"predicting-flu-outcome-comparing-eight-classification-algorithms.html","id":"comparing-machine-learning-algorithms","chapter":"28 Predicting Flu outcome comparing eight classification algorithms","heading":"28.5 Comparing Machine Learning algorithms","text":"try predict outcome unknown cases, testing models’ accuracy validation datasets couple algorithms. chosen well known algorithms, caret implements many .chosen preprocessing worried different data distributions continuous variables (e.g. age) binary variables (.e. 0, 1 classification e.g. hospitalisation) lead problems.Random ForestGLM netk-Nearest NeighborsPenalized Discriminant AnalysisStabilized Linear Discriminant AnalysisNearest Shrunken CentroidsSingle C5.0 TreePartial Least Squares","code":"\ntrain_control <- trainControl(method = \"repeatedcv\", \n                              number = 10, \n                              repeats = 10, \n                              verboseIter = FALSE)"},{"path":"predicting-flu-outcome-comparing-eight-classification-algorithms.html","id":"random-forest-2","chapter":"28 Predicting Flu outcome comparing eight classification algorithms","heading":"28.5.1 Random Forest","text":"Random Forests predictions based generation multiple classification trees. model classified 14 23 cases correctly.","code":"\nset.seed(27)\nmodel_rf <- caret::train(outcome ~ .,\n                             data = val_train_data,\n                             method = \"rf\",\n                             preProcess = NULL,\n                             trControl = train_control)\nmodel_rf\n#> Random Forest \n#> \n#> 56 samples\n#> 11 predictors\n#>  2 classes: 'Death', 'Recover' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold, repeated 10 times) \n#> Summary of sample sizes: 51, 49, 50, 51, 49, 51, ... \n#> Resampling results across tuning parameters:\n#> \n#>   mtry  Accuracy  Kappa\n#>    2    0.687     0.340\n#>    6    0.732     0.432\n#>   11    0.726     0.423\n#> \n#> Accuracy was used to select the optimal model using the largest value.\n#> The final value used for the model was mtry = 6.\nconfusionMatrix(predict(model_rf, val_test_data[, -1]), val_test_data$outcome)\n#> Confusion Matrix and Statistics\n#> \n#>           Reference\n#> Prediction Death Recover\n#>    Death       3       0\n#>    Recover     6      14\n#>                                         \n#>                Accuracy : 0.739         \n#>                  95% CI : (0.516, 0.898)\n#>     No Information Rate : 0.609         \n#>     P-Value [Acc > NIR] : 0.1421        \n#>                                         \n#>                   Kappa : 0.378         \n#>                                         \n#>  Mcnemar's Test P-Value : 0.0412        \n#>                                         \n#>             Sensitivity : 0.333         \n#>             Specificity : 1.000         \n#>          Pos Pred Value : 1.000         \n#>          Neg Pred Value : 0.700         \n#>              Prevalence : 0.391         \n#>          Detection Rate : 0.130         \n#>    Detection Prevalence : 0.130         \n#>       Balanced Accuracy : 0.667         \n#>                                         \n#>        'Positive' Class : Death         \n#> "},{"path":"predicting-flu-outcome-comparing-eight-classification-algorithms.html","id":"glm-net","chapter":"28 Predicting Flu outcome comparing eight classification algorithms","heading":"28.5.2 GLM net","text":"Lasso elastic net regularization generalized linear model regression based linear regression models useful feature correlation model.model classified 13 23 cases correctly.","code":"\nset.seed(27)\nmodel_glmnet <- caret::train(outcome ~ .,\n                             data = val_train_data,\n                             method = \"glmnet\",\n                             preProcess = NULL,\n                             trControl = train_control)\nmodel_glmnet\n#> glmnet \n#> \n#> 56 samples\n#> 11 predictors\n#>  2 classes: 'Death', 'Recover' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold, repeated 10 times) \n#> Summary of sample sizes: 51, 49, 50, 51, 49, 51, ... \n#> Resampling results across tuning parameters:\n#> \n#>   alpha  lambda    Accuracy  Kappa\n#>   0.10   0.000491  0.671     0.324\n#>   0.10   0.004909  0.669     0.318\n#>   0.10   0.049093  0.680     0.339\n#>   0.55   0.000491  0.671     0.324\n#>   0.55   0.004909  0.671     0.322\n#>   0.55   0.049093  0.695     0.365\n#>   1.00   0.000491  0.671     0.324\n#>   1.00   0.004909  0.672     0.326\n#>   1.00   0.049093  0.714     0.414\n#> \n#> Accuracy was used to select the optimal model using the largest value.\n#> The final values used for the model were alpha = 1 and lambda = 0.0491.\nconfusionMatrix(predict(model_glmnet, val_test_data[, -1]), val_test_data$outcome)\n#> Confusion Matrix and Statistics\n#> \n#>           Reference\n#> Prediction Death Recover\n#>    Death       3       2\n#>    Recover     6      12\n#>                                         \n#>                Accuracy : 0.652         \n#>                  95% CI : (0.427, 0.836)\n#>     No Information Rate : 0.609         \n#>     P-Value [Acc > NIR] : 0.422         \n#>                                         \n#>                   Kappa : 0.207         \n#>                                         \n#>  Mcnemar's Test P-Value : 0.289         \n#>                                         \n#>             Sensitivity : 0.333         \n#>             Specificity : 0.857         \n#>          Pos Pred Value : 0.600         \n#>          Neg Pred Value : 0.667         \n#>              Prevalence : 0.391         \n#>          Detection Rate : 0.130         \n#>    Detection Prevalence : 0.217         \n#>       Balanced Accuracy : 0.595         \n#>                                         \n#>        'Positive' Class : Death         \n#> "},{"path":"predicting-flu-outcome-comparing-eight-classification-algorithms.html","id":"k-nearest-neighbors","chapter":"28 Predicting Flu outcome comparing eight classification algorithms","heading":"28.5.3 k-Nearest Neighbors","text":"k-nearest neighbors predicts based point distances predefined constants. model classified 14 23 cases correctly.","code":"\nset.seed(27)\nmodel_kknn <- caret::train(outcome ~ .,\n                             data = val_train_data,\n                             method = \"kknn\",\n                             preProcess = NULL,\n                             trControl = train_control)\nmodel_kknn\n#> k-Nearest Neighbors \n#> \n#> 56 samples\n#> 11 predictors\n#>  2 classes: 'Death', 'Recover' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold, repeated 10 times) \n#> Summary of sample sizes: 51, 49, 50, 51, 49, 51, ... \n#> Resampling results across tuning parameters:\n#> \n#>   kmax  Accuracy  Kappa\n#>   5     0.666     0.313\n#>   7     0.653     0.274\n#>   9     0.648     0.263\n#> \n#> Tuning parameter 'distance' was held constant at a value of 2\n#> Tuning parameter 'kernel' was held constant at a value of optimal\n#> Accuracy was used to select the optimal model using the largest value.\n#> The final values used for the model were kmax = 5, distance = 2 and kernel = optimal.\nconfusionMatrix(predict(model_kknn, val_test_data[, -1]), val_test_data$outcome)\n#> Confusion Matrix and Statistics\n#> \n#>           Reference\n#> Prediction Death Recover\n#>    Death       5       3\n#>    Recover     4      11\n#>                                         \n#>                Accuracy : 0.696         \n#>                  95% CI : (0.471, 0.868)\n#>     No Information Rate : 0.609         \n#>     P-Value [Acc > NIR] : 0.264         \n#>                                         \n#>                   Kappa : 0.348         \n#>                                         \n#>  Mcnemar's Test P-Value : 1.000         \n#>                                         \n#>             Sensitivity : 0.556         \n#>             Specificity : 0.786         \n#>          Pos Pred Value : 0.625         \n#>          Neg Pred Value : 0.733         \n#>              Prevalence : 0.391         \n#>          Detection Rate : 0.217         \n#>    Detection Prevalence : 0.348         \n#>       Balanced Accuracy : 0.671         \n#>                                         \n#>        'Positive' Class : Death         \n#> "},{"path":"predicting-flu-outcome-comparing-eight-classification-algorithms.html","id":"penalized-discriminant-analysis","chapter":"28 Predicting Flu outcome comparing eight classification algorithms","heading":"28.5.4 Penalized Discriminant Analysis","text":"Penalized Discriminant Analysis penalized linear discriminant analysis also useful highly correlated features. model classified 14 23 cases correctly.","code":"\nset.seed(27)\nmodel_pda <- caret::train(outcome ~ .,\n                             data = val_train_data,\n                             method = \"pda\",\n                             preProcess = NULL,\n                             trControl = train_control)\nmodel_pda\n#> Penalized Discriminant Analysis \n#> \n#> 56 samples\n#> 11 predictors\n#>  2 classes: 'Death', 'Recover' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold, repeated 10 times) \n#> Summary of sample sizes: 51, 49, 50, 51, 49, 51, ... \n#> Resampling results across tuning parameters:\n#> \n#>   lambda  Accuracy  Kappa\n#>   0e+00     NaN       NaN\n#>   1e-04   0.681     0.343\n#>   1e-01   0.681     0.343\n#> \n#> Accuracy was used to select the optimal model using the largest value.\n#> The final value used for the model was lambda = 1e-04.\nconfusionMatrix(predict(model_pda, val_test_data[, -1]), val_test_data$outcome)\n#> Confusion Matrix and Statistics\n#> \n#>           Reference\n#> Prediction Death Recover\n#>    Death       3       2\n#>    Recover     6      12\n#>                                         \n#>                Accuracy : 0.652         \n#>                  95% CI : (0.427, 0.836)\n#>     No Information Rate : 0.609         \n#>     P-Value [Acc > NIR] : 0.422         \n#>                                         \n#>                   Kappa : 0.207         \n#>                                         \n#>  Mcnemar's Test P-Value : 0.289         \n#>                                         \n#>             Sensitivity : 0.333         \n#>             Specificity : 0.857         \n#>          Pos Pred Value : 0.600         \n#>          Neg Pred Value : 0.667         \n#>              Prevalence : 0.391         \n#>          Detection Rate : 0.130         \n#>    Detection Prevalence : 0.217         \n#>       Balanced Accuracy : 0.595         \n#>                                         \n#>        'Positive' Class : Death         \n#> "},{"path":"predicting-flu-outcome-comparing-eight-classification-algorithms.html","id":"stabilized-linear-discriminant-analysis","chapter":"28 Predicting Flu outcome comparing eight classification algorithms","heading":"28.5.5 Stabilized Linear Discriminant Analysis","text":"Stabilized Linear Discriminant Analysis designed high-dimensional data correlated co-variables. model classified 15 23 cases correctly.","code":"\nset.seed(27)\nmodel_slda <- caret::train(outcome ~ .,\n                             data = val_train_data,\n                             method = \"slda\",\n                             preProcess = NULL,\n                             trControl = train_control)\nmodel_slda\n#> Stabilized Linear Discriminant Analysis \n#> \n#> 56 samples\n#> 11 predictors\n#>  2 classes: 'Death', 'Recover' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold, repeated 10 times) \n#> Summary of sample sizes: 51, 49, 50, 51, 49, 51, ... \n#> Resampling results:\n#> \n#>   Accuracy  Kappa\n#>   0.682     0.358\nconfusionMatrix(predict(model_slda, val_test_data[, -1]), val_test_data$outcome)\n#> Confusion Matrix and Statistics\n#> \n#>           Reference\n#> Prediction Death Recover\n#>    Death       3       3\n#>    Recover     6      11\n#>                                         \n#>                Accuracy : 0.609         \n#>                  95% CI : (0.385, 0.803)\n#>     No Information Rate : 0.609         \n#>     P-Value [Acc > NIR] : 0.590         \n#>                                         \n#>                   Kappa : 0.127         \n#>                                         \n#>  Mcnemar's Test P-Value : 0.505         \n#>                                         \n#>             Sensitivity : 0.333         \n#>             Specificity : 0.786         \n#>          Pos Pred Value : 0.500         \n#>          Neg Pred Value : 0.647         \n#>              Prevalence : 0.391         \n#>          Detection Rate : 0.130         \n#>    Detection Prevalence : 0.261         \n#>       Balanced Accuracy : 0.560         \n#>                                         \n#>        'Positive' Class : Death         \n#> "},{"path":"predicting-flu-outcome-comparing-eight-classification-algorithms.html","id":"nearest-shrunken-centroids","chapter":"28 Predicting Flu outcome comparing eight classification algorithms","heading":"28.5.6 Nearest Shrunken Centroids","text":"Nearest Shrunken Centroids computes standardized centroid class shrinks centroid toward overall centroid classes. model classified 15 23 cases correctly.","code":"\nset.seed(27)\nmodel_pam <- caret::train(outcome ~ .,\n                             data = val_train_data,\n                             method = \"pam\",\n                             preProcess = NULL,\n                             trControl = train_control)\n#> 12345678910111213141516171819202122232425262728293011111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\nmodel_pam\n#> Nearest Shrunken Centroids \n#> \n#> 56 samples\n#> 11 predictors\n#>  2 classes: 'Death', 'Recover' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold, repeated 10 times) \n#> Summary of sample sizes: 51, 49, 50, 51, 49, 51, ... \n#> Resampling results across tuning parameters:\n#> \n#>   threshold  Accuracy  Kappa\n#>   0.142      0.709     0.416\n#>   2.065      0.714     0.382\n#>   3.987      0.590     0.000\n#> \n#> Accuracy was used to select the optimal model using the largest value.\n#> The final value used for the model was threshold = 2.06.\nconfusionMatrix(predict(model_pam, val_test_data[, -1]), val_test_data$outcome)\n#> Confusion Matrix and Statistics\n#> \n#>           Reference\n#> Prediction Death Recover\n#>    Death       1       3\n#>    Recover     8      11\n#>                                         \n#>                Accuracy : 0.522         \n#>                  95% CI : (0.306, 0.732)\n#>     No Information Rate : 0.609         \n#>     P-Value [Acc > NIR] : 0.857         \n#>                                         \n#>                   Kappa : -0.115        \n#>                                         \n#>  Mcnemar's Test P-Value : 0.228         \n#>                                         \n#>             Sensitivity : 0.1111        \n#>             Specificity : 0.7857        \n#>          Pos Pred Value : 0.2500        \n#>          Neg Pred Value : 0.5789        \n#>              Prevalence : 0.3913        \n#>          Detection Rate : 0.0435        \n#>    Detection Prevalence : 0.1739        \n#>       Balanced Accuracy : 0.4484        \n#>                                         \n#>        'Positive' Class : Death         \n#> "},{"path":"predicting-flu-outcome-comparing-eight-classification-algorithms.html","id":"single-c5.0-tree","chapter":"28 Predicting Flu outcome comparing eight classification algorithms","heading":"28.5.7 Single C5.0 Tree","text":"C5.0 another tree-based modeling algorithm. model classified 15 23 cases correctly.","code":"\nset.seed(27)\nmodel_C5.0Tree <- caret::train(outcome ~ .,\n                             data = val_train_data,\n                             method = \"C5.0Tree\",\n                             preProcess = NULL,\n                             trControl = train_control)\nmodel_C5.0Tree\n#> Single C5.0 Tree \n#> \n#> 56 samples\n#> 11 predictors\n#>  2 classes: 'Death', 'Recover' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold, repeated 10 times) \n#> Summary of sample sizes: 51, 49, 50, 51, 49, 51, ... \n#> Resampling results:\n#> \n#>   Accuracy  Kappa\n#>   0.696     0.359\nconfusionMatrix(predict(model_C5.0Tree, val_test_data[, -1]), val_test_data$outcome)\n#> Confusion Matrix and Statistics\n#> \n#>           Reference\n#> Prediction Death Recover\n#>    Death       4       1\n#>    Recover     5      13\n#>                                         \n#>                Accuracy : 0.739         \n#>                  95% CI : (0.516, 0.898)\n#>     No Information Rate : 0.609         \n#>     P-Value [Acc > NIR] : 0.142         \n#>                                         \n#>                   Kappa : 0.405         \n#>                                         \n#>  Mcnemar's Test P-Value : 0.221         \n#>                                         \n#>             Sensitivity : 0.444         \n#>             Specificity : 0.929         \n#>          Pos Pred Value : 0.800         \n#>          Neg Pred Value : 0.722         \n#>              Prevalence : 0.391         \n#>          Detection Rate : 0.174         \n#>    Detection Prevalence : 0.217         \n#>       Balanced Accuracy : 0.687         \n#>                                         \n#>        'Positive' Class : Death         \n#> "},{"path":"predicting-flu-outcome-comparing-eight-classification-algorithms.html","id":"partial-least-squares","chapter":"28 Predicting Flu outcome comparing eight classification algorithms","heading":"28.5.8 Partial Least Squares","text":"modeling correlated features. model classified 15 23 cases correctly.","code":"\nset.seed(27)\nmodel_pls <- caret::train(outcome ~ .,\n                             data = val_train_data,\n                             method = \"pls\",\n                             preProcess = NULL,\n                             trControl = train_control)\nmodel_pls\n#> Partial Least Squares \n#> \n#> 56 samples\n#> 11 predictors\n#>  2 classes: 'Death', 'Recover' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold, repeated 10 times) \n#> Summary of sample sizes: 51, 49, 50, 51, 49, 51, ... \n#> Resampling results across tuning parameters:\n#> \n#>   ncomp  Accuracy  Kappa\n#>   1      0.663     0.315\n#>   2      0.676     0.341\n#>   3      0.691     0.376\n#> \n#> Accuracy was used to select the optimal model using the largest value.\n#> The final value used for the model was ncomp = 3.\nconfusionMatrix(predict(model_pls, val_test_data[, -1]), val_test_data$outcome)\n#> Confusion Matrix and Statistics\n#> \n#>           Reference\n#> Prediction Death Recover\n#>    Death       2       3\n#>    Recover     7      11\n#>                                         \n#>                Accuracy : 0.565         \n#>                  95% CI : (0.345, 0.768)\n#>     No Information Rate : 0.609         \n#>     P-Value [Acc > NIR] : 0.742         \n#>                                         \n#>                   Kappa : 0.009         \n#>                                         \n#>  Mcnemar's Test P-Value : 0.343         \n#>                                         \n#>             Sensitivity : 0.222         \n#>             Specificity : 0.786         \n#>          Pos Pred Value : 0.400         \n#>          Neg Pred Value : 0.611         \n#>              Prevalence : 0.391         \n#>          Detection Rate : 0.087         \n#>    Detection Prevalence : 0.217         \n#>       Balanced Accuracy : 0.504         \n#>                                         \n#>        'Positive' Class : Death         \n#> "},{"path":"predicting-flu-outcome-comparing-eight-classification-algorithms.html","id":"comparing-accuracy-of-models-1","chapter":"28 Predicting Flu outcome comparing eight classification algorithms","heading":"28.6 Comparing accuracy of models","text":"models similarly accurate.","code":""},{"path":"predicting-flu-outcome-comparing-eight-classification-algorithms.html","id":"summary-accuracy-and-kappa","chapter":"28 Predicting Flu outcome comparing eight classification algorithms","heading":"28.6.1 Summary Accuracy and Kappa","text":"","code":"\n# Create a list of models\nmodels <- list(rf       = model_rf, \n               glmnet   = model_glmnet, \n               kknn     = model_kknn, \n               pda      = model_pda, \n               slda     = model_slda,\n               pam      = model_pam, \n               C5.0Tree = model_C5.0Tree, \n               pls      = model_pls)\n\n# Resample the models\nresample_results <- resamples(models)\n\n# Generate a summary\nsummary(resample_results, metric = c(\"Kappa\", \"Accuracy\"))\n#> \n#> Call:\n#> summary.resamples(object = resample_results, metric = c(\"Kappa\", \"Accuracy\"))\n#> \n#> Models: rf, glmnet, kknn, pda, slda, pam, C5.0Tree, pls \n#> Number of resamples: 100 \n#> \n#> Kappa \n#>            Min. 1st Qu. Median  Mean 3rd Qu. Max. NA's\n#> rf       -0.500   0.167  0.545 0.432   0.667    1    0\n#> glmnet   -0.667   0.167  0.545 0.414   0.667    1    0\n#> kknn     -0.667   0.125  0.333 0.313   0.615    1    0\n#> pda      -0.667   0.142  0.333 0.343   0.615    1    0\n#> slda     -0.667   0.167  0.367 0.358   0.615    1    0\n#> pam      -0.667   0.167  0.545 0.382   0.571    1    0\n#> C5.0Tree -0.667   0.167  0.333 0.359   0.615    1    0\n#> pls      -0.667   0.167  0.333 0.376   0.667    1    0\n#> \n#> Accuracy \n#>           Min. 1st Qu. Median  Mean 3rd Qu. Max. NA's\n#> rf       0.333   0.600  0.800 0.732   0.833    1    0\n#> glmnet   0.167   0.600  0.800 0.714   0.833    1    0\n#> kknn     0.167   0.600  0.667 0.666   0.800    1    0\n#> pda      0.167   0.593  0.667 0.681   0.800    1    0\n#> slda     0.167   0.600  0.667 0.682   0.800    1    0\n#> pam      0.200   0.600  0.800 0.714   0.800    1    0\n#> C5.0Tree 0.167   0.600  0.667 0.696   0.800    1    0\n#> pls      0.167   0.600  0.667 0.691   0.833    1    0\nbwplot(resample_results , metric = c(\"Kappa\",\"Accuracy\"))"},{"path":"predicting-flu-outcome-comparing-eight-classification-algorithms.html","id":"combined-results-of-predicting-validation-test-samples","chapter":"28 Predicting Flu outcome comparing eight classification algorithms","heading":"28.6.2 Combined results of predicting validation test samples","text":"compare predictions models, summed prediction probabilities Death Recovery models calculated log2 ratio summed probabilities Recovery summed probabilities Death. cases log2 ratio bigger 1.5 defined Recover, cases \\(log2\\) ratio -1.5 defined Death, remaining cases defined uncertain.predictions based models either correct uncertain.","code":"\nresults <- data.frame(\n  randomForest = predict(model_rf, newdata = val_test_data[, -1], type=\"prob\"),\n  glmnet = predict(model_glmnet, newdata = val_test_data[, -1], type=\"prob\"),\n  kknn = predict(model_kknn, newdata = val_test_data[, -1], type=\"prob\"),\n  pda = predict(model_pda, newdata = val_test_data[, -1], type=\"prob\"),\n  slda = predict(model_slda, newdata = val_test_data[, -1], type=\"prob\"),\n  pam = predict(model_pam, newdata = val_test_data[, -1], type=\"prob\"),\n  C5.0Tree = predict(model_C5.0Tree, newdata = val_test_data[, -1], type=\"prob\"),\n  pls = predict(model_pls, newdata = val_test_data[, -1], type=\"prob\"))\n\nresults$sum_Death <- rowSums(results[, grep(\"Death\", colnames(results))])\nresults$sum_Recover <- rowSums(results[, grep(\"Recover\", colnames(results))])\nresults$log2_ratio <- log2(results$sum_Recover/results$sum_Death)\nresults$true_outcome <- val_test_data$outcome\nresults$pred_outcome <- ifelse(results$log2_ratio > 1.5, \"Recover\", \n              ifelse(results$log2_ratio < -1.5, \"Death\", \"uncertain\"))\nresults$prediction <- ifelse(results$pred_outcome == results$true_outcome, \"CORRECT\", \n              ifelse(results$pred_outcome == \"uncertain\", \"uncertain\", \"wrong\"))\nresults[, -c(1:16)]\n#>          sum_Death sum_Recover log2_ratio true_outcome pred_outcome prediction\n#> case_10      4.237        3.76    -0.1709        Death    uncertain  uncertain\n#> case_11      5.181        2.82    -0.8778        Death    uncertain  uncertain\n#> case_116     2.412        5.59     1.2123      Recover    uncertain  uncertain\n#> case_12      5.219        2.78    -0.9085        Death    uncertain  uncertain\n#> case_121     2.356        5.64     1.2606        Death    uncertain  uncertain\n#> case_127     0.694        7.31     3.3972      Recover      Recover    CORRECT\n#> case_131     0.685        7.31     3.4164      Recover      Recover    CORRECT\n#> case_133     0.649        7.35     3.5024      Recover      Recover    CORRECT\n#> case_135     2.027        5.97     1.5589        Death      Recover      wrong\n#> case_2       2.161        5.84     1.4337        Death    uncertain  uncertain\n#> case_20      3.144        4.86     0.6272      Recover    uncertain  uncertain\n#> case_30      4.493        3.51    -0.3576      Recover    uncertain  uncertain\n#> case_45      2.594        5.41     1.0590        Death    uncertain  uncertain\n#> case_5       3.019        4.98     0.7227      Recover    uncertain  uncertain\n#> case_55      3.925        4.08     0.0543      Recover    uncertain  uncertain\n#> case_59      1.894        6.11     1.6886      Recover      Recover    CORRECT\n#> case_72      2.545        5.46     1.1002      Recover    uncertain  uncertain\n#> case_74      2.339        5.66     1.2748      Recover    uncertain  uncertain\n#> case_77      0.845        7.15     3.0819      Recover      Recover    CORRECT\n#> case_8       2.237        5.76     1.3650        Death    uncertain  uncertain\n#> case_89      1.712        6.29     1.8772      Recover      Recover    CORRECT\n#> case_97      2.959        5.04     0.7687      Recover    uncertain  uncertain\n#> case_98      4.798        3.20    -0.5833        Death    uncertain  uncertain"},{"path":"predicting-flu-outcome-comparing-eight-classification-algorithms.html","id":"predicting-unknown-outcomes","chapter":"28 Predicting Flu outcome comparing eight classification algorithms","heading":"28.7 Predicting unknown outcomes","text":"models now used predict outcome cases unknown fate., accuracy similar models.","code":"\n\ntrain_control <- trainControl(method = \"repeatedcv\", \n                              number = 10, \n                              repeats = 10, \n                              verboseIter = FALSE)\n\nset.seed(27)\nmodel_rf <- caret::train(outcome ~ .,\n                             data = train_data,\n                             method = \"rf\",\n                             preProcess = NULL,\n                             trControl = train_control)\nmodel_glmnet <- caret::train(outcome ~ .,\n                             data = train_data,\n                             method = \"glmnet\",\n                             preProcess = NULL,\n                             trControl = train_control)\nmodel_kknn <- caret::train(outcome ~ .,\n                             data = train_data,\n                             method = \"kknn\",\n                             preProcess = NULL,\n                             trControl = train_control)\nmodel_pda <- caret::train(outcome ~ .,\n                             data = train_data,\n                             method = \"pda\",\n                             preProcess = NULL,\n                             trControl = train_control)\n#> Warning: predictions failed for Fold01.Rep01: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold02.Rep01: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold03.Rep01: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold04.Rep01: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold05.Rep01: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold06.Rep01: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold07.Rep01: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold08.Rep01: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold09.Rep01: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold10.Rep01: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold01.Rep02: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold02.Rep02: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold03.Rep02: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold04.Rep02: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold05.Rep02: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold06.Rep02: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold07.Rep02: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold08.Rep02: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold09.Rep02: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold10.Rep02: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold01.Rep03: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold02.Rep03: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold03.Rep03: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold04.Rep03: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold05.Rep03: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold06.Rep03: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold07.Rep03: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold08.Rep03: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold09.Rep03: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold10.Rep03: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold01.Rep04: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold02.Rep04: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold03.Rep04: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold04.Rep04: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold05.Rep04: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold06.Rep04: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold07.Rep04: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold08.Rep04: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold09.Rep04: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold10.Rep04: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold01.Rep05: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold02.Rep05: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold03.Rep05: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold04.Rep05: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold05.Rep05: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold06.Rep05: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold07.Rep05: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold08.Rep05: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold09.Rep05: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold10.Rep05: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold01.Rep06: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold02.Rep06: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold03.Rep06: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold04.Rep06: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold05.Rep06: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold06.Rep06: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold07.Rep06: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold08.Rep06: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold09.Rep06: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold10.Rep06: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold01.Rep07: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold02.Rep07: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold03.Rep07: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold04.Rep07: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold05.Rep07: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold06.Rep07: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold07.Rep07: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold08.Rep07: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold09.Rep07: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold10.Rep07: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold01.Rep08: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold02.Rep08: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold03.Rep08: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold04.Rep08: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold05.Rep08: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold06.Rep08: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold07.Rep08: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold08.Rep08: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold09.Rep08: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold10.Rep08: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold01.Rep09: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold02.Rep09: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold03.Rep09: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold04.Rep09: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold05.Rep09: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold06.Rep09: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold07.Rep09: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold08.Rep09: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold09.Rep09: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold10.Rep09: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold01.Rep10: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold02.Rep10: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold03.Rep10: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold04.Rep10: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold05.Rep10: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold06.Rep10: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold07.Rep10: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold08.Rep10: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold09.Rep10: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning: predictions failed for Fold10.Rep10: lambda=0e+00 Error in mindist[l] <- ndist[l] : \n#>   NAs are not allowed in subscripted assignments\n#> Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : There were missing values in resampled performance measures.\n#> Warning in train.default(x, y, weights = w, ...): missing values found in aggregated results\nmodel_slda <- caret::train(outcome ~ .,\n                             data = train_data,\n                             method = \"slda\",\n                             preProcess = NULL,\n                             trControl = train_control)\nmodel_pam <- caret::train(outcome ~ .,\n                             data = train_data,\n                             method = \"pam\",\n                             preProcess = NULL,\n                             trControl = train_control)\n#> 12345678910111213141516171819202122232425262728293011111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\nmodel_C5.0Tree <- caret::train(outcome ~ .,\n                             data = train_data,\n                             method = \"C5.0Tree\",\n                             preProcess = NULL,\n                             trControl = train_control)\nmodel_pls <- caret::train(outcome ~ .,\n                             data = train_data,\n                             method = \"pls\",\n                             preProcess = NULL,\n                             trControl = train_control)\n\nmodels <- list(rf = model_rf, \n               glmnet   = model_glmnet, \n               kknn     = model_kknn, \n               pda      = model_pda, \n               slda     = model_slda,\n               pam      = model_pam, \n               C5.0Tree = model_C5.0Tree, \n               pls      = model_pls)\n\n# Resample the models\nresample_results <- resamples(models)\n\nbwplot(resample_results , metric = c(\"Kappa\",\"Accuracy\"))"},{"path":"predicting-flu-outcome-comparing-eight-classification-algorithms.html","id":"final-results","chapter":"28 Predicting Flu outcome comparing eight classification algorithms","heading":"28.7.1 Final results","text":"final results calculated described .","code":"\nresults <- data.frame(\n  randomForest = predict(model_rf, newdata = test_data, type=\"prob\"),\n  glmnet = predict(model_glmnet, newdata = test_data, type=\"prob\"),\n  kknn = predict(model_kknn, newdata = test_data, type=\"prob\"),\n  pda = predict(model_pda, newdata = test_data, type=\"prob\"),\n  slda = predict(model_slda, newdata = test_data, type=\"prob\"),\n  pam = predict(model_pam, newdata = test_data, type=\"prob\"),\n  C5.0Tree = predict(model_C5.0Tree, newdata = test_data, type=\"prob\"),\n  pls = predict(model_pls, newdata = test_data, type=\"prob\"))\n\nresults$sum_Death <- rowSums(results[, grep(\"Death\", colnames(results))])\nresults$sum_Recover <- rowSums(results[, grep(\"Recover\", colnames(results))])\nresults$log2_ratio <- log2(results$sum_Recover/results$sum_Death)\nresults$predicted_outcome <- ifelse(results$log2_ratio > 1.5, \"Recover\", \n                                    ifelse(results$log2_ratio < -1.5, \"Death\", \n                                           \"uncertain\"))\nresults[, -c(1:16)]\n#>          sum_Death sum_Recover log2_ratio predicted_outcome\n#> case_100     1.854        6.15     1.7287           Recover\n#> case_101     5.432        2.57    -1.0807         uncertain\n#> case_102     2.806        5.19     0.8887         uncertain\n#> case_103     2.342        5.66     1.2729         uncertain\n#> case_104     1.744        6.26     1.8432           Recover\n#> case_105     0.955        7.04     2.8828           Recover\n#> case_108     4.489        3.51    -0.3543         uncertain\n#> case_109     4.515        3.48    -0.3736         uncertain\n#> case_110     2.411        5.59     1.2132         uncertain\n#> case_112     2.632        5.37     1.0281         uncertain\n#> case_113     2.198        5.80     1.4004         uncertain\n#> case_114     3.339        4.66     0.4810         uncertain\n#> case_115     1.112        6.89     2.6307           Recover\n#> case_118     2.778        5.22     0.9109         uncertain\n#> case_120     2.213        5.79     1.3868         uncertain\n#> case_122     3.235        4.77     0.5590         uncertain\n#> case_126     3.186        4.81     0.5952         uncertain\n#> case_130     2.300        5.70     1.3091         uncertain\n#> case_132     4.473        3.53    -0.3427         uncertain\n#> case_136     3.281        4.72     0.5243         uncertain\n#> case_15      2.270        5.73     1.3355         uncertain\n#> case_16      2.820        5.18     0.8772         uncertain\n#> case_22      4.779        3.22    -0.5689         uncertain\n#> case_28      2.862        5.14     0.8445         uncertain\n#> case_31      2.412        5.59     1.2117         uncertain\n#> case_32      2.591        5.41     1.0616         uncertain\n#> case_38      2.060        5.94     1.5280           Recover\n#> case_39      4.749        3.25    -0.5466         uncertain\n#> case_4       5.342        2.66    -1.0074         uncertain\n#> case_40      6.550        1.45    -2.1750             Death\n#> case_41      4.611        3.39    -0.4441         uncertain\n#> case_42      5.570        2.43    -1.1966         uncertain\n#> case_47      2.563        5.44     1.0850         uncertain\n#> case_48      4.850        3.15    -0.6224         uncertain\n#> case_52      4.709        3.29    -0.5173         uncertain\n#> case_54      2.718        5.28     0.9586         uncertain\n#> case_56      6.394        1.61    -1.9933             Death\n#> case_62      6.048        1.95    -1.6319             Death\n#> case_63      2.337        5.66     1.2766         uncertain\n#> case_66      2.176        5.82     1.4205         uncertain\n#> case_67      1.893        6.11     1.6895           Recover\n#> case_68      3.907        4.09     0.0672         uncertain\n#> case_69      4.465        3.53    -0.3370         uncertain\n#> case_70      3.885        4.12     0.0833         uncertain\n#> case_71      2.524        5.48     1.1172         uncertain\n#> case_80      2.759        5.24     0.9261         uncertain\n#> case_84      3.661        4.34     0.2448         uncertain\n#> case_85      4.921        3.08    -0.6762         uncertain\n#> case_86      3.563        4.44     0.3164         uncertain\n#> case_88      0.566        7.43     3.7160           Recover\n#> case_9       5.981        2.02    -1.5665             Death\n#> case_90      3.570        4.43     0.3116         uncertain\n#> case_92      4.664        3.34    -0.4835         uncertain\n#> case_93      2.056        5.94     1.5319           Recover\n#> case_95      4.495        3.50    -0.3589         uncertain\n#> case_96      1.313        6.69     2.3482           Recover\n#> case_99      4.799        3.20    -0.5839         uncertain\nwrite.table(results, \"results_prediction_unknown_outcome_ML_part1.txt\", \n            col.names = T, sep = \"\\t\")\nresults %>% \n  filter(predicted_outcome == \"Recover\") %>% \n  select(-c(1:16))\n#>   sum_Death sum_Recover log2_ratio predicted_outcome\n#> 1     1.854        6.15       1.73           Recover\n#> 2     1.744        6.26       1.84           Recover\n#> 3     0.955        7.04       2.88           Recover\n#> 4     1.112        6.89       2.63           Recover\n#> 5     2.060        5.94       1.53           Recover\n#> 6     1.893        6.11       1.69           Recover\n#> 7     0.566        7.43       3.72           Recover\n#> 8     2.056        5.94       1.53           Recover\n#> 9     1.313        6.69       2.35           Recover"},{"path":"predicting-flu-outcome-comparing-eight-classification-algorithms.html","id":"predicted-outcome","chapter":"28 Predicting Flu outcome comparing eight classification algorithms","heading":"28.7.2 Predicted outcome","text":"57 cases, 14 defined Recover, 3 Death 40 uncertain.comparison date onset, data hospitalization, gender age predicted outcome shows predicted deaths associated older age predicted recoveries. Date onset show obvious bias either direction.","code":"\nresults %>% \n  group_by(predicted_outcome) %>% \n  summarize(n = n())\n#> # A tibble: 3 x 2\n#>   predicted_outcome     n\n#>   <chr>             <int>\n#> 1 Death                 4\n#> 2 Recover               9\n#> 3 uncertain            44\nresults_combined <- merge(results[, -c(1:16)],\n                          fluH7N9_china_2013[which(fluH7N9_china_2013$case_ID \n                                                   %in% rownames(results)), ], \n                          by.x = \"row.names\", by.y = \"case_ID\")\n# results_combined <- results_combined[, -c(2, 3, 8, 9)]\nresults_combined <- results_combined[, -c(1, 2, 3, 9, 10)]\nresults_combined\n#>    log2_ratio predicted_outcome case_id date_of_onset date_of_hospitalisation gender age  province\n#> 1      1.7287           Recover     100    2013-04-16                    <NA>      m  58  Zhejiang\n#> 2     -1.0807         uncertain     101    2013-04-13                    <NA>      f  79  Zhejiang\n#> 3      0.8887         uncertain     102    2013-04-12                    <NA>      m  81  Zhejiang\n#> 4      1.2729         uncertain     103    2013-04-13              2013-04-19      m  68   Jiangsu\n#> 5      1.8432           Recover     104    2013-04-16                    <NA>      f  54  Zhejiang\n#> 6      2.8828           Recover     105    2013-04-14                    <NA>      m  32  Zhejiang\n#> 7     -0.3543         uncertain     108    2013-04-15                    <NA>      m  84  Zhejiang\n#> 8     -0.3736         uncertain     109    2013-04-15                    <NA>      m  62  Zhejiang\n#> 9      1.2132         uncertain     110    2013-04-12              2013-04-16      m  53    Taiwan\n#> 10     1.0281         uncertain     112    2013-04-17                    <NA>      m  69   Jiangxi\n#> 11     1.4004         uncertain     113    2013-04-15                    <NA>      f  60  Zhejiang\n#> 12     0.4810         uncertain     114    2013-04-18                    <NA>      f  50  Zhejiang\n#> 13     2.6307           Recover     115    2013-04-17                    <NA>      m  38  Zhejiang\n#> 14     0.9109         uncertain     118    2013-04-17                    <NA>      m  49   Jiangsu\n#> 15     1.3868         uncertain     120    2013-03-08                    <NA>      m  60   Jiangsu\n#> 16     0.5590         uncertain     122    2013-04-18                    <NA>      m  38  Zhejiang\n#> 17     0.5952         uncertain     126    2013-04-17              2013-04-27      m  80    Fujian\n#> 18     1.3091         uncertain     130    2013-04-29              2013-04-30      m  69    Fujian\n#> 19    -0.3427         uncertain     132    2013-05-03              2013-05-03      f  79   Jiangxi\n#> 20     0.5243         uncertain     136    2013-07-27              2013-07-28      f  51 Guangdong\n#> 21     1.3355         uncertain      15    2013-03-20                    <NA>      f  61   Jiangsu\n#> 22     0.8772         uncertain      16    2013-03-21                    <NA>      m  79   Jiangsu\n#> 23    -0.5689         uncertain      22    2013-03-28              2013-04-01      m  85   Jiangsu\n#> 24     0.8445         uncertain      28    2013-03-29                    <NA>      m  79  Zhejiang\n#> 25     1.2117         uncertain      31    2013-03-29                    <NA>      m  70   Jiangsu\n#> 26     1.0616         uncertain      32    2013-04-02                    <NA>      m  74   Jiangsu\n#> 27     1.5280           Recover      38    2013-04-03                    <NA>      m  56   Jiangsu\n#> 28    -0.5466         uncertain      39    2013-04-08              2013-04-08      m  66  Zhejiang\n#> 29    -1.0074         uncertain       4    2013-03-19              2013-03-27      f  45   Jiangsu\n#> 30    -2.1750             Death      40    2013-04-06              2013-04-11      m  74  Zhejiang\n#> 31    -0.4441         uncertain      41    2013-04-06              2013-04-12      f  54  Zhejiang\n#> 32    -1.1966         uncertain      42    2013-04-03              2013-04-10      m  53  Shanghai\n#> 33     1.0850         uncertain      47    2013-04-01                    <NA>      m  72   Jiangsu\n#> 34    -0.6224         uncertain      48    2013-04-03              2013-04-09      m  65  Zhejiang\n#> 35    -0.5173         uncertain      52    2013-04-06                    <NA>      f  64  Zhejiang\n#> 36     0.9586         uncertain      54    2013-04-06                    <NA>      m  75  Zhejiang\n#> 37    -1.9933             Death      56    2013-04-05              2013-04-11      m  73  Shanghai\n#> 38    -1.6319             Death      62    2013-04-03                    <NA>      f  68  Zhejiang\n#> 39     1.2766         uncertain      63    2013-04-10                    <NA>      m  60     Anhui\n#> 40     1.4205         uncertain      66          <NA>                    <NA>      m  72   Jiangsu\n#> 41     1.6895           Recover      67    2013-04-12                    <NA>      m  56  Zhejiang\n#> 42     0.0672         uncertain      68    2013-04-10                    <NA>      m  57  Zhejiang\n#> 43    -0.3370         uncertain      69    2013-04-10                    <NA>      m  62  Zhejiang\n#> 44     0.0833         uncertain      70    2013-04-11                    <NA>      f  58  Zhejiang\n#> 45     1.1172         uncertain      71    2013-04-10                    <NA>      f  72  Zhejiang\n#> 46     0.9261         uncertain      80    2013-04-08                    <NA>      m  74  Zhejiang\n#> 47     0.2448         uncertain      84          <NA>                    <NA>      f  26   Jiangsu\n#> 48    -0.6762         uncertain      85    2013-04-09              2013-04-16      f  80  Shanghai\n#> 49     0.3164         uncertain      86    2013-04-13                    <NA>      f  54  Zhejiang\n#> 50     3.7160           Recover      88          <NA>                    <NA>      m   4   Beijing\n#> 51    -1.5665             Death       9    2013-03-25              2013-03-25      m  67  Zhejiang\n#> 52     0.3116         uncertain      90    2013-04-12              2013-04-15      m  43  Zhejiang\n#> 53    -0.4835         uncertain      92    2013-04-10              2013-04-17      f  66  Zhejiang\n#> 54     1.5319           Recover      93    2013-04-11                    <NA>      m  56  Zhejiang\n#> 55    -0.3589         uncertain      95    2013-03-30                    <NA>      m  37  Zhejiang\n#> 56     2.3482           Recover      96    2013-04-07                    <NA>      m  43   Jiangsu\n#> 57    -0.5839         uncertain      99    2013-04-12                    <NA>      f  68  Zhejiang\n# tidy dataframe for plotting\nresults_combined_gather <- results_combined %>%\n  gather(group_dates, date, date_of_onset:date_of_hospitalisation)\n\nresults_combined_gather$group_dates <- factor(results_combined_gather$group_dates, \n                          levels = c(\"date_of_onset\", \"date_of_hospitalisation\"))\n\nresults_combined_gather$group_dates <- mapvalues(results_combined_gather$group_dates, \n                            from = c(\"date_of_onset\", \"date_of_hospitalisation\"), \n                              to = c(\"Date of onset\", \"Date of hospitalisation\"))\n\nresults_combined_gather$gender <- mapvalues(results_combined_gather$gender, \n                                            from = c(\"f\", \"m\"), \n                                             to = c(\"Female\", \"Male\"))\nlevels(results_combined_gather$gender) <- c(levels(results_combined_gather$gender), \n                                            \"unknown\")\nresults_combined_gather$gender[is.na(results_combined_gather$gender)] <- \"unknown\"\nggplot(data = results_combined_gather, aes(x = date, y = log2_ratio, \n                                           color = predicted_outcome)) +\n  geom_jitter(aes(size = as.numeric(age)), alpha = 0.3) +\n  geom_rug() +\n  facet_grid(gender ~ group_dates) +\n  labs(\n    color = \"Predicted outcome\",\n    size = \"Age\",\n    x = \"Date in 2013\",\n    y = \"log2 ratio of prediction Recover vs Death\",\n    title = \"2013 Influenza A H7N9 cases in China\",\n    subtitle = \"Predicted outcome\",\n    caption = \"\"\n  ) +\n  my_theme() +\n  scale_shape_manual(values = c(15, 16, 17)) +\n  scale_color_brewer(palette=\"Set1\") +\n  scale_fill_brewer(palette=\"Set1\")\n#> Warning: Removed 42 rows containing missing values (geom_point)."},{"path":"predicting-flu-outcome-comparing-eight-classification-algorithms.html","id":"conclusions-1","chapter":"28 Predicting Flu outcome comparing eight classification algorithms","heading":"28.8 Conclusions","text":"dataset posed couple difficulties begin , like unequal distribution data points across variables missing data. makes modeling inherently prone flaws. However, real life data isn’t perfect either, went ahead tested modeling success anyway.accounting uncertain classification low predictions probability, validation data classified accurately. However, accurate model, cases don’t give enough information reliably predict outcome. cases, information (.e. features) fewer missing data improve modeling outcome.Also, example applicable specific case flu. order able draw general conclusions flu outcome, cases additional information, example medical parameters like preexisting medical conditions, disase parameters, demographic information, etc. necessary., dataset served nice example possibilities (pitfalls) machine learning applications showcases basic workflow building prediction models R.comparison feature selection methods see .see mistakes tips tricks improvement, please don’t hesitate let know! Thanks. :-)","code":"\nsessionInfo()\n#> R version 3.6.3 (2020-02-29)\n#> Platform: x86_64-pc-linux-gnu (64-bit)\n#> Running under: Debian GNU/Linux 10 (buster)\n#> \n#> Matrix products: default\n#> BLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.3.5.so\n#> \n#> locale:\n#>  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C               LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8     LC_MONETARY=en_US.UTF-8    LC_MESSAGES=C              LC_PAPER=en_US.UTF-8       LC_NAME=C                  LC_ADDRESS=C               LC_TELEPHONE=C             LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n#> \n#> attached base packages:\n#> [1] grid      stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] RColorBrewer_1.1-2 rpart.plot_3.0.8   rattle_5.3.0       rpart_4.1-15       caret_6.0-86       lattice_0.20-38    mice_3.8.0         dplyr_0.8.5        gridExtra_2.3      ggplot2_3.3.0      plyr_1.8.6         tidyr_1.0.2        outbreaks_1.5.0   \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] nlme_3.1-144         fs_1.4.1             lubridate_1.7.8      C50_0.1.3            tools_3.6.3          backports_1.1.6      bslib_0.2.2.9000     utf8_1.1.4           R6_2.4.1             colorspace_1.4-1     nnet_7.3-12          withr_2.2.0          tidyselect_1.0.0     downlit_0.2.1.9000   compiler_3.6.3       mda_0.4-10           cli_2.0.2            glmnet_3.0-2         xml2_1.3.2           isoband_0.2.1        Cubist_0.2.3         labeling_0.3         bookdown_0.21.4      sass_0.2.0.9005      scales_1.1.0         mvtnorm_1.1-0        randomForest_4.6-14  rappdirs_0.3.1       stringr_1.4.0        digest_0.6.25        rmarkdown_2.5.3      pkgconfig_2.0.3      htmltools_0.5.0.9003 rlang_0.4.5          shape_1.4.4          jquerylib_0.1.2      farver_2.0.3         generics_0.0.2       jsonlite_1.6.1       ModelMetrics_1.2.2.2 magrittr_1.5         Formula_1.2-3        Matrix_1.2-18        fansi_0.4.1          Rcpp_1.0.4.6         munsell_0.5.0        partykit_1.2-7      \n#> [48] lifecycle_0.2.0      stringi_1.4.6        pROC_1.16.2          yaml_2.2.1           inum_1.0-1           MASS_7.3-51.5        pamr_1.56.1          recipes_0.1.10       pls_2.7-2            crayon_1.3.4         splines_3.6.3        knitr_1.28           pillar_1.4.3         igraph_1.2.5         reshape2_1.4.4       codetools_0.2-16     stats4_3.6.3         glue_1.4.0           evaluate_0.14        data.table_1.12.8    vctrs_0.2.4          foreach_1.5.0        gtable_0.3.0         purrr_0.3.4          assertthat_0.2.1     xfun_0.19.4          gower_0.2.1          prodlim_2019.11.13   libcoin_1.0-5        broom_0.5.6          e1071_1.7-3          class_7.3-15         survival_3.1-8       timeDate_3043.102    tibble_3.0.1         iterators_1.0.12     kknn_1.3.1           cluster_2.1.0        lava_1.6.7           ellipsis_0.3.0       ipred_0.9-9"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"a-detailed-study-of-bike-sharing-demand","chapter":"29 A detailed study of bike sharing demand","heading":"29 A detailed study of bike sharing demand","text":"Datasets: bike-sharing-demandAlgorithms:\nDecision Trees\nConditional Inference Tree\nRandom Forest\nDecision TreesConditional Inference TreeRandom ForestSource: https://www.analyticsvidhya.com/blog/2015/06/solution-kaggle-competition-bike-sharing-demand/","code":"\n#loading the required libraries\nlibrary(rpart)\nlibrary(rattle)\n#> Rattle: A free graphical interface for data science with R.\n#> Version 5.3.0 Copyright (c) 2006-2018 Togaware Pty Ltd.\n#> Type 'rattle()' to shake, rattle, and roll your data.\nlibrary(rpart.plot)\nlibrary(RColorBrewer)\nlibrary(randomForest)\n#> randomForest 4.6-14\n#> Type rfNews() to see new features/changes/bug fixes.\n#> \n#> Attaching package: 'randomForest'\n#> The following object is masked from 'package:rattle':\n#> \n#>     importance\nlibrary(corrplot)\n#> corrplot 0.84 loaded\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following object is masked from 'package:randomForest':\n#> \n#>     combine\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\nlibrary(tictoc)"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"hypothesis-generation","chapter":"29 A detailed study of bike sharing demand","heading":"29.1 Hypothesis Generation","text":"exploring data understand relationship variables, ’d recommend focus hypothesis generation first. Now, might sound counter-intuitive solving data science problem, one thing learnt years, . exploring data, spend time thinking business problem, gaining domain knowledge may gaining first hand experience problem (travel North America!)help? practice usually helps form better features later , biased data available dataset. stage, expected posses structured thinking .e. thinking process takes consideration possible aspects particular problem.hypothesis thought influence demand bikes:Hourly trend: must high demand office timings. Early morning late evening can different trend (cyclist) low demand 10:00 pm 4:00 .Hourly trend: must high demand office timings. Early morning late evening can different trend (cyclist) low demand 10:00 pm 4:00 .Daily Trend: Registered users demand bike weekdays compared weekend holiday.Daily Trend: Registered users demand bike weekdays compared weekend holiday.Rain: demand bikes lower rainy day compared sunny day. Similarly, higher humidity cause lower demand vice versa.Rain: demand bikes lower rainy day compared sunny day. Similarly, higher humidity cause lower demand vice versa.Temperature: high low temperature encourage disencourage bike riding?Temperature: high low temperature encourage disencourage bike riding?Pollution: pollution level city starts soaring, people may start using Bike (may influenced government / company policies increased awareness).Pollution: pollution level city starts soaring, people may start using Bike (may influenced government / company policies increased awareness).Time: Total demand higher contribution registered user compared casual registered user base increase time.Time: Total demand higher contribution registered user compared casual registered user base increase time.Traffic: can positively correlated Bike demand. Higher traffic may force people use bike compared road transport medium like car, taxi etcTraffic: can positively correlated Bike demand. Higher traffic may force people use bike compared road transport medium like car, taxi etc","code":""},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"understanding-the-data-set","chapter":"29 A detailed study of bike sharing demand","heading":"29.2 Understanding the Data Set","text":"dataset shows hourly rental data two years (2011 2012). training data set first 19 days month. test dataset 20th day month’s end. required predict total count bikes rented hour covered test set.training data set, separately given bike demand registered, casual users sum given count.Training data set 12 variables (see ) Test 9 (excluding registered, casual count).","code":""},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"independent-variables","chapter":"29 A detailed study of bike sharing demand","heading":"29.2.1 Independent variables","text":"","code":"datetime:   date and hour in \"mm/dd/yyyy hh:mm\" format\nseason:     Four categories-> 1 = spring, 2 = summer, 3 = fall, 4 = winter\nholiday:    whether the day is a holiday or not (1/0)\nworkingday: whether the day is neither a weekend nor holiday (1/0)\nweather:    Four Categories of weather\n            1-> Clear, Few clouds, Partly cloudy, Partly cloudy\n            2-> Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n            3-> Light Snow and Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n            4-> Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\ntemp:       hourly temperature in Celsius\natemp:      \"feels like\" temperature in Celsius\nhumidity:   relative humidity\nwindspeed:  wind speed"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"dependent-variables","chapter":"29 A detailed study of bike sharing demand","heading":"29.2.2 Dependent variables","text":"","code":"registered: number of registered user\ncasual:     number of non-registered user\ncount:      number of total rentals (registered + casual)"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"importing-the-dataset-and-data-exploration","chapter":"29 A detailed study of bike sharing demand","heading":"29.3 Importing the dataset and Data Exploration","text":"solution, used R (R Studio 0.99.442) Windows Environment.steps import perform data exploration. new concept, can refer guide Data Exploration RImport Train Test Data SetCombine Train Test Data set (understand distribution independent variable together).Variable Type IdentificationFind missing values dataset anyNo NAs dataset.Understand distribution numerical variables generate frequency table numeric variables. Analyze distribution.","code":"\n# https://www.kaggle.com/c/bike-sharing-demand/data\ntrain = read.csv(file.path(data_raw_dir, \"bike_train.csv\"))\ntest = read.csv(file.path(data_raw_dir, \"bike_test.csv\"))\nglimpse(train)\n#> Rows: 10,886\n#> Columns: 12\n#> $ datetime   <fct> 2011-01-01 00:00:00, 2011-01-01 01:00:00, 2011-01-01 02:00…\n#> $ season     <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n#> $ holiday    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#> $ workingday <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#> $ weather    <int> 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3…\n#> $ temp       <dbl> 9.84, 9.02, 9.02, 9.84, 9.84, 9.84, 9.02, 8.20, 9.84, 13.1…\n#> $ atemp      <dbl> 14.4, 13.6, 13.6, 14.4, 14.4, 12.9, 13.6, 12.9, 14.4, 17.4…\n#> $ humidity   <int> 81, 80, 80, 75, 75, 75, 80, 86, 75, 76, 76, 81, 77, 72, 72…\n#> $ windspeed  <dbl> 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 17, 19, 19, 20, 19, 20, 20, …\n#> $ casual     <int> 3, 8, 5, 3, 0, 0, 2, 1, 1, 8, 12, 26, 29, 47, 35, 40, 41, …\n#> $ registered <int> 13, 32, 27, 10, 1, 1, 0, 2, 7, 6, 24, 30, 55, 47, 71, 70, …\n#> $ count      <int> 16, 40, 32, 13, 1, 1, 2, 3, 8, 14, 36, 56, 84, 94, 106, 11…\nglimpse(test)\n#> Rows: 6,493\n#> Columns: 9\n#> $ datetime   <fct> 2011-01-20 00:00:00, 2011-01-20 01:00:00, 2011-01-20 02:00…\n#> $ season     <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n#> $ holiday    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#> $ workingday <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n#> $ weather    <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1…\n#> $ temp       <dbl> 10.66, 10.66, 10.66, 10.66, 10.66, 9.84, 9.02, 9.02, 9.02,…\n#> $ atemp      <dbl> 11.4, 13.6, 13.6, 12.9, 12.9, 11.4, 10.6, 10.6, 10.6, 11.4…\n#> $ humidity   <int> 56, 56, 56, 56, 56, 60, 60, 55, 55, 52, 48, 45, 42, 45, 45…\n#> $ windspeed  <dbl> 26, 0, 0, 11, 11, 15, 15, 15, 19, 15, 20, 11, 0, 7, 9, 13,…\n# add variables to test dataset before merging\ntest$registered=0\ntest$casual=0\ntest$count=0\n\ndata = rbind(train,test)\nstr(data)\n#> 'data.frame':    17379 obs. of  12 variables:\n#>  $ datetime  : Factor w/ 17379 levels \"2011-01-01 00:00:00\",..: 1 2 3 4 5 6 7 8 9 10 ...\n#>  $ season    : int  1 1 1 1 1 1 1 1 1 1 ...\n#>  $ holiday   : int  0 0 0 0 0 0 0 0 0 0 ...\n#>  $ workingday: int  0 0 0 0 0 0 0 0 0 0 ...\n#>  $ weather   : int  1 1 1 1 1 2 1 1 1 1 ...\n#>  $ temp      : num  9.84 9.02 9.02 9.84 9.84 ...\n#>  $ atemp     : num  14.4 13.6 13.6 14.4 14.4 ...\n#>  $ humidity  : int  81 80 80 75 75 75 80 86 75 76 ...\n#>  $ windspeed : num  0 0 0 0 0 ...\n#>  $ casual    : num  3 8 5 3 0 0 2 1 1 8 ...\n#>  $ registered: num  13 32 27 10 1 1 0 2 7 6 ...\n#>  $ count     : num  16 40 32 13 1 1 2 3 8 14 ...\ntable(is.na(data))\n#> \n#>  FALSE \n#> 208548"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"histograms","chapter":"29 A detailed study of bike sharing demand","heading":"29.3.1 histograms","text":"","code":"\n# histograms each attribute\npar(mfrow=c(2,4))\nfor(i in 2:9) {\n    hist(data[,i], main = names(data)[i])\n}"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"density-plots","chapter":"29 A detailed study of bike sharing demand","heading":"29.3.2 density plots","text":"","code":"\n# density plot for each attribute\npar(mfrow=c(2,4))\nfor(i in 2:9) {\n    plot(density(data[,i]), main=names(data)[i])\n}"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"boxplots","chapter":"29 A detailed study of bike sharing demand","heading":"29.3.3 boxplots","text":"","code":"\n# boxplots for each attribute\npar(mfrow=c(2,4))\nfor(i in 2:9) {\n    boxplot(data[,i], main=names(data)[i])\n}"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"unique-values-of-discrete-variables","chapter":"29 A detailed study of bike sharing demand","heading":"29.3.4 Unique values of discrete variables","text":"Humidity integer discrete variable; continuous numeric variable.","code":"\n# the discrete variables in this case are integers\nints <- unlist(lapply(data, is.integer))\nnames(data)[ints]\n#> [1] \"season\"     \"holiday\"    \"workingday\" \"weather\"    \"humidity\"\n# convert humidity to numeric\ndata$humidity <- as.numeric(data$humidity)\n# list unique values of integer variables\nints <- unlist(lapply(data, is.integer))\nint_vars <- names(data)[ints]\n\nsapply(int_vars, function(x) unique(data[x]))\n#> $season.season\n#> [1] 1 2 3 4\n#> \n#> $holiday.holiday\n#> [1] 0 1\n#> \n#> $workingday.workingday\n#> [1] 0 1\n#> \n#> $weather.weather\n#> [1] 1 2 3 4"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"inferences","chapter":"29 A detailed study of bike sharing demand","heading":"29.3.5 Inferences","text":"variables season, holiday, workingday weather discrete (integer).Activity even seasons.activitity happens non-holidays.Activity doubles working days.Activity happens mostly clear (1) weather.temp, atemp humidity continuous variables (numeric).","code":""},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"hypothesis-testing-using-multivariate-analysis","chapter":"29 A detailed study of bike sharing demand","heading":"29.4 Hypothesis Testing (using multivariate analysis)","text":"Till now, got fair understanding data set. Now, let’s test hypothesis generated earlier. added additional hypothesis dataset. Let’s test one one:","code":""},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"hourly-trend","chapter":"29 A detailed study of bike sharing demand","heading":"29.4.1 Hourly trend","text":"must high demand office timings. Early morning late evening can different trend (cyclist) low demand 10:00 pm 4:00 .don’t variable ‘hour’ us. can extract using datetime column.","code":"\nhead(data$datetime)\n#> [1] 2011-01-01 00:00:00 2011-01-01 01:00:00 2011-01-01 02:00:00\n#> [4] 2011-01-01 03:00:00 2011-01-01 04:00:00 2011-01-01 05:00:00\n#> 17379 Levels: 2011-01-01 00:00:00 2011-01-01 01:00:00 ... 2012-12-31 23:00:00\nclass(data$datetime)\n#> [1] \"factor\"\n# show hour and day from the variable datetime\nhead(substr(data$datetime, 12, 13))  # hour\n#> [1] \"00\" \"01\" \"02\" \"03\" \"04\" \"05\"\nhead(substr(data$datetime, 9, 10))   # day\n#> [1] \"01\" \"01\" \"01\" \"01\" \"01\" \"01\"\n# extracting hour\ndata$hour = substr(data$datetime,12,13)\ndata$hour = as.factor(data$hour)\nhead(data$hour)\n#> [1] 00 01 02 03 04 05\n#> 24 Levels: 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 ... 23\n### dividing again in train and test\n# the train dataset is for the first 19 days\ntrain = data[as.integer(substr(data$datetime, 9, 10)) < 20,]\n\n# the test dataset is from day 20 to the end of the month\ntest = data[as.integer(substr(data$datetime, 9, 10)) > 19,]"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"boxplot-count-vs-hour-in-training-set","chapter":"29 A detailed study of bike sharing demand","heading":"29.4.2 boxplot count vs hour in training set","text":"Rides increase 6 6pm, office hours.","code":"\nboxplot(train$count ~ train$hour, xlab=\"hour\", ylab=\"count of users\")\n# casual users\ncasual <- data[data$casual > 0, ]\nregistered <- data[data$registered > 0, ]\n\ndim(casual)\n#> [1] 9900   13\ndim(registered)\n#> [1] 10871    13"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"boxplot-hourly-casual-vs-registered-users-in-the-training-set","chapter":"29 A detailed study of bike sharing demand","heading":"29.4.3 Boxplot hourly: casual vs registered users in the training set","text":"Casual Registered users different distributions. Casual users tend rent office hours.","code":"\n# by hour: casual vs registered users\npar(mfrow=c(2,1))\nboxplot(train$casual ~ train$hour, xlab=\"hour\", ylab=\"casual users\")\nboxplot(train$registered ~ train$hour, xlab=\"hour\", ylab=\"registered users\")"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"outliers-in-the-training-set","chapter":"29 A detailed study of bike sharing demand","heading":"29.4.4 outliers in the training set","text":"","code":"\npar(mfrow=c(2,1))\nboxplot(train$count ~ train$hour, xlab=\"hour\", ylab=\"count of users\")\nboxplot(log(train$count) ~ train$hour,xlab=\"hour\",ylab=\"log(count)\")"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"daily-trend","chapter":"29 A detailed study of bike sharing demand","heading":"29.4.5 Daily trend","text":"Registered users demand bike weekdays compared weekend holiday.","code":"\n# extracting days of week\ndate <- substr(data$datetime, 1, 10)\ndays <- weekdays(as.Date(date))\ndata$day <- days\n# split the dataset again at day 20 of the month, before and after\ntrain = data[as.integer(substr(data$datetime,9,10)) < 20,]\ntest  = data[as.integer(substr(data$datetime,9,10)) > 19,]"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"boxplot-daily-trend-casual-vs-registered-users-training-set","chapter":"29 A detailed study of bike sharing demand","heading":"29.4.6 Boxplot daily trend: casual vs registered users, training set","text":"Demand casual users increases weekend, contrary registered users.","code":"\n# creating boxplots for rentals with different variables to see the variation\npar(mfrow=c(2,1))\nboxplot(train$casual ~ train$day, xlab=\"day\", ylab=\"casual users\")\nboxplot(train$registered ~ train$day, xlab=\"day\", ylab=\"registered users\")"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"rain","chapter":"29 A detailed study of bike sharing demand","heading":"29.4.7 Rain","text":"demand bikes lower rainy day compared sunny day. Similarly, higher humidity cause lower demand vice versa.use variable weather (1 4) analyze riding rain conditions.","code":""},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"boxplot-of-rain-effect-on-bike-riding-training-set","chapter":"29 A detailed study of bike sharing demand","heading":"29.4.7.1 Boxplot of rain effect on bike riding, training set","text":"Registered used tend ride even rain.","code":"\npar(mfrow=c(2,1))\nboxplot(train$casual ~ train$weather, xlab=\"day\", ylab=\"casual users\")\nboxplot(train$registered ~ train$weather, xlab=\"day\", ylab=\"registered users\")"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"temperature","chapter":"29 A detailed study of bike sharing demand","heading":"29.4.8 Temperature","text":"high low temperature encourage disencourage bike riding?","code":""},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"boxplot-of-temperature-effect-training-set","chapter":"29 A detailed study of bike sharing demand","heading":"29.4.8.1 boxplot of temperature effect, training set","text":"Casual users tend ride milder temperatures registered users ride even low temperatures.","code":"\npar(mfrow=c(2,1))\nboxplot(train$casual ~ train$temp, xlab=\"temp\", ylab=\"casual users\")\nboxplot(train$registered ~ train$temp, xlab=\"temp\", ylab=\"registered users\")"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"correlation-2","chapter":"29 A detailed study of bike sharing demand","heading":"29.4.9 Correlation","text":"correlation casual atemp, temp.Strong correlation temp atemp.","code":"\nsub = data.frame(train$registered, train$casual, train$count, train$temp,\n                 train$humidity, train$atemp, train$windspeed)\ncor(sub)\n#>                  train.registered train.casual train.count train.temp\n#> train.registered           1.0000       0.4972       0.971     0.3186\n#> train.casual               0.4972       1.0000       0.690     0.4671\n#> train.count                0.9709       0.6904       1.000     0.3945\n#> train.temp                 0.3186       0.4671       0.394     1.0000\n#> train.humidity            -0.2655      -0.3482      -0.317    -0.0649\n#> train.atemp                0.3146       0.4621       0.390     0.9849\n#> train.windspeed            0.0911       0.0923       0.101    -0.0179\n#>                  train.humidity train.atemp train.windspeed\n#> train.registered        -0.2655      0.3146          0.0911\n#> train.casual            -0.3482      0.4621          0.0923\n#> train.count             -0.3174      0.3898          0.1014\n#> train.temp              -0.0649      0.9849         -0.0179\n#> train.humidity           1.0000     -0.0435         -0.3186\n#> train.atemp             -0.0435      1.0000         -0.0575\n#> train.windspeed         -0.3186     -0.0575          1.0000\n# do not show the diagonal\ncorrplot(cor(sub), diag = FALSE)"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"activity-by-year","chapter":"29 A detailed study of bike sharing demand","heading":"29.4.10 Activity by year","text":"","code":""},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"year-extraction","chapter":"29 A detailed study of bike sharing demand","heading":"29.4.10.1 Year extraction","text":"","code":"\n# extracting year\ndata$year = substr(data$datetime, 1, 4)\ndata$year = as.factor(data$year)\n# ignore the division of data again and again, this could have been done together also\ntrain = data[as.integer(substr(data$datetime,9,10)) < 20,]\ntest = data[as.integer(substr(data$datetime,9,10)) > 19,]"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"trend-by-year-training-set","chapter":"29 A detailed study of bike sharing demand","heading":"29.4.10.2 Trend by year, training set","text":"Activity increased 2012.","code":"\npar(mfrow=c(2,1))\n# again some boxplots with different variables\n# these boxplots give important information about the dependent variable with respect to the independent variables\nboxplot(train$casual ~ train$year, xlab=\"year\", ylab=\"casual users\")\nboxplot(train$registered ~ train$year, xlab=\"year\", ylab=\"registered users\")"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"trend-by-windspeed-training-set","chapter":"29 A detailed study of bike sharing demand","heading":"29.4.10.3 trend by windspeed, training set","text":"Casual users ride even stron winds.","code":"\npar(mfrow=c(2,1))\nboxplot(train$casual ~ train$windspeed, xlab=\"windspeed\", ylab=\"casual users\")\nboxplot(train$registered ~ train$windspeed, xlab=\"windspeed\", ylab=\"registered users\")"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"trend-by-humidity-training-set","chapter":"29 A detailed study of bike sharing demand","heading":"29.4.10.4 trend by humidity, training set","text":"Casual users prefer ride humid weather.","code":"\npar(mfrow=c(2,1))\nboxplot(train$casual ~ train$humidity, xlab=\"humidity\", ylab=\"casual users\")\nboxplot(train$registered ~ train$humidity, xlab=\"humidity\", ylab=\"registered users\")"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"feature-engineering-1","chapter":"29 A detailed study of bike sharing demand","heading":"29.5 Feature Engineering","text":"","code":""},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"prepare-data-1","chapter":"29 A detailed study of bike sharing demand","heading":"29.5.1 Prepare data","text":"","code":"\n# factoring some variables from integer\ndata$season     <- as.factor(data$season)\ndata$weather    <- as.factor(data$weather)\ndata$holiday    <- as.factor(data$holiday)\ndata$workingday <- as.factor(data$workingday)\n# new column\ndata$hour <- as.integer(data$hour)\n# created this variable to divide a day into parts, but did not finally use it\ndata$day_part <- 0\n# split in training and test sets again\ntrain <- data[as.integer(substr(data$datetime, 9, 10)) < 20,]\ntest  <- data[as.integer(substr(data$datetime, 9, 10)) > 19,]\n# combine the sets\ndata <- rbind(train, test)"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"build-hour-bins","chapter":"29 A detailed study of bike sharing demand","heading":"29.5.2 Build hour bins","text":"","code":"\n# for registered users\nd = rpart(registered ~ hour, data = train)\nfancyRpartPlot(d)\n# for casual users\nd = rpart(casual ~ hour, data = train)\nfancyRpartPlot(d)\n# Assign the timings according to tree\n# fill the hour bins\ndata = rbind(train,test)\n\n# create hour buckets for registered users\n# 0,1,2,3,4,5,6,7 < 7.5\n# 22,23,24 >=22\n# 10,11,12,13,14,15,16,17: h>=9.5 & h<18\n# h<9.5 & h<8.5 : 8\n# h<9.5 & h>=8.5 : 9\n# h>=20: 20,21\n# h < 20: 18,19\n\ndata$dp_reg = 0\ndata$dp_reg[data$hour < 8] = 1\ndata$dp_reg[data$hour >= 22] = 2\ndata$dp_reg[data$hour > 9 & data$hour < 18] = 3\ndata$dp_reg[data$hour == 8] = 4\ndata$dp_reg[data$hour == 9] = 5\ndata$dp_reg[data$hour == 20 | data$hour == 21] = 6\ndata$dp_reg[data$hour == 19 | data$hour == 18] = 7\n# casual users\n# h<11, h<8.5: 0,1,2,3,4,5,6,7,8\n# h>=8.5 & h<11: 9, 10 \n# h >=11 & h>=21: 21,22,23,24\n# h >=11 & h<21: 11,12,13,14,15,16,17,18,19,20\ndata$dp_cas = 0\ndata$dp_cas[data$hour < 11 & data$hour >= 8] = 1\ndata$dp_cas[data$hour == 9 | data$hour == 10] = 2\ndata$dp_cas[data$hour >= 11 & data$hour < 21] = 3\ndata$dp_cas[data$hour >= 21] = 4"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"temperature-bins","chapter":"29 A detailed study of bike sharing demand","heading":"29.5.3 Temperature bins","text":"","code":"\n# partition the data by temperature, registered users\nf = rpart(registered ~ temp, data=train)\nfancyRpartPlot(f)\n# partition the data by temperature,, casual users\nf=rpart(casual ~ temp, data=train)\nfancyRpartPlot(f)"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"assign-temperature-ranges-accoding-to-trees","chapter":"29 A detailed study of bike sharing demand","heading":"29.5.3.1 Assign temperature ranges accoding to trees","text":"","code":"\ndata$temp_reg = 0\ndata$temp_reg[data$temp < 13] = 1\ndata$temp_reg[data$temp >= 13 & data$temp < 23] = 2\ndata$temp_reg[data$temp >= 23 & data$temp < 30] = 3\ndata$temp_reg[data$temp >= 30] = 4\ndata$temp_cas = 0\ndata$temp_cas[data$temp < 15] = 1\ndata$temp_cas[data$temp >= 15 & data$temp < 23] = 2\ndata$temp_cas[data$temp >= 23 & data$temp < 30] = 3\ndata$temp_cas[data$temp >= 30] = 4"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"year-bins-by-quarter","chapter":"29 A detailed study of bike sharing demand","heading":"29.5.4 Year bins by quarter","text":"","code":"\n# add new variable with the month number\ndata$month <- substr(data$datetime, 6, 7)\ndata$month <- as.integer(data$month)\n# bin by quarter manually\ndata$year_part[data$year=='2011']                = 1\ndata$year_part[data$year=='2011' & data$month>3] = 2\ndata$year_part[data$year=='2011' & data$month>6] = 3\ndata$year_part[data$year=='2011' & data$month>9] = 4\ndata$year_part[data$year=='2012']                = 5\ndata$year_part[data$year=='2012' & data$month>3] = 6\ndata$year_part[data$year=='2012' & data$month>6] = 7\ndata$year_part[data$year=='2012' & data$month>9] = 8\ntable(data$year_part)\n#> \n#>    1    2    3    4    5    6    7    8 \n#> 2067 2183 2192 2203 2176 2182 2208 2168"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"day-type","chapter":"29 A detailed study of bike sharing demand","heading":"29.5.5 Day Type","text":"Created variable categories like “weekday”, “weekend” “holiday”.","code":"\n# creating another variable day_type which may affect our accuracy as weekends and weekdays are important in deciding rentals\ndata$day_type = 0\ndata$day_type[data$holiday==0 & data$workingday==0] = \"weekend\"\ndata$day_type[data$holiday==1]                      = \"holiday\"\ndata$day_type[data$holiday==0 & data$workingday==1] = \"working day\"\n# split dataset again\ntrain = data[as.integer(substr(data$datetime,9,10)) < 20,]\ntest = data[as.integer(substr(data$datetime,9,10)) > 19,]\npar(mfrow=c(2,1))\nboxplot(train$casual ~ train$dp_cas, xlab = \"day partition\", ylab=\"casual users\")\nboxplot(train$registered ~ train$dp_reg, xlab = \"day partition\", ylab=\"registered users\")\npar(mfrow=c(2,1))\nboxplot(train$casual ~ train$day_type, xlab = \"day type\", \n        ylab=\"casual users\", ylim = c(0,900))\nboxplot(train$registered ~ train$day_type, xlab = \"day type\", \n        ylab=\"registered users\", ylim = c(0,900))\npar(mfrow=c(2,1))\nboxplot(train$casual ~ train$year_part, xlab = \"year partition, quarter\", \n        ylab=\"casual users\", ylim = c(0,900))\nboxplot(train$registered ~ train$year_part, xlab = \"year partition, quarter\", \n        ylab=\"registered users\", ylim = c(0,900))"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"temperatures","chapter":"29 A detailed study of bike sharing demand","heading":"29.5.6 Temperatures","text":"","code":"\npar(mfrow=c(2,1))\nboxplot(train$casual ~ train$temp, xlab = \"temperature\", \n        ylab=\"casual users\", ylim = c(0,900))\nboxplot(train$registered ~ train$temp, xlab = \"temperature\", \n        ylab=\"registered users\", ylim = c(0,900))\nplot(train$temp, train$count)\ndata <- rbind(train, test)\n# data$month <- substr(data$datetime, 6, 7)\n# data$month <- as.integer(data$month)"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"imputting-missing-data-to-wind-speed","chapter":"29 A detailed study of bike sharing demand","heading":"29.5.7 Imputting missing data to wind speed","text":"","code":"\n# dividing total data depending on windspeed to impute/predict the missing values\ntable(data$windspeed == 0)\n#> \n#> FALSE  TRUE \n#> 15199  2180\n    # FALSE  TRUE \n    # 15199  2180 \n\nk = data$windspeed == 0\n\nwind_0 = subset(data, k)    # windspeed is zero\nwind_1 = subset(data, !k)   # windspeed not zero\ntic()\n# predicting missing values in windspeed using a random forest model\n# this is a different approach to impute missing values rather than \n# just using the mean or median or some other statistic for imputation\n\nset.seed(415)\nfit <- randomForest(windspeed ~ season + weather + humidity + month + temp + \n                        year + atemp, \n                    data = wind_1, \n                    importance = TRUE, \n                    ntree = 250)\n\npred = predict(fit, wind_0)\nwind_0$windspeed = pred       # fill with wind speed predictions\ntoc()\n#> 44.717 sec elapsed\n# recompose the whole dataset\ndata = rbind(wind_0, wind_1)\n# how many zero values now?\nsum(data$windspeed == 0)\n#> [1] 0"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"weekend-variable","chapter":"29 A detailed study of bike sharing demand","heading":"29.5.8 Weekend variable","text":"Created separate variable weekend (0/1)","code":"\ndata$weekend = 0\ndata$weekend[data$day==\"Sunday\" | data$day==\"Saturday\" ] = 1"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"model-building","chapter":"29 A detailed study of bike sharing demand","heading":"29.6 Model Building","text":"first attempt, applied decision tree, conditional inference tree random forest algorithms found random forest performing best. can also go regression, boosted regression, neural network find one working well .executing random forest model code, followed following steps:Convert discrete variables factor (weather, season, hour, holiday, working day, month, day)","code":"\nstr(data)\n#> 'data.frame':    17379 obs. of  24 variables:\n#>  $ datetime  : Factor w/ 17379 levels \"2011-01-01 00:00:00\",..: 1 2 3 4 5 7 8 9 10 65 ...\n#>  $ season    : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ holiday   : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ workingday: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 2 ...\n#>  $ weather   : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ temp      : num  9.84 9.02 9.02 9.84 9.84 ...\n#>  $ atemp     : num  14.4 13.6 13.6 14.4 14.4 ...\n#>  $ humidity  : num  81 80 80 75 75 80 86 75 76 47 ...\n#>  $ windspeed : num  9.03 9.05 9.05 9.15 9.15 ...\n#>  $ casual    : num  3 8 5 3 0 2 1 1 8 8 ...\n#>  $ registered: num  13 32 27 10 1 0 2 7 6 102 ...\n#>  $ count     : num  16 40 32 13 1 2 3 8 14 110 ...\n#>  $ hour      : int  1 2 3 4 5 7 8 9 10 20 ...\n#>  $ day       : chr  \"Saturday\" \"Saturday\" \"Saturday\" \"Saturday\" ...\n#>  $ year      : Factor w/ 2 levels \"2011\",\"2012\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ day_part  : num  0 0 0 0 0 0 0 0 0 0 ...\n#>  $ dp_reg    : num  1 1 1 1 1 1 4 5 3 6 ...\n#>  $ dp_cas    : num  0 0 0 0 0 0 1 2 2 3 ...\n#>  $ temp_reg  : num  1 1 1 1 1 1 1 1 2 1 ...\n#>  $ temp_cas  : num  1 1 1 1 1 1 1 1 1 1 ...\n#>  $ month     : int  1 1 1 1 1 1 1 1 1 1 ...\n#>  $ year_part : num  1 1 1 1 1 1 1 1 1 1 ...\n#>  $ day_type  : chr  \"weekend\" \"weekend\" \"weekend\" \"weekend\" ...\n#>  $ weekend   : num  1 1 1 1 1 1 1 1 1 0 ..."},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"convert-variables-to-factors","chapter":"29 A detailed study of bike sharing demand","heading":"29.6.1 Convert variables to factors","text":"know dependent variables natural outliers predict log dependent variables.know dependent variables natural outliers predict log dependent variables.Predict bike demand registered casual users separately.\n\\(y1 = \\log(casual+1)\\) \\(y2 = \\log(registered+1)\\), added 1 deal zero values casual registered columns.Predict bike demand registered casual users separately.\n\\(y1 = \\log(casual+1)\\) \\(y2 = \\log(registered+1)\\), added 1 deal zero values casual registered columns.","code":"\n# converting all relevant categorical variables into factors to feed to our random forest model\ndata$season     = as.factor(data$season)\ndata$holiday    = as.factor(data$holiday)\ndata$workingday = as.factor(data$workingday)\ndata$weather    = as.factor(data$weather)\ndata$hour       = as.factor(data$hour)\ndata$month      = as.factor(data$month)\ndata$day_part   = as.factor(data$dp_cas)\ndata$day_type   = as.factor(data$dp_reg)\ndata$day        = as.factor(data$day)\ndata$temp_cas   = as.factor(data$temp_cas)\ndata$temp_reg   = as.factor(data$temp_reg)\nstr(data)\n#> 'data.frame':    17379 obs. of  24 variables:\n#>  $ datetime  : Factor w/ 17379 levels \"2011-01-01 00:00:00\",..: 1 2 3 4 5 7 8 9 10 65 ...\n#>  $ season    : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ holiday   : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ workingday: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 2 ...\n#>  $ weather   : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ temp      : num  9.84 9.02 9.02 9.84 9.84 ...\n#>  $ atemp     : num  14.4 13.6 13.6 14.4 14.4 ...\n#>  $ humidity  : num  81 80 80 75 75 80 86 75 76 47 ...\n#>  $ windspeed : num  9.03 9.05 9.05 9.15 9.15 ...\n#>  $ casual    : num  3 8 5 3 0 2 1 1 8 8 ...\n#>  $ registered: num  13 32 27 10 1 0 2 7 6 102 ...\n#>  $ count     : num  16 40 32 13 1 2 3 8 14 110 ...\n#>  $ hour      : Factor w/ 24 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 7 8 9 10 20 ...\n#>  $ day       : Factor w/ 7 levels \"Friday\",\"Monday\",..: 3 3 3 3 3 3 3 3 3 2 ...\n#>  $ year      : Factor w/ 2 levels \"2011\",\"2012\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ day_part  : Factor w/ 5 levels \"0\",\"1\",\"2\",\"3\",..: 1 1 1 1 1 1 2 3 3 4 ...\n#>  $ dp_reg    : num  1 1 1 1 1 1 4 5 3 6 ...\n#>  $ dp_cas    : num  0 0 0 0 0 0 1 2 2 3 ...\n#>  $ temp_reg  : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 1 1 1 1 1 1 1 2 1 ...\n#>  $ temp_cas  : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ month     : Factor w/ 12 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ year_part : num  1 1 1 1 1 1 1 1 1 1 ...\n#>  $ day_type  : Factor w/ 7 levels \"1\",\"2\",\"3\",\"4\",..: 1 1 1 1 1 1 4 5 3 6 ...\n#>  $ weekend   : num  1 1 1 1 1 1 1 1 1 0 ...\n# separate again as train and test set\ntrain = data[as.integer(substr(data$datetime, 9, 10)) < 20,]\ntest = data[as.integer(substr(data$datetime, 9, 10)) > 19,]"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"log-transform","chapter":"29 A detailed study of bike sharing demand","heading":"29.6.2 Log transform","text":"","code":"\n# log transformation for some skewed variables, \n# which can be seen from their distribution\ntrain$reg1   = train$registered + 1\ntrain$cas1   = train$casual + 1\ntrain$logcas = log(train$cas1)\ntrain$logreg = log(train$reg1)\ntest$logreg  = 0\ntest$logcas  = 0"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"plot-by-weather-by-season","chapter":"29 A detailed study of bike sharing demand","heading":"29.6.2.1 Plot by weather, by season","text":"","code":"\n# cartesian plot\npar(mfrow=c(2,1))\nboxplot(train$registered ~ train$weather, xlab=\"weather\", ylab=\"registered users\")\nboxplot(train$registered ~ train$season, xlab=\"season\", ylab=\"registered users\")\n# semilog plot\npar(mfrow=c(2,1))\nboxplot(train$logreg ~ train$weather, xlab = \"weather\")\nboxplot(train$logreg ~ train$season, xlab = \"season\")"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"predicting-for-registered-and-casual-users-test-dataset","chapter":"29 A detailed study of bike sharing demand","heading":"29.6.3 Predicting for registered and casual users, test dataset","text":"","code":"\ntic()\n# final model building using random forest\n# note that we build different models for predicting for \n# registered and casual users\n# this was seen as giving best result after a lot of experimentation\nset.seed(415)\nfit1 <- randomForest(logreg ~ hour + workingday + day + holiday + day_type +\n                         temp_reg + humidity + atemp + windspeed + season + \n                         weather + dp_reg + weekend + year + year_part, \n                     data = train, \n                     importance = TRUE, \n                     ntree = 250)\n\npred1 = predict(fit1, test)\ntest$logreg = pred1\ntoc()\n#> 98.575 sec elapsed\n# casual users\nset.seed(415)\nfit2 <- randomForest(logcas ~ hour + day_type + day + humidity + atemp + \n                         temp_cas + windspeed + season + weather + holiday +\n                         workingday + dp_cas + weekend + year + year_part, \n                     data = train, importance = TRUE, ntree = 250)\n\npred2 = predict(fit2, test)\ntest$logcas = pred2"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"preparing-and-exporting-results","chapter":"29 A detailed study of bike sharing demand","heading":"29.6.4 Preparing and exporting results","text":"following steps mentioned , can score 0.38675 Kaggle leaderboard .e. top 5 percentile total participants. might seen, applied extraordinary science getting level. , real competition starts . like see, can improve use features advanced modeling techniques.","code":"\n# creating the final submission file\n# reverse log conversion\ntest$registered <- exp(test$logreg) - 1\ntest$casual     <- exp(test$logcas) - 1\ntest$count      <- test$casual + test$registered\n\nr <- data.frame(datetime = test$datetime, \n                casual = test$casual, \n                registered = test$registered)\n\nprint(sum(r$casual))\n#> [1] 205804\nprint(sum(r$registered))\n#> [1] 962834\n\ns <- data.frame(datetime = test$datetime, count = test$count)\nwrite.csv(s, file =file.path(data_out_dir, \"bike-submit.csv\"), row.names = FALSE)\n\n# sum(cas+reg) = 1168638\n# month number now is correct"},{"path":"a-detailed-study-of-bike-sharing-demand.html","id":"end-notes","chapter":"29 A detailed study of bike sharing demand","heading":"29.7 End Notes","text":"article, looked structured approach problem solving method can help improve performance. recommend generate hypothesis deep dive data set technique limit thought process. can improve performance applying advanced techniques (ensemble methods) understand data trend better.can find complete solution : GitHub Link","code":"\n# this is the older submission. months were incomplete\nold <- read.csv(file = file.path(data_raw_dir, \"bike-submit-old.csv\"))\nsum(old$count)\n#> [1] 1164829"},{"path":"prediction-of-arrhythmia-with-deep-neural-nets.html","id":"prediction-of-arrhythmia-with-deep-neural-nets","chapter":"30 Prediction of arrhythmia with deep neural nets","heading":"30 Prediction of arrhythmia with deep neural nets","text":"","code":""},{"path":"prediction-of-arrhythmia-with-deep-neural-nets.html","id":"introduction-14","chapter":"30 Prediction of arrhythmia with deep neural nets","heading":"30.1 Introduction","text":"27 February 2017This week, showing build feed-forward deep neural networks multilayer perceptrons. models example built classify ECG data either healthy hearts someone suffering arrhythmia. show prepare dataset modeling, setting weights modeling parameters, finally, evaluate model performance h2o package.","code":""},{"path":"prediction-of-arrhythmia-with-deep-neural-nets.html","id":"deep-learning-with-neural-networks","chapter":"30 Prediction of arrhythmia with deep neural nets","heading":"30.1.1 Deep learning with neural networks","text":"Deep learning neural networks arguably one rapidly growing applications machine learning AI today. allow building complex models consist multiple hidden layers within artificial networks able find non-linear patterns unstructured data. Deep neural networks usually feed-forward, means layer feeds output subsequent layers, recurrent feed-back neural networks can also built. Feed-forward neural networks also called multilayer perceptrons (MLPs).","code":""},{"path":"prediction-of-arrhythmia-with-deep-neural-nets.html","id":"h2o","chapter":"30 Prediction of arrhythmia with deep neural nets","heading":"30.1.2 H2O","text":"R package h2o provides convenient interface H2O, open-source machine learning deep learning platform. H2O distributes wide range common machine learning algorithms classification, regression deep learning.","code":""},{"path":"prediction-of-arrhythmia-with-deep-neural-nets.html","id":"preparing-the-r-session","chapter":"30 Prediction of arrhythmia with deep neural nets","heading":"30.1.3 Preparing the R session","text":"First, need load packages.","code":"\nlibrary(dplyr)\nlibrary(h2o)\nlibrary(ggplot2)\nlibrary(ggrepel)\nlibrary(h2o)\n\nh2o.init()\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         24 minutes 57 seconds \n#>     H2O cluster timezone:       Etc/UTC \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.30.0.1 \n#>     H2O cluster version age:    7 months and 16 days !!! \n#>     H2O cluster name:           H2O_started_from_R_root_mwl453 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   7.25 GB \n#>     H2O cluster total cores:    8 \n#>     H2O cluster allowed cores:  8 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4 \n#>     R Version:                  R version 3.6.3 (2020-02-29)\n#> Warning in h2o.clusterInfo(): \n#> Your H2O cluster version is too old (7 months and 16 days)!\n#> Please download and install the latest version from http://h2o.ai/download/\nmy_theme <- function(base_size = 12, base_family = \"sans\"){\n  theme_minimal(base_size = base_size, base_family = base_family) +\n  theme(\n    axis.text = element_text(size = 12),\n    axis.title = element_text(size = 14),\n    panel.grid.major = element_line(color = \"grey\"),\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"aliceblue\"),\n    strip.background = element_rect(fill = \"darkgrey\", color = \"grey\", size = 1),\n    strip.text = element_text(face = \"bold\", size = 12, color = \"white\"),\n    legend.position = \"right\",\n    legend.justification = \"top\", \n    panel.border = element_rect(color = \"grey\", fill = NA, size = 0.5)\n  )\n}"},{"path":"prediction-of-arrhythmia-with-deep-neural-nets.html","id":"arrhythmia-data","chapter":"30 Prediction of arrhythmia with deep neural nets","heading":"30.2 Arrhythmia data","text":"data using demonstrate building neural nets arrhythmia dataset UC Irvine’s machine learning database. contains 279 features ECG heart rhythm diagnostics one output column. going rename feature columns many descriptions complex. Also, don’t need know specifically features looking building models.description feature, see https://archive.ics.uci.edu/ml/machine-learning-databases/arrhythmia/arrhythmia.names.output column defines 16 classes: class 1 samples healthy ECGs, remaining classes belong different types arrhythmia, class 16 remaining arrhythmia cases didn’t fit distinct classes.usual, want get acquainted data explore ’s properties building model. , first going look distribution classes healthy arrhythmia samples.interested distinguishing healthy arrhythmia ECGs, converting output binary format combining arrhythmia cases one class.binary classification, almost numbers healthy arrhythmia cases dataset.also interested much normal arrhythmia cases cluster Principal Component Analysis (PCA). first preparing PCA plotting function run feature data.Find plot PCAs.PCA shows big overlap healthy arrhythmia samples, .e. seem major global differences features. class distinct others seems class 9.want give arrhythmia cases different rest stronger weight neural network, define weight column every sample outside central PCA cluster get “2”, effect used twice model.also want know variance within features.Features low variance less likely strongly contribute differentiation healthy arrhythmia cases, going remove . also concatenating weights column:","code":"\narrhythmia <- read.table(file.path(data_raw_dir, \"arrhythmia.data.txt\"), sep = \",\")\narrhythmia[arrhythmia == \"?\"] <- NA\n\n# making sure, that all feature columns are numeric\narrhythmia[-280] <- lapply(arrhythmia[-280], as.character)\narrhythmia[-280] <- lapply(arrhythmia[-280], as.numeric)\n\n#  renaming output column and converting to factor\ncolnames(arrhythmia)[280] <- \"class\"\narrhythmia$class <- as.factor(arrhythmia$class)\np1 <- ggplot(arrhythmia, aes(x = class)) +\n  geom_bar(fill = \"navy\", alpha = 0.7) +\n  my_theme()\n#  all arrhythmia cases into one class\narrhythmia$diagnosis <- ifelse(arrhythmia$class == 1, \"healthy\", \"arrhythmia\")\narrhythmia$diagnosis <- as.factor(arrhythmia$diagnosis)\np2 <- ggplot(arrhythmia, aes(x = diagnosis)) +\n  geom_bar(fill = \"navy\", alpha = 0.7) +\n  my_theme()\nlibrary(gridExtra)\n#> \n#> Attaching package: 'gridExtra'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     combine\nlibrary(grid)\n\ngrid.arrange(p1, p2, ncol = 2)\nlibrary(pcaGoPromoter)\n\npca_func <- function(pcaOutput2, group_name){\n    centroids <- aggregate(cbind(PC1, PC2) ~ groups, pcaOutput2, mean)\n    conf.rgn  <- do.call(rbind, lapply(unique(pcaOutput2$groups), function(t)\n          data.frame(groups = as.character(t),\n                     ellipse(cov(pcaOutput2[pcaOutput2$groups == t, 1:2]),\n                           centre = as.matrix(centroids[centroids$groups == t, 2:3]),\n                           level = 0.95),\n                     stringsAsFactors = FALSE)))\n        \n    plot <- ggplot(data = pcaOutput2, aes(x = PC1, y = PC2, group = groups, \n                                          color = groups)) + \n      geom_polygon(data = conf.rgn, aes(fill = groups), alpha = 0.2) +\n      geom_point(size = 2, alpha = 0.5) + \n      labs(color = paste(group_name),\n           fill = paste(group_name),\n           x = paste0(\"PC1: \", round(pcaOutput$pov[1], digits = 2) * 100, \"% variance\"),\n           y = paste0(\"PC2: \", round(pcaOutput$pov[2], digits = 2) * 100, \"% variance\")) +\n      my_theme()\n    \n    return(plot)\n}\n# Find what columns have NAs and the quantity\nfor (col in names(arrhythmia)) {\n    n_nas <- length(which(is.na(arrhythmia[, col])))\n    if (n_nas > 0) cat(col, n_nas, \"\\n\")\n}\n#> V11 8 \n#> V12 22 \n#> V13 1 \n#> V14 376 \n#> V15 1\n# Replace NAs with zeros\narrhythmia[is.na(arrhythmia)] <- 0\npcaOutput <- pca(t(arrhythmia[-c(280, 281)]), printDropped=FALSE, \n                 scale=TRUE, \n                 center = TRUE)\n\npcaOutput2 <- as.data.frame(pcaOutput$scores)\n\npcaOutput2$groups <- arrhythmia$class\np1 <- pca_func(pcaOutput2, group_name = \"class\")\n\npcaOutput2$groups <- arrhythmia$diagnosis\np2 <- pca_func(pcaOutput2, group_name = \"diagnosis\")\n\ngrid.arrange(p1, p2, ncol = 2)\nweights <- ifelse(pcaOutput2$PC1 < -5 & abs(pcaOutput2$PC2) > 10, 2, 1)\nlibrary(matrixStats)\n#> \n#> Attaching package: 'matrixStats'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     count\n\ncolvars <- data.frame(feature = colnames(arrhythmia[-c(280, 281)]),\n                      variance = colVars(as.matrix(arrhythmia[-c(280, 281)])))\n\nsubset(colvars, variance > 50) %>%\n  mutate(feature = factor(feature, levels = colnames(arrhythmia[-c(280, 281)]))) %>%\n  ggplot(aes(x = feature, y = variance)) +\n    geom_bar(stat = \"identity\", fill = \"navy\", alpha = 0.7) +\n    my_theme() +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\narrhythmia_subset <- cbind(weights, \n                           arrhythmia[, c(281, 280, which(colvars$variance > 50))])"},{"path":"prediction-of-arrhythmia-with-deep-neural-nets.html","id":"converting-the-dataframe-to-a-h2o-object","chapter":"30 Prediction of arrhythmia with deep neural nets","heading":"30.3 Converting the dataframe to a h2o object","text":"Now final data frame modeling, working h2o functions, data needs converted DataFrame H2O Frame. done as_h2o_frame() function.can now access functions h2o package built work h2o Frames. useful function h2o.describe(). similar base R’s summary() function outputs many descriptive measures data. get good overview measures, going plot .also interested correlation features output. can use h2o.cor() function calculate correlation matrix. much easier understand data visualize , going create another plot.","code":"#as_h2o_frame(arrhythmia_subset)\narrhythmia_hf <- as.h2o(arrhythmia_subset, key=\"arrhtythmia.hex\")\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\nlibrary(tidyr) # for gathering\n#> \n#> Attaching package: 'tidyr'\n#> The following object is masked from 'package:S4Vectors':\n#> \n#>     expand\nh2o.describe(arrhythmia_hf[, -1]) %>% # excluding the weights column\n  gather(x, y, Zeros:Sigma) %>%\n  mutate(group = ifelse(\n    x %in% c(\"Min\", \"Max\", \"Mean\"), \"min, mean, max\", \n    ifelse(x %in% c(\"NegInf\", \"PosInf\"), \"Inf\", \"sigma, zeros\"))) %>% \n  # separating them into facets makes them easier to see\n  mutate(Label = factor(Label, levels = colnames(arrhythmia_hf[, -1]))) %>%\n  ggplot(aes(x = Label, y = as.numeric(y), color = x)) +\n    geom_point(size = 4, alpha = 0.6) +\n    scale_color_brewer(palette = \"Set1\") +\n    my_theme() +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +\n    facet_grid(group ~ ., scales = \"free\") +\n    labs(x = \"Feature\",\n         y = \"Value\",\n         color = \"\")\n#> Warning: Removed 2 rows containing missing values (geom_point).\nlibrary(reshape2) # for melting\n#> \n#> Attaching package: 'reshape2'\n#> The following object is masked from 'package:tidyr':\n#> \n#>     smiths\n\n# diagnosis is now a characer column and we need to convert it again\narrhythmia_hf[, 2] <- h2o.asfactor(arrhythmia_hf[, 2]) \narrhythmia_hf[, 3] <- h2o.asfactor(arrhythmia_hf[, 3]) # same for class\n\ncor <- h2o.cor(arrhythmia_hf[, -c(1, 3)])\nrownames(cor) <- colnames(cor)\n\nmelt(cor) %>%\n  mutate(Var2 = rep(rownames(cor), nrow(cor))) %>%\n  mutate(Var2 = factor(Var2, levels = colnames(cor))) %>%\n  mutate(variable = factor(variable, levels = colnames(cor))) %>%\n  ggplot(aes(x = variable, y = Var2, fill = value)) + \n    geom_tile(width = 0.9, height = 0.9) +\n    scale_fill_gradient2(low = \"white\", high = \"red\", name = \"Cor.\") +\n    my_theme() +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +\n    labs(x = \"\", \n         y = \"\")\n#> No id variables; using all as measure variables"},{"path":"prediction-of-arrhythmia-with-deep-neural-nets.html","id":"training-test-and-validation-data","chapter":"30 Prediction of arrhythmia with deep neural nets","heading":"30.4 Training, test and validation data","text":"Now can use h2o.splitFrame() function split data training, validation test data., using 70% training 15% validation testing. also just split data two sections, training test set sufficient samples, good idea evaluate model performance independent test set top training validation set. can easily overfit model, want get idea generalizable - can assess looking well works previously unknown data.also defining response, features weights column names now.categorical features, use h2o.interaction() function define interaction terms, since numeric features , don’t need .can also run PCA training data, using h2o.prcomp() function calculate singular value decomposition Gram matrix power method.","code":"\nsplits <- h2o.splitFrame(arrhythmia_hf, \n                         ratios = c(0.7, 0.15), \n                         seed = 1)\n\ntrain <- splits[[1]]\nvalid <- splits[[2]]\ntest <- splits[[3]]\n\nresponse <- \"diagnosis\"\nweights <- \"weights\"\nfeatures <- setdiff(colnames(train), c(response, weights, \"class\"))\nsummary(train$diagnosis, exact_quantiles = TRUE)\n#>  diagnosis      \n#>  healthy   :163 \n#>  arrhythmia:155\nsummary(valid$diagnosis, exact_quantiles = TRUE)\n#>  diagnosis     \n#>  healthy   :43 \n#>  arrhythmia:25\nsummary(test$diagnosis, exact_quantiles = TRUE)\n#>  diagnosis     \n#>  healthy   :39 \n#>  arrhythmia:27pca <- h2o.prcomp(training_frame = train,\n           x = features,\n           validation_frame = valid,\n           transform = \"NORMALIZE\",\n           k = 3,\n           seed = 42)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |======================================================================| 100%\n#> Warning in doTryCatch(return(expr), name, parentenv, handler): _train: Dataset\n#> used may contain fewer number of rows due to removal of rows with NA/missing\n#> values. If this is not desirable, set impute_missing argument in pca call to\n#> TRUE/True/true/... depending on the client language.\npca\n#> Model Details:\n#> ==============\n#> \n#> H2ODimReductionModel: pca\n#> Model ID:  PCA_model_R_1605840408949_2800 \n#> Importance of components: \n#>                             pc1      pc2      pc3\n#> Standard deviation     0.582620 0.507796 0.421869\n#> Proportion of Variance 0.164697 0.125110 0.086351\n#> Cumulative Proportion  0.164697 0.289808 0.376159\n#> \n#> \n#> H2ODimReductionMetrics: pca\n#> \n#> No model metrics available for PCA\n#> H2ODimReductionMetrics: pca\n#> \n#> No model metrics available for PCA\neigenvec <- as.data.frame(pca@model$eigenvectors)\neigenvec$label <- features\n\nggplot(eigenvec, aes(x = pc1, y = pc2, label = label)) +\n  geom_point(color = \"navy\", alpha = 0.7) +\n  geom_text_repel() +\n  my_theme()"},{"path":"prediction-of-arrhythmia-with-deep-neural-nets.html","id":"modeling","chapter":"30 Prediction of arrhythmia with deep neural nets","heading":"30.5 Modeling","text":"Now, can build deep neural network model. can specify quite parameters, likeCross-validation: Cross validation can tell us training validation errors model. final model overwritten best model, don’t specify otherwise.Cross-validation: Cross validation can tell us training validation errors model. final model overwritten best model, don’t specify otherwise.Adaptive learning rate: deep learning h2o, default use stochastic gradient descent optimization adaptive learning rate. two corresponding parameters rho epsilon help us find global (near enough) optima.Adaptive learning rate: deep learning h2o, default use stochastic gradient descent optimization adaptive learning rate. two corresponding parameters rho epsilon help us find global (near enough) optima.Activation function: activation function defines node output relative given set inputs. want activation function non-linear continuously differentiable.Activation function: activation function defines node output relative given set inputs. want activation function non-linear continuously differentiable.Hidden nodes: Defines number hidden layers number nodes per layer.Hidden nodes: Defines number hidden layers number nodes per layer.Epochs: Increasing number epochs (one full training cycle training samples) can increase model performance, also run risk overfitting. determine optimal number epochs, need use early stopping.Epochs: Increasing number epochs (one full training cycle training samples) can increase model performance, also run risk overfitting. determine optimal number epochs, need use early stopping.Early stopping: default, early stopping enabled. means training stopped reach certain validation error prevent overfitting.Early stopping: default, early stopping enabled. means training stopped reach certain validation error prevent overfitting.course, need quite bit experience intuition hit good combination parameters. ’s usually makes sense grid search hyper-parameter tuning. , want focus building evaluating deep learning models, though. cover grid search next week’s post.training can take , depending many samples, features, nodes hidden layers training , good idea save model.can re-load model time check model quality make predictions new data.","code":"# this will take some time and all CPUs\ndl_model <- h2o.deeplearning(x = features,\n                             y = response,\n                             weights_column = weights,\n                             model_id = \"dl_model\",\n                             training_frame = train,\n                             validation_frame = valid,\n                             nfolds = 15,                                   # 10x cross validation\n                             keep_cross_validation_fold_assignment = TRUE,\n                             fold_assignment = \"Stratified\",\n                             activation = \"RectifierWithDropout\",\n                             score_each_iteration = TRUE,\n                             hidden = c(200, 200, 200, 200, 200),           # 5 hidden layers, each of 200 neurons\n                             epochs = 100,\n                             variable_importances = TRUE,\n                             export_weights_and_biases = TRUE,\n                             seed = 42)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |======================================================================| 100%\n# if file exists, overwrite it\nh2o.saveModel(dl_model, path = file.path(data_out_dir, \"dl_model\"), force = TRUE)\n#> [1] \"/home/rstudio/all/output/data/dl_model/dl_model\"\ndl_model <- h2o.loadModel(file.path(data_out_dir, \"dl_model/dl_model\"))"},{"path":"prediction-of-arrhythmia-with-deep-neural-nets.html","id":"model-performance","chapter":"30 Prediction of arrhythmia with deep neural nets","heading":"30.6 Model performance","text":"now want know model performed validation data. summary() function give us detailed overview model. showing output , quite extensive.One performance metric usually interested mean per class error training validation data.confusion matrix tells us, many classes predicted correctly many predictions accurate. , see errors predictions validation data.can also plot classification error epochs samples.Next classification error, usually interested logistic loss (negative log-likelihood log loss). describes sum errors sample training validation data negative logarithm likelihood error given prediction/ classification. Simply put, lower loss, better model (ignore potential overfitting).can also plot mean squared error (MSE). MSE tells us average prediction errors squared, .e. estimator’s variance bias. closer zero, better model.Next, want know area curve (AUC). AUC important metric measuring binary classification model performances. gives area curve, .e. integral, true positive vs false positive rates. closer 1, better model.weights connecting two adjacent layers per-neuron biases specified model save, can accessed :Variable importance can extracted well (keep mind, variable importance deep neural networks difficult assess considered rough estimates).","code":"\nsum_model <- summary(dl_model)\n#> Model Details:\n#> ==============\n#> \n#> H2OBinomialModel: deeplearning\n#> Model Key:  dl_model \n#> Status of Neuron Layers: predicting diagnosis, 2-class classification, bernoulli distribution, CrossEntropy loss, 179,402 weights/biases, 2.1 MB, 34,090 training samples, mini-batch size 1\n#>   layer units             type dropout       l1       l2 mean_rate rate_rms\n#> 1     1    90            Input  0.00 %       NA       NA        NA       NA\n#> 2     2   200 RectifierDropout 50.00 % 0.000000 0.000000  0.004410 0.003566\n#> 3     3   200 RectifierDropout 50.00 % 0.000000 0.000000  0.007427 0.004335\n#> 4     4   200 RectifierDropout 50.00 % 0.000000 0.000000  0.010675 0.005708\n#> 5     5   200 RectifierDropout 50.00 % 0.000000 0.000000  0.009960 0.004888\n#> 6     6   200 RectifierDropout 50.00 % 0.000000 0.000000  0.016891 0.033925\n#> 7     7     2          Softmax      NA 0.000000 0.000000  0.002348 0.001300\n#>   momentum mean_weight weight_rms mean_bias bias_rms\n#> 1       NA          NA         NA        NA       NA\n#> 2 0.000000    0.003335   0.096079  0.425264 0.057396\n#> 3 0.000000   -0.008360   0.074794  0.953139 0.052764\n#> 4 0.000000   -0.008134   0.072302  0.965284 0.025436\n#> 5 0.000000   -0.005446   0.071378  0.980205 0.032167\n#> 6 0.000000   -0.009832   0.071224  0.960465 0.031353\n#> 7 0.000000   -0.041027   0.379064 -0.001154 0.102214\n#> \n#> H2OBinomialMetrics: deeplearning\n#> ** Reported on training data. **\n#> ** Metrics reported on full training frame **\n#> \n#> MSE:  0.0186\n#> RMSE:  0.137\n#> LogLoss:  0.0818\n#> Mean Per-Class Error:  0.0183\n#> AUC:  0.986\n#> AUCPR:  0.964\n#> Gini:  0.972\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>            arrhythmia healthy    Error    Rate\n#> arrhythmia        158       6 0.036585  =6/164\n#> healthy             0     163 0.000000  =0/163\n#> Totals            158     169 0.018349  =6/327\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold      value idx\n#> 1                       max f1  0.418479   0.981928 168\n#> 2                       max f2  0.418479   0.992692 168\n#> 3                 max f0point5  0.777120   0.975460 162\n#> 4                 max accuracy  0.418479   0.981651 168\n#> 5                max precision  0.994204   1.000000   0\n#> 6                   max recall  0.418479   1.000000 168\n#> 7              max specificity  0.994204   1.000000   0\n#> 8             max absolute_mcc  0.418479   0.963956 168\n#> 9   max min_per_class_accuracy  0.777120   0.975460 162\n#> 10 max mean_per_class_accuracy  0.418479   0.981707 168\n#> 11                     max tns  0.994204 164.000000   0\n#> 12                     max fns  0.994204 162.000000   0\n#> 13                     max fps  0.000000 164.000000 317\n#> 14                     max tps  0.418479 163.000000 168\n#> 15                     max tnr  0.994204   1.000000   0\n#> 16                     max fnr  0.994204   0.993865   0\n#> 17                     max fpr  0.000000   1.000000 317\n#> 18                     max tpr  0.418479   1.000000 168\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: deeplearning\n#> ** Reported on validation data. **\n#> ** Metrics reported on full validation frame **\n#> \n#> MSE:  0.185\n#> RMSE:  0.43\n#> LogLoss:  1.07\n#> Mean Per-Class Error:  0.232\n#> AUC:  0.856\n#> AUCPR:  0.904\n#> Gini:  0.712\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>            arrhythmia healthy    Error    Rate\n#> arrhythmia         14      11 0.440000  =11/25\n#> healthy             1      42 0.023256   =1/43\n#> Totals             15      53 0.176471  =12/68\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold     value idx\n#> 1                       max f1  0.000010  0.875000  52\n#> 2                       max f2  0.000010  0.933333  52\n#> 3                 max f0point5  0.532411  0.844749  43\n#> 4                 max accuracy  0.026911  0.823529  46\n#> 5                max precision  0.994281  1.000000   0\n#> 6                   max recall  0.000001  1.000000  60\n#> 7              max specificity  0.994281  1.000000   0\n#> 8             max absolute_mcc  0.000010  0.624149  52\n#> 9   max min_per_class_accuracy  0.837920  0.744186  37\n#> 10 max mean_per_class_accuracy  0.026911  0.793488  46\n#> 11                     max tns  0.994281 25.000000   0\n#> 12                     max fns  0.994281 42.000000   0\n#> 13                     max fps  0.000000 25.000000  67\n#> 14                     max tps  0.000001 43.000000  60\n#> 15                     max tnr  0.994281  1.000000   0\n#> 16                     max fnr  0.994281  0.976744   0\n#> 17                     max fpr  0.000000  1.000000  67\n#> 18                     max tpr  0.000001  1.000000  60\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: deeplearning\n#> ** Reported on cross-validation data. **\n#> ** 15-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n#> \n#> MSE:  0.179\n#> RMSE:  0.424\n#> LogLoss:  0.612\n#> Mean Per-Class Error:  0.22\n#> AUC:  0.832\n#> AUCPR:  0.791\n#> Gini:  0.664\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>            arrhythmia healthy    Error     Rate\n#> arrhythmia        118      46 0.280488  =46/164\n#> healthy            26     137 0.159509  =26/163\n#> Totals            144     183 0.220183  =72/327\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold      value idx\n#> 1                       max f1  0.467012   0.791908 182\n#> 2                       max f2  0.010805   0.890487 251\n#> 3                 max f0point5  0.818005   0.767511 126\n#> 4                 max accuracy  0.479766   0.779817 180\n#> 5                max precision  0.990047   1.000000   0\n#> 6                   max recall  0.000494   1.000000 285\n#> 7              max specificity  0.990047   1.000000   0\n#> 8             max absolute_mcc  0.467012   0.564026 182\n#> 9   max min_per_class_accuracy  0.642316   0.760736 162\n#> 10 max mean_per_class_accuracy  0.467012   0.780001 182\n#> 11                     max tns  0.990047 164.000000   0\n#> 12                     max fns  0.990047 162.000000   0\n#> 13                     max fps  0.000002 164.000000 317\n#> 14                     max tps  0.000494 163.000000 285\n#> 15                     max tnr  0.990047   1.000000   0\n#> 16                     max fnr  0.990047   0.993865   0\n#> 17                     max fpr  0.000002   1.000000 317\n#> 18                     max tpr  0.000494   1.000000 285\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> Cross-Validation Metrics Summary: \n#>                 mean         sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid\n#> accuracy   0.8264511 0.06905811 0.71428573  0.7647059 0.93333334 0.82608694\n#> auc       0.85387987 0.08763333 0.72727275 0.73333335  0.9814815 0.88095236\n#> aucpr     0.81194234 0.16594325  0.6327996 0.41917738  0.9882933  0.9155332\n#> err       0.17354885 0.06905811  0.2857143 0.23529412 0.06666667 0.17391305\n#> err_count  4.0666666  2.3441925        8.0        4.0        1.0        4.0\n#>           cv_5_valid cv_6_valid cv_7_valid cv_8_valid cv_9_valid cv_10_valid\n#> accuracy   0.8888889     0.9375  0.8181818 0.90909094  0.7692308         0.8\n#> auc        0.9220779   0.984375      0.875  0.9285714 0.78431374   0.8348214\n#> aucpr      0.9566689  0.9852771  0.8248434  0.9663477  0.8781041    0.787485\n#> err       0.11111111     0.0625 0.18181819 0.09090909 0.23076923         0.2\n#> err_count        2.0        1.0        4.0        1.0        6.0         6.0\n#>           cv_11_valid cv_12_valid cv_13_valid cv_14_valid cv_15_valid\n#> accuracy    0.8181818   0.7619048        0.75  0.83870965   0.8666667\n#> auc        0.88429755         0.7   0.8203125      0.8625   0.8888889\n#> aucpr       0.9034981  0.55518734  0.81944454   0.7740876   0.7723875\n#> err        0.18181819  0.23809524        0.25  0.16129032  0.13333334\n#> err_count         4.0         5.0         8.0         5.0         2.0\n#> \n#> ---\n#>                   mean          sd  cv_1_valid   cv_2_valid cv_3_valid\n#> pr_auc      0.81194234  0.16594325   0.6327996   0.41917738  0.9882933\n#> precision   0.77556306 0.123630576  0.57894737    0.5714286        0.9\n#> r2          0.28461012  0.25423092 -0.15165131 -0.032088812 0.74034876\n#> recall      0.92027056 0.094280414         1.0          0.8        1.0\n#> rmse         0.4067296 0.074735165    0.524111   0.46289793 0.24963234\n#> specificity  0.7232667  0.18531013   0.5294118         0.75  0.8333333\n#>             cv_4_valid cv_5_valid cv_6_valid cv_7_valid cv_8_valid cv_9_valid\n#> pr_auc       0.9155332  0.9566689  0.9852771  0.8248434  0.9663477  0.8781041\n#> precision    0.7777778 0.90909094  0.8888889       0.75        1.0 0.73913044\n#> r2           0.3932612 0.43196946   0.566897 0.31167182  0.5744002 0.16804448\n#> recall             1.0 0.90909094        1.0       0.75 0.85714287        1.0\n#> rmse         0.3801529 0.36741653 0.32905284 0.39910218 0.31382465 0.43393275\n#> specificity  0.5555556 0.85714287      0.875 0.85714287        1.0 0.33333334\n#>             cv_10_valid cv_11_valid  cv_12_valid cv_13_valid cv_14_valid\n#> pr_auc         0.787485   0.9034981   0.55518734  0.81944454   0.7740876\n#> precision          0.75   0.8181818    0.6666667   0.6666667   0.8666667\n#> r2              0.27393   0.3500686 -0.105942845  0.11054755  0.37114796\n#> recall       0.85714287   0.8181818          1.0         1.0      0.8125\n#> rmse         0.42510086   0.4030916    0.5252226  0.47155392   0.3962946\n#> specificity        0.75   0.8181818   0.54545456         0.5   0.8666667\n#>             cv_15_valid\n#> pr_auc        0.7723875\n#> precision          0.75\n#> r2            0.2665479\n#> recall              1.0\n#> rmse          0.4195575\n#> specificity   0.7777778\n#> \n#> Scoring History: \n#>              timestamp          duration training_speed    epochs iterations\n#> 1  2020-11-20 03:13:16         0.000 sec             NA   0.00000          0\n#> 2  2020-11-20 03:13:17  1 min 17.796 sec   6407 obs/sec  10.72013          1\n#> 3  2020-11-20 03:13:17  1 min 18.386 sec   6336 obs/sec  21.44025          2\n#> 4  2020-11-20 03:13:18  1 min 19.009 sec   6209 obs/sec  32.16038          3\n#> 5  2020-11-20 03:13:18  1 min 19.593 sec   6255 obs/sec  42.88050          4\n#> 6  2020-11-20 03:13:19  1 min 20.122 sec   6407 obs/sec  53.60063          5\n#> 7  2020-11-20 03:13:20  1 min 20.672 sec   6470 obs/sec  64.32075          6\n#> 8  2020-11-20 03:13:20  1 min 21.161 sec   6626 obs/sec  75.04088          7\n#> 9  2020-11-20 03:13:21  1 min 21.713 sec   6651 obs/sec  85.76101          8\n#> 10 2020-11-20 03:13:21  1 min 22.206 sec   6753 obs/sec  96.48113          9\n#> 11 2020-11-20 03:13:22  1 min 22.691 sec   6849 obs/sec 107.20126         10\n#>         samples training_rmse training_logloss training_r2 training_auc\n#> 1      0.000000            NA               NA          NA           NA\n#> 2   3409.000000       0.38710          0.50069     0.40061      0.87805\n#> 3   6818.000000       0.37006          0.45518     0.45221      0.91819\n#> 4  10227.000000       0.31307          0.33984     0.60796      0.93371\n#> 5  13636.000000       0.29218          0.28780     0.65851      0.94389\n#> 6  17045.000000       0.27788          0.26995     0.69114      0.95470\n#> 7  20454.000000       0.24824          0.21679     0.75350      0.96263\n#> 8  23863.000000       0.22656          0.17875     0.79468      0.97187\n#> 9  27272.000000       0.20888          0.15915     0.82547      0.97684\n#> 10 30681.000000       0.20676          0.14910     0.82901      0.97943\n#> 11 34090.000000       0.13655          0.08185     0.92541      0.98593\n#>    training_pr_auc training_lift training_classification_error validation_rmse\n#> 1               NA            NA                            NA              NA\n#> 2          0.85608       2.00613                       0.20183         0.42497\n#> 3          0.88384       2.00613                       0.13761         0.44540\n#> 4          0.90412       2.00613                       0.11927         0.42070\n#> 5          0.91572       2.00613                       0.10703         0.39320\n#> 6          0.92469       2.00613                       0.08869         0.40088\n#> 7          0.93161       2.00613                       0.07339         0.41127\n#> 8          0.95640       2.00613                       0.06422         0.42758\n#> 9          0.96058       2.00613                       0.04281         0.40838\n#> 10         0.96071       2.00613                       0.04893         0.40080\n#> 11         0.96352       2.00613                       0.01835         0.42967\n#>    validation_logloss validation_r2 validation_auc validation_pr_auc\n#> 1                  NA            NA             NA                NA\n#> 2             0.65975       0.22316        0.83349           0.86349\n#> 3             0.71988       0.14669        0.87163           0.90336\n#> 4             0.68549       0.23869        0.86140           0.90503\n#> 5             0.60864       0.33498        0.88465           0.92204\n#> 6             0.61984       0.30874        0.88465           0.92387\n#> 7             0.73716       0.27246        0.85953           0.90783\n#> 8             0.78238       0.21362        0.84744           0.89505\n#> 9             0.93037       0.28265        0.85581           0.89635\n#> 10            0.80015       0.30901        0.87721           0.92082\n#> 11            1.06884       0.20591        0.85581           0.90389\n#>    validation_lift validation_classification_error\n#> 1               NA                              NA\n#> 2          1.58140                         0.20588\n#> 3          1.58140                         0.16176\n#> 4          1.58140                         0.17647\n#> 5          1.58140                         0.17647\n#> 6          1.58140                         0.17647\n#> 7          1.58140                         0.19118\n#> 8          1.58140                         0.17647\n#> 9          1.58140                         0.17647\n#> 10         1.58140                         0.17647\n#> 11         1.58140                         0.17647\n#> \n#> Variable Importances: (Extract with `h2o.varimp`) \n#> =================================================\n#> \n#> Variable Importances: \n#>   variable relative_importance scaled_importance percentage\n#> 1     V169            1.000000          1.000000   0.013985\n#> 2      V15            0.981259          0.981259   0.013723\n#> 3     V239            0.954730          0.954730   0.013352\n#> 4     V136            0.952538          0.952538   0.013322\n#> 5      V42            0.949366          0.949366   0.013277\n#> \n#> ---\n#>    variable relative_importance scaled_importance percentage\n#> 85     V218            0.717785          0.717785   0.010038\n#> 86     V188            0.713244          0.713244   0.009975\n#> 87     V219            0.713025          0.713025   0.009972\n#> 88      V12            0.705674          0.705674   0.009869\n#> 89     V179            0.680337          0.680337   0.009515\n#> 90     V269            0.671023          0.671023   0.009384\nh2o.mean_per_class_error(dl_model, train = TRUE, valid = TRUE, xval = TRUE)\n#>  train  valid   xval \n#> 0.0183 0.2316 0.2200\nh2o.confusionMatrix(dl_model, valid = TRUE)\n#> Confusion Matrix (vertical: actual; across: predicted)  for max f1 @ threshold = 9.50154319453689e-06:\n#>            arrhythmia healthy    Error    Rate\n#> arrhythmia         14      11 0.440000  =11/25\n#> healthy             1      42 0.023256   =1/43\n#> Totals             15      53 0.176471  =12/68\nplot(dl_model,\n     timestep = \"epochs\",\n     metric = \"classification_error\")\nplot(dl_model,\n     timestep = \"samples\",\n     metric = \"classification_error\")\nplot(dl_model,\n     timestep = \"epochs\",\n     metric = \"logloss\")\nplot(dl_model,\n     timestep = \"epochs\",\n     metric = \"rmse\")\nh2o.auc(dl_model, train = TRUE)\n#> [1] 0.986\nh2o.auc(dl_model, valid = TRUE)\n#> [1] 0.856\nh2o.auc(dl_model, xval = TRUE)\n#> [1] 0.832\nw <- h2o.weights(dl_model, matrix_id = 1)\nb <- h2o.biases(dl_model, vector_id = 1)\nh2o.varimp(dl_model)\n#> Variable Importances: \n#>   variable relative_importance scaled_importance percentage\n#> 1     V169            1.000000          1.000000   0.013985\n#> 2      V15            0.981259          0.981259   0.013723\n#> 3     V239            0.954730          0.954730   0.013352\n#> 4     V136            0.952538          0.952538   0.013322\n#> 5      V42            0.949366          0.949366   0.013277\n#> \n#> ---\n#>    variable relative_importance scaled_importance percentage\n#> 85     V218            0.717785          0.717785   0.010038\n#> 86     V188            0.713244          0.713244   0.009975\n#> 87     V219            0.713025          0.713025   0.009972\n#> 88      V12            0.705674          0.705674   0.009869\n#> 89     V179            0.680337          0.680337   0.009515\n#> 90     V269            0.671023          0.671023   0.009384\nh2o.varimp_plot(dl_model)"},{"path":"prediction-of-arrhythmia-with-deep-neural-nets.html","id":"test-data","chapter":"30 Prediction of arrhythmia with deep neural nets","heading":"30.7 Test data","text":"Now good idea model performance validation data, want know performed unseen test data. good model find optimal balance accuracy training test data. model 0% error training data 40% error test data effect useless. overfit training data thus able generalize unknown data.Plotting test performance’s AUC plot shows us approximately good predictions .confusion matrix alone can seen h2o.confusionMatrix() function, also part performance summary.final predictions probabilities can extracted h2o.predict() function. Beware though, number correct wrong classifications can slightly different confusion matrix ., combine predictions actual test diagnoses classes data frame. plotting also want column, tells whether predictions correct. default, prediction probability 0.5 get scored prediction respective category. find often makes sense stringent , though set higher threshold. Therefore, creating another column stringent predictions, count predictions made 80% probability. Everything fall within range gets scored “uncertain”. stringent predictions, also creating column tells whether accurate.get better overview, going plot predictions (default stringent):stringent prediction threshold slightly reduced number errors much.also want know whether certain classes arrhythmia especially prone misclassified:obvious biases towards classes small number samples classes, difficult assess.","code":"\nperf <- h2o.performance(dl_model, test)\nperf\n#> H2OBinomialMetrics: deeplearning\n#> \n#> MSE:  0.241\n#> RMSE:  0.491\n#> LogLoss:  1.68\n#> Mean Per-Class Error:  0.244\n#> AUC:  0.791\n#> AUCPR:  0.814\n#> Gini:  0.582\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>            arrhythmia healthy    Error    Rate\n#> arrhythmia         18       9 0.333333   =9/27\n#> healthy             6      33 0.153846   =6/39\n#> Totals             24      42 0.227273  =15/66\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold     value idx\n#> 1                       max f1  0.021631  0.814815  41\n#> 2                       max f2  0.000001  0.915493  56\n#> 3                 max f0point5  0.960972  0.817610  29\n#> 4                 max accuracy  0.021631  0.772727  41\n#> 5                max precision  0.992921  1.000000   0\n#> 6                   max recall  0.000001  1.000000  56\n#> 7              max specificity  0.992921  1.000000   0\n#> 8             max absolute_mcc  0.021631  0.524142  41\n#> 9   max min_per_class_accuracy  0.705627  0.740741  35\n#> 10 max mean_per_class_accuracy  0.960972  0.759259  29\n#> 11                     max tns  0.992921 27.000000   0\n#> 12                     max fns  0.992921 38.000000   0\n#> 13                     max fps  0.000000 27.000000  65\n#> 14                     max tps  0.000001 39.000000  56\n#> 15                     max tnr  0.992921  1.000000   0\n#> 16                     max fnr  0.992921  0.974359   0\n#> 17                     max fpr  0.000000  1.000000  65\n#> 18                     max tpr  0.000001  1.000000  56\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\nplot(perf)\nh2o.logloss(perf)\n#> [1] 1.68\nh2o.mse(perf)\n#> [1] 0.241\nh2o.auc(perf)\n#> [1] 0.791\nhead(h2o.metric(perf))\n#> Metrics for Thresholds: Binomial metrics as a function of classification thresholds\n#>   threshold       f1       f2 f0point5 accuracy precision   recall specificity\n#> 1  0.992921 0.050000 0.031847 0.116279 0.424242  1.000000 0.025641    1.000000\n#> 2  0.992818 0.097561 0.063291 0.212766 0.439394  1.000000 0.051282    1.000000\n#> 3  0.992377 0.142857 0.094340 0.294118 0.454545  1.000000 0.076923    1.000000\n#> 4  0.992375 0.186047 0.125000 0.363636 0.469697  1.000000 0.102564    1.000000\n#> 5  0.992079 0.181818 0.124224 0.338983 0.454545  0.800000 0.102564    0.962963\n#> 6  0.992010 0.222222 0.154321 0.396825 0.469697  0.833333 0.128205    0.962963\n#>   absolute_mcc min_per_class_accuracy mean_per_class_accuracy tns fns fps tps\n#> 1     0.103203               0.025641                0.512821  27  38   0   1\n#> 2     0.147087               0.051282                0.525641  27  37   0   2\n#> 3     0.181568               0.076923                0.538462  27  36   0   3\n#> 4     0.211341               0.102564                0.551282  27  35   0   4\n#> 5     0.121754               0.102564                0.532764  26  35   1   4\n#> 6     0.155921               0.128205                0.545584  26  34   1   5\n#>        tnr      fnr      fpr      tpr idx\n#> 1 1.000000 0.974359 0.000000 0.025641   0\n#> 2 1.000000 0.948718 0.000000 0.051282   1\n#> 3 1.000000 0.923077 0.000000 0.076923   2\n#> 4 1.000000 0.897436 0.000000 0.102564   3\n#> 5 0.962963 0.897436 0.037037 0.102564   4\n#> 6 0.962963 0.871795 0.037037 0.128205   5\nh2o.confusionMatrix(dl_model, test)\n#> Confusion Matrix (vertical: actual; across: predicted)  for max f1 @ threshold = 0.0216308394253861:\n#>            arrhythmia healthy    Error    Rate\n#> arrhythmia         18       9 0.333333   =9/27\n#> healthy             6      33 0.153846   =6/39\n#> Totals             24      42 0.227273  =15/66finalRf_predictions <- data.frame(class = as.vector(test$class), \n                                  actual = as.vector(test$diagnosis), \n                                  as.data.frame(h2o.predict(object = dl_model, \n                                                            newdata = test)))\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\nfinalRf_predictions$accurate <- ifelse(\n  finalRf_predictions$actual == finalRf_predictions$predict, \"yes\", \"no\")\n\nfinalRf_predictions$predict_stringent <- ifelse(\n  finalRf_predictions$arrhythmia > 0.8, \"arrhythmia\",           \n  ifelse(finalRf_predictions$healthy > 0.8, \"healthy\", \"uncertain\"))\n\nfinalRf_predictions$accurate_stringent <- ifelse(\n  finalRf_predictions$actual == finalRf_predictions$predict_stringent, \"yes\", \n  ifelse(finalRf_predictions$predict_stringent == \"uncertain\", \"na\", \"no\"))\nfinalRf_predictions %>%\n  group_by(actual, predict) %>%\n  summarise(n = n())\n#> # A tibble: 4 x 3\n#> # Groups:   actual [2]\n#>   actual     predict        n\n#>   <fct>      <fct>      <int>\n#> 1 arrhythmia arrhythmia    11\n#> 2 arrhythmia healthy       16\n#> 3 healthy    arrhythmia     3\n#> 4 healthy    healthy       36\nfinalRf_predictions %>%\n  group_by(actual, predict_stringent) %>%\n  summarise(n = n())\n#> # A tibble: 6 x 3\n#> # Groups:   actual [2]\n#>   actual     predict_stringent     n\n#>   <fct>      <chr>             <int>\n#> 1 arrhythmia arrhythmia           19\n#> 2 arrhythmia healthy               6\n#> 3 arrhythmia uncertain             2\n#> 4 healthy    arrhythmia           10\n#> 5 healthy    healthy              28\n#> 6 healthy    uncertain             1\np1 <- finalRf_predictions %>%\n  ggplot(aes(x = actual, fill = accurate)) +\n    geom_bar(position = \"dodge\") +\n    scale_fill_brewer(palette = \"Set1\") +\n    my_theme() +\n    labs(fill = \"Were\\npredictions\\naccurate?\",\n         title = \"Default predictions\")\n\np2 <- finalRf_predictions %>%\n  subset(accurate_stringent != \"na\") %>%\n  ggplot(aes(x = actual, fill = accurate_stringent)) +\n    geom_bar(position = \"dodge\") +\n    scale_fill_brewer(palette = \"Set1\") +\n    my_theme() +\n    labs(fill = \"Were\\npredictions\\naccurate?\",\n         title = \"Stringent predictions\")\n\ngrid.arrange(p1, p2, ncol = 2)\np1 <- subset(finalRf_predictions, actual == \"arrhythmia\") %>%\n  ggplot(aes(x = predict, fill = class)) +\n    geom_bar(position = \"dodge\") +\n    my_theme() +\n    labs(title = \"Prediction accuracy of arrhythmia cases\",\n         subtitle = \"Default predictions\",\n         x = \"predicted to be\")\n\np2 <- subset(finalRf_predictions, actual == \"arrhythmia\") %>%\n  ggplot(aes(x = predict_stringent, fill = class)) +\n    geom_bar(position = \"dodge\") +\n    my_theme() +\n    labs(title = \"Prediction accuracy of arrhythmia cases\",\n         subtitle = \"Stringent predictions\",\n         x = \"predicted to be\")\n\ngrid.arrange(p1, p2, ncol = 2)"},{"path":"prediction-of-arrhythmia-with-deep-neural-nets.html","id":"final-conclusions-how-useful-is-the-model","chapter":"30 Prediction of arrhythmia with deep neural nets","heading":"30.8 Final conclusions: How useful is the model?","text":"samples classified correctly, total error particularly good. Moreover, evaluating usefulness specific model, need keep mind want achieve questions want answer. wanted deploy model clinical setting, assist diagnosing patients. , need think consequences wrong classifications . better optimize high sensitivity, example many arrhythmia cases possible get detected - drawback probably also diagnose healthy people? want maximize precision, meaning confident patient got predicted arrhythmia indeed , accepting arrhythmia cases remain undiagnosed? consider stringent predictions, model correctly classified 19 27 arrhythmia cases, 6 misdiagnosed. mean patients actually sick, wouldn’t gotten correct treatment (decided solely based model). real-life application, obviously sufficient!Next week, ’ll trying improve model grid search hyper-parameter tuning., stay tuned… (sorry, couldn’t resist ;-))","code":"\nsessionInfo()\n#> R version 3.6.3 (2020-02-29)\n#> Platform: x86_64-pc-linux-gnu (64-bit)\n#> Running under: Debian GNU/Linux 10 (buster)\n#> \n#> Matrix products: default\n#> BLAS/LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.3.5.so\n#> \n#> locale:\n#>  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n#>  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n#>  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=C             \n#>  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n#>  [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n#> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n#> \n#> attached base packages:\n#>  [1] stats4    parallel  grid      stats     graphics  grDevices utils    \n#>  [8] datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] reshape2_1.4.4       tidyr_1.0.2          matrixStats_0.56.0  \n#>  [4] pcaGoPromoter_1.30.0 Biostrings_2.54.0    XVector_0.26.0      \n#>  [7] IRanges_2.20.2       S4Vectors_0.24.4     BiocGenerics_0.32.0 \n#> [10] ellipse_0.4.1        gridExtra_2.3        ggrepel_0.8.2       \n#> [13] ggplot2_3.3.0        h2o_3.30.0.1         dplyr_0.8.5         \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] Rcpp_1.0.4.6         utf8_1.1.4           assertthat_0.2.1    \n#>  [4] digest_0.6.25        plyr_1.8.6           R6_2.4.1            \n#>  [7] RSQLite_2.2.0        evaluate_0.14        pillar_1.4.3        \n#> [10] zlibbioc_1.32.0      rlang_0.4.5          data.table_1.12.8   \n#> [13] jquerylib_0.1.2      blob_1.2.1           rmarkdown_2.5.3     \n#> [16] labeling_0.3         stringr_1.4.0        RCurl_1.98-1.2      \n#> [19] bit_1.1-15.2         munsell_0.5.0        compiler_3.6.3      \n#> [22] xfun_0.19.4          pkgconfig_2.0.3      htmltools_0.5.0.9003\n#> [25] downlit_0.2.1.9000   tidyselect_1.0.0     tibble_3.0.1        \n#> [28] bookdown_0.21.4      fansi_0.4.1          crayon_1.3.4        \n#> [31] withr_2.2.0          bitops_1.0-6         rappdirs_0.3.1      \n#> [34] jsonlite_1.6.1       gtable_0.3.0         lifecycle_0.2.0     \n#> [37] DBI_1.1.0            magrittr_1.5         scales_1.1.0        \n#> [40] cli_2.0.2            stringi_1.4.6        farver_2.0.3        \n#> [43] fs_1.4.1             xml2_1.3.2           bslib_0.2.2.9000    \n#> [46] ellipsis_0.3.0       vctrs_0.2.4          RColorBrewer_1.1-2  \n#> [49] tools_3.6.3          bit64_0.9-7          Biobase_2.46.0      \n#> [52] glue_1.4.0           purrr_0.3.4          yaml_2.2.1          \n#> [55] AnnotationDbi_1.48.0 colorspace_1.4-1     memoise_1.1.0       \n#> [58] knitr_1.28           sass_0.2.0.9005"},{"path":"linear-regression-with-islr.html","id":"linear-regression-with-islr","chapter":"31 Linear Regression with ISLR","heading":"31 Linear Regression with ISLR","text":"Dataset: Advertising.csvVideos, slides:https://www.r-bloggers.com/-depth-introduction--machine-learning--15-hours--expert-videos/Data:http://www-bcf.usc.edu/~gareth/ISL/Advertising.csvcode:http://subasish.github.io/pages/ISLwithR/http://math480-s15-zarringhalam.wikispaces.umb.edu/R+Codehttps://github.com/yahwes/ISLRhttps://www.tau.ac.il/~saharon/IntroStatLearn.htmlhttps://www.waxworksmath.com/Authors/G_M/James/WWW/chapter_3.htmlhttps://github.com/asadoughi/stat-learningplots:https://onlinecourses.science.psu.edu/stat857/node/28/Advertising data set. plot displays sales, thousands\nunits, function TV, radio, newspaper budgets, thousands \ndollars, 200 diﬀerent markets.plot show simple least squares\nﬁt sales variable, described Chapter 3. words, blue\nline represents simple model can used predict sales using TV, radio,\nnewspaper, respectively.Recall Advertising data Chapter 2. Figure 2.1 displays sales\n(thousands units) particular product function advertis-\ning budgets (thousands dollars) TV, radio, newspaper media.\nSuppose role statistical consultants asked suggest,\nbasis data, marketing plan next year result \nhigh product sales. information useful order provide\nrecommendation? important questions might\nseek address:relationship advertising budget sales?relationship advertising budget sales?strong relationship advertising budget sales?strong relationship advertising budget sales?media contribute sales?media contribute sales?accurately can estimate eﬀect medium sales?accurately can estimate eﬀect medium sales?Advertising data, least squares fit regression\nsales onto TV shown. fit found minimizing sum squared\nerrors. grey line segment represents error, fit makes compro-\nmise averaging squares. case linear fit captures essence \nrelationship, although somewhat deficient left plot.","code":"\nlibrary(readr)\n\nadvertising <- read_csv(file.path(data_raw_dir, \"Advertising.csv\"))\n#> Warning: Missing column names filled in: 'X1' [1]\n#> Parsed with column specification:\n#> cols(\n#>   X1 = col_double(),\n#>   TV = col_double(),\n#>   radio = col_double(),\n#>   newspaper = col_double(),\n#>   sales = col_double()\n#> )\nadvertising\n#> # A tibble: 200 x 5\n#>      X1    TV radio newspaper sales\n#>   <dbl> <dbl> <dbl>     <dbl> <dbl>\n#> 1     1 230.   37.8      69.2  22.1\n#> 2     2  44.5  39.3      45.1  10.4\n#> 3     3  17.2  45.9      69.3   9.3\n#> 4     4 152.   41.3      58.5  18.5\n#> 5     5 181.   10.8      58.4  12.9\n#> 6     6   8.7  48.9      75     7.2\n#> # … with 194 more rows\npar(mfrow=c(1,3))\nplot(advertising$TV, advertising$sales, xlab = \"TV\", ylab = \"Sales\", col = \"red\")\nplot(advertising$radio, advertising$sales, xlab=\"Radio\", ylab=\"Sales\", col=\"red\")\nplot(advertising$radio, advertising$newspaper, xlab=\"Newspaper\", \n     ylab=\"Sales\", col=\"red\")\npar(mfrow=c(1,3))\ntv_model <- lm(sales ~ TV, data = advertising)\nradio_model <- lm(sales ~ radio, data = advertising)\nnewspaper_model <- lm(sales ~ newspaper, data = advertising)\n\nplot(advertising$TV, advertising$sales, xlab = \"TV\", ylab = \"Sales\", col = \"red\")\nabline(tv_model, col = \"blue\")\nplot(advertising$radio, advertising$sales, xlab=\"Radio\", ylab=\"Sales\", col=\"red\")\nabline(radio_model)\nplot(advertising$newspaper, advertising$sales, xlab=\"Newspaper\", \n     ylab=\"Sales\", col=\"red\")\nabline(newspaper_model)\ntv_model <- lm(sales ~ TV, data = advertising)\nplot(advertising$TV, advertising$sales, xlab = \"TV\", ylab = \"Sales\", \n     col = \"red\", pch=16)\nabline(tv_model, col = \"blue\", lwd=2)\nsegments(advertising$TV, advertising$sales, advertising$TV, predict(tv_model), \n         col = \"gray\")\nsmry <- summary(tv_model)\nsmry\n#> \n#> Call:\n#> lm(formula = sales ~ TV, data = advertising)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -8.386 -1.955 -0.191  2.067  7.212 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  7.03259    0.45784    15.4   <2e-16 ***\n#> TV           0.04754    0.00269    17.7   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 3.26 on 198 degrees of freedom\n#> Multiple R-squared:  0.612,  Adjusted R-squared:  0.61 \n#> F-statistic:  312 on 1 and 198 DF,  p-value: <2e-16\nlibrary(lattice)\n\nminRss <- sqrt(abs(min(smry$residuals))) * sign(min(smry$residuals))\nmaxRss <- sqrt(max(smry$residuals))\n\ntwovar <- function(x, y) { \n  x^2 + y^2 }\n\nmat <- outer( seq(minRss, maxRss, length = 100),  \n                seq(minRss, maxRss, length = 100), \n                Vectorize( function(x,y) twovar(x, y) ) )\n\n\n\ncontourplot(mat, at = c(1,2,3))\ntv_model\n#> \n#> Call:\n#> lm(formula = sales ~ TV, data = advertising)\n#> \n#> Coefficients:\n#> (Intercept)           TV  \n#>      7.0326       0.0475\ntv.lm <- lm(sales ~ poly(sales, TV, degree=2), data = advertising)\n# contour(tv.lm, sales ~ TV)\nlibrary(rsm)\nmpg.lm <- lm(mpg ~ poly(hp, disp, degree = 3), data = mtcars)\ncontour(mpg.lm, hp ~ disp)\nx <- -6:16\nop <- par(mfrow = c(2, 2))\ncontour(outer(x, x), method = \"flattest\", vfont = c(\"sans serif\", \"plain\"))"},{"path":"evaluation-of-three-linear-regression-models.html","id":"evaluation-of-three-linear-regression-models","chapter":"32 Evaluation of three linear regression models","heading":"32 Evaluation of three linear regression models","text":"Dataset: iris.csvAlgorithms:\nSimple Linear Regression\nMultiple Regression\nNeural Networks\nSimple Linear RegressionMultiple RegressionNeural Networks","code":""},{"path":"evaluation-of-three-linear-regression-models.html","id":"introduction-15","chapter":"32 Evaluation of three linear regression models","heading":"32.1 Introduction","text":"https://www.matthewrenze.com/workshops/practical-machine-learning--r/lab-3a-regression.html","code":""},{"path":"evaluation-of-three-linear-regression-models.html","id":"explore-the-data","chapter":"32 Evaluation of three linear regression models","heading":"32.2 Explore the Data","text":"Load Iris dataPlot scatterplotPlot correlogramCreate scatterplot matrix","code":"\ndata(iris)\nwrite.csv(iris, file.path(data_raw_dir, \"iris.csv\"))\nplot(iris[1:4])\nlibrary(corrgram)\n#> Registered S3 method overwritten by 'seriation':\n#>   method         from \n#>   reorder.hclust gclus\ncorrgram(iris[1:4])\ncor(iris[1:4])\n#>              Sepal.Length Sepal.Width Petal.Length Petal.Width\n#> Sepal.Length        1.000      -0.118        0.872       0.818\n#> Sepal.Width        -0.118       1.000       -0.428      -0.366\n#> Petal.Length        0.872      -0.428        1.000       0.963\n#> Petal.Width         0.818      -0.366        0.963       1.000\ncor(\n  x = iris$Petal.Length, \n  y = iris$Petal.Width)\n#> [1] 0.963\nplot(\n  x = iris$Petal.Length, \n  y = iris$Petal.Width,\n  xlim = c(0.25, 7),\n  ylim = c(0.25, 2.5))"},{"path":"evaluation-of-three-linear-regression-models.html","id":"create-training-and-test-sets","chapter":"32 Evaluation of three linear regression models","heading":"32.3 Create Training and Test Sets","text":"","code":"\nset.seed(42)\nindexes <- sample(\n  x = 1:150, \n  size = 100)\ntrain <- iris[indexes, ]\ntest <- iris[-indexes, ]"},{"path":"evaluation-of-three-linear-regression-models.html","id":"predict-with-simple-linear-regression","chapter":"32 Evaluation of three linear regression models","heading":"32.4 Predict with Simple Linear Regression","text":"","code":"\nsimpleModel <- lm(\n  formula = Petal.Width ~ Petal.Length,\n  data = train)\nplot(\n  x = iris$Petal.Length, \n  y = iris$Petal.Width,\n  xlim = c(0.25, 7),\n  ylim = c(0.25, 2.5))\n  \nlines(\n  x = train$Petal.Length,\n  y = simpleModel$fitted, \n  col = \"red\",\n  lwd = 3)\nsummary(simpleModel)\n#> \n#> Call:\n#> lm(formula = Petal.Width ~ Petal.Length, data = train)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -0.5684 -0.1279 -0.0307  0.1280  0.6385 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   -0.3486     0.0476   -7.33  6.7e-11 ***\n#> Petal.Length   0.4137     0.0119   34.80  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.209 on 98 degrees of freedom\n#> Multiple R-squared:  0.925,  Adjusted R-squared:  0.924 \n#> F-statistic: 1.21e+03 on 1 and 98 DF,  p-value: <2e-16\nsimplePredictions <- predict(\n  object = simpleModel,\n  newdata = test)\nplot(\n  x = iris$Petal.Length, \n  y = iris$Petal.Width,\n  xlim = c(0.25, 7),\n  ylim = c(0.25, 2.5))\n  \npoints(\n  x = test$Petal.Length,\n  y = simplePredictions,\n  col = \"blue\",\n  pch = 4,\n  lwd = 2)\n\npoints(\n  x = test$Petal.Length,\n  y = test$Petal.Width,\n  col = \"red\",\n  pch = 16)\nsimpleRMSE <- sqrt(mean((test$Petal.Width - simplePredictions)^2))\nprint(simpleRMSE)\n#> [1] 0.201"},{"path":"evaluation-of-three-linear-regression-models.html","id":"predict-with-multiple-regression","chapter":"32 Evaluation of three linear regression models","heading":"32.5 Predict with Multiple Regression","text":"","code":"\nmultipleModel <- lm(\n  formula = Petal.Width ~ .,\n  data = train)\nsummary(multipleModel)\n#> \n#> Call:\n#> lm(formula = Petal.Width ~ ., data = train)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -0.5769 -0.0843 -0.0066  0.0978  0.4731 \n#> \n#> Coefficients:\n#>                   Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)        -0.5088     0.2277   -2.23  0.02779 *  \n#> Sepal.Length       -0.0486     0.0593   -0.82  0.41435    \n#> Sepal.Width         0.2032     0.0594    3.42  0.00092 ***\n#> Petal.Length        0.2103     0.0641    3.28  0.00146 ** \n#> Speciesversicolor   0.6769     0.1583    4.28  4.5e-05 ***\n#> Speciesvirginica    1.0762     0.2126    5.06  2.1e-06 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.176 on 94 degrees of freedom\n#> Multiple R-squared:  0.949,  Adjusted R-squared:  0.947 \n#> F-statistic:  352 on 5 and 94 DF,  p-value: <2e-16\nmultiplePredictions <- predict(\n  object = multipleModel,\n  newdata = test)\nplot(\n  x = iris$Petal.Length, \n  y = iris$Petal.Width,\n  xlim = c(0.25, 7),\n  ylim = c(0.25, 2.5))\n  \npoints(\n  x = test$Petal.Length,\n  y = multiplePredictions,\n  col = \"blue\",\n  pch = 4,\n  lwd = 2)\n\npoints(\n  x = test$Petal.Length,\n  y = test$Petal.Width,\n  col = \"red\",\n  pch = 16)\nmultipleRMSE <- sqrt(mean((test$Petal.Width - multiplePredictions)^2))\nprint(multipleRMSE)\n#> [1] 0.15"},{"path":"evaluation-of-three-linear-regression-models.html","id":"predict-with-neural-network-regression","chapter":"32 Evaluation of three linear regression models","heading":"32.6 5. Predict with Neural Network Regression","text":"","code":"\nnormalize <- function(x) {\n  (x - min(x)) / (max(x) - min(x)) - 0.5\n}\ndenormalize <- function(x, y) {\n  ((x + 0.5) * (max(y) - min(y))) + min(y)\n}\nscaledIris <- data.frame(\n  Sepal.Length = normalize(iris$Sepal.Length),\n  Sepal.Width = normalize(iris$Sepal.Width),\n  Petal.Length = normalize(iris$Petal.Length),\n  Petal.Width = normalize(iris$Petal.Width),\n  Species = iris$Species)\nscaledTrain <- scaledIris[indexes, ]\nscaledTest <- scaledIris[-indexes, ]\nlibrary(nnet)\n\nneuralRegressor <- nnet(\n  formula = Petal.Width ~ .,\n  data = scaledTrain,\n  linout = TRUE,\n  skip = TRUE,\n  size = 4,\n  decay = 0.0001,\n  maxit = 500)\n#> # weights:  34\n#> initial  value 64.175158 \n#> iter  10 value 0.498340\n#> iter  20 value 0.439307\n#> iter  30 value 0.419373\n#> iter  40 value 0.415119\n#> iter  50 value 0.412305\n#> iter  60 value 0.410862\n#> iter  70 value 0.404854\n#> iter  80 value 0.402606\n#> iter  90 value 0.397903\n#> iter 100 value 0.396295\n#> iter 110 value 0.394292\n#> iter 120 value 0.392628\n#> iter 130 value 0.390306\n#> iter 140 value 0.389577\n#> iter 150 value 0.388916\n#> iter 160 value 0.387607\n#> iter 170 value 0.382857\n#> iter 180 value 0.377332\n#> iter 190 value 0.371974\n#> iter 200 value 0.366019\n#> iter 210 value 0.357405\n#> iter 220 value 0.351831\n#> iter 230 value 0.347613\n#> iter 240 value 0.344466\n#> iter 250 value 0.341515\n#> iter 260 value 0.340828\n#> iter 270 value 0.340236\n#> iter 280 value 0.338736\n#> iter 290 value 0.337991\n#> iter 300 value 0.336182\n#> iter 310 value 0.333793\n#> iter 320 value 0.331206\n#> iter 330 value 0.330171\n#> iter 340 value 0.329803\n#> iter 350 value 0.329587\n#> iter 360 value 0.329343\n#> iter 370 value 0.328909\n#> iter 380 value 0.327579\n#> iter 390 value 0.326227\n#> iter 400 value 0.323911\n#> iter 410 value 0.322154\n#> iter 420 value 0.320878\n#> iter 430 value 0.320122\n#> iter 440 value 0.319153\n#> iter 450 value 0.318239\n#> iter 460 value 0.316869\n#> iter 470 value 0.315668\n#> iter 480 value 0.314685\n#> iter 490 value 0.314604\n#> iter 500 value 0.314257\n#> final  value 0.314257 \n#> stopped after 500 iterations\nlibrary(NeuralNetTools)\n\nplotnet(neuralRegressor)\nscaledPredictions <- predict(\n  object = neuralRegressor, \n  newdata = scaledTest)\nneuralPredictions <- denormalize(\n  x = scaledPredictions, \n  y = iris$Petal.Width)\nplot(\n  x = iris$Petal.Length, \n  y = iris$Petal.Width,\n  xlim = c(0.25, 7),\n  ylim = c(0.25, 2.5))\n  \npoints(\n  x = test$Petal.Length,\n  y = neuralPredictions,\n  col = \"blue\",\n  pch = 4,\n  lwd = 2)\n\npoints(\n  x = test$Petal.Length,\n  y = test$Petal.Width,\n  col = \"red\",\n  pch = 16)\nneuralRMSE <- sqrt(mean((test$Petal.Width - neuralPredictions)^2))\nprint(neuralRMSE)\n#> [1] 0.188"},{"path":"evaluation-of-three-linear-regression-models.html","id":"evaluate-all-the-regression-models","chapter":"32 Evaluation of three linear regression models","heading":"32.7 6. Evaluate all the regression Models","text":"","code":"\nprint(simpleRMSE)\n#> [1] 0.201\nprint(multipleRMSE)\n#> [1] 0.15\nprint(neuralRMSE)\n#> [1] 0.188"},{"path":"comparison-of-six-linear-regression-algorithms.html","id":"comparison-of-six-linear-regression-algorithms","chapter":"33 Comparison of six Linear Regression algorithms","heading":"33 Comparison of six Linear Regression algorithms","text":"Datasets: BostonAlgorithms:\nLM, GKM, GLMNET, SVM, CART, KNN\nLM, GKM, GLMNET, SVM, CART, KNNObjective: Comparison various algorithms","code":""},{"path":"comparison-of-six-linear-regression-algorithms.html","id":"introduction-16","chapter":"33 Comparison of six Linear Regression algorithms","heading":"33.1 Introduction","text":"algorithms used:LMGLMGLMNETSVMCARTKNN","code":"\n# load packages\nlibrary(mlbench)\nlibrary(caret)\n#> Loading required package: lattice\n#> Loading required package: ggplot2\nlibrary(corrplot)\n#> corrplot 0.84 loaded\n\n# attach the BostonHousing dataset\ndata(BostonHousing)\ndplyr::glimpse(BostonHousing)\n#> Rows: 506\n#> Columns: 14\n#> $ crim    <dbl> 0.00632, 0.02731, 0.02729, 0.03237, 0.06905, 0.02985, 0.08829…\n#> $ zn      <dbl> 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.5, 12.5, 12.5, 12.5, 12.5, …\n#> $ indus   <dbl> 2.31, 7.07, 7.07, 2.18, 2.18, 2.18, 7.87, 7.87, 7.87, 7.87, 7…\n#> $ chas    <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#> $ nox     <dbl> 0.538, 0.469, 0.469, 0.458, 0.458, 0.458, 0.524, 0.524, 0.524…\n#> $ rm      <dbl> 6.58, 6.42, 7.18, 7.00, 7.15, 6.43, 6.01, 6.17, 5.63, 6.00, 6…\n#> $ age     <dbl> 65.2, 78.9, 61.1, 45.8, 54.2, 58.7, 66.6, 96.1, 100.0, 85.9, …\n#> $ dis     <dbl> 4.09, 4.97, 4.97, 6.06, 6.06, 6.06, 5.56, 5.95, 6.08, 6.59, 6…\n#> $ rad     <dbl> 1, 2, 2, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4…\n#> $ tax     <dbl> 296, 242, 242, 222, 222, 222, 311, 311, 311, 311, 311, 311, 3…\n#> $ ptratio <dbl> 15.3, 17.8, 17.8, 18.7, 18.7, 18.7, 15.2, 15.2, 15.2, 15.2, 1…\n#> $ b       <dbl> 397, 397, 393, 395, 397, 394, 396, 397, 387, 387, 393, 397, 3…\n#> $ lstat   <dbl> 4.98, 9.14, 4.03, 2.94, 5.33, 5.21, 12.43, 19.15, 29.93, 17.1…\n#> $ medv    <dbl> 24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 1…\ntibble::as_tibble(BostonHousing)\n#> # A tibble: 506 x 14\n#>      crim    zn indus chas    nox    rm   age   dis   rad   tax ptratio     b\n#>     <dbl> <dbl> <dbl> <fct> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl> <dbl>\n#> 1 0.00632    18  2.31 0     0.538  6.58  65.2  4.09     1   296    15.3  397.\n#> 2 0.0273      0  7.07 0     0.469  6.42  78.9  4.97     2   242    17.8  397.\n#> 3 0.0273      0  7.07 0     0.469  7.18  61.1  4.97     2   242    17.8  393.\n#> 4 0.0324      0  2.18 0     0.458  7.00  45.8  6.06     3   222    18.7  395.\n#> 5 0.0690      0  2.18 0     0.458  7.15  54.2  6.06     3   222    18.7  397.\n#> 6 0.0298      0  2.18 0     0.458  6.43  58.7  6.06     3   222    18.7  394.\n#> # … with 500 more rows, and 2 more variables: lstat <dbl>, medv <dbl>"},{"path":"comparison-of-six-linear-regression-algorithms.html","id":"workflow-4","chapter":"33 Comparison of six Linear Regression algorithms","heading":"33.2 Workflow","text":"Load datasetCreate train test datasets, 80/20Inspect dataset:DimensionclassesskimrAnalyze featurescorrelationVisualize featureshistogramsdensity plotspairwisecorrelogramTrain -isSet train control \n10 cross-validations\n3 repetitions\nMetric: RMSE\n10 cross-validations3 repetitionsMetric: RMSETrain modelsCompare accuracy modelsVisual comparison\ndot plot\ndot plotTrain Feature selectionFeature selection\nfindCorrelation\ngenerate new dataset\nfindCorrelationgenerate new datasetTrain models againCompare RMSE againVisual comparison\ndot plot\ndot plotTrain dataset transformationdata transformatiom\nCenter\nScale\nBoxCox\nCenterScaleBoxCoxTrain modelsCompare RMSEVisual comparison\ndot plot\ndot plotTune best modelSet train control \n10 cross-validations\n3 repetitions\nMetric: RMSE\n10 cross-validations3 repetitionsMetric: RMSETrain models\nRadial SVM\nSigma vector\n.C\nBoxCox\n9, Ensembling\nRadial SVMSigma vector.CBoxCox\n9, EnsemblingSelect algorithms\nRandom Forest\nStochastic Gradient Boosting\nCubist\nRandom ForestStochastic Gradient BoostingCubistNumeric comparison\nresample\nsummary\nresamplesummaryVisual comparisondot plotTune best model: CubistSet train control \n10 cross-validations\n3 repetitions\nMetric: RMSE\n10 cross-validations3 repetitionsMetric: RMSETrain models\nCubist\n.committees\n.neighbors\nBoxCox\nCubist.committees.neighborsBoxCoxEvaluate tuning parameters\nNumeric comparison\nprint tuned model\n\nVisual comparison\nscatter plot\n\nNumeric comparison\nprint tuned model\nprint tuned modelVisual comparison\nscatter plot\nscatter plotFinalize modelBack transformationSummaryApply model validation set\nTransform dataset\nMake prediction\nCalculate RMSE\nTransform datasetMake predictionCalculate RMSE","code":""},{"path":"comparison-of-six-linear-regression-algorithms.html","id":"preparing-the-data","chapter":"33 Comparison of six Linear Regression algorithms","heading":"33.3 Preparing the data","text":"factors character variables","code":"\n# Split out validation dataset\n# create a list of 80% of the rows in the original dataset we can use for training\nset.seed(7)\nvalidationIndex <- createDataPartition(BostonHousing$medv, \n                                       p=0.80, list=FALSE)\n\n# select 20% of the data for validation\nvalidation <- BostonHousing[-validationIndex,]\n\n# use the remaining 80% of data to training and testing the models\ndataset <- BostonHousing[validationIndex,]\n# dimensions of dataset\ndim(validation)\n#> [1] 99 14\ndim(dataset)\n#> [1] 407  14\n# list types for each attribute\nsapply(dataset, class)\n#>      crim        zn     indus      chas       nox        rm       age       dis \n#> \"numeric\" \"numeric\" \"numeric\"  \"factor\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \n#>       rad       tax   ptratio         b     lstat      medv \n#> \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\" \"numeric\"\n# take a peek at the first 20 rows of the data\nhead(dataset, n=20)\n#>       crim   zn indus chas   nox   rm   age  dis rad tax ptratio   b lstat medv\n#> 1  0.00632 18.0  2.31    0 0.538 6.58  65.2 4.09   1 296    15.3 397  4.98 24.0\n#> 2  0.02731  0.0  7.07    0 0.469 6.42  78.9 4.97   2 242    17.8 397  9.14 21.6\n#> 3  0.02729  0.0  7.07    0 0.469 7.18  61.1 4.97   2 242    17.8 393  4.03 34.7\n#> 4  0.03237  0.0  2.18    0 0.458 7.00  45.8 6.06   3 222    18.7 395  2.94 33.4\n#> 5  0.06905  0.0  2.18    0 0.458 7.15  54.2 6.06   3 222    18.7 397  5.33 36.2\n#> 6  0.02985  0.0  2.18    0 0.458 6.43  58.7 6.06   3 222    18.7 394  5.21 28.7\n#> 7  0.08829 12.5  7.87    0 0.524 6.01  66.6 5.56   5 311    15.2 396 12.43 22.9\n#> 10 0.17004 12.5  7.87    0 0.524 6.00  85.9 6.59   5 311    15.2 387 17.10 18.9\n#> 11 0.22489 12.5  7.87    0 0.524 6.38  94.3 6.35   5 311    15.2 393 20.45 15.0\n#> 12 0.11747 12.5  7.87    0 0.524 6.01  82.9 6.23   5 311    15.2 397 13.27 18.9\n#> 13 0.09378 12.5  7.87    0 0.524 5.89  39.0 5.45   5 311    15.2 390 15.71 21.7\n#> 14 0.62976  0.0  8.14    0 0.538 5.95  61.8 4.71   4 307    21.0 397  8.26 20.4\n#> 15 0.63796  0.0  8.14    0 0.538 6.10  84.5 4.46   4 307    21.0 380 10.26 18.2\n#> 17 1.05393  0.0  8.14    0 0.538 5.93  29.3 4.50   4 307    21.0 387  6.58 23.1\n#> 20 0.72580  0.0  8.14    0 0.538 5.73  69.5 3.80   4 307    21.0 391 11.28 18.2\n#> 21 1.25179  0.0  8.14    0 0.538 5.57  98.1 3.80   4 307    21.0 377 21.02 13.6\n#> 22 0.85204  0.0  8.14    0 0.538 5.96  89.2 4.01   4 307    21.0 393 13.83 19.6\n#> 23 1.23247  0.0  8.14    0 0.538 6.14  91.7 3.98   4 307    21.0 397 18.72 15.2\n#> 24 0.98843  0.0  8.14    0 0.538 5.81 100.0 4.10   4 307    21.0 395 19.88 14.5\n#> 25 0.75026  0.0  8.14    0 0.538 5.92  94.1 4.40   4 307    21.0 394 16.30 15.6\nlibrary(skimr)\nprint(skim_with(numeric = list(hist = NULL)))\n#> Creating new skimming functions for the following classes: hist.\n#> They did not have recognized defaults. Call get_default_skimmers() for more information.\n#> function(data, ...) {\n#>     data_name <- rlang::expr_label(substitute(data))\n#>     if (!inherits(data, \"data.frame\")) {\n#>       data <- as.data.frame(data)\n#>     }\n#>     stopifnot(inherits(data, \"data.frame\"))\n#> \n#>     .vars <- rlang::quos(...)\n#>     cols <- names(data)\n#>     if (length(.vars) == 0) {\n#>       selected <- cols\n#>     } else {\n#>       selected <- tidyselect::vars_select(cols, !!!.vars)\n#>     }\n#> \n#>     grps <- dplyr::groups(data)\n#>     if (length(grps) > 0) {\n#>       group_variables <- selected %in% as.character(grps)\n#>       selected <- selected[!group_variables]\n#>     } else {\n#>       attr(data, \"groups\") <- list()\n#>     }\n#> \n#> \n#>     skimmers <- purrr::map(\n#>       selected, get_final_skimmers, data, local_skimmers, append\n#>     )\n#>     types <- purrr::map_chr(skimmers, \"skim_type\")\n#>     unique_skimmers <- reduce_skimmers(skimmers, types)\n#>     combined_skimmers <- purrr::map(unique_skimmers, join_with_base, base)\n#>     ready_to_skim <- tibble::tibble(\n#>       skim_type = unique(types),\n#>       skimmers = purrr::map(combined_skimmers, mangle_names, names(base$funs)),\n#>       skim_variable = split(selected, types)[unique(types)]\n#>     )\n#>     grouped <- dplyr::group_by(ready_to_skim, .data$skim_type)\n#>     nested <- dplyr::summarize(\n#>       grouped,\n#>       skimmed = purrr::map2(\n#>         .data$skimmers, .data$skim_variable, skim_by_type, data\n#>       )\n#>     )\n#>     structure(\n#>       tidyr::unnest(nested, .data$skimmed),\n#>       class = c(\"skim_df\", \"tbl_df\", \"tbl\", \"data.frame\"),\n#>       data_rows = nrow(data),\n#>       data_cols = ncol(data),\n#>       df_name = data_name,\n#>       groups = dplyr::groups(data),\n#>       base_skimmers = names(base$funs),\n#>       skimmers_used = get_skimmers_used(unique_skimmers)\n#>     )\n#>   }\n#> <bytecode: 0x55a71f623430>\n#> <environment: 0x55a71f5c8db8>\nprint(skim(dataset))\n#> ── Data Summary ────────────────────────\n#>                            Values \n#> Name                       dataset\n#> Number of rows             407    \n#> Number of columns          14     \n#> _______________________           \n#> Column type frequency:            \n#>   factor                   1      \n#>   numeric                  13     \n#> ________________________          \n#> Group variables            None   \n#> \n#> ── Variable type: factor ───────────────────────────────────────────────────────\n#>   skim_variable n_missing complete_rate ordered n_unique top_counts   \n#> 1 chas                  0             1 FALSE          2 0: 378, 1: 29\n#> \n#> ── Variable type: numeric ──────────────────────────────────────────────────────\n#>    skim_variable n_missing complete_rate    mean      sd        p0      p25\n#>  1 crim                  0             1   3.64    8.80    0.00632   0.0796\n#>  2 zn                    0             1  11.9    24.2     0         0     \n#>  3 indus                 0             1  11.0     6.87    0.74      4.93  \n#>  4 nox                   0             1   0.555   0.117   0.385     0.448 \n#>  5 rm                    0             1   6.29    0.704   3.56      5.89  \n#>  6 age                   0             1  68.4    28.2     6.2      42.7   \n#>  7 dis                   0             1   3.82    2.12    1.13      2.11  \n#>  8 rad                   0             1   9.59    8.77    1         4     \n#>  9 tax                   0             1 409.    169.    187       280.    \n#> 10 ptratio               0             1  18.4     2.18   12.6      17     \n#> 11 b                     0             1 357.     89.7     0.32    374.    \n#> 12 lstat                 0             1  12.6     7.03    1.92      7.06  \n#> 13 medv                  0             1  22.5     8.96    5        17.0   \n#>        p50     p75    p100 hist \n#>  1   0.268   3.69   89.0   ▇▁▁▁▁\n#>  2   0      15     100     ▇▁▁▁▁\n#>  3   8.56   18.1    27.7   ▇▆▁▇▁\n#>  4   0.538   0.631   0.871 ▇▇▆▃▁\n#>  5   6.21    6.62    8.78  ▁▁▇▂▁\n#>  6  77.3    94.2   100     ▂▃▂▂▇\n#>  7   3.15    5.21   12.1   ▇▅▂▁▁\n#>  8   5      24      24     ▇▂▁▁▃\n#>  9 334     666     711     ▇▇▃▁▇\n#> 10  19      20.2    22     ▁▃▅▅▇\n#> 11 391.    396.    397.    ▁▁▁▁▇\n#> 12  11.3    16.5    38.0   ▇▇▃▂▁\n#> 13  21.2    25      50     ▂▇▃▁▁\ndataset[,4] <- as.numeric(as.character(dataset[,4]))\nprint(skim(dataset))\n#> ── Data Summary ────────────────────────\n#>                            Values \n#> Name                       dataset\n#> Number of rows             407    \n#> Number of columns          14     \n#> _______________________           \n#> Column type frequency:            \n#>   numeric                  14     \n#> ________________________          \n#> Group variables            None   \n#> \n#> ── Variable type: numeric ──────────────────────────────────────────────────────\n#>    skim_variable n_missing complete_rate     mean      sd        p0      p25\n#>  1 crim                  0             1   3.64     8.80    0.00632   0.0796\n#>  2 zn                    0             1  11.9     24.2     0         0     \n#>  3 indus                 0             1  11.0      6.87    0.74      4.93  \n#>  4 chas                  0             1   0.0713   0.258   0         0     \n#>  5 nox                   0             1   0.555    0.117   0.385     0.448 \n#>  6 rm                    0             1   6.29     0.704   3.56      5.89  \n#>  7 age                   0             1  68.4     28.2     6.2      42.7   \n#>  8 dis                   0             1   3.82     2.12    1.13      2.11  \n#>  9 rad                   0             1   9.59     8.77    1         4     \n#> 10 tax                   0             1 409.     169.    187       280.    \n#> 11 ptratio               0             1  18.4      2.18   12.6      17     \n#> 12 b                     0             1 357.      89.7     0.32    374.    \n#> 13 lstat                 0             1  12.6      7.03    1.92      7.06  \n#> 14 medv                  0             1  22.5      8.96    5        17.0   \n#>        p50     p75    p100 hist \n#>  1   0.268   3.69   89.0   ▇▁▁▁▁\n#>  2   0      15     100     ▇▁▁▁▁\n#>  3   8.56   18.1    27.7   ▇▆▁▇▁\n#>  4   0       0       1     ▇▁▁▁▁\n#>  5   0.538   0.631   0.871 ▇▇▆▃▁\n#>  6   6.21    6.62    8.78  ▁▁▇▂▁\n#>  7  77.3    94.2   100     ▂▃▂▂▇\n#>  8   3.15    5.21   12.1   ▇▅▂▁▁\n#>  9   5      24      24     ▇▂▁▁▃\n#> 10 334     666     711     ▇▇▃▁▇\n#> 11  19      20.2    22     ▁▃▅▅▇\n#> 12 391.    396.    397.    ▁▁▁▁▇\n#> 13  11.3    16.5    38.0   ▇▇▃▂▁\n#> 14  21.2    25      50     ▂▇▃▁▁"},{"path":"comparison-of-six-linear-regression-algorithms.html","id":"variables-correlation","chapter":"33 Comparison of six Linear Regression algorithms","heading":"33.3.1 Variables correlation","text":"","code":"\n# find correlation between variables\ncor(dataset[,1:13])\n#>            crim      zn   indus     chas     nox     rm     age    dis      rad\n#> crim     1.0000 -0.1996  0.4076 -0.05507  0.4099 -0.194  0.3524 -0.376  0.60834\n#> zn      -0.1996  1.0000 -0.5314 -0.02987 -0.5202  0.311 -0.5845  0.680 -0.32273\n#> indus    0.4076 -0.5314  1.0000  0.06583  0.7733 -0.383  0.6512 -0.711  0.61998\n#> chas    -0.0551 -0.0299  0.0658  1.00000  0.0934  0.127  0.0735 -0.099 -0.00245\n#> nox      0.4099 -0.5202  0.7733  0.09340  1.0000 -0.296  0.7338 -0.769  0.62760\n#> rm      -0.1940  0.3111 -0.3826  0.12677 -0.2961  1.000 -0.2262  0.207 -0.22126\n#> age      0.3524 -0.5845  0.6512  0.07350  0.7338 -0.226  1.0000 -0.749  0.46896\n#> dis     -0.3756  0.6799 -0.7113 -0.09905 -0.7693  0.207 -0.7492  1.000 -0.50372\n#> rad      0.6083 -0.3227  0.6200 -0.00245  0.6276 -0.221  0.4690 -0.504  1.00000\n#> tax      0.5711 -0.3184  0.7185 -0.03064  0.6758 -0.295  0.5058 -0.526  0.92005\n#> ptratio  0.2897 -0.3888  0.3782 -0.12283  0.1888 -0.365  0.2709 -0.228  0.47971\n#> b       -0.3442  0.1747 -0.3644  0.03782 -0.3684  0.126 -0.2742  0.284 -0.42314\n#> lstat    0.4229 -0.4219  0.6136 -0.08430  0.5839 -0.612  0.6066 -0.501  0.50251\n#>             tax ptratio       b   lstat\n#> crim     0.5711   0.290 -0.3442  0.4229\n#> zn      -0.3184  -0.389  0.1747 -0.4219\n#> indus    0.7185   0.378 -0.3644  0.6136\n#> chas    -0.0306  -0.123  0.0378 -0.0843\n#> nox      0.6758   0.189 -0.3684  0.5839\n#> rm      -0.2953  -0.365  0.1260 -0.6120\n#> age      0.5058   0.271 -0.2742  0.6066\n#> dis     -0.5264  -0.228  0.2843 -0.5013\n#> rad      0.9201   0.480 -0.4231  0.5025\n#> tax      1.0000   0.469 -0.4303  0.5538\n#> ptratio  0.4691   1.000 -0.1700  0.4093\n#> b       -0.4303  -0.170  1.0000 -0.3509\n#> lstat    0.5538   0.409 -0.3509  1.0000\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\n\nm <- cor(dataset[,1:13])\ndiag(m) <- 0\n# select variables with correlation 0.7 and above\nthreshold <- 0.7\nok <- apply(abs(m) >= threshold, 1, any)\nm[ok, ok]\n#>        indus    nox    age    dis    rad    tax\n#> indus  0.000  0.773  0.651 -0.711  0.620  0.719\n#> nox    0.773  0.000  0.734 -0.769  0.628  0.676\n#> age    0.651  0.734  0.000 -0.749  0.469  0.506\n#> dis   -0.711 -0.769 -0.749  0.000 -0.504 -0.526\n#> rad    0.620  0.628  0.469 -0.504  0.000  0.920\n#> tax    0.719  0.676  0.506 -0.526  0.920  0.000\n# values of correlation >= 0.7\nind <- sapply(1:13, function(x) abs(m[, x]) > 0.7)\nm[ind]\n#>  [1]  0.773 -0.711  0.719  0.773  0.734 -0.769  0.734 -0.749 -0.711 -0.769\n#> [11] -0.749  0.920  0.719  0.920\n# defining a index for selecting if the condition is met\ncind <- apply(m, 2, function(x) any(abs(x) > 0.7))\ncm <- m[, cind] # since col6 only has values less than 0.5 it is not taken\ncm\n#>           indus     nox     age    dis      rad     tax\n#> crim     0.4076  0.4099  0.3524 -0.376  0.60834  0.5711\n#> zn      -0.5314 -0.5202 -0.5845  0.680 -0.32273 -0.3184\n#> indus    0.0000  0.7733  0.6512 -0.711  0.61998  0.7185\n#> chas     0.0658  0.0934  0.0735 -0.099 -0.00245 -0.0306\n#> nox      0.7733  0.0000  0.7338 -0.769  0.62760  0.6758\n#> rm      -0.3826 -0.2961 -0.2262  0.207 -0.22126 -0.2953\n#> age      0.6512  0.7338  0.0000 -0.749  0.46896  0.5058\n#> dis     -0.7113 -0.7693 -0.7492  0.000 -0.50372 -0.5264\n#> rad      0.6200  0.6276  0.4690 -0.504  0.00000  0.9201\n#> tax      0.7185  0.6758  0.5058 -0.526  0.92005  0.0000\n#> ptratio  0.3782  0.1888  0.2709 -0.228  0.47971  0.4691\n#> b       -0.3644 -0.3684 -0.2742  0.284 -0.42314 -0.4303\n#> lstat    0.6136  0.5839  0.6066 -0.501  0.50251  0.5538\nrind <- apply(cm, 1, function(x) any(abs(x) > 0.7))  \nrm <- cm[rind, ]\nrm\n#>        indus    nox    age    dis    rad    tax\n#> indus  0.000  0.773  0.651 -0.711  0.620  0.719\n#> nox    0.773  0.000  0.734 -0.769  0.628  0.676\n#> age    0.651  0.734  0.000 -0.749  0.469  0.506\n#> dis   -0.711 -0.769 -0.749  0.000 -0.504 -0.526\n#> rad    0.620  0.628  0.469 -0.504  0.000  0.920\n#> tax    0.719  0.676  0.506 -0.526  0.920  0.000"},{"path":"comparison-of-six-linear-regression-algorithms.html","id":"a-look-at-the-variables","chapter":"33 Comparison of six Linear Regression algorithms","heading":"33.3.2 A look at the variables","text":"","code":"\n# histograms for each attribute\npar(mfrow=c(3,5))\nfor(i in 1:13) {\n    hist(dataset[,i], main=names(dataset)[i])\n}\n# density plot for each attribute\npar(mfrow=c(3,5))\nfor(i in 1:13) {\nplot(density(dataset[,i]), main=names(dataset)[i])\n}\n# boxplots for each attribute\npar(mfrow=c(3,5))\nfor(i in 1:13) {\nboxplot(dataset[,i], main=names(dataset)[i])\n}\n# scatter plot matrix\npairs(dataset[,1:13])\n# correlation plot\ncorrelations <- cor(dataset[,1:13])\ncorrplot(correlations, method=\"circle\")"},{"path":"comparison-of-six-linear-regression-algorithms.html","id":"evaluation-of-algorithms","chapter":"33 Comparison of six Linear Regression algorithms","heading":"33.4 Evaluation of algorithms","text":"","code":"\n# Run algorithms using 10-fold cross-validation\ntrainControl <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\nmetric <- \"RMSE\"\n# LM\nset.seed(7)\nfit.lm <- train(medv~., data=dataset, method=\"lm\", \n                metric=metric, preProc=c(\"center\", \"scale\"), \n                trControl=trainControl)\n# GLM\nset.seed(7)\nfit.glm <- train(medv~., data=dataset, method=\"glm\", \n                 metric=metric, preProc=c(\"center\", \"scale\"), \n                 trControl=trainControl)\n# GLMNET\nset.seed(7)\nfit.glmnet <- train(medv~., data=dataset, method=\"glmnet\", \n                    metric=metric, \n                    preProc=c(\"center\", \"scale\"), \n                    trControl=trainControl)\n# SVM\nset.seed(7)\nfit.svm <- train(medv~., data=dataset, method=\"svmRadial\", \n                 metric=metric, \n                 preProc=c(\"center\", \"scale\"), \n                 trControl=trainControl)\n# CART\nset.seed(7)\ngrid <- expand.grid(.cp=c(0, 0.05, 0.1))\nfit.cart <- train(medv~., data=dataset, method=\"rpart\", \n                  metric=metric, tuneGrid=grid, \n                  preProc=c(\"center\", \"scale\"), \n                  trControl=trainControl)\n# KNN\nset.seed(7)\nfit.knn <- train(medv~., data=dataset, method=\"knn\", \n                 metric=metric, preProc=c(\"center\", \"scale\"), \n                 trControl=trainControl)\n# Compare algorithms\nresults <- resamples(list(LM     = fit.lm, \n                          GLM    = fit.glm, \n                          GLMNET = fit.glmnet, \n                          SVM    = fit.svm, \n                          CART   = fit.cart, \n                          KNN    = fit.knn))\nsummary(results)\n#> \n#> Call:\n#> summary.resamples(object = results)\n#> \n#> Models: LM, GLM, GLMNET, SVM, CART, KNN \n#> Number of resamples: 30 \n#> \n#> MAE \n#>        Min. 1st Qu. Median Mean 3rd Qu. Max. NA's\n#> LM     2.30    2.90   3.37 3.32    3.70 4.64    0\n#> GLM    2.30    2.90   3.37 3.32    3.70 4.64    0\n#> GLMNET 2.30    2.88   3.34 3.30    3.70 4.63    0\n#> SVM    1.42    1.99   2.52 2.39    2.65 3.35    0\n#> CART   2.22    2.62   2.88 2.93    3.08 4.16    0\n#> KNN    1.98    2.69   2.87 2.95    3.24 4.00    0\n#> \n#> RMSE \n#>        Min. 1st Qu. Median Mean 3rd Qu. Max. NA's\n#> LM     2.99    3.87   4.63 4.63    5.32 6.69    0\n#> GLM    2.99    3.87   4.63 4.63    5.32 6.69    0\n#> GLMNET 2.99    3.88   4.62 4.62    5.32 6.69    0\n#> SVM    2.05    2.95   3.81 3.91    4.46 6.98    0\n#> CART   2.77    3.38   4.00 4.20    4.60 7.09    0\n#> KNN    2.65    3.74   4.42 4.48    5.06 6.98    0\n#> \n#> Rsquared \n#>         Min. 1st Qu. Median  Mean 3rd Qu.  Max. NA's\n#> LM     0.505   0.674  0.747 0.740   0.813 0.900    0\n#> GLM    0.505   0.674  0.747 0.740   0.813 0.900    0\n#> GLMNET 0.503   0.673  0.747 0.741   0.816 0.904    0\n#> SVM    0.519   0.762  0.845 0.810   0.896 0.970    0\n#> CART   0.514   0.737  0.816 0.778   0.842 0.899    0\n#> KNN    0.519   0.748  0.804 0.770   0.829 0.931    0\ndotplot(results)"},{"path":"comparison-of-six-linear-regression-algorithms.html","id":"feature-selection-1","chapter":"33 Comparison of six Linear Regression algorithms","heading":"33.5 Feature selection","text":"Comparing results, can see made RMSE worse linear nonlinear algorithms. correlated attributes removed contributing accuracy models.","code":"\n# remove correlated attributes\n# find attributes that are highly correlated\nset.seed(7)\ncutoff <- 0.70\ncorrelations <- cor(dataset[,1:13])\nhighlyCorrelated <- findCorrelation(correlations, cutoff=cutoff)\n\nfor (value in highlyCorrelated) {\n    print(names(dataset)[value])\n}\n#> [1] \"indus\"\n#> [1] \"nox\"\n#> [1] \"tax\"\n#> [1] \"dis\"\n\n# create a new dataset without highly correlated features\ndatasetFeatures <- dataset[,-highlyCorrelated]\ndim(datasetFeatures)\n#> [1] 407  10\n# Run algorithms using 10-fold cross-validation\ntrainControl <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\nmetric <- \"RMSE\"\n\n# LM\nset.seed(7)\nfit.lm <- train(medv~., data=dataset, method=\"lm\", \n                metric=metric, preProc=c(\"center\", \"scale\"), \n                trControl=trainControl)\n# GLM\nset.seed(7)\nfit.glm <- train(medv~., data=dataset, method=\"glm\", \n                 metric=metric, preProc=c(\"center\", \"scale\"), \n                 trControl=trainControl)\n# GLMNET\nset.seed(7)\nfit.glmnet <- train(medv~., data=dataset, method=\"glmnet\", \n                    metric=metric, \n                    preProc=c(\"center\", \"scale\"), \n                    trControl=trainControl)\n# SVM\nset.seed(7)\nfit.svm <- train(medv~., data=dataset, method=\"svmRadial\", \n                 metric=metric, \n                 preProc=c(\"center\", \"scale\"), \n                 trControl=trainControl)\n# CART\nset.seed(7)\ngrid <- expand.grid(.cp=c(0, 0.05, 0.1))\nfit.cart <- train(medv~., data=dataset, method=\"rpart\", \n                  metric=metric, tuneGrid=grid, \n                  preProc=c(\"center\", \"scale\"), \n                  trControl=trainControl)\n# KNN\nset.seed(7)\nfit.knn <- train(medv~., data=dataset, method=\"knn\", \n                 metric=metric, preProc=c(\"center\", \"scale\"), \n                 trControl=trainControl)\n\n# Compare algorithms\nfeature_results <- resamples(list(LM     = fit.lm, \n                                  GLM    = fit.glm, \n                                  GLMNET = fit.glmnet, \n                                  SVM    = fit.svm, \n                                  CART   = fit.cart, \n                                  KNN    = fit.knn))\nsummary(feature_results)\n#> \n#> Call:\n#> summary.resamples(object = feature_results)\n#> \n#> Models: LM, GLM, GLMNET, SVM, CART, KNN \n#> Number of resamples: 30 \n#> \n#> MAE \n#>        Min. 1st Qu. Median Mean 3rd Qu. Max. NA's\n#> LM     2.30    2.90   3.37 3.32    3.70 4.64    0\n#> GLM    2.30    2.90   3.37 3.32    3.70 4.64    0\n#> GLMNET 2.30    2.88   3.34 3.30    3.70 4.63    0\n#> SVM    1.42    1.99   2.52 2.39    2.65 3.35    0\n#> CART   2.22    2.62   2.88 2.93    3.08 4.16    0\n#> KNN    1.98    2.69   2.87 2.95    3.24 4.00    0\n#> \n#> RMSE \n#>        Min. 1st Qu. Median Mean 3rd Qu. Max. NA's\n#> LM     2.99    3.87   4.63 4.63    5.32 6.69    0\n#> GLM    2.99    3.87   4.63 4.63    5.32 6.69    0\n#> GLMNET 2.99    3.88   4.62 4.62    5.32 6.69    0\n#> SVM    2.05    2.95   3.81 3.91    4.46 6.98    0\n#> CART   2.77    3.38   4.00 4.20    4.60 7.09    0\n#> KNN    2.65    3.74   4.42 4.48    5.06 6.98    0\n#> \n#> Rsquared \n#>         Min. 1st Qu. Median  Mean 3rd Qu.  Max. NA's\n#> LM     0.505   0.674  0.747 0.740   0.813 0.900    0\n#> GLM    0.505   0.674  0.747 0.740   0.813 0.900    0\n#> GLMNET 0.503   0.673  0.747 0.741   0.816 0.904    0\n#> SVM    0.519   0.762  0.845 0.810   0.896 0.970    0\n#> CART   0.514   0.737  0.816 0.778   0.842 0.899    0\n#> KNN    0.519   0.748  0.804 0.770   0.829 0.931    0\ndotplot(feature_results)"},{"path":"comparison-of-six-linear-regression-algorithms.html","id":"evaluate-algorithms-box-cox-transform","chapter":"33 Comparison of six Linear Regression algorithms","heading":"33.6 Evaluate Algorithms: Box-Cox Transform","text":"know attributes skew others perhaps \nexponential distribution. One option explore squaring log\ntransforms respectively (try !). Another approach use power transform let figure amount correct attribute. One example Box-Cox power transform. Let’s try using transform rescale original data evaluate effect 6 algorithms. also leave centering scaling benefit instance-based methods.","code":"\n# Run algorithms using 10-fold cross-validation\ntrainControl <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\nmetric <- \"RMSE\"\n\n# lm\nset.seed(7)\nfit.lm <- train(medv~., data=dataset, method=\"lm\", metric=metric, \n                preProc=c(\"center\", \"scale\", \"BoxCox\"), \n                trControl=trainControl)\n# GLM\nset.seed(7)\nfit.glm <- train(medv~., data=dataset, method=\"glm\", metric=metric, \n                 preProc=c(\"center\", \"scale\", \"BoxCox\"), \n                 trControl=trainControl)\n# GLMNET\nset.seed(7)\nfit.glmnet <- train(medv~., data=dataset, method=\"glmnet\", metric=metric, \n                    preProc=c(\"center\", \"scale\", \"BoxCox\"),\n                    trControl=trainControl)\n# SVM\nset.seed(7)\nfit.svm <- train(medv~., data=dataset, method=\"svmRadial\", metric=metric, \n                 preProc=c(\"center\", \"scale\", \"BoxCox\"),\n                 trControl=trainControl)\n# CART\nset.seed(7)\ngrid <- expand.grid(.cp=c(0, 0.05, 0.1))\nfit.cart <- train(medv~., data=dataset, method=\"rpart\", metric=metric,\n                  tuneGrid=grid,\n                  preProc=c(\"center\", \"scale\", \"BoxCox\"),\n                  trControl=trainControl)\n# KNN\nset.seed(7)\nfit.knn <- train(medv~., data=dataset, method=\"knn\", metric=metric, \n                 preProc=c(\"center\", \"scale\", \"BoxCox\"), \n                 trControl=trainControl)\n\n# Compare algorithms\ntransformResults <- resamples(list(LM     = fit.lm, \n                                  GLM    = fit.glm, \n                                  GLMNET = fit.glmnet, \n                                  SVM    = fit.svm, \n                                  CART   = fit.cart, \n                                  KNN    = fit.knn))\nsummary(transformResults)\n#> \n#> Call:\n#> summary.resamples(object = transformResults)\n#> \n#> Models: LM, GLM, GLMNET, SVM, CART, KNN \n#> Number of resamples: 30 \n#> \n#> MAE \n#>        Min. 1st Qu. Median Mean 3rd Qu. Max. NA's\n#> LM     2.10    2.80   3.21 3.18    3.45 4.44    0\n#> GLM    2.10    2.80   3.21 3.18    3.45 4.44    0\n#> GLMNET 2.11    2.80   3.21 3.17    3.45 4.43    0\n#> SVM    1.30    1.95   2.25 2.26    2.48 3.19    0\n#> CART   2.22    2.62   2.89 2.94    3.11 4.16    0\n#> KNN    2.33    2.66   2.82 2.97    3.27 3.96    0\n#> \n#> RMSE \n#>        Min. 1st Qu. Median Mean 3rd Qu. Max. NA's\n#> LM     2.82    3.81   4.43 4.37    5.00 6.16    0\n#> GLM    2.82    3.81   4.43 4.37    5.00 6.16    0\n#> GLMNET 2.83    3.79   4.42 4.36    5.00 6.18    0\n#> SVM    1.80    2.73   3.41 3.70    4.24 6.73    0\n#> CART   2.77    3.38   4.00 4.23    4.83 7.09    0\n#> KNN    3.01    3.73   4.37 4.52    5.02 7.30    0\n#> \n#> Rsquared \n#>         Min. 1st Qu. Median  Mean 3rd Qu.  Max. NA's\n#> LM     0.560   0.712  0.775 0.766   0.825 0.910    0\n#> GLM    0.560   0.712  0.775 0.766   0.825 0.910    0\n#> GLMNET 0.556   0.713  0.775 0.766   0.826 0.910    0\n#> SVM    0.524   0.778  0.854 0.827   0.907 0.979    0\n#> CART   0.514   0.727  0.816 0.774   0.843 0.899    0\n#> KNN    0.492   0.723  0.792 0.762   0.842 0.937    0\ndotplot(transformResults)"},{"path":"comparison-of-six-linear-regression-algorithms.html","id":"tune-svm","chapter":"33 Comparison of six Linear Regression algorithms","heading":"33.7 Tune SVM","text":"Let’s design grid search around C value 1. might see small trend decreasing RMSE increasing C, let’s try integer C values 1 10. Another parameter caret let us tune sigma parameter. smoothing parameter. Good sigma values often start around 0.1, try numbers .","code":"\nprint(fit.svm)\n#> Support Vector Machines with Radial Basis Function Kernel \n#> \n#> 407 samples\n#>  13 predictor\n#> \n#> Pre-processing: centered (13), scaled (13), Box-Cox transformation (11) \n#> Resampling: Cross-Validated (10 fold, repeated 3 times) \n#> Summary of sample sizes: 365, 366, 366, 367, 366, 366, ... \n#> Resampling results across tuning parameters:\n#> \n#>   C     RMSE  Rsquared  MAE \n#>   0.25  4.54  0.772     2.73\n#>   0.50  4.07  0.802     2.46\n#>   1.00  3.70  0.827     2.26\n#> \n#> Tuning parameter 'sigma' was held constant at a value of 0.116\n#> RMSE was used to select the optimal model using the smallest value.\n#> The final values used for the model were sigma = 0.116 and C = 1.\n# tune SVM sigma and C parametres\ntrainControl <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\nmetric <- \"RMSE\"\nset.seed(7)\n\ngrid <- expand.grid(.sigma = c(0.025, 0.05, 0.1, 0.15), \n                    .C = seq(1, 10, by=1))\n\nfit.svm <- train(medv~., data=dataset, method=\"svmRadial\", metric=metric, \n                 tuneGrid=grid, \n                 preProc=c(\"BoxCox\"), trControl=trainControl)\nprint(fit.svm)\n#> Support Vector Machines with Radial Basis Function Kernel \n#> \n#> 407 samples\n#>  13 predictor\n#> \n#> Pre-processing: Box-Cox transformation (11) \n#> Resampling: Cross-Validated (10 fold, repeated 3 times) \n#> Summary of sample sizes: 365, 366, 366, 367, 366, 366, ... \n#> Resampling results across tuning parameters:\n#> \n#>   sigma  C   RMSE  Rsquared  MAE \n#>   0.025   1  3.67  0.830     2.34\n#>   0.025   2  3.49  0.840     2.21\n#>   0.025   3  3.45  0.842     2.17\n#>   0.025   4  3.42  0.844     2.14\n#>   0.025   5  3.41  0.845     2.13\n#>   0.025   6  3.40  0.846     2.12\n#>   0.025   7  3.39  0.846     2.11\n#>   0.025   8  3.39  0.846     2.11\n#>   0.025   9  3.38  0.846     2.11\n#>   0.025  10  3.37  0.847     2.10\n#>   0.050   1  3.61  0.833     2.25\n#>   0.050   2  3.44  0.843     2.17\n#>   0.050   3  3.36  0.848     2.11\n#>   0.050   4  3.30  0.852     2.08\n#>   0.050   5  3.25  0.856     2.05\n#>   0.050   6  3.20  0.860     2.03\n#>   0.050   7  3.16  0.862     2.02\n#>   0.050   8  3.13  0.865     2.02\n#>   0.050   9  3.11  0.866     2.01\n#>   0.050  10  3.10  0.867     2.01\n#>   0.100   1  3.68  0.829     2.26\n#>   0.100   2  3.37  0.848     2.12\n#>   0.100   3  3.23  0.858     2.06\n#>   0.100   4  3.17  0.862     2.04\n#>   0.100   5  3.14  0.865     2.04\n#>   0.100   6  3.11  0.866     2.04\n#>   0.100   7  3.09  0.868     2.04\n#>   0.100   8  3.09  0.868     2.04\n#>   0.100   9  3.08  0.868     2.04\n#>   0.100  10  3.08  0.868     2.05\n#>   0.150   1  3.79  0.822     2.30\n#>   0.150   2  3.42  0.846     2.14\n#>   0.150   3  3.30  0.854     2.09\n#>   0.150   4  3.26  0.857     2.09\n#>   0.150   5  3.24  0.858     2.09\n#>   0.150   6  3.23  0.858     2.10\n#>   0.150   7  3.23  0.857     2.12\n#>   0.150   8  3.24  0.856     2.13\n#>   0.150   9  3.26  0.855     2.15\n#>   0.150  10  3.27  0.854     2.17\n#> \n#> RMSE was used to select the optimal model using the smallest value.\n#> The final values used for the model were sigma = 0.1 and C = 9.\nplot(fit.svm)"},{"path":"comparison-of-six-linear-regression-algorithms.html","id":"ensembling","chapter":"33 Comparison of six Linear Regression algorithms","heading":"33.8 Ensembling","text":"can try ensemble methods problem see can get decrease RMSE.Random Forest, bagging (RF).Gradient Boosting Machines (GBM).Cubist, boosting (CUBIST).Let’s dive deeper Cubist see can tune get skill . Cubist two parameters tunable caret: committees number boosting operations neighbors used prediction number instances used correct rule-based prediction (although documentation perhaps little ambiguous ).Let’s use grid search tune around values. ’ll try committees 15 25 spot-check neighbors value 5.can see achieved accurate model RMSE 2.822 using committees = 18 neighbors = 3.looks like results Cubist algorithm accurate. Let’s finalize creating new standalone Cubist model parameters trained using whole dataset. must also use Box-Cox power transform.","code":"\n# try ensembles\nseed <- 7\ntrainControl <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\nmetric <- \"RMSE\"\n\n# Random Forest\nset.seed(seed)\nfit.rf <- train(medv~., data=dataset, method=\"rf\", metric=metric, \n                preProc=c(\"BoxCox\"),\n                trControl=trainControl)\n\n# Stochastic Gradient Boosting\nset.seed(seed)\nfit.gbm <- train(medv~., data=dataset, method=\"gbm\", metric=metric, \n                 preProc=c(\"BoxCox\"), \n                 trControl=trainControl, verbose=FALSE)\n# Cubist\nset.seed(seed)\nfit.cubist <- train(medv~., data=dataset, method=\"cubist\", metric=metric, \n                    preProc=c(\"BoxCox\"), trControl=trainControl)\n# Compare algorithms\nensembleResults <- resamples(list(RF  = fit.rf, \n                                  GBM = fit.gbm, \n                                  CUBIST = fit.cubist))\nsummary(ensembleResults)\n#> \n#> Call:\n#> summary.resamples(object = ensembleResults)\n#> \n#> Models: RF, GBM, CUBIST \n#> Number of resamples: 30 \n#> \n#> MAE \n#>        Min. 1st Qu. Median Mean 3rd Qu. Max. NA's\n#> RF     1.64    1.98   2.20 2.21    2.30 3.22    0\n#> GBM    1.65    1.98   2.27 2.33    2.55 3.75    0\n#> CUBIST 1.31    1.75   1.95 2.00    2.17 2.89    0\n#> \n#> RMSE \n#>        Min. 1st Qu. Median Mean 3rd Qu. Max. NA's\n#> RF     2.11    2.58   3.08 3.22    3.70 6.52    0\n#> GBM    1.92    2.54   3.31 3.38    3.67 6.85    0\n#> CUBIST 1.79    2.38   2.74 3.09    3.78 5.79    0\n#> \n#> Rsquared \n#>         Min. 1st Qu. Median  Mean 3rd Qu.  Max. NA's\n#> RF     0.597   0.852  0.900 0.869   0.919 0.972    0\n#> GBM    0.558   0.815  0.889 0.851   0.912 0.964    0\n#> CUBIST 0.681   0.818  0.909 0.875   0.932 0.970    0\ndotplot(ensembleResults)\n# look at parameters used for Cubist\nprint(fit.cubist)\n#> Cubist \n#> \n#> 407 samples\n#>  13 predictor\n#> \n#> Pre-processing: Box-Cox transformation (11) \n#> Resampling: Cross-Validated (10 fold, repeated 3 times) \n#> Summary of sample sizes: 365, 366, 366, 367, 366, 366, ... \n#> Resampling results across tuning parameters:\n#> \n#>   committees  neighbors  RMSE  Rsquared  MAE \n#>    1          0          3.94  0.805     2.50\n#>    1          5          3.66  0.828     2.24\n#>    1          9          3.69  0.825     2.26\n#>   10          0          3.45  0.848     2.29\n#>   10          5          3.19  0.868     2.04\n#>   10          9          3.23  0.864     2.07\n#>   20          0          3.34  0.858     2.25\n#>   20          5          3.09  0.875     2.00\n#>   20          9          3.12  0.872     2.03\n#> \n#> RMSE was used to select the optimal model using the smallest value.\n#> The final values used for the model were committees = 20 and neighbors = 5.\nlibrary(Cubist)\n# Tune the Cubist algorithm\ntrainControl <- trainControl(method=\"repeatedcv\", number=10, repeats=3)\nmetric <- \"RMSE\"\nset.seed(7)\ngrid <- expand.grid(.committees = seq(15, 25, by=1), \n                    .neighbors = c(3, 5, 7))\n\ntune.cubist <- train(medv~., data=dataset, method = \"cubist\", metric=metric, \n                     preProc=c(\"BoxCox\"), \n                     tuneGrid=grid, trControl=trainControl)\nprint(tune.cubist)\n#> Cubist \n#> \n#> 407 samples\n#>  13 predictor\n#> \n#> Pre-processing: Box-Cox transformation (11) \n#> Resampling: Cross-Validated (10 fold, repeated 3 times) \n#> Summary of sample sizes: 365, 366, 366, 367, 366, 366, ... \n#> Resampling results across tuning parameters:\n#> \n#>   committees  neighbors  RMSE  Rsquared  MAE \n#>   15          3          3.07  0.877     2.00\n#>   15          5          3.13  0.873     2.02\n#>   15          7          3.14  0.871     2.03\n#>   16          3          3.05  0.878     1.99\n#>   16          5          3.11  0.874     2.01\n#>   16          7          3.12  0.872     2.02\n#>   17          3          3.04  0.879     1.98\n#>   17          5          3.09  0.875     2.00\n#>   17          7          3.11  0.873     2.01\n#>   18          3          3.03  0.880     1.97\n#>   18          5          3.08  0.876     2.00\n#>   18          7          3.10  0.874     2.01\n#>   19          3          3.03  0.880     1.97\n#>   19          5          3.08  0.876     1.99\n#>   19          7          3.10  0.874     2.01\n#>   20          3          3.03  0.879     1.98\n#>   20          5          3.09  0.875     2.00\n#>   20          7          3.10  0.874     2.01\n#>   21          3          3.03  0.879     1.98\n#>   21          5          3.09  0.876     2.00\n#>   21          7          3.10  0.874     2.02\n#>   22          3          3.03  0.879     1.98\n#>   22          5          3.09  0.875     2.00\n#>   22          7          3.10  0.874     2.02\n#>   23          3          3.03  0.880     1.98\n#>   23          5          3.09  0.876     2.01\n#>   23          7          3.10  0.874     2.02\n#>   24          3          3.03  0.879     1.98\n#>   24          5          3.09  0.875     2.01\n#>   24          7          3.11  0.873     2.02\n#>   25          3          3.03  0.880     1.98\n#>   25          5          3.09  0.876     2.01\n#>   25          7          3.10  0.874     2.02\n#> \n#> RMSE was used to select the optimal model using the smallest value.\n#> The final values used for the model were committees = 25 and neighbors = 3.\nplot(tune.cubist)"},{"path":"comparison-of-six-linear-regression-algorithms.html","id":"finalize-the-model","chapter":"33 Comparison of six Linear Regression algorithms","heading":"33.9 Finalize the model","text":"can now use model evaluate held-validation dataset. , must prepare input data using Box-Cox transform.can see estimated RMSE unseen data 2.666, lower dissimilar expected RMSE 2.822.","code":"\n# prepare the data transform using training data\nset.seed(7)\nx <- dataset[,1:13]\ny <- dataset[,14]\n\n# transform\npreprocessParams <- preProcess(x, method=c(\"BoxCox\"))\ntransX <- predict(preprocessParams, x)\n\n# train the final model\nfinalModel <- cubist(x = transX, y=y, committees=18)\nsummary(finalModel)\n#> \n#> Call:\n#> cubist.default(x = transX, y = y, committees = 18)\n#> \n#> \n#> Cubist [Release 2.07 GPL Edition]  Fri Nov 20 03:19:17 2020\n#> ---------------------------------\n#> \n#>     Target attribute `outcome'\n#> \n#> Read 407 cases (14 attributes) from undefined.data\n#> \n#> Model 1:\n#> \n#>   Rule 1/1: [84 cases, mean 14.29, range 5 to 27.5, est err 1.97]\n#> \n#>     if\n#>  nox > -0.4864544\n#>     then\n#>  outcome = 35.08 - 2.45 crim - 4.31 lstat + 2.1e-05 b\n#> \n#>   Rule 1/2: [163 cases, mean 19.37, range 7 to 31, est err 2.10]\n#> \n#>     if\n#>  nox <= -0.4864544\n#>  lstat > 2.848535\n#>     then\n#>  outcome = 186.8 - 2.34 lstat - 3.3 dis - 88 tax + 2 rad + 4.4 rm\n#>            - 0.033 ptratio - 0.0116 age + 3.3e-05 b\n#> \n#>   Rule 1/3: [24 cases, mean 21.65, range 18.2 to 25.3, est err 1.19]\n#> \n#>     if\n#>  rm <= 3.326479\n#>  dis > 1.345056\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = 43.83 + 14.5 rm - 2.29 lstat - 3.8 dis - 30 tax\n#>            - 0.014 ptratio - 1.4 nox + 0.017 zn + 0.4 rad + 0.15 crim\n#>            - 0.0025 age + 8e-06 b\n#> \n#>   Rule 1/4: [7 cases, mean 27.66, range 20.7 to 50, est err 7.89]\n#> \n#>     if\n#>  rm > 3.326479\n#>  ptratio > 193.545\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = 19.64 + 7.8 rm - 3.4 dis - 1.62 lstat + 0.27 crim - 0.006 age\n#>            + 0.023 zn - 7 tax - 0.003 ptratio\n#> \n#>   Rule 1/5: [141 cases, mean 30.60, range 15 to 50, est err 2.09]\n#> \n#>     if\n#>  rm > 3.326479\n#>  ptratio <= 193.545\n#>     then\n#>  outcome = 137.95 + 21.7 rm - 3.43 lstat - 4.9 dis - 87 tax - 0.0162 age\n#>            - 0.039 ptratio + 0.06 crim + 0.005 zn\n#> \n#>   Rule 1/6: [8 cases, mean 32.16, range 22.1 to 50, est err 8.67]\n#> \n#>     if\n#>  rm <= 3.326479\n#>  dis <= 1.345056\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = -19.71 + 18.58 lstat - 15.9 dis + 5.6 rm\n#> \n#> Model 2:\n#> \n#>   Rule 2/1: [23 cases, mean 10.57, range 5 to 15, est err 3.06]\n#> \n#>     if\n#>  crim > 2.086391\n#>  dis <= 0.6604174\n#>  b > 67032.41\n#>     then\n#>  outcome = 37.22 - 4.83 crim - 7 dis - 1.9 lstat - 1.9e-05 b - 0.7 rm\n#> \n#>   Rule 2/2: [70 cases, mean 14.82, range 5 to 50, est err 3.90]\n#> \n#>     if\n#>  rm <= 3.620525\n#>  dis <= 0.6604174\n#>     then\n#>  outcome = 74.6 - 21 dis - 5.09 lstat - 15 tax - 0.0017 age + 6e-06 b\n#> \n#>   Rule 2/3: [18 cases, mean 18.03, range 7.5 to 50, est err 6.81]\n#> \n#>     if\n#>  crim > 2.086391\n#>  dis <= 0.6604174\n#>  b <= 67032.41\n#>     then\n#>  outcome = 94.95 - 40.1 dis - 8.15 crim - 7.14 lstat - 3.5e-05 b - 1.3 rm\n#> \n#>   Rule 2/4: [258 cases, mean 20.74, range 9.5 to 36.2, est err 1.92]\n#> \n#>     if\n#>  rm <= 3.620525\n#>  dis > 0.6604174\n#>  lstat > 1.805082\n#>     then\n#>  outcome = 61.89 - 2.56 lstat + 5.5 rm - 2.8 dis + 7.3e-05 b - 0.0132 age\n#>            - 26 tax - 0.11 indus - 0.004 ptratio + 0.05 crim\n#> \n#>   Rule 2/5: [37 cases, mean 31.66, range 10.4 to 50, est err 3.70]\n#> \n#>     if\n#>  rm > 3.620525\n#>  lstat > 1.805082\n#>     then\n#>  outcome = 370.03 - 180 tax - 2.19 lstat - 1.7 dis + 2.6 rm\n#>            - 0.016 ptratio - 0.25 indus + 0.12 crim - 0.0021 age\n#>            + 9e-06 b - 0.5 nox\n#> \n#>   Rule 2/6: [42 cases, mean 38.23, range 22.8 to 50, est err 3.70]\n#> \n#>     if\n#>  lstat <= 1.805082\n#>     then\n#>  outcome = -73.87 + 32.4 rm - 9.4e-05 b - 1.8 dis + 0.028 zn\n#>            - 0.013 ptratio\n#> \n#>   Rule 2/7: [4 cases, mean 40.20, range 37.6 to 42.8, est err 7.33]\n#> \n#>     if\n#>  rm > 4.151791\n#>  dis > 1.114486\n#>     then\n#>  outcome = 35.8\n#> \n#>   Rule 2/8: [8 cases, mean 47.45, range 41.3 to 50, est err 10.01]\n#> \n#>     if\n#>  dis <= 1.114486\n#>  lstat <= 1.805082\n#>     then\n#>  outcome = 48.96 + 7.53 crim - 4.1e-05 b - 0.8 dis + 1.2 rm + 0.008 zn\n#> \n#> Model 3:\n#> \n#>   Rule 3/1: [81 cases, mean 13.93, range 5 to 23.2, est err 2.24]\n#> \n#>     if\n#>  nox > -0.4864544\n#>  lstat > 2.848535\n#>     then\n#>  outcome = 55.03 - 0.0631 age - 2.11 crim + 12 nox - 4.16 lstat\n#>            + 3.2e-05 b\n#> \n#>   Rule 3/2: [163 cases, mean 19.37, range 7 to 31, est err 2.29]\n#> \n#>     if\n#>  nox <= -0.4864544\n#>  lstat > 2.848535\n#>     then\n#>  outcome = 77.73 - 0.059 ptratio + 5.8 rm - 3.2 dis - 0.0139 age\n#>            - 1.15 lstat - 30 tax - 1.1 nox + 0.4 rad\n#> \n#>   Rule 3/3: [62 cases, mean 24.01, range 18.2 to 50, est err 3.56]\n#> \n#>     if\n#>  rm <= 3.448196\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = 94.86 + 18.2 rm + 0.63 crim - 68 tax - 2.3 dis - 3 nox\n#>            - 0.0098 age - 0.41 indus - 0.011 ptratio\n#> \n#>   Rule 3/4: [143 cases, mean 28.76, range 16.5 to 50, est err 2.53]\n#> \n#>     if\n#>  dis > 0.9547035\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = 269.46 + 17.9 rm - 6.1 dis - 153 tax + 0.96 crim - 0.0217 age\n#>            - 5.5 nox - 0.62 indus - 0.028 ptratio - 0.89 lstat + 0.4 rad\n#>            + 0.004 zn\n#> \n#>   Rule 3/5: [10 cases, mean 35.13, range 21.9 to 50, est err 9.31]\n#> \n#>     if\n#>  dis <= 0.6492998\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = 58.69 - 56.8 dis - 8.4 nox\n#> \n#>   Rule 3/6: [10 cases, mean 41.67, range 22 to 50, est err 9.89]\n#> \n#>     if\n#>  dis > 0.6492998\n#>  dis <= 0.9547035\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = 47.93\n#> \n#> Model 4:\n#> \n#>   Rule 4/1: [69 cases, mean 12.69, range 5 to 27.5, est err 2.55]\n#> \n#>     if\n#>  dis <= 0.719156\n#>  lstat > 3.508535\n#>     then\n#>  outcome = 180.13 - 7.2 dis + 0.039 age - 3.78 lstat - 83 tax\n#> \n#>   Rule 4/2: [164 cases, mean 19.42, range 12 to 31, est err 1.96]\n#> \n#>     if\n#>  dis > 0.719156\n#>  lstat > 2.848535\n#>     then\n#>  outcome = 52.75 + 7.1 rm - 2.05 lstat - 3.6 dis + 8.2e-05 b - 0.0152 age\n#>            - 25 tax + 0.5 rad - 1.2 nox - 0.008 ptratio\n#> \n#>   Rule 4/3: [11 cases, mean 20.39, range 15 to 27.9, est err 3.51]\n#> \n#>     if\n#>  dis <= 0.719156\n#>  lstat > 2.848535\n#>  lstat <= 3.508535\n#>     then\n#>  outcome = 21.69\n#> \n#>   Rule 4/4: [63 cases, mean 23.22, range 16.5 to 31.5, est err 1.67]\n#> \n#>     if\n#>  rm <= 3.483629\n#>  dis > 0.9731624\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = 59.35 - 3.96 lstat - 3.1 dis + 1 rm - 14 tax + 0.3 rad\n#>            - 0.7 nox - 0.005 ptratio + 6e-06 b\n#> \n#>   Rule 4/5: [8 cases, mean 33.08, range 22 to 50, est err 23.91]\n#> \n#>     if\n#>  rm > 3.369183\n#>  dis <= 0.9731624\n#>  lstat > 2.254579\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = -322.28 + 64.9 lstat + 56.8 rm - 30.2 dis\n#> \n#>   Rule 4/6: [7 cases, mean 33.87, range 22.1 to 50, est err 13.21]\n#> \n#>     if\n#>  rm <= 3.369183\n#>  dis <= 0.9731624\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = -52.11 + 43.45 lstat - 30.8 dis\n#> \n#>   Rule 4/7: [91 cases, mean 34.43, range 21.9 to 50, est err 3.32]\n#> \n#>     if\n#>  rm > 3.483629\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = -33.09 + 22 rm - 5.02 lstat - 0.038 ptratio - 0.9 dis\n#>            + 0.005 zn\n#> \n#>   Rule 4/8: [22 cases, mean 36.99, range 21.9 to 50, est err 13.21]\n#> \n#>     if\n#>  dis <= 0.9731624\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = 80.3 - 17.43 lstat - 0.134 ptratio + 2.5 rm - 1.2 dis\n#>            + 0.008 zn\n#> \n#> Model 5:\n#> \n#>   Rule 5/1: [84 cases, mean 14.29, range 5 to 27.5, est err 2.81]\n#> \n#>     if\n#>  nox > -0.4864544\n#>     then\n#>  outcome = 56.48 + 28.5 nox - 0.0875 age - 3.58 crim - 5.9 dis\n#>            - 2.96 lstat + 0.073 ptratio + 1.7e-05 b\n#> \n#>   Rule 5/2: [163 cases, mean 19.37, range 7 to 31, est err 2.38]\n#> \n#>     if\n#>  nox <= -0.4864544\n#>  lstat > 2.848535\n#>     then\n#>  outcome = 61.59 - 0.064 ptratio + 5.9 rm - 3.1 dis - 0.0142 age\n#>            - 0.77 lstat - 21 tax\n#> \n#>   Rule 5/3: [163 cases, mean 29.94, range 16.5 to 50, est err 3.65]\n#> \n#>     if\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = 264.17 + 21.9 rm - 8 dis - 155 tax - 0.0317 age\n#>            - 0.032 ptratio + 0.29 crim - 1.6 nox - 0.25 indus\n#> \n#>   Rule 5/4: [10 cases, mean 35.13, range 21.9 to 50, est err 11.79]\n#> \n#>     if\n#>  dis <= 0.6492998\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = 68.19 - 73.4 dis + 1.1 rm + 0.11 crim - 0.6 nox - 0.1 indus\n#>            - 0.0017 age - 0.12 lstat\n#> \n#> Model 6:\n#> \n#>   Rule 6/1: [71 cases, mean 15.57, range 5 to 50, est err 4.42]\n#> \n#>     if\n#>  dis <= 0.6443245\n#>  lstat > 1.793385\n#>     then\n#>  outcome = 45.7 - 20.6 dis - 5.38 lstat\n#> \n#>   Rule 6/2: [159 cases, mean 19.53, range 8.3 to 36.2, est err 2.08]\n#> \n#>     if\n#>  rm <= 3.329365\n#>  dis > 0.6443245\n#>     then\n#>  outcome = 24.33 + 8.8 rm + 0.000118 b - 0.0146 age - 2.5 dis\n#>            - 0.95 lstat + 0.37 crim - 0.32 indus + 0.02 zn - 16 tax\n#>            + 0.2 rad - 0.5 nox - 0.004 ptratio\n#> \n#>   Rule 6/3: [175 cases, mean 27.80, range 9.5 to 50, est err 2.95]\n#> \n#>     if\n#>  rm > 3.329365\n#>  dis > 0.6443245\n#>     then\n#>  outcome = 0.11 + 18.7 rm - 3.11 lstat + 8.1e-05 b - 1.1 dis + 0.19 crim\n#>            - 20 tax - 0.19 indus + 0.3 rad - 0.7 nox - 0.005 ptratio\n#>            + 0.006 zn\n#> \n#>   Rule 6/4: [8 cases, mean 32.50, range 21.9 to 50, est err 10.34]\n#> \n#>     if\n#>  dis <= 0.6443245\n#>  lstat > 1.793385\n#>  lstat <= 2.894121\n#>     then\n#>  outcome = 69.38 - 71.2 dis - 0.14 lstat\n#> \n#>   Rule 6/5: [34 cases, mean 37.55, range 22.8 to 50, est err 3.55]\n#> \n#>     if\n#>  rm <= 4.151791\n#>  lstat <= 1.793385\n#>     then\n#>  outcome = -125.14 + 41.7 rm + 4.3 rad + 1.48 indus - 0.014 ptratio\n#> \n#>   Rule 6/6: [7 cases, mean 43.66, range 37.6 to 50, est err 3.12]\n#> \n#>     if\n#>  rm > 4.151791\n#>  lstat <= 1.793385\n#>     then\n#>  outcome = -137.67 + 44.6 rm - 0.064 ptratio\n#> \n#> Model 7:\n#> \n#>   Rule 7/1: [84 cases, mean 14.29, range 5 to 27.5, est err 2.91]\n#> \n#>     if\n#>  nox > -0.4864544\n#>     then\n#>  outcome = 46.85 - 3.45 crim - 0.0621 age + 14.2 nox + 4.4 dis\n#>            - 2.01 lstat + 2.5e-05 b\n#> \n#>   Rule 7/2: [323 cases, mean 24.66, range 7 to 50, est err 3.68]\n#> \n#>     if\n#>  nox <= -0.4864544\n#>     then\n#>  outcome = 57.59 - 0.065 ptratio - 4.4 dis + 6.8 rm - 0.0143 age\n#>            - 1.36 lstat - 19 tax - 0.8 nox - 0.12 crim + 0.09 indus\n#> \n#>   Rule 7/3: [132 cases, mean 28.24, range 16.5 to 50, est err 2.55]\n#> \n#>     if\n#>  dis > 1.063503\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = 270.92 + 24.5 rm - 0.0418 age - 165 tax - 5.7 dis\n#>            - 0.028 ptratio + 0.26 crim + 0.017 zn\n#> \n#>   Rule 7/4: [7 cases, mean 36.01, range 23.3 to 50, est err 3.87]\n#> \n#>     if\n#>  dis <= 0.6002641\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = 57.18 - 69.5 dis - 6.5 nox + 1.9 rm - 0.015 ptratio\n#> \n#>   Rule 7/5: [24 cases, mean 37.55, range 21.9 to 50, est err 8.66]\n#> \n#>     if\n#>  dis > 0.6002641\n#>  dis <= 1.063503\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = -3.76 - 14.8 dis - 2.93 crim - 0.16 ptratio + 17.5 rm - 15 nox\n#> \n#> Model 8:\n#> \n#>   Rule 8/1: [80 cases, mean 13.75, range 5 to 27.9, est err 3.51]\n#> \n#>     if\n#>  dis <= 0.719156\n#>  lstat > 2.848535\n#>     then\n#>  outcome = 123.46 - 11.3 dis - 5.06 lstat - 45 tax + 0.9 rad + 1.7e-05 b\n#> \n#>   Rule 8/2: [164 cases, mean 19.42, range 12 to 31, est err 2.05]\n#> \n#>     if\n#>  dis > 0.719156\n#>  lstat > 2.848535\n#>     then\n#>  outcome = 227.11 - 120 tax + 6.4 rm + 9.3e-05 b - 3.3 dis + 2 rad\n#>            - 0.0183 age - 0.93 lstat + 0.05 crim - 0.3 nox\n#> \n#>   Rule 8/3: [163 cases, mean 29.94, range 16.5 to 50, est err 3.54]\n#> \n#>     if\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = 158.14 - 5.73 lstat + 10.8 rm - 4 dis - 83 tax - 4.1 nox\n#>            + 0.61 crim - 0.54 indus + 1 rad + 3.6e-05 b\n#> \n#>   Rule 8/4: [7 cases, mean 36.01, range 23.3 to 50, est err 11.44]\n#> \n#>     if\n#>  dis <= 0.6002641\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = 72.89 - 87.2 dis + 0.6 rm - 0.13 lstat\n#> \n#>   Rule 8/5: [47 cases, mean 38.44, range 15 to 50, est err 5.71]\n#> \n#>     if\n#>  rm > 3.726352\n#>     then\n#>  outcome = 602.95 - 10.4 lstat + 21 rm - 326 tax - 0.093 ptratio\n#> \n#> Model 9:\n#> \n#>   Rule 9/1: [81 cases, mean 13.93, range 5 to 23.2, est err 2.91]\n#> \n#>     if\n#>  nox > -0.4864544\n#>  lstat > 2.848535\n#>     then\n#>  outcome = 41.11 - 3.98 crim - 4.42 lstat + 6.7 nox\n#> \n#>   Rule 9/2: [163 cases, mean 19.37, range 7 to 31, est err 2.49]\n#> \n#>     if\n#>  nox <= -0.4864544\n#>  lstat > 2.848535\n#>     then\n#>  outcome = 44.98 - 0.068 ptratio - 4.4 dis + 6.6 rm - 1.25 lstat\n#>            - 0.0118 age - 0.9 nox - 12 tax - 0.08 crim + 0.06 indus\n#> \n#>   Rule 9/3: [132 cases, mean 28.24, range 16.5 to 50, est err 2.35]\n#> \n#>     if\n#>  dis > 1.063503\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = 157.67 + 22.2 rm - 0.0383 age - 104 tax - 0.033 ptratio\n#>            - 2.2 dis\n#> \n#>   Rule 9/4: [7 cases, mean 30.76, range 21.9 to 50, est err 6.77]\n#> \n#>     if\n#>  dis <= 1.063503\n#>  b <= 66469.73\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = 48.52 - 56.1 dis - 12.9 nox - 0.032 ptratio + 2.7 rm\n#> \n#>   Rule 9/5: [24 cases, mean 39.09, range 22 to 50, est err 6.20]\n#> \n#>     if\n#>  dis <= 1.063503\n#>  b > 66469.73\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = -5.49 - 34.8 dis - 20.7 nox + 18.2 rm - 0.051 ptratio\n#> \n#> Model 10:\n#> \n#>   Rule 10/1: [327 cases, mean 19.45, range 5 to 50, est err 2.77]\n#> \n#>     if\n#>  rm <= 3.617282\n#>  lstat > 1.805082\n#>     then\n#>  outcome = 270.78 - 4.09 lstat - 131 tax + 2.9 rad + 5.3e-05 b - 0.6 dis\n#>            - 0.16 indus + 0.7 rm - 0.3 nox\n#> \n#>   Rule 10/2: [38 cases, mean 31.57, range 10.4 to 50, est err 4.71]\n#> \n#>     if\n#>  rm > 3.617282\n#>  lstat > 1.805082\n#>     then\n#>  outcome = 308.44 - 150 tax - 2.63 lstat + 1.6 rad - 1.9 dis - 0.49 indus\n#>            + 2.5 rm + 3e-05 b - 1.2 nox + 0.14 crim - 0.005 ptratio\n#> \n#>   Rule 10/3: [35 cases, mean 37.15, range 22.8 to 50, est err 2.76]\n#> \n#>     if\n#>  rm <= 4.151791\n#>  lstat <= 1.805082\n#>     then\n#>  outcome = -71.65 + 33.4 rm - 0.017 ptratio - 0.34 lstat + 0.2 rad\n#>            - 0.3 dis - 7 tax - 0.4 nox\n#> \n#>   Rule 10/4: [10 cases, mean 42.63, range 21.9 to 50, est err 7.11]\n#> \n#>     if\n#>  rm > 4.151791\n#>     then\n#>  outcome = -92.51 + 32.8 rm - 0.03 ptratio\n#> \n#> Model 11:\n#> \n#>   Rule 11/1: [84 cases, mean 14.29, range 5 to 27.5, est err 4.13]\n#> \n#>     if\n#>  nox > -0.4864544\n#>     then\n#>  outcome = 42.75 - 4.12 crim + 18.1 nox - 0.045 age + 6.8 dis\n#>            - 1.86 lstat\n#> \n#>   Rule 11/2: [244 cases, mean 17.56, range 5 to 31, est err 4.29]\n#> \n#>     if\n#>  lstat > 2.848535\n#>     then\n#>  outcome = 34.83 - 5.2 dis - 0.058 ptratio - 0.0228 age + 5.8 rm\n#>            - 0.56 lstat - 0.07 crim - 0.4 nox - 5 tax\n#> \n#>   Rule 11/3: [163 cases, mean 29.94, range 16.5 to 50, est err 3.49]\n#> \n#>     if\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = 151.5 + 23.3 rm - 5.5 dis + 1.01 crim - 0.0211 age\n#>            - 0.052 ptratio - 98 tax + 0.031 zn\n#> \n#>   Rule 11/4: [10 cases, mean 35.13, range 21.9 to 50, est err 25.19]\n#> \n#>     if\n#>  dis <= 0.6492998\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = 130.87 - 157.1 dis - 15.76 crim\n#> \n#> Model 12:\n#> \n#>   Rule 12/1: [80 cases, mean 13.75, range 5 to 27.9, est err 4.76]\n#> \n#>     if\n#>  dis <= 0.719156\n#>  lstat > 2.894121\n#>     then\n#>  outcome = 182.68 - 6.03 lstat - 7.6 dis - 76 tax + 1.3 rad - 0.52 indus\n#>            + 2.6e-05 b\n#> \n#>   Rule 12/2: [300 cases, mean 19.10, range 5 to 50, est err 2.76]\n#> \n#>     if\n#>  rm <= 3.50716\n#>  lstat > 1.793385\n#>     then\n#>  outcome = 83.61 - 3 lstat + 9.6e-05 b - 0.0072 age - 33 tax + 0.7 rad\n#>            + 0.32 indus\n#> \n#>   Rule 12/3: [10 cases, mean 24.25, range 15.7 to 36.2, est err 13.88]\n#> \n#>     if\n#>  rm <= 3.50716\n#>  tax <= 1.865769\n#>     then\n#>  outcome = 35.46\n#> \n#>   Rule 12/4: [10 cases, mean 32.66, range 21.9 to 50, est err 6.28]\n#> \n#>     if\n#>  dis <= 0.719156\n#>  lstat > 1.793385\n#>  lstat <= 2.894121\n#>     then\n#>  outcome = 82.78 - 69.5 dis - 3.66 indus\n#> \n#>   Rule 12/5: [89 cases, mean 32.75, range 13.4 to 50, est err 3.39]\n#> \n#>     if\n#>  rm > 3.50716\n#>  dis > 0.719156\n#>     then\n#>  outcome = 313.22 + 13.7 rm - 174 tax - 3.06 lstat + 4.8e-05 b - 1.5 dis\n#>            - 0.41 indus + 0.7 rad - 0.0055 age + 0.22 crim\n#> \n#>   Rule 12/6: [34 cases, mean 37.55, range 22.8 to 50, est err 3.25]\n#> \n#>     if\n#>  rm <= 4.151791\n#>  lstat <= 1.793385\n#>     then\n#>  outcome = -86.8 + 36 rm - 0.3 lstat - 5 tax\n#> \n#>   Rule 12/7: [7 cases, mean 43.66, range 37.6 to 50, est err 5.79]\n#> \n#>     if\n#>  rm > 4.151791\n#>  lstat <= 1.793385\n#>     then\n#>  outcome = -158.68 + 47.4 rm - 0.02 ptratio\n#> \n#> Model 13:\n#> \n#>   Rule 13/1: [84 cases, mean 14.29, range 5 to 27.5, est err 2.87]\n#> \n#>     if\n#>  nox > -0.4864544\n#>     then\n#>  outcome = 54.69 - 3.79 crim - 0.0644 age + 11.4 nox - 2.53 lstat\n#> \n#>   Rule 13/2: [8 cases, mean 17.76, range 7 to 27.9, est err 13.69]\n#> \n#>     if\n#>  nox <= -0.4864544\n#>  age > 296.3423\n#>  b <= 60875.57\n#>     then\n#>  outcome = -899.55 + 3.0551 age\n#> \n#>   Rule 13/3: [31 cases, mean 17.94, range 7 to 27.9, est err 5.15]\n#> \n#>     if\n#>  nox <= -0.4864544\n#>  b <= 60875.57\n#>  lstat > 2.848535\n#>     then\n#>  outcome = 44.43 - 3.51 lstat - 0.054 ptratio - 1.4 dis - 0.26 crim\n#>            - 0.0042 age - 0.21 indus + 0.9 rm\n#> \n#>   Rule 13/4: [163 cases, mean 19.37, range 7 to 31, est err 3.37]\n#> \n#>     if\n#>  nox <= -0.4864544\n#>  lstat > 2.848535\n#>     then\n#>  outcome = -5.76 + 0.000242 b + 8.9 rm - 5.2 dis - 0.0209 age\n#>            - 0.042 ptratio - 0.63 indus\n#> \n#>   Rule 13/5: [163 cases, mean 29.94, range 16.5 to 50, est err 3.45]\n#> \n#>     if\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = 178.84 + 23.8 rm - 0.0343 age - 4.5 dis - 114 tax + 0.88 crim\n#>            - 0.048 ptratio + 0.026 zn\n#> \n#>   Rule 13/6: [7 cases, mean 36.01, range 23.3 to 50, est err 14.09]\n#> \n#>     if\n#>  dis <= 0.6002641\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = 45.82 - 70.3 dis - 9.9 nox + 5.1 rm + 1.5 rad\n#> \n#>   Rule 13/7: [31 cases, mean 37.21, range 21.9 to 50, est err 7.73]\n#> \n#>     if\n#>  dis <= 1.063503\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = 95.05 - 4.52 lstat - 7.5 dis + 8.8 rm - 0.064 ptratio\n#>            - 6.2 nox - 36 tax\n#> \n#> Model 14:\n#> \n#>   Rule 14/1: [49 cases, mean 16.06, range 8.4 to 22.7, est err 3.17]\n#> \n#>     if\n#>  nox > -0.4205732\n#>  lstat > 2.848535\n#>     then\n#>  outcome = 12.83 + 42.3 nox - 4.77 lstat + 9.7 rm + 7.8e-05 b\n#> \n#>   Rule 14/2: [78 cases, mean 16.36, range 5 to 50, est err 5.17]\n#> \n#>     if\n#>  dis <= 0.6604174\n#>     then\n#>  outcome = 110.6 - 10.4 dis - 4.85 lstat + 0.0446 age - 46 tax + 0.8 rad\n#> \n#>   Rule 14/3: [57 cases, mean 18.40, range 9.5 to 31, est err 2.43]\n#> \n#>     if\n#>  nox > -0.9365134\n#>  nox <= -0.4205732\n#>  age > 245.2507\n#>  dis > 0.6604174\n#>  lstat > 2.848535\n#>     then\n#>  outcome = 206.69 - 0.1012 age - 7.05 lstat + 12.2 nox - 67 tax + 0.3 rad\n#>            + 0.5 rm - 0.3 dis\n#> \n#>   Rule 14/4: [230 cases, mean 20.19, range 9.5 to 36.2, est err 2.09]\n#> \n#>     if\n#>  rm <= 3.483629\n#>  dis > 0.6492998\n#>     then\n#>  outcome = 119.15 - 2.61 lstat + 5.2 rm - 57 tax - 1.8 dis - 2.4 nox\n#>            + 0.7 rad + 0.24 crim + 0.003 age - 0.007 ptratio + 9e-06 b\n#> \n#>   Rule 14/5: [48 cases, mean 20.28, range 10.2 to 24.5, est err 2.13]\n#> \n#>     if\n#>  nox > -0.9365134\n#>  nox <= -0.4205732\n#>  age <= 245.2507\n#>  dis > 0.6604174\n#>  lstat > 2.848535\n#>     then\n#>  outcome = 19.4 - 1.91 lstat + 1.02 indus - 0.013 age + 2.7 rm + 2.6 nox\n#>            - 0.009 ptratio\n#> \n#>   Rule 14/6: [44 cases, mean 20.69, range 14.4 to 29.6, est err 2.26]\n#> \n#>     if\n#>  nox <= -0.9365134\n#>  lstat > 2.848535\n#>     then\n#>  outcome = 87.55 - 0.000315 b - 6.5 dis + 2.6 rad - 0.59 lstat - 18 tax\n#> \n#>   Rule 14/7: [102 cases, mean 32.44, range 13.4 to 50, est err 3.35]\n#> \n#>     if\n#>  rm > 3.483629\n#>  dis > 0.6492998\n#>     then\n#>  outcome = 126.92 + 22.7 rm - 4.68 lstat - 85 tax - 0.036 ptratio\n#>            - 1.1 dis + 0.007 zn\n#> \n#>   Rule 14/8: [84 cases, mean 33.40, range 21 to 50, est err 2.44]\n#> \n#>     if\n#>  rm > 3.483629\n#>  tax <= 1.896025\n#>     then\n#>  outcome = 347.12 + 25.2 rm - 213 tax - 3.5 lstat - 0.013 ptratio\n#> \n#>   Rule 14/9: [10 cases, mean 35.13, range 21.9 to 50, est err 12.13]\n#> \n#>     if\n#>  dis <= 0.6492998\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = 72.65 - 77.8 dis\n#> \n#> Model 15:\n#> \n#>   Rule 15/1: [28 cases, mean 12.35, range 5 to 27.9, est err 4.09]\n#> \n#>     if\n#>  crim > 2.405809\n#>  b > 16084.5\n#>     then\n#>  outcome = 53.45 - 7.8 crim - 3.5 lstat - 0.0189 age\n#> \n#>   Rule 15/2: [11 cases, mean 13.56, range 8.3 to 27.5, est err 5.99]\n#> \n#>     if\n#>  crim > 2.405809\n#>  b <= 16084.5\n#>     then\n#>  outcome = 8.73 + 0.001756 b\n#> \n#>   Rule 15/3: [244 cases, mean 17.56, range 5 to 31, est err 2.73]\n#> \n#>     if\n#>  lstat > 2.848535\n#>     then\n#>  outcome = 103.02 - 0.0251 age - 2.37 lstat - 3.5 dis + 6.8e-05 b + 4 rm\n#>            - 0.035 ptratio - 41 tax - 0.25 crim\n#> \n#>   Rule 15/4: [131 cases, mean 28.22, range 16.5 to 50, est err 2.59]\n#> \n#>     if\n#>  dis > 1.086337\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = 267.07 + 17.7 rm - 0.0421 age - 150 tax - 5.5 dis + 0.88 crim\n#>            - 0.035 ptratio + 0.031 zn - 0.12 lstat - 0.3 nox\n#> \n#>   Rule 15/5: [13 cases, mean 33.08, range 22 to 50, est err 4.44]\n#> \n#>     if\n#>  nox <= -0.7229691\n#>  dis <= 1.086337\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = 148.52 - 0.002365 b - 85.9 nox - 1 dis + 0.16 crim + 0.8 rm\n#>            + 0.007 zn - 0.0016 age - 7 tax - 0.003 ptratio\n#> \n#>   Rule 15/6: [7 cases, mean 36.01, range 23.3 to 50, est err 7.00]\n#> \n#>     if\n#>  dis <= 0.6002641\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = 50.55 - 68.1 dis - 11.4 nox + 0.00012 b + 1 rm - 0.008 ptratio\n#> \n#>   Rule 15/7: [12 cases, mean 41.77, range 21.9 to 50, est err 9.73]\n#> \n#>     if\n#>  nox > -0.7229691\n#>  dis > 0.6002641\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = 13.74 - 92 nox - 40.5 dis - 0.023 ptratio + 2.6 rm\n#> \n#> Model 16:\n#> \n#>   Rule 16/1: [60 cases, mean 15.95, range 7.2 to 27.5, est err 3.16]\n#> \n#>     if\n#>  nox > -0.4344906\n#>     then\n#>  outcome = 46.98 - 6.53 lstat - 6.9 dis - 1.1 rm\n#> \n#>   Rule 16/2: [45 cases, mean 16.89, range 5 to 50, est err 5.45]\n#> \n#>     if\n#>  nox <= -0.4344906\n#>  dis <= 0.6557049\n#>     then\n#>  outcome = 35.33 - 37 dis - 51.7 nox - 7.38 lstat - 0.4 rm\n#> \n#>   Rule 16/3: [128 cases, mean 19.97, range 9.5 to 36.2, est err 2.52]\n#> \n#>     if\n#>  rm <= 3.626081\n#>  dis > 0.6557049\n#>  dis <= 1.298828\n#>  lstat > 2.133251\n#>     then\n#>  outcome = 61.65 - 3.35 lstat + 4.9 dis + 1.6 rm - 1.3 nox - 22 tax\n#>            + 0.5 rad + 1.8e-05 b + 0.09 crim - 0.004 ptratio\n#> \n#>   Rule 16/4: [140 cases, mean 21.93, range 12.7 to 35.1, est err 2.19]\n#> \n#>     if\n#>  rm <= 3.626081\n#>  dis > 1.298828\n#>     then\n#>  outcome = 54.16 - 3.58 lstat + 2.2 rad - 1.6 dis - 1.9 nox + 1.8 rm\n#>            - 17 tax + 1.3e-05 b + 0.06 crim - 0.003 ptratio\n#> \n#>   Rule 16/5: [30 cases, mean 21.97, range 14.4 to 29.1, est err 2.41]\n#> \n#>     if\n#>  rm <= 3.626081\n#>  dis > 1.298828\n#>  tax <= 1.879832\n#>  lstat > 2.133251\n#>     then\n#>  outcome = -1065.35 + 566 tax + 8.7 rm - 0.13 lstat - 0.2 dis - 0.3 nox\n#> \n#>   Rule 16/6: [22 cases, mean 30.88, range 10.4 to 50, est err 4.51]\n#> \n#>     if\n#>  rm > 3.626081\n#>  lstat > 2.133251\n#>     then\n#>  outcome = 42.24 + 18.7 rm - 1.5 indus - 1.84 lstat - 2.5 nox - 1.6 dis\n#>            - 39 tax + 0.7 rad - 0.012 ptratio + 0.0035 age + 1.2e-05 b\n#>            + 0.11 crim\n#> \n#>   Rule 16/7: [73 cases, mean 34.52, range 20.6 to 50, est err 3.36]\n#> \n#>     if\n#>  lstat <= 2.133251\n#>     then\n#>  outcome = 50.6 + 19.6 rm - 2.77 lstat - 3.2 nox - 1.7 dis - 45 tax\n#>            + 1 rad + 0.007 age - 0.014 ptratio\n#> \n#> Model 17:\n#> \n#>   Rule 17/1: [116 cases, mean 15.37, range 5 to 27.9, est err 2.55]\n#> \n#>     if\n#>  crim > 0.4779842\n#>  lstat > 2.944963\n#>     then\n#>  outcome = 35.96 - 3.68 crim - 3.41 lstat + 0.3 nox\n#> \n#>   Rule 17/2: [112 cases, mean 19.13, range 7 to 31, est err 2.14]\n#> \n#>     if\n#>  crim <= 0.4779842\n#>  lstat > 2.944963\n#>     then\n#>  outcome = 184.65 - 0.0365 age + 9 rm - 4.1 dis - 97 tax + 8.4e-05 b\n#>            - 0.024 ptratio\n#> \n#>   Rule 17/3: [9 cases, mean 28.37, range 15 to 50, est err 11.17]\n#> \n#>     if\n#>  dis <= 0.9547035\n#>  b <= 66469.73\n#>  lstat <= 2.944963\n#>     then\n#>  outcome = -1.12 + 0.000454 b\n#> \n#>   Rule 17/4: [179 cases, mean 29.28, range 15 to 50, est err 3.35]\n#> \n#>     if\n#>  lstat <= 2.944963\n#>     then\n#>  outcome = 278.16 + 20 rm - 7.4 dis - 0.0356 age - 161 tax + 0.051 zn\n#>            - 0.61 lstat + 0.17 crim - 0.008 ptratio\n#> \n#>   Rule 17/5: [23 cases, mean 36.10, range 15 to 50, est err 10.83]\n#> \n#>     if\n#>  dis <= 0.9547035\n#>  lstat <= 2.944963\n#>     then\n#>  outcome = 233.74 - 8.5 dis + 12.1 rm + 1.15 crim - 2.42 lstat - 113 tax\n#>            - 0.0221 age + 0.068 zn - 0.031 ptratio\n#> \n#> Model 18:\n#> \n#>   Rule 18/1: [84 cases, mean 14.29, range 5 to 27.5, est err 2.44]\n#> \n#>     if\n#>  nox > -0.4864544\n#>     then\n#>  outcome = 41.55 - 6.2 lstat + 14.6 nox + 3.8e-05 b\n#> \n#>   Rule 18/2: [163 cases, mean 19.37, range 7 to 31, est err 2.44]\n#> \n#>     if\n#>  nox <= -0.4864544\n#>  lstat > 2.848535\n#>     then\n#>  outcome = 172.79 - 3.67 lstat + 3.1 rad - 3.5 dis - 72 tax - 0.72 indus\n#>            - 0.033 ptratio - 1.2 nox + 0.0027 age + 0.6 rm + 0.05 crim\n#>            + 5e-06 b\n#> \n#>   Rule 18/3: [106 cases, mean 25.41, range 16.5 to 50, est err 2.76]\n#> \n#>     if\n#>  rm <= 3.626081\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = 10.71 - 4.6 dis - 2.21 lstat + 2.3 rad + 5.5 rm - 5.3 nox\n#>            - 0.83 indus - 0.003 ptratio\n#> \n#>   Rule 18/4: [4 cases, mean 33.47, range 30.1 to 36.2, est err 5.61]\n#> \n#>     if\n#>  rm <= 3.626081\n#>  tax <= 1.863917\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = 36.84\n#> \n#>   Rule 18/5: [10 cases, mean 35.13, range 21.9 to 50, est err 17.40]\n#> \n#>     if\n#>  dis <= 0.6492998\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = 84.58 - 94.7 dis - 0.15 lstat\n#> \n#>   Rule 18/6: [57 cases, mean 38.38, range 21.9 to 50, est err 3.97]\n#> \n#>     if\n#>  rm > 3.626081\n#>  lstat <= 2.848535\n#>     then\n#>  outcome = 100.34 + 22.3 rm - 5.79 lstat - 0.062 ptratio - 69 tax\n#>            + 0.3 rad - 0.5 nox - 0.3 dis + 0.0011 age\n#> \n#> \n#> Evaluation on training data (407 cases):\n#> \n#>     Average  |error|               1.72\n#>     Relative |error|               0.26\n#>     Correlation coefficient        0.96\n#> \n#> \n#>  Attribute usage:\n#>    Conds  Model\n#> \n#>     72%    84%    lstat\n#>     38%    85%    dis\n#>     35%    80%    rm\n#>     27%    55%    nox\n#>      4%    58%    crim\n#>      2%    49%    b\n#>      2%    68%    ptratio\n#>      1%    78%    tax\n#>      1%    67%    age\n#>            41%    rad\n#>            36%    indus\n#>            20%    zn\n#> \n#> \n#> Time: 0.1 secs\n# transform the validation dataset\nset.seed(7)\nvalX <- validation[,1:13]\ntrans_valX <- predict(preprocessParams, valX)\nvalY <- validation[,14]\n\n# use final model to make predictions on the validation dataset\npredictions <- predict(finalModel, newdata = trans_valX, neighbors=3)\n\n# calculate RMSE\nrmse <- RMSE(predictions, valY)\nr2 <- R2(predictions, valY)\nprint(rmse)\n#> [1] 3.24"},{"path":"comparing-regression-models.html","id":"comparing-regression-models","chapter":"34 Comparing regression models","heading":"34 Comparing regression models","text":"Dataset: Rates.csvAlgorithms: SLR, MLR, NN","code":""},{"path":"comparing-regression-models.html","id":"introduction-17","chapter":"34 Comparing regression models","heading":"34.1 Introduction","text":"line 29 plotSource: https://www.matthewrenze.com/workshops/practical-machine-learning--r/lab-3b-regression.html","code":"\nlibrary(readr)\n\npolicies <- read_csv(file.path(data_raw_dir, \"Rates.csv\"))\n#> Parsed with column specification:\n#> cols(\n#>   Gender = col_character(),\n#>   State = col_character(),\n#>   State.Rate = col_double(),\n#>   Height = col_double(),\n#>   Weight = col_double(),\n#>   BMI = col_double(),\n#>   Age = col_double(),\n#>   Rate = col_double()\n#> )\npolicies\n#> # A tibble: 1,942 x 8\n#>   Gender State State.Rate Height Weight   BMI   Age   Rate\n#>   <chr>  <chr>      <dbl>  <dbl>  <dbl> <dbl> <dbl>  <dbl>\n#> 1 Male   MA        0.100     184   67.8  20.0    77 0.332 \n#> 2 Male   VA        0.142     163   89.4  33.6    82 0.869 \n#> 3 Male   NY        0.0908    170   81.2  28.1    31 0.01  \n#> 4 Male   TN        0.120     175   99.7  32.6    39 0.0215\n#> 5 Male   FL        0.110     184   72.1  21.3    68 0.150 \n#> 6 Male   WA        0.163     166   98.4  35.7    64 0.211 \n#> # … with 1,936 more rows\nsummary(policies)\n#>     Gender             State             State.Rate        Height   \n#>  Length:1942        Length:1942        Min.   :0.001   Min.   :150  \n#>  Class :character   Class :character   1st Qu.:0.110   1st Qu.:162  \n#>  Mode  :character   Mode  :character   Median :0.128   Median :170  \n#>                                        Mean   :0.138   Mean   :170  \n#>                                        3rd Qu.:0.144   3rd Qu.:176  \n#>                                        Max.   :0.318   Max.   :190  \n#>      Weight           BMI            Age            Rate      \n#>  Min.   : 44.1   Min.   :16.0   Min.   :18.0   Min.   :0.001  \n#>  1st Qu.: 68.6   1st Qu.:23.7   1st Qu.:34.0   1st Qu.:0.015  \n#>  Median : 81.3   Median :28.1   Median :51.0   Median :0.046  \n#>  Mean   : 81.2   Mean   :28.3   Mean   :50.8   Mean   :0.138  \n#>  3rd Qu.: 93.8   3rd Qu.:32.5   3rd Qu.:68.0   3rd Qu.:0.173  \n#>  Max.   :116.5   Max.   :46.8   Max.   :84.0   Max.   :0.999\nlibrary(RColorBrewer)\npalette <- brewer.pal(9, \"Reds\")\n# plot(\n#   x = policies,\n#   col = palette[cut(x = policies$Rate, breaks = 9)]\n#   )\nlibrary(corrgram)\n#> Registered S3 method overwritten by 'seriation':\n#>   method         from \n#>   reorder.hclust gclus\n\ncorrgram(policies)\ncor(policies[3:8])\n#>            State.Rate  Height  Weight     BMI     Age    Rate\n#> State.Rate    1.00000 -0.0165 0.00923  0.0192  0.1123  0.2269\n#> Height       -0.01652  1.0000 0.23809 -0.3170 -0.1648 -0.1286\n#> Weight        0.00923  0.2381 1.00000  0.8396  0.0117  0.0609\n#> BMI           0.01924 -0.3170 0.83963  1.0000  0.1023  0.1405\n#> Age           0.11235 -0.1648 0.01168  0.1023  1.0000  0.7801\n#> Rate          0.22685 -0.1286 0.06094  0.1405  0.7801  1.0000\ncor(\n  x = policies$Age, \n  y = policies$Rate)\n#> [1] 0.78\nplot(\n  x = policies$Age, \n  y = policies$Rate)"},{"path":"comparing-regression-models.html","id":"split-the-data-into-test-and-training-sets","chapter":"34 Comparing regression models","heading":"34.2 Split the Data into Test and Training Sets","text":"","code":"\nset.seed(42)\nlibrary(caret)\n#> Loading required package: lattice\n#> \n#> Attaching package: 'lattice'\n#> The following object is masked from 'package:corrgram':\n#> \n#>     panel.fill\n#> Loading required package: ggplot2\n\nindexes <- createDataPartition(\n  y = policies$Rate,\n  p = 0.80,\n  list = FALSE)\n\ntrain <- policies[indexes, ]\n#> Warning: The `i` argument of ``[`()` can't be a matrix as of tibble 3.0.0.\n#> Convert to a vector.\n#> This warning is displayed once every 8 hours.\n#> Call `lifecycle::last_warnings()` to see where this warning was generated.\ntest <- policies[-indexes, ]\nprint(nrow(train))\n#> [1] 1555\nprint(nrow(test))\n#> [1] 387"},{"path":"comparing-regression-models.html","id":"predict-with-simple-linear-regression-1","chapter":"34 Comparing regression models","heading":"34.3 Predict with Simple Linear Regression","text":"","code":"\nsimpleModel <- lm(\n  formula = Rate ~ Age,\n  data = train)\nplot(\n  x = policies$Age, \n  y = policies$Rate)\n  \nlines(\n  x = train$Age,\n  y = simpleModel$fitted, \n  col = \"red\",\n  lwd = 3)\nsummary(simpleModel)\n#> \n#> Call:\n#> lm(formula = Rate ~ Age, data = train)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -0.1799 -0.0881 -0.0208  0.0617  0.6300 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -0.265244   0.008780   -30.2   <2e-16 ***\n#> Age          0.007928   0.000161    49.3   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.123 on 1553 degrees of freedom\n#> Multiple R-squared:  0.61,   Adjusted R-squared:  0.609 \n#> F-statistic: 2.43e+03 on 1 and 1553 DF,  p-value: <2e-16\nsimplePredictions <- predict(\n  object = simpleModel,\n  newdata = test)\nplot(\n  x = policies$Age, \n  y = policies$Rate)\n\n\npoints(\n  x = test$Age,\n  y = simplePredictions,\n  col = \"blue\",\n  pch = 4,\n  lwd = 2)\nsimpleRMSE <- sqrt(mean((test$Rate - simplePredictions)^2))\nprint(simpleRMSE)\n#> [1] 0.119"},{"path":"comparing-regression-models.html","id":"predict-with-multiple-linear-regression","chapter":"34 Comparing regression models","heading":"34.4 Predict with Multiple Linear Regression","text":"","code":"\nmultipleModel <- lm(\n  formula = Rate ~ Age + Gender + State.Rate + BMI,\n  data = train)\nsummary(multipleModel)\n#> \n#> Call:\n#> lm(formula = Rate ~ Age + Gender + State.Rate + BMI, data = train)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -0.2255 -0.0865 -0.0292  0.0590  0.6053 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) -0.428141   0.018742  -22.84  < 2e-16 ***\n#> Age          0.007703   0.000156   49.28  < 2e-16 ***\n#> GenderMale   0.030350   0.006001    5.06  4.8e-07 ***\n#> State.Rate   0.613139   0.068330    8.97  < 2e-16 ***\n#> BMI          0.002634   0.000518    5.09  4.1e-07 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.118 on 1550 degrees of freedom\n#> Multiple R-squared:  0.64,   Adjusted R-squared:  0.639 \n#> F-statistic:  688 on 4 and 1550 DF,  p-value: <2e-16\nmultiplePredictions <- predict(\n  object = multipleModel,\n  newdata = test)\nplot(\n  x = policies$Age, \n  y = policies$Rate)\n\npoints(\n  x = test$Age,\n  y = multiplePredictions,\n  col = \"blue\",\n  pch = 4,\n  lwd = 2)\nmultipleRMSE <- sqrt(mean((test$Rate - multiplePredictions)^2))\nprint(multipleRMSE)\n#> [1] 0.114"},{"path":"comparing-regression-models.html","id":"predict-with-neural-network-regression-1","chapter":"34 Comparing regression models","heading":"34.5 Predict with Neural Network Regression","text":"","code":"\nnormalize <- function(x) {\n  (x - min(x)) / (max(x) - min(x)) - 0.5\n}\ndenormalize <- function(x, y) {\n  ((x + 0.5) * (max(y) - min(y))) + min(y)\n}\nscaledPolicies <- data.frame(\n  Gender = policies$Gender,\n  State.Rate = normalize(policies$State.Rate),\n  BMI = normalize(policies$BMI),\n  Age = normalize(policies$Age),\n  Rate = normalize(policies$Rate))\nscaledTrain <- scaledPolicies[indexes, ]\nscaledTest <- scaledPolicies[-indexes, ]\nlibrary(nnet)\n\nneuralRegressor <- nnet(\n  formula = Rate ~ .,\n  data = scaledTrain,\n  linout = TRUE,\n  size = 5,\n  decay = 0.0001,\n  maxit = 1000)\n#> # weights:  31\n#> initial  value 548.090539 \n#> iter  10 value 10.610284\n#> iter  20 value 3.927378\n#> iter  30 value 3.735266\n#> iter  40 value 3.513899\n#> iter  50 value 3.073390\n#> iter  60 value 2.547202\n#> iter  70 value 2.296126\n#> iter  80 value 2.166120\n#> iter  90 value 2.106996\n#> iter 100 value 2.092654\n#> iter 110 value 2.058596\n#> iter 120 value 2.039404\n#> iter 130 value 2.023721\n#> iter 140 value 2.018777\n#> iter 150 value 2.006895\n#> iter 160 value 1.999130\n#> iter 170 value 1.993921\n#> iter 180 value 1.990505\n#> iter 190 value 1.989212\n#> iter 200 value 1.988818\n#> iter 210 value 1.988007\n#> iter 220 value 1.987710\n#> iter 230 value 1.987671\n#> iter 240 value 1.987599\n#> iter 250 value 1.987575\n#> iter 260 value 1.987553\n#> iter 270 value 1.987537\n#> iter 280 value 1.987528\n#> final  value 1.987522 \n#> converged\nscaledPredictions <- predict(\n  object = neuralRegressor, \n  newdata = scaledTest)\nneuralPredictions <- denormalize(\n  x = scaledPredictions, \n  y = policies$Rate)\nplot(\n  x = train$Age, \n  y = train$Rate)\n\npoints(\n  x = test$Age,\n  y = neuralPredictions,\n  col = \"blue\",\n  pch = 4,\n  lwd = 2)\nlibrary(NeuralNetTools)\n\nplotnet(neuralRegressor)\nneuralRMSE <- sqrt(mean((test$Rate - neuralPredictions)^2))\nprint(neuralRMSE)\n#> [1] 0.0368"},{"path":"comparing-regression-models.html","id":"evaluate-the-regression-models","chapter":"34 Comparing regression models","heading":"34.6 Evaluate the Regression Models","text":"","code":"\nprint(simpleRMSE)\n#> [1] 0.119\nprint(multipleRMSE)\n#> [1] 0.114\nprint(neuralRMSE)\n#> [1] 0.0368"},{"path":"finding-the-factors-of-happiness.html","id":"finding-the-factors-of-happiness","chapter":"35 Finding the factors of happiness","heading":"35 Finding the factors of happiness","text":"Dataset: World Happiness, happiness","code":""},{"path":"finding-the-factors-of-happiness.html","id":"introduction-18","chapter":"35 Finding the factors of happiness","heading":"35.1 Introduction","text":"Source: http://enhancedatascience.com/2017/04/25/r-basics-linear-regression--r/\nData: https://www.kaggle.com/unsdsn/world-happinessLinear regression one basics statistics machine learning. Hence, must-know perform linear regression R interpret results.Linear regression algorithm fit best straight line fits data? , minimise squared distance points dataset fitted line.tutorial, use World Happiness report dataset Kaggle. report analyses Happiness country according several factors wealth, health, family life, … goal find important factors happiness. noble goal!","code":""},{"path":"finding-the-factors-of-happiness.html","id":"a-quick-exploration-of-the-data","chapter":"35 Finding the factors of happiness","heading":"35.2 A quick exploration of the data","text":"fitting model, need know data better. First, let’s import data R. Please download dataset Kaggle put working directory.code imports data data.table clean column names (lot . appearing original ones)Now, let’s plot Scatter Plot Matrix get grasp variables related one another. , GGally package great.variables positively correlated Happiness score. can expect coefficients linear regression positive. However, correlation variable often 0.5, can expect multicollinearity appear regression.data, also access Country score computed. Even ’s useful regression, let’s plot data map!code classic code map. important points:reordered point plotting avoid artefacts.\nmerge right outer join, points map need kept. Otherwise, points missing mess map.\nvariable rescaled facet_wrap can used. , absolute level variable primary interest. relative level variable countries want visualise.distinction North South quite visible. addition , countries suffered crisis also really visible.","code":"\nrequire(data.table)\n#> Loading required package: data.table\ndata_happiness_dir <- file.path(data_raw_dir, \"happiness\")\n\nHappiness_Data = data.table(read.csv(file.path(data_happiness_dir, '2016.csv')))\ncolnames(Happiness_Data) <- gsub('.','',colnames(Happiness_Data), fixed=T)\nrequire(ggplot2)\n#> Loading required package: ggplot2\nrequire(GGally)\n#> Loading required package: GGally\n#> Registered S3 method overwritten by 'GGally':\n#>   method from   \n#>   +.gg   ggplot2\nggpairs(Happiness_Data[,c(4,7:13), with=F], lower = list( continuous = \"smooth\"))\nrequire('rworldmap')\n#> Loading required package: rworldmap\n#> Loading required package: sp\n#> ### Welcome to rworldmap ###\n#> For a short introduction type :   vignette('rworldmap')\nlibrary(reshape2)\n#> \n#> Attaching package: 'reshape2'\n#> The following objects are masked from 'package:data.table':\n#> \n#>     dcast, melt\n\nmap.world <- map_data(map=\"world\")\n\ndataPlot<- melt(Happiness_Data, id.vars ='Country', \n                measure.vars = colnames(Happiness_Data)[c(4,7:13)])\n\n#Correcting names that are different\ndataPlot[Country == 'United States', Country:='USA']\ndataPlot[Country == 'United Kingdoms', Country:='UK']\n\n##Rescaling each variable to have nice gradient\ndataPlot[,value:=value/max(value), by=variable]\ndataMap = data.table(merge(map.world, dataPlot, \n                           by.x='region', \n                           by.y='Country', \n                           all.x=T))\ndataMap = dataMap[order(order)]\ndataMap = dataMap[order(order)][!is.na(variable)]\ngg <- ggplot()\ngg <- gg + \n    geom_map(data=dataMap, map=dataMap, \n             aes(map_id = region, x=long, y=lat, fill=value)) +\n    # facet_wrap(~variable, scale='free')\n    facet_wrap(~variable)\n#> Warning: Ignoring unknown aesthetics: x, y\ngg <- gg + scale_fill_gradient(low = \"navy\", high = \"lightblue\")\ngg <- gg + coord_equal()\ngg"},{"path":"finding-the-factors-of-happiness.html","id":"linear-regression-with-r","chapter":"35 Finding the factors of happiness","heading":"35.3 Linear regression with R","text":"Now taken look data, first model can fitted. explanatory variables DGP per capita, life expectancy, level freedom trust government.","code":"\n##First model\nmodel1 <- lm(HappinessScore ~ EconomyGDPperCapita + Family + \n                 HealthLifeExpectancy + Freedom + TrustGovernmentCorruption, \n             data=Happiness_Data)"},{"path":"finding-the-factors-of-happiness.html","id":"regression-summary","chapter":"35 Finding the factors of happiness","heading":"35.4 Regression summary","text":"summary function provides easy way assess linear regression R.quick interpretation:coefficient significative .05 thresholdThe overall model also significativeIt explains 78.7% Happiness datasetAs expected relationship explanatory variables output variable positives.model well!can also easily get given indicator model performance, R², different coefficients p-value overall model.","code":"\nrequire(stargazer)\n#> Loading required package: stargazer\n#> \n#> Please cite as:\n#>  Hlavac, Marek (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n#>  R package version 5.2.2. https://CRAN.R-project.org/package=stargazer\n\n##Quick summary\nsum1=summary(model1)\nsum1\n#> \n#> Call:\n#> lm(formula = HappinessScore ~ EconomyGDPperCapita + Family + \n#>     HealthLifeExpectancy + Freedom + TrustGovernmentCorruption, \n#>     data = Happiness_Data)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -1.4833 -0.2817 -0.0277  0.3280  1.4615 \n#> \n#> Coefficients:\n#>                           Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)                  2.212      0.150   14.73  < 2e-16 ***\n#> EconomyGDPperCapita          0.697      0.209    3.33   0.0011 ** \n#> Family                       1.234      0.229    5.39  2.6e-07 ***\n#> HealthLifeExpectancy         1.462      0.343    4.26  3.5e-05 ***\n#> Freedom                      1.559      0.373    4.18  5.0e-05 ***\n#> TrustGovernmentCorruption    0.959      0.455    2.11   0.0365 *  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.535 on 151 degrees of freedom\n#> Multiple R-squared:  0.787,  Adjusted R-squared:  0.78 \n#> F-statistic:  112 on 5 and 151 DF,  p-value: <2e-16\n\nstargazer(model1,type='text')\n#> \n#> =====================================================\n#>                               Dependent variable:    \n#>                           ---------------------------\n#>                                 HappinessScore       \n#> -----------------------------------------------------\n#> EconomyGDPperCapita                0.697***          \n#>                                     (0.209)          \n#>                                                      \n#> Family                             1.230***          \n#>                                     (0.229)          \n#>                                                      \n#> HealthLifeExpectancy               1.460***          \n#>                                     (0.343)          \n#>                                                      \n#> Freedom                            1.560***          \n#>                                     (0.373)          \n#>                                                      \n#> TrustGovernmentCorruption           0.959**          \n#>                                     (0.455)          \n#>                                                      \n#> Constant                           2.210***          \n#>                                     (0.150)          \n#>                                                      \n#> -----------------------------------------------------\n#> Observations                          157            \n#> R2                                   0.787           \n#> Adjusted R2                          0.780           \n#> Residual Std. Error            0.535 (df = 151)      \n#> F Statistic                112.000*** (df = 5; 151)  \n#> =====================================================\n#> Note:                     *p<0.1; **p<0.05; ***p<0.01\n##R²\nsum1$r.squared*100\n#> [1] 78.7\n##Coefficients\nsum1$coefficients\n#>                           Estimate Std. Error t value Pr(>|t|)\n#> (Intercept)                  2.212      0.150   14.73 5.20e-31\n#> EconomyGDPperCapita          0.697      0.209    3.33 1.10e-03\n#> Family                       1.234      0.229    5.39 2.62e-07\n#> HealthLifeExpectancy         1.462      0.343    4.26 3.53e-05\n#> Freedom                      1.559      0.373    4.18 5.01e-05\n#> TrustGovernmentCorruption    0.959      0.455    2.11 3.65e-02\n##p-value\ndf(sum1$fstatistic[1],sum1$fstatistic[2],sum1$fstatistic[3])\n#>    value \n#> 3.39e-49\n \n##Confidence interval of the coefficient\nconfint(model1,level = 0.95)\n#>                            2.5 % 97.5 %\n#> (Intercept)               1.9152   2.51\n#> EconomyGDPperCapita       0.2833   1.11\n#> Family                    0.7821   1.69\n#> HealthLifeExpectancy      0.7846   2.14\n#> Freedom                   0.8212   2.30\n#> TrustGovernmentCorruption 0.0609   1.86\nconfint(model1,level = 0.99)\n#>                            0.5 % 99.5 %\n#> (Intercept)                1.820   2.60\n#> EconomyGDPperCapita        0.151   1.24\n#> Family                     0.637   1.83\n#> HealthLifeExpectancy       0.568   2.36\n#> Freedom                    0.585   2.53\n#> TrustGovernmentCorruption -0.227   2.14\nconfint(model1,level = 0.90)\n#>                             5 % 95 %\n#> (Intercept)               1.963 2.46\n#> EconomyGDPperCapita       0.350 1.04\n#> Family                    0.856 1.61\n#> HealthLifeExpectancy      0.895 2.03\n#> Freedom                   0.941 2.18\n#> TrustGovernmentCorruption 0.207 1.71"},{"path":"finding-the-factors-of-happiness.html","id":"regression-analysis","chapter":"35 Finding the factors of happiness","heading":"35.5 Regression analysis","text":"","code":""},{"path":"finding-the-factors-of-happiness.html","id":"residual-analysis","chapter":"35 Finding the factors of happiness","heading":"35.5.1 Residual analysis","text":"Now regression done, analysis validity result can analysed. Let’s begin residuals assumption normality homoscedasticity.residual versus fitted plot used see residuals behave different value output (.e, variance mean). plot shows strong evidence heteroscedasticity.","code":"\n# Visualisation of residuals\nggplot(model1, aes(model1$residuals)) + \n    geom_histogram(bins=20, aes(y = ..density..)) + \n    geom_density(color='blue', fill = 'blue', alpha = 0.2) + \n    geom_vline(xintercept = mean(model1$residuals), color='red') + \n    stat_function(fun=dnorm, color=\"red\", size=1, \n                  args = list(mean = mean(model1$residuals), \n                            sd = sd(model1$residuals))) + \n    xlab('residuals values')\nggplot(model1, aes(model1$fitted.values, model1$residuals)) + \n    geom_point() + \n    geom_hline(yintercept = c(1.96 * sd(model1$residuals), \n                              - 1.96 * sd(model1$residuals)), color='red') + \n    xlab('fitted value') + \n    ylab('residuals values')"},{"path":"finding-the-factors-of-happiness.html","id":"analysis-of-colinearity","chapter":"35 Finding the factors of happiness","heading":"35.6 Analysis of colinearity","text":"colinearity can assessed using VIF, car package provides function compute directly.VIF less 5, hence sign colinearity.","code":"\nrequire('car')\n#> Loading required package: car\n#> Loading required package: carData\nvif(model1)\n#>       EconomyGDPperCapita                    Family      HealthLifeExpectancy \n#>                      4.07                      2.03                      3.37 \n#>                   Freedom TrustGovernmentCorruption \n#>                      1.61                      1.39"},{"path":"finding-the-factors-of-happiness.html","id":"what-drives-happiness","chapter":"35 Finding the factors of happiness","heading":"35.7 What drives happiness","text":"Now let’s compute standardised betas see really drives happiness.Though code may seem complicated, just computing standardised betas variables std_beta=beta*sd(x)/sd(y).top three coefficients Health Life expectancy, Family GDP per Capita. Though money make happiness among top three factors Happiness!Now know perform linear regression R!","code":"\n##Standardized betas\nstd_betas = sum1$coefficients[-1,1] * \n    data.table(model1$model)[, lapply(.SD, sd), .SDcols=2:6] / \n    sd(model1$model$HappinessScore)\n\nstd_betas\n#>    EconomyGDPperCapita Family HealthLifeExpectancy Freedom\n#> 1:               0.252  0.288                0.294   0.199\n#>    TrustGovernmentCorruption\n#> 1:                    0.0933"},{"path":"regression-with-a-neural-network.html","id":"regression-with-a-neural-network","chapter":"36 Regression with a neural network","heading":"36 Regression with a neural network","text":"Dataset: BostonHousingAlgorithms:\nNeural Network (nnet)\nLinear Regression\nNeural Network (nnet)Linear Regression","code":"\n###\n### prepare data\n###\nlibrary(mlbench)\ndata(BostonHousing)\n \n# inspect the range which is 1-50\nsummary(BostonHousing$medv)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>     5.0    17.0    21.2    22.5    25.0    50.0\n \n \n##\n## model linear regression\n##\n \nlm.fit <- lm(medv ~ ., data=BostonHousing)\n \nlm.predict <- predict(lm.fit)\n \n# mean squared error: 21.89483\nmean((lm.predict - BostonHousing$medv)^2) \n#> [1] 21.9\n \nplot(BostonHousing$medv, lm.predict,\n    main=\"Linear regression predictions vs actual\",\n    xlab=\"Actual\")\n##\n## model neural network\n##\nrequire(nnet)\n#> Loading required package: nnet\n \n# scale inputs: divide by 50 to get 0-1 range\nnnet.fit <- nnet(medv/50 ~ ., data=BostonHousing, size=2) \n#> # weights:  31\n#> initial  value 17.039194 \n#> iter  10 value 13.754559\n#> iter  20 value 13.537235\n#> iter  30 value 13.537183\n#> iter  40 value 13.530522\n#> final  value 13.529736 \n#> converged\n \n# multiply 50 to restore original scale\nnnet.predict <- predict(nnet.fit)*50 \n \n# mean squared error: 16.40581\nmean((nnet.predict - BostonHousing$medv)^2) \n#> [1] 66.8\n \nplot(BostonHousing$medv, nnet.predict,\n    main=\"Neural network predictions vs actual\",\n    xlab=\"Actual\")"},{"path":"regression-with-a-neural-network.html","id":"neural-network","chapter":"36 Regression with a neural network","heading":"36.1 Neural Network","text":"Now, let’s use function train() package caret optimize neural network hyperparameters decay size, Also, caret performs resampling give better estimate error. case scale linear regression value, error statistics directly comparable.","code":"\n library(mlbench)\n data(BostonHousing)\n \nrequire(caret)\n#> Loading required package: caret\n#> Loading required package: lattice\n#> Loading required package: ggplot2\n \nmygrid <- expand.grid(.decay=c(0.5, 0.1), .size=c(4,5,6))\nnnetfit <- train(medv/50 ~ ., data=BostonHousing, method=\"nnet\", maxit=1000, tuneGrid=mygrid, trace=F) \nprint(nnetfit)\n#> Neural Network \n#> \n#> 506 samples\n#>  13 predictor\n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (25 reps) \n#> Summary of sample sizes: 506, 506, 506, 506, 506, 506, ... \n#> Resampling results across tuning parameters:\n#> \n#>   decay  size  RMSE    Rsquared  MAE   \n#>   0.1    4     0.0830  0.790     0.0571\n#>   0.1    5     0.0814  0.798     0.0559\n#>   0.1    6     0.0799  0.806     0.0549\n#>   0.5    4     0.0908  0.757     0.0626\n#>   0.5    5     0.0897  0.762     0.0622\n#>   0.5    6     0.0890  0.766     0.0620\n#> \n#> RMSE was used to select the optimal model using the smallest value.\n#> The final values used for the model were size = 6 and decay = 0.1.506 samples\n 13 predictors\n \nNo pre-processing\nResampling: Bootstrap (25 reps) \n \nSummary of sample sizes: 506, 506, 506, 506, 506, 506, ... \n \nResampling results across tuning parameters:\n \n  size  decay  RMSE    Rsquared  RMSE SD  Rsquared SD\n  4     0.1    0.0852  0.785     0.00863  0.0406     \n  4     0.5    0.0923  0.753     0.00891  0.0436     \n  5     0.1    0.0836  0.792     0.00829  0.0396     \n  5     0.5    0.0899  0.765     0.00858  0.0399     \n  6     0.1    0.0835  0.793     0.00804  0.0318     \n  6     0.5    0.0895  0.768     0.00789  0.0344   "},{"path":"regression-with-a-neural-network.html","id":"linear-regression","chapter":"36 Regression with a neural network","heading":"36.2 Linear Regression","text":"tuned neural network RMSE 0.0835 compared linear regression’s RMSE 0.0994.","code":"\n lmfit <- train(medv/50 ~ ., data=BostonHousing, method=\"lm\") \n print(lmfit)\n#> Linear Regression \n#> \n#> 506 samples\n#>  13 predictor\n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (25 reps) \n#> Summary of sample sizes: 506, 506, 506, 506, 506, 506, ... \n#> Resampling results:\n#> \n#>   RMSE    Rsquared  MAE   \n#>   0.0988  0.726     0.0692\n#> \n#> Tuning parameter 'intercept' was held constant at a value of TRUE506 samples\n 13 predictors\n \nNo pre-processing\nResampling: Bootstrap (25 reps) \n \nSummary of sample sizes: 506, 506, 506, 506, 506, 506, ... \n \nResampling results\n \n  RMSE    Rsquared  RMSE SD  Rsquared SD\n  0.0994  0.703     0.00741  0.0389    "},{"path":"comparing-multiple-regression-vs-a-neural-network.html","id":"comparing-multiple-regression-vs-a-neural-network","chapter":"37 Comparing Multiple Regression vs a Neural Network","heading":"37 Comparing Multiple Regression vs a Neural Network","text":"Dataset: diamondsAlgorithms:\nNeural Networks (RSNNS)\nMultiple Regression\nNeural Networks (RSNNS)Multiple Regression","code":""},{"path":"comparing-multiple-regression-vs-a-neural-network.html","id":"introduction-19","chapter":"37 Comparing Multiple Regression vs a Neural Network","heading":"37.1 Introduction","text":"Source: http://beyondvalence.blogspot.com/2014/04/r-comparing-multiple--neural-network.htmlHere compare evaluate results multiple regression neural network diamonds data set ggplot2 package R. Consisting 53,940 observations 10 variables, diamonds contains data carat, cut, color, clarity, price, diamond dimensions. variables particular effect price, like see can predict price various diamonds.cut, color, clarity variables factors, must treated dummy variables multiple neural network regressions. Let us start multiple regression.","code":"\nlibrary(ggplot2)\nlibrary(RSNNS)\n#> Loading required package: Rcpp\nlibrary(MASS)\nlibrary(caret)\n#> Loading required package: lattice\n#> \n#> Attaching package: 'caret'\n#> The following objects are masked from 'package:RSNNS':\n#> \n#>     confusionMatrix, train\n# library(diamonds)\n\nhead(diamonds)\n#> # A tibble: 6 x 10\n#>   carat cut       color clarity depth table price     x     y     z\n#>   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n#> 1 0.23  Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n#> 2 0.21  Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n#> 3 0.23  Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n#> 4 0.290 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n#> 5 0.31  Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n#> 6 0.24  Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\ndplyr::glimpse(diamonds)\n#> Rows: 53,940\n#> Columns: 10\n#> $ carat   <dbl> 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0…\n#> $ cut     <ord> Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ve…\n#> $ color   <ord> E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I…\n#> $ clarity <ord> SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1,…\n#> $ depth   <dbl> 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 6…\n#> $ table   <dbl> 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 5…\n#> $ price   <int> 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 3…\n#> $ x       <dbl> 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4…\n#> $ y       <dbl> 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4…\n#> $ z       <dbl> 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2…"},{"path":"comparing-multiple-regression-vs-a-neural-network.html","id":"multiple-regression","chapter":"37 Comparing Multiple Regression vs a Neural Network","heading":"37.2 Multiple Regression","text":"First ready Multiple Regression sampling rows randomize observations, create sample index 0’s 1’s separate training test sets. Note depth table columns (5, 6) removed linear combinations dimensions, x, y, z. See observations training test sets approximate 70% 30% total observations, sampled set probabilities.Now move next stage multiple regression via train() function caret library, instead regular lm() function. specify predictors, response variable (price), “lm” method, cross validation resampling method.call train(ed) object, can see attributes training set, resampling, sample sizes, results. Note root mean square error value 1150. low enough take heavy weight TEAM: Neural Network? visualize training diamond prices predicted prices ggplot().see axis, predicted prices high values compared actual prices. Also, predicted prices 0, possible observed, set TEAM: Multiple Regression back points.Next use ggplot() visualize predicted observed diamond prices test data, train linear regression model.Similar training prices plot, see test prices model predicts larger values also predicted negative price values. order Multiple Regression win, Neural Network wild prediction values.Lastly, calculate root mean square error, taking mean squared difference predicted observed diamond prices. resulting RMSE 1110.843, similar RMSE training set.detailed output model summary, coefficients residuals. Observe carat best predictor, highest t value 191.7, every increase 1 carat holding variables equal, results 10,873 dollar increase value. look factor variables, see reliable increase coefficients increases level value.Now move neural network regression.","code":"\nset.seed(1234567)\ndiamonds <- diamonds[sample(1:nrow(diamonds), nrow(diamonds)),]\nd.index = sample(0:1, nrow(diamonds), prob=c(0.3, 0.7), rep = TRUE)\nd.train <- diamonds[d.index==1, c(-5,-6)]\nd.test <- diamonds[d.index==0, c(-5,-6)]\ndim(d.train)\n#> [1] 37502     8\ndim(d.test)\n#> [1] 16438     8\nx <- d.train[,-5]\ny <- as.numeric(d.train[,5]$price)\n\nds.lm <- caret::train(x, y, method = \"lm\",\n                      trainControl = trainControl(method = \"cv\"))\n#> Warning: Setting row names on a tibble is deprecated.\n#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n#>  extra argument 'trainControl' will be disregarded\n#> Warning: Setting row names on a tibble is deprecated.\n#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n#>  extra argument 'trainControl' will be disregarded\n#> Warning: Setting row names on a tibble is deprecated.\n#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n#>  extra argument 'trainControl' will be disregarded\n#> Warning: Setting row names on a tibble is deprecated.\n#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n#>  extra argument 'trainControl' will be disregarded\n#> Warning: Setting row names on a tibble is deprecated.\n#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n#>  extra argument 'trainControl' will be disregarded\n#> Warning: Setting row names on a tibble is deprecated.\n#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n#>  extra argument 'trainControl' will be disregarded\n#> Warning: Setting row names on a tibble is deprecated.\n#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n#>  extra argument 'trainControl' will be disregarded\n#> Warning: Setting row names on a tibble is deprecated.\n#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n#>  extra argument 'trainControl' will be disregarded\n#> Warning: Setting row names on a tibble is deprecated.\n#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n#>  extra argument 'trainControl' will be disregarded\n#> Warning: Setting row names on a tibble is deprecated.\n#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n#>  extra argument 'trainControl' will be disregarded\n#> Warning: Setting row names on a tibble is deprecated.\n#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n#>  extra argument 'trainControl' will be disregarded\n#> Warning: Setting row names on a tibble is deprecated.\n#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n#>  extra argument 'trainControl' will be disregarded\n#> Warning: Setting row names on a tibble is deprecated.\n#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n#>  extra argument 'trainControl' will be disregarded\n#> Warning: Setting row names on a tibble is deprecated.\n#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n#>  extra argument 'trainControl' will be disregarded\n#> Warning: Setting row names on a tibble is deprecated.\n#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n#>  extra argument 'trainControl' will be disregarded\n#> Warning: Setting row names on a tibble is deprecated.\n#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n#>  extra argument 'trainControl' will be disregarded\n#> Warning: Setting row names on a tibble is deprecated.\n#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n#>  extra argument 'trainControl' will be disregarded\n#> Warning: Setting row names on a tibble is deprecated.\n#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n#>  extra argument 'trainControl' will be disregarded\n#> Warning: Setting row names on a tibble is deprecated.\n#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n#>  extra argument 'trainControl' will be disregarded\n#> Warning: Setting row names on a tibble is deprecated.\n#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n#>  extra argument 'trainControl' will be disregarded\n#> Warning: Setting row names on a tibble is deprecated.\n#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n#>  extra argument 'trainControl' will be disregarded\n#> Warning: Setting row names on a tibble is deprecated.\n#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n#>  extra argument 'trainControl' will be disregarded\n#> Warning: Setting row names on a tibble is deprecated.\n#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n#>  extra argument 'trainControl' will be disregarded\n#> Warning: Setting row names on a tibble is deprecated.\n#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n#>  extra argument 'trainControl' will be disregarded\n#> Warning: Setting row names on a tibble is deprecated.\n#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n#>  extra argument 'trainControl' will be disregarded\n#> Warning: Setting row names on a tibble is deprecated.\n#> Warning: In lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :\n#>  extra argument 'trainControl' will be disregarded\nds.lm                      \n#> Linear Regression \n#> \n#> 37502 samples\n#>     7 predictor\n#> \n#> No pre-processing\n#> Resampling: Bootstrapped (25 reps) \n#> Summary of sample sizes: 37502, 37502, 37502, 37502, 37502, 37502, ... \n#> Resampling results:\n#> \n#>   RMSE  Rsquared  MAE\n#>   1140  0.919     745\n#> \n#> Tuning parameter 'intercept' was held constant at a value of TRUE\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following object is masked from 'package:MASS':\n#> \n#>     select\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\n\ndata.frame(obs = y, pred = ds.lm$finalModel$fitted.values) %>% \n  ggplot(aes(x = obs, y = pred)) +\n  geom_point(alpha=0.1) +\n  geom_abline(color=\"blue\") +\n  labs(title=\"Diamond train price\", x=\"observed\", y=\"predicted\")\n# predict on test set\nds.lm.p <- predict(ds.lm, d.test[,-5], type=\"raw\")\n\n# compare observed vs predicted prices in the test set\ndata.frame(obs = d.test[,5]$price, pred = ds.lm.p) %>% \n  ggplot(aes(x = obs, y = pred)) +\n  geom_point(alpha=0.1) +\n  geom_abline(color=\"blue\")+\n  labs(\"Diamond Test Price\", x=\"observed\", y=\"predicted\")\nds.lm.mse <- (1 / nrow(d.test)) * sum((ds.lm.p - d.test[,5])^2)\nlm.rmse <- sqrt(ds.lm.mse)\nlm.rmse\n#> [1] 1168\nsummary(ds.lm)\n#> \n#> Call:\n#> lm(formula = .outcome ~ ., data = dat, trainControl = ..1)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -21090   -598   -183    378  10778 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)     3.68      94.63    0.04   0.9690    \n#> carat       11142.68      57.43  194.02  < 2e-16 ***\n#> cut.L         767.70      24.31   31.58  < 2e-16 ***\n#> cut.Q        -336.63      21.41  -15.72  < 2e-16 ***\n#> cut.C         157.31      18.81    8.36  < 2e-16 ***\n#> cut^4         -22.81      14.78   -1.54   0.1228    \n#> color.L     -1950.28      20.66  -94.42  < 2e-16 ***\n#> color.Q      -665.60      18.82  -35.37  < 2e-16 ***\n#> color.C      -147.16      17.61   -8.36  < 2e-16 ***\n#> color^4        44.64      16.20    2.76   0.0059 ** \n#> color^5       -91.21      15.32   -5.95  2.7e-09 ***\n#> color^6       -54.74      13.92   -3.93  8.5e-05 ***\n#> clarity.L    4115.45      36.68  112.19  < 2e-16 ***\n#> clarity.Q   -1959.71      34.33  -57.09  < 2e-16 ***\n#> clarity.C     990.60      29.29   33.83  < 2e-16 ***\n#> clarity^4    -370.82      23.30  -15.92  < 2e-16 ***\n#> clarity^5     240.60      18.91   12.72  < 2e-16 ***\n#> clarity^6      -7.99      16.37   -0.49   0.6253    \n#> clarity^7      80.62      14.48    5.57  2.6e-08 ***\n#> x           -1400.26      95.70  -14.63  < 2e-16 ***\n#> y             545.42      94.57    5.77  8.1e-09 ***\n#> z            -190.86      31.20   -6.12  9.6e-10 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1130 on 37480 degrees of freedom\n#> Multiple R-squared:  0.92,   Adjusted R-squared:  0.92 \n#> F-statistic: 2.05e+04 on 21 and 37480 DF,  p-value: <2e-16"},{"path":"comparing-multiple-regression-vs-a-neural-network.html","id":"neural-network-1","chapter":"37 Comparing Multiple Regression vs a Neural Network","heading":"37.3 Neural Network","text":"neural networks operate terms 0 1, -1 1, must first normalize price variable 0 1, making lowest value 0 highest value 1. accomplished using normalizeData() function. Save price output order revert normalization training data. Also, take factor variables turn numeric labels using toNumericClassLabels(). see normalized prices split training test set splitForTrainingAndTest() function.Now Neural Network ready multi-layer perceptron (MLP) regression. define training inputs (predictor variables) targets (prices), size layer (5), incremented learning parameter (0.1), max iterations (100 epochs), also test input/targets.spectators dealt mlp() , know summary output can quite lenghty, omitted (dislike commercials ). move visual description MLP model iterative sum square error training test sets. Additionally, plot regression error (predicted vs observed) training test prices.Time Neural Network show statistical muscles! First , iterative sum square error epoch, noting specified maximum 100 MLP model. see immediate drop SSE first iterations, SSE leveling around 50. test SSE, red, fluctuations just 50 well. Since SSE began plateau, model fit well well, since want avoid fitting model. 100 iterations good choice.Second, observe regression plot fitted (predicted) target (observed) prices training set. prices fit reasonably well, see red model regression line close black (y=x) optimal line. Note middle prices predicted model, negative prices, unlike linear regression model.Third, look predicted observed prices test set. red regression line approximates optimal black line, price values predicted model. , negative predicted prices, good sign.Now calculate RMSE training set, get 692.5155. looks promising Neural Network!Naturally want calculate RMSE test set, note real world, luxury knowing real test values. arrive 679.5265.model better predicting diamond price? linear regression model 10 fold cross validation, multi-layer perceptron model 5 nodes run 100 iterations? won rumble?RUMBLE RESULTSFrom calculating two RMSE’s training test sets two TEAMS, wrap list. named TEAM: Multiple Regression linear, TEAM: Neural Network regression neural.can evaluate models RMSE values.Looking training RMSE first, see clear difference linear RMSE 66% larger neural RMSE, 1,152.393 versus 692.5155. Peeking test sets, similar 63% larger linear RMSE neural RMSE, 1,110.843 679.5265 respectively. TEAM: Neural Network begins gain upper hand evaluation round.One important difference two models range predictions. Recall training test plots linear regression model predicted negative price values, whereas MLP model predicted positive prices. devastating blow Multiple Regression. Also, -prediction prices existed models, however linear regression model predicted middle values higher anticipated maximum price values.Sometimes simple models optimal, times complicated models better. time, neural network model prevailed predicting diamond prices.","code":"\ndiamonds[,3] <- toNumericClassLabels(diamonds[,3]$color)\ndiamonds[,4] <- toNumericClassLabels(diamonds[,4]$clarity)\nprices <- normalizeData(diamonds[,7], type=\"0_1\")\nhead(prices)\n#>        [,1]\n#> [1,] 0.0841\n#> [2,] 0.1491\n#> [3,] 0.0237\n#> [4,] 0.3247\n#> [5,] 0.0280\n#> [6,] 0.0252\ndsplit <- splitForTrainingAndTest(diamonds[, c(-2,-5,-6,-7,-9,-10)], prices, ratio=0.3)\n# mlp model\nd.nn <- mlp(dsplit$inputsTrain,\n            dsplit$targetsTrain,\n            size = c(5), learnFuncParams = c(0.1), maxit=100,\n            inputsTest = dsplit$inputsTest,\n            targetsTest = dsplit$targetsTest,\n            metric = \"RMSE\",\n            linout = FALSE)\n# SSE error\nplotIterativeError(d.nn, main = \"Diamonds RSNNS-SSE\")\n# regression  errors\nplotRegressionError(dsplit$targetsTrain, d.nn$fitted.values,\n                    main = \"Diamonds Training Fit\")\nplotRegressionError(dsplit$targetsTest, d.nn$fittedTestValues,\n                    main = \"Diamonds Test Fit\")\n# train set\ntrain.pred <- denormalizeData(d.nn$fitted.values,\n                              getNormParameters(prices))\n\ntrain.obs <- denormalizeData(dsplit$targetsTrain,\n                             getNormParameters(prices))\n\ntrain.mse <- (1 / nrow(dsplit$inputsTrain)) * sum((train.pred - train.obs)^2)\n\nrsnns.train.rmse <- sqrt(train.mse)\nrsnns.train.rmse\n#> [1] 739\n# test set\ntest.pred <- denormalizeData(d.nn$fittedTestValues,\n                             getNormParameters(prices))\n\ntest.obs <- denormalizeData(dsplit$targetsTest,\n                            getNormParameters(prices))\n\ntest.mse <- (1 / nrow(dsplit$inputsTest)) * sum((test.pred - test.obs)^2)\n\nrsnns.test.rmse <- sqrt(test.mse)\nrsnns.test.rmse\n#> [1] 751\n# aggregate all rmse\nd.rmse <- list(linear.train = ds.lm$results$RMSE,\n               linear.test = lm.rmse,\n               neural.train = rsnns.train.rmse,\n               neural.test = rsnns.test.rmse)\nd.rmse\n#> $linear.train\n#> [1] 1140\n#> \n#> $linear.test\n#> [1] 1168\n#> \n#> $neural.train\n#> [1] 739\n#> \n#> $neural.test\n#> [1] 751"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"temperature-modeling-using-nested-dataframes","chapter":"38 Temperature modeling using nested dataframes","heading":"38 Temperature modeling using nested dataframes","text":"Dataset: temperature.csv","code":""},{"path":"temperature-modeling-using-nested-dataframes.html","id":"introduction-20","chapter":"38 Temperature modeling using nested dataframes","heading":"38.1 Introduction","text":"http://ijlyttle.github.io/isugg_purrr/presentation.html#(1)","code":""},{"path":"temperature-modeling-using-nested-dataframes.html","id":"packages-to-run-this-presentation","chapter":"38 Temperature modeling using nested dataframes","heading":"38.1.1 Packages to run this presentation","text":"","code":"\nlibrary(\"readr\")\nlibrary(\"tibble\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"stringr\")\nlibrary(\"ggplot2\")\nlibrary(\"purrr\")\nlibrary(\"broom\")"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"motivation","chapter":"38 Temperature modeling using nested dataframes","heading":"38.1.2 Motivation","text":"know, purrr recent package Hadley Wickham, focused lists functional programming, like dplyr focused data-frames.figure good way learn new package try solve problem, dataset:can view downloadyou can view downloadyou can download source presentationyou can download source presentationthese three temperatures recorded simultaneously piece electronicsthese three temperatures recorded simultaneously piece electronicsit valuable able characterize transient temperature sensorit valuable able characterize transient temperature sensorwe want apply set models across three sensorswe want apply set models across three sensorsit easier show using picturesit easier show using pictures","code":""},{"path":"temperature-modeling-using-nested-dataframes.html","id":"prepare-the-data","chapter":"38 Temperature modeling using nested dataframes","heading":"38.2 Prepare the data","text":"","code":""},{"path":"temperature-modeling-using-nested-dataframes.html","id":"lets-get-the-data-into-shape","chapter":"38 Temperature modeling using nested dataframes","heading":"38.2.1 Let’s get the data into shape","text":"Using readr package","code":"\ntemperature_wide <- \n  read_csv(file.path(data_raw_dir, \"temperature.csv\")) %>%\n  print()\n#> Parsed with column specification:\n#> cols(\n#>   instant = col_datetime(format = \"\"),\n#>   temperature_a = col_double(),\n#>   temperature_b = col_double(),\n#>   temperature_c = col_double()\n#> )\n#> # A tibble: 327 x 4\n#>   instant             temperature_a temperature_b temperature_c\n#>   <dttm>                      <dbl>         <dbl>         <dbl>\n#> 1 2015-11-13 06:10:19          116.          91.7          84.2\n#> 2 2015-11-13 06:10:23          116.          91.7          84.2\n#> 3 2015-11-13 06:10:27          116.          91.6          84.2\n#> 4 2015-11-13 06:10:31          116.          91.7          84.2\n#> 5 2015-11-13 06:10:36          116.          91.7          84.2\n#> 6 2015-11-13 06:10:41          116.          91.6          84.2\n#> # … with 321 more rows"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"is-temperature_wide-tidy","chapter":"38 Temperature modeling using nested dataframes","heading":"38.2.2 Is temperature_wide “tidy”?","text":"?","code":"#> # A tibble: 327 x 4\n#>   instant             temperature_a temperature_b temperature_c\n#>   <dttm>                      <dbl>         <dbl>         <dbl>\n#> 1 2015-11-13 06:10:19          116.          91.7          84.2\n#> 2 2015-11-13 06:10:23          116.          91.7          84.2\n#> 3 2015-11-13 06:10:27          116.          91.6          84.2\n#> 4 2015-11-13 06:10:31          116.          91.7          84.2\n#> 5 2015-11-13 06:10:36          116.          91.7          84.2\n#> 6 2015-11-13 06:10:41          116.          91.6          84.2\n#> # … with 321 more rows"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"tidy-data","chapter":"38 Temperature modeling using nested dataframes","heading":"38.2.3 Tidy data","text":"column variableEach row observationEach cell value(http://www.jstatsoft.org/v59/i10/paper)personal observation “tidy” can depend context, want data.","code":""},{"path":"temperature-modeling-using-nested-dataframes.html","id":"lets-get-this-into-a-tidy-form","chapter":"38 Temperature modeling using nested dataframes","heading":"38.2.4 Let’s get this into a tidy form","text":"","code":"\ntemperature_tall <-\n  temperature_wide %>%\n  gather(key = \"id_sensor\", value = \"temperature\", starts_with(\"temp\")) %>%\n  mutate(id_sensor = str_replace(id_sensor, \"temperature_\", \"\")) %>%\n  print()\n#> # A tibble: 981 x 3\n#>   instant             id_sensor temperature\n#>   <dttm>              <chr>           <dbl>\n#> 1 2015-11-13 06:10:19 a                116.\n#> 2 2015-11-13 06:10:23 a                116.\n#> 3 2015-11-13 06:10:27 a                116.\n#> 4 2015-11-13 06:10:31 a                116.\n#> 5 2015-11-13 06:10:36 a                116.\n#> 6 2015-11-13 06:10:41 a                116.\n#> # … with 975 more rows"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"now-its-easier-to-visualize","chapter":"38 Temperature modeling using nested dataframes","heading":"38.2.5 Now, it’s easier to visualize","text":"","code":"\ntemperature_tall %>%\n  ggplot(aes(x = instant, y = temperature, color = id_sensor)) +\n  geom_line()"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"calculate-delta-time-delta-t-and-delta-temperature-delta-t","chapter":"38 Temperature modeling using nested dataframes","heading":"38.2.6 Calculate delta time (\\(\\Delta t\\)) and delta temperature (\\(\\Delta T\\))","text":"delta_time \\(\\Delta t\\)change time since event started, sdelta_temperature: \\(\\Delta T\\)change temperature since event started, °C","code":"\ndelta <- \n  temperature_tall %>%\n  arrange(id_sensor, instant) %>%\n  group_by(id_sensor) %>%\n  mutate(\n    delta_time = as.numeric(instant) - as.numeric(instant[[1]]),\n    delta_temperature = temperature - temperature[[1]]\n  ) %>%\n  select(id_sensor, delta_time, delta_temperature)"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"lets-have-a-look","chapter":"38 Temperature modeling using nested dataframes","heading":"38.2.7 Let’s have a look","text":"","code":"\n# plot delta time vs delta temperature, by sensor\ndelta %>%\n  ggplot(aes(x = delta_time, y = delta_temperature, color = id_sensor)) +\n  geom_line()  "},{"path":"temperature-modeling-using-nested-dataframes.html","id":"define-the-models","chapter":"38 Temperature modeling using nested dataframes","heading":"38.3 Define the models","text":"want see three different curve-fits might perform three data-sets:","code":""},{"path":"temperature-modeling-using-nested-dataframes.html","id":"newtonian-cooling","chapter":"38 Temperature modeling using nested dataframes","heading":"38.3.0.1 Newtonian cooling","text":"\\[\\Delta T = \\Delta {T_0} * (1 - e^{-\\frac{\\delta t}{\\tau_0}})\\]","code":""},{"path":"temperature-modeling-using-nested-dataframes.html","id":"semi-infinite-solid","chapter":"38 Temperature modeling using nested dataframes","heading":"38.3.1 Semi-infinite solid","text":"\\[\\Delta T = \\Delta T_0 * erfc(\\sqrt{\\frac{\\tau_0}{\\delta t}}))\\]","code":""},{"path":"temperature-modeling-using-nested-dataframes.html","id":"semi-infinite-solid-with-convection","chapter":"38 Temperature modeling using nested dataframes","heading":"38.3.2 Semi-infinite solid with convection","text":"\\[\\Delta T = \\Delta T_0 * \\big [ \\operatorname erfc(\\sqrt{\\frac{\\tau_0}{\\delta t}}) - e^ {Bi_0 + (\\frac {Bi_0}{2})^2 \\frac {\\delta t}{\\tau_0}} * \\operatorname erfc (\\sqrt \\frac{\\tau_0}{\\delta t} + \\frac {Bi_0}{2} * \\sqrt \\frac{\\delta t }{\\tau_0} \\big]\\]","code":""},{"path":"temperature-modeling-using-nested-dataframes.html","id":"erf-and-erfc-functions","chapter":"38 Temperature modeling using nested dataframes","heading":"38.3.3 erf and erfc functions","text":"","code":"\n# reference: http://stackoverflow.com/questions/29067916/r-error-function-erfz\n# (see Abramowitz and Stegun 29.2.29)\nerf <- function(x) 2 * pnorm(x * sqrt(2)) - 1\nerfc <- function(x) 2 * pnorm(x * sqrt(2), lower = FALSE)"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"newton-cooling-equation","chapter":"38 Temperature modeling using nested dataframes","heading":"38.3.4 Newton cooling equation","text":"","code":"\nnewton_cooling <- function(x) {\n  nls(\n    delta_temperature ~ delta_temperature_0 * (1 - exp(-delta_time/tau_0)),\n    start = list(delta_temperature_0 = -10, tau_0 = 50),\n    data = x\n  )\n}"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"temperature-models-simple-and-convection","chapter":"38 Temperature modeling using nested dataframes","heading":"38.3.5 Temperature models: simple and convection","text":"","code":"\nsemi_infinite_simple <- function(x) {\n  nls(\n    delta_temperature ~ delta_temperature_0 * erfc(sqrt(tau_0 / delta_time)),\n    start = list(delta_temperature_0 = -10, tau_0 = 50),\n    data = x\n  )    \n}\n\nsemi_infinite_convection <- function(x){\n  nls(\n    delta_temperature ~\n      delta_temperature_0 * (\n        erfc(sqrt(tau_0 / delta_time)) -\n        exp(Bi_0 + (Bi_0/2)^2 * delta_time / tau_0) *\n          erfc(sqrt(tau_0 / delta_time) + \n        (Bi_0/2) * sqrt(delta_time / tau_0))\n      ),\n    start = list(delta_temperature_0 = -5, tau_0 = 50, Bi_0 = 1.e6),\n    data = x\n  )\n}"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"test-modeling-on-one-dataset","chapter":"38 Temperature modeling using nested dataframes","heading":"38.4 Test modeling on one dataset","text":"","code":""},{"path":"temperature-modeling-using-nested-dataframes.html","id":"before-going-into-purrr","chapter":"38 Temperature modeling using nested dataframes","heading":"38.4.1 Before going into purrr","text":"anything, want show can something one dataset one model-function:","code":"\n# only one sensor; it is a test\ntmp_data <- delta %>% filter(id_sensor == \"a\")\n\ntmp_model <- newton_cooling(tmp_data)\n\nsummary(tmp_model)\n#> \n#> Formula: delta_temperature ~ delta_temperature_0 * (1 - exp(-delta_time/tau_0))\n#> \n#> Parameters:\n#>                     Estimate Std. Error t value Pr(>|t|)    \n#> delta_temperature_0 -15.0608     0.0526    -286   <2e-16 ***\n#> tau_0               500.0138     4.8367     103   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.327 on 325 degrees of freedom\n#> \n#> Number of iterations to convergence: 7 \n#> Achieved convergence tolerance: 4.14e-06"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"look-at-predictions","chapter":"38 Temperature modeling using nested dataframes","heading":"38.4.2 Look at predictions","text":"","code":"\n# apply prediction and make it tidy\ntmp_pred <- \n  tmp_data %>%\n  mutate(modeled = predict(tmp_model, data = .)) %>%\n  select(id_sensor, delta_time, measured = delta_temperature, modeled) %>%\n  gather(\"type\", \"delta_temperature\", measured:modeled) %>%\n  print()\n#> # A tibble: 654 x 4\n#> # Groups:   id_sensor [1]\n#>   id_sensor delta_time type     delta_temperature\n#>   <chr>          <dbl> <chr>                <dbl>\n#> 1 a                  0 measured             0    \n#> 2 a                  4 measured             0    \n#> 3 a                  8 measured            -0.06 \n#> 4 a                 12 measured            -0.06 \n#> 5 a                 17 measured            -0.211\n#> 6 a                 22 measured            -0.423\n#> # … with 648 more rows"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"plot-newton-model","chapter":"38 Temperature modeling using nested dataframes","heading":"38.4.3 Plot Newton model","text":"","code":"\ntmp_pred %>%\n  ggplot(aes(x = delta_time, y = delta_temperature, linetype = type)) +\n  geom_line() +\n  labs(title = \"Newton temperature model\", subtitle = \"One sensor: a\")"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"regular-data-frame-deltas","chapter":"38 Temperature modeling using nested dataframes","heading":"38.4.4 “Regular” data-frame (deltas)","text":"column dataframe vector - case, character vector two doubles","code":"\nprint(delta)\n#> # A tibble: 981 x 3\n#> # Groups:   id_sensor [3]\n#>   id_sensor delta_time delta_temperature\n#>   <chr>          <dbl>             <dbl>\n#> 1 a                  0             0    \n#> 2 a                  4             0    \n#> 3 a                  8            -0.06 \n#> 4 a                 12            -0.06 \n#> 5 a                 17            -0.211\n#> 6 a                 22            -0.423\n#> # … with 975 more rows"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"making-a-nested-dataframe","chapter":"38 Temperature modeling using nested dataframes","heading":"38.5 Making a nested dataframe","text":"","code":""},{"path":"temperature-modeling-using-nested-dataframes.html","id":"how-to-make-a-weird-data-frame","chapter":"38 Temperature modeling using nested dataframes","heading":"38.5.1 How to make a weird data-frame","text":"’s fun starts - column data-frame can list.use tidyr::nest() makes column data, list data-framesuse tidyr::nest() makes column data, list data-framesthis seems like stronger expression dplyr::group_by() ideathis seems like stronger expression dplyr::group_by() idea","code":"\n# nest delta_time and delta_temperature variables\ndelta_nested <- \n  delta %>%\n  nest(-id_sensor) %>%\n  print()\n#> Warning: All elements of `...` must be named.\n#> Did you want `data = c(delta_time, delta_temperature)`?\n#> # A tibble: 3 x 2\n#> # Groups:   id_sensor [3]\n#>   id_sensor data              \n#>   <chr>     <list>            \n#> 1 a         <tibble [327 × 2]>\n#> 2 b         <tibble [327 × 2]>\n#> 3 c         <tibble [327 × 2]>"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"map-dataframes-to-a-modeling-function-newton","chapter":"38 Temperature modeling using nested dataframes","heading":"38.5.2 Map dataframes to a modeling function (Newton)","text":"map() like lapply()map() like lapply()map() returns list-column (keeps weirdness)map() returns list-column (keeps weirdness)get additional list-column model.","code":"\nmodel_nested <-\n  delta_nested %>%\n  mutate(model = map(data, newton_cooling)) %>%\n  print()\n#> # A tibble: 3 x 3\n#> # Groups:   id_sensor [3]\n#>   id_sensor data               model \n#>   <chr>     <list>             <list>\n#> 1 a         <tibble [327 × 2]> <nls> \n#> 2 b         <tibble [327 × 2]> <nls> \n#> 3 c         <tibble [327 × 2]> <nls>"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"we-can-use-map2-to-make-the-predictions","chapter":"38 Temperature modeling using nested dataframes","heading":"38.5.3 We can use map2() to make the predictions","text":"map2() like mapply()map2() like mapply()designed map two colunms (model, data) function predict()designed map two colunms (model, data) function predict()Another list-column pred prediction results.","code":"\npredict_nested <-\n  model_nested %>%\n  mutate(pred = map2(model, data, predict)) %>%\n  print()\n#> # A tibble: 3 x 4\n#> # Groups:   id_sensor [3]\n#>   id_sensor data               model  pred       \n#>   <chr>     <list>             <list> <list>     \n#> 1 a         <tibble [327 × 2]> <nls>  <dbl [327]>\n#> 2 b         <tibble [327 × 2]> <nls>  <dbl [327]>\n#> 3 c         <tibble [327 × 2]> <nls>  <dbl [327]>"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"we-need-to-get-out-of-the-weirdness","chapter":"38 Temperature modeling using nested dataframes","heading":"38.5.4 We need to get out of the weirdness","text":"use unnest() get back regular data-frame","code":"\npredict_unnested <- \n  predict_nested %>%\n  unnest(data, pred) %>% \n  print()\n#> Warning: unnest() has a new interface. See ?unnest for details.\n#> Try `df %>% unnest(c(data, pred))`, with `mutate()` if needed\n#> # A tibble: 981 x 5\n#> # Groups:   id_sensor [3]\n#>   id_sensor delta_time delta_temperature model    pred\n#>   <chr>          <dbl>             <dbl> <list>  <dbl>\n#> 1 a                  0             0     <nls>   0    \n#> 2 a                  4             0     <nls>  -0.120\n#> 3 a                  8            -0.06  <nls>  -0.239\n#> 4 a                 12            -0.06  <nls>  -0.357\n#> 5 a                 17            -0.211 <nls>  -0.503\n#> 6 a                 22            -0.423 <nls>  -0.648\n#> # … with 975 more rows"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"we-can-wrangle-the-predictions","chapter":"38 Temperature modeling using nested dataframes","heading":"38.5.5 We can wrangle the predictions","text":"get form makes easier plot","code":"\npredict_tall <- \n  predict_unnested %>%\n  rename(modeled = pred, measured = delta_temperature) %>%\n  gather(\"type\", \"delta_temperature\", modeled, measured) %>%\n  print()\n#> # A tibble: 1,962 x 5\n#> # Groups:   id_sensor [3]\n#>   id_sensor delta_time model  type    delta_temperature\n#>   <chr>          <dbl> <list> <chr>               <dbl>\n#> 1 a                  0 <nls>  modeled             0    \n#> 2 a                  4 <nls>  modeled            -0.120\n#> 3 a                  8 <nls>  modeled            -0.239\n#> 4 a                 12 <nls>  modeled            -0.357\n#> 5 a                 17 <nls>  modeled            -0.503\n#> 6 a                 22 <nls>  modeled            -0.648\n#> # … with 1,956 more rows"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"we-can-visualize-the-predictions","chapter":"38 Temperature modeling using nested dataframes","heading":"38.5.6 We can visualize the predictions","text":"","code":"\npredict_tall %>%\n  ggplot(aes(x = delta_time, y = delta_temperature)) +\n  geom_line(aes(color = id_sensor, linetype = type)) +\n  labs(title = \"Newton temperature modeling\", \n       subtitle = \"Three sensors: a, b, c\")"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"apply-multiple-models-on-a-nested-structure","chapter":"38 Temperature modeling using nested dataframes","heading":"38.6 Apply multiple models on a nested structure","text":"","code":""},{"path":"temperature-modeling-using-nested-dataframes.html","id":"step-1-selection-of-models","chapter":"38 Temperature modeling using nested dataframes","heading":"38.6.1 Step 1: Selection of models","text":"Make list functions model:","code":"\nlist_model <-\n  list(\n    newton_cooling = newton_cooling,\n    semi_infinite_simple = semi_infinite_simple,\n    semi_infinite_convection = semi_infinite_convection\n  )"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"step-2-write-a-function-to-define-the-inner-loop","chapter":"38 Temperature modeling using nested dataframes","heading":"38.6.2 Step 2: write a function to define the “inner” loop","text":"given model-function given (weird) data-frame, return modified version data-frame column model, model-function applied element data-frame’s data column (list data-frames)given model-function given (weird) data-frame, return modified version data-frame column model, model-function applied element data-frame’s data column (list data-frames)purrr functions safely() possibly() interesting. think useful outside purrr friendlier way error-handling.purrr functions safely() possibly() interesting. think useful outside purrr friendlier way error-handling.","code":"\n# add additional variable with the model name\n\nfn_model <- function(.model, df) {\n  # one parameter for the model in the list, the second for the data\n  # safer to avoid non-standard evaluation\n  # df %>% mutate(model = map(data, .model)) \n  \n  df$model <- map(df$data, possibly(.model, NULL))\n  df\n}"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"step-3-use-map_df-to-define-the-outer-loop","chapter":"38 Temperature modeling using nested dataframes","heading":"38.6.3 Step 3: Use map_df() to define the “outer” loop","text":"element list model-functions, run inner-loop function, row-bind results data-framefor element list model-functions, run inner-loop function, row-bind results data-framewe want discard rows model failedwe want discard rows model failedwe also want investigate failed, ’s different talkwe also want investigate failed, ’s different talk","code":"\n# this dataframe will be the second input of fn_model\ndelta_nested %>% \n  print()\n#> # A tibble: 3 x 2\n#> # Groups:   id_sensor [3]\n#>   id_sensor data              \n#>   <chr>     <list>            \n#> 1 a         <tibble [327 × 2]>\n#> 2 b         <tibble [327 × 2]>\n#> 3 c         <tibble [327 × 2]>\n# fn_model is receiving two inputs: one from list_model and from delta_nested\nmodel_nested_new <-\n  list_model %>%\n  map_df(fn_model, delta_nested, .id = \"id_model\") %>%\n  print()\n#> # A tibble: 9 x 4\n#> # Groups:   id_sensor [3]\n#>   id_model             id_sensor data               model \n#>   <chr>                <chr>     <list>             <list>\n#> 1 newton_cooling       a         <tibble [327 × 2]> <nls> \n#> 2 newton_cooling       b         <tibble [327 × 2]> <nls> \n#> 3 newton_cooling       c         <tibble [327 × 2]> <nls> \n#> 4 semi_infinite_simple a         <tibble [327 × 2]> <nls> \n#> 5 semi_infinite_simple b         <tibble [327 × 2]> <nls> \n#> 6 semi_infinite_simple c         <tibble [327 × 2]> <nls> \n#> # … with 3 more rows"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"step-4-use-map-to-identify-the-null-models","chapter":"38 Temperature modeling using nested dataframes","heading":"38.6.4 Step 4: Use map() to identify the null models","text":"using map(model, .null) returns list columnto use filter(), escape weirdness","code":"\nmodel_nested_new <-\n  list_model %>%\n  map_df(fn_model, delta_nested, .id = \"id_model\") %>%\n  mutate(is_null = map(model, is.null)) %>%\n  print()\n#> # A tibble: 9 x 5\n#> # Groups:   id_sensor [3]\n#>   id_model             id_sensor data               model  is_null  \n#>   <chr>                <chr>     <list>             <list> <list>   \n#> 1 newton_cooling       a         <tibble [327 × 2]> <nls>  <lgl [1]>\n#> 2 newton_cooling       b         <tibble [327 × 2]> <nls>  <lgl [1]>\n#> 3 newton_cooling       c         <tibble [327 × 2]> <nls>  <lgl [1]>\n#> 4 semi_infinite_simple a         <tibble [327 × 2]> <nls>  <lgl [1]>\n#> 5 semi_infinite_simple b         <tibble [327 × 2]> <nls>  <lgl [1]>\n#> 6 semi_infinite_simple c         <tibble [327 × 2]> <nls>  <lgl [1]>\n#> # … with 3 more rows"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"step-5-map_lgl-to-identify-nulls-and-get-out-of-the-weirdness","chapter":"38 Temperature modeling using nested dataframes","heading":"38.6.5 Step 5: map_lgl() to identify nulls and get out of the weirdness","text":"using map_lgl(model, .null) returns vector column","code":"\nmodel_nested_new <-\n  list_model %>%\n  map_df(fn_model, delta_nested, .id = \"id_model\") %>%\n  mutate(is_null = map_lgl(model, is.null)) %>%\n  print()\n#> # A tibble: 9 x 5\n#> # Groups:   id_sensor [3]\n#>   id_model             id_sensor data               model  is_null\n#>   <chr>                <chr>     <list>             <list> <lgl>  \n#> 1 newton_cooling       a         <tibble [327 × 2]> <nls>  FALSE  \n#> 2 newton_cooling       b         <tibble [327 × 2]> <nls>  FALSE  \n#> 3 newton_cooling       c         <tibble [327 × 2]> <nls>  FALSE  \n#> 4 semi_infinite_simple a         <tibble [327 × 2]> <nls>  FALSE  \n#> 5 semi_infinite_simple b         <tibble [327 × 2]> <nls>  FALSE  \n#> 6 semi_infinite_simple c         <tibble [327 × 2]> <nls>  FALSE  \n#> # … with 3 more rows"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"step-6-filter-nulls-and-select-variables-to-clean-up","chapter":"38 Temperature modeling using nested dataframes","heading":"38.6.6 Step 6: filter() nulls and select() variables to clean up","text":"","code":"\nmodel_nested_new <-\n  list_model %>%\n  map_df(fn_model, delta_nested, .id = \"id_model\") %>%\n  mutate(is_null = map_lgl(model, is.null)) %>%\n  filter(!is_null) %>%\n  select(-is_null) %>%\n  print()\n#> # A tibble: 6 x 4\n#> # Groups:   id_sensor [3]\n#>   id_model             id_sensor data               model \n#>   <chr>                <chr>     <list>             <list>\n#> 1 newton_cooling       a         <tibble [327 × 2]> <nls> \n#> 2 newton_cooling       b         <tibble [327 × 2]> <nls> \n#> 3 newton_cooling       c         <tibble [327 × 2]> <nls> \n#> 4 semi_infinite_simple a         <tibble [327 × 2]> <nls> \n#> 5 semi_infinite_simple b         <tibble [327 × 2]> <nls> \n#> 6 semi_infinite_simple c         <tibble [327 × 2]> <nls>"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"step-7-calculate-predictions-on-nested-dataframe","chapter":"38 Temperature modeling using nested dataframes","heading":"38.6.7 Step 7: Calculate predictions on nested dataframe","text":"","code":"\npredict_nested <- \n  model_nested_new %>%\n  mutate(pred = map2(model, data, predict)) %>%\n  print()\n#> # A tibble: 6 x 5\n#> # Groups:   id_sensor [3]\n#>   id_model             id_sensor data               model  pred       \n#>   <chr>                <chr>     <list>             <list> <list>     \n#> 1 newton_cooling       a         <tibble [327 × 2]> <nls>  <dbl [327]>\n#> 2 newton_cooling       b         <tibble [327 × 2]> <nls>  <dbl [327]>\n#> 3 newton_cooling       c         <tibble [327 × 2]> <nls>  <dbl [327]>\n#> 4 semi_infinite_simple a         <tibble [327 × 2]> <nls>  <dbl [327]>\n#> 5 semi_infinite_simple b         <tibble [327 × 2]> <nls>  <dbl [327]>\n#> 6 semi_infinite_simple c         <tibble [327 × 2]> <nls>  <dbl [327]>"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"unnest-make-it-tall-and-tidy","chapter":"38 Temperature modeling using nested dataframes","heading":"38.6.8 unnest(), make it tall and tidy","text":"","code":"\npredict_tall <-\n  predict_nested %>%\n  unnest(data, pred) %>% \n  rename(modeled = pred, measured = delta_temperature) %>%\n  gather(\"type\", \"delta_temperature\", modeled, measured) %>%\n  print()\n#> Warning: unnest() has a new interface. See ?unnest for details.\n#> Try `df %>% unnest(c(data, pred))`, with `mutate()` if needed\n#> # A tibble: 3,924 x 6\n#> # Groups:   id_sensor [3]\n#>   id_model       id_sensor delta_time model  type    delta_temperature\n#>   <chr>          <chr>          <dbl> <list> <chr>               <dbl>\n#> 1 newton_cooling a                  0 <nls>  modeled             0    \n#> 2 newton_cooling a                  4 <nls>  modeled            -0.120\n#> 3 newton_cooling a                  8 <nls>  modeled            -0.239\n#> 4 newton_cooling a                 12 <nls>  modeled            -0.357\n#> 5 newton_cooling a                 17 <nls>  modeled            -0.503\n#> 6 newton_cooling a                 22 <nls>  modeled            -0.648\n#> # … with 3,918 more rows"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"visualize-the-predictions","chapter":"38 Temperature modeling using nested dataframes","heading":"38.6.9 Visualize the predictions","text":"","code":"\npredict_tall %>%\n  ggplot(aes(x = delta_time, y = delta_temperature)) +\n  geom_line(aes(color = id_sensor, linetype = type)) +\n  facet_grid(id_model ~ .) +\n  labs(title = \"Newton and Semi-infinite temperature modeling\", \n       subtitle = \"Three sensors: a, b, c\")"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"lets-get-the-residuals","chapter":"38 Temperature modeling using nested dataframes","heading":"38.6.10 Let’s get the residuals","text":"","code":"\nresid <-\n  model_nested_new %>%\n  mutate(resid = map(model, resid)) %>%\n  unnest(data, resid) %>%\n  print()\n#> Warning: unnest() has a new interface. See ?unnest for details.\n#> Try `df %>% unnest(c(data, resid))`, with `mutate()` if needed\n#> # A tibble: 1,962 x 6\n#> # Groups:   id_sensor [3]\n#>   id_model       id_sensor delta_time delta_temperature model  resid\n#>   <chr>          <chr>          <dbl>             <dbl> <list> <dbl>\n#> 1 newton_cooling a                  0             0     <nls>  0    \n#> 2 newton_cooling a                  4             0     <nls>  0.120\n#> 3 newton_cooling a                  8            -0.06  <nls>  0.179\n#> 4 newton_cooling a                 12            -0.06  <nls>  0.297\n#> 5 newton_cooling a                 17            -0.211 <nls>  0.292\n#> 6 newton_cooling a                 22            -0.423 <nls>  0.225\n#> # … with 1,956 more rows"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"and-visualize-them","chapter":"38 Temperature modeling using nested dataframes","heading":"38.6.11 And visualize them","text":"","code":"\nresid %>%\n  ggplot(aes(x = delta_time, y = resid)) +\n  geom_line(aes(color = id_sensor)) +\n  facet_grid(id_model ~ .) +\n  labs(title = \"Residuals for Newton and Semi-infinite models\")"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"using-broom-package-to-look-at-model-statistics","chapter":"38 Temperature modeling using nested dataframes","heading":"38.7 Using broom package to look at model-statistics","text":"use previous defined dataframe model data:tidy() function extracts statistics model.","code":"\nmodel_nested_new %>% \n  print()\n#> # A tibble: 6 x 4\n#> # Groups:   id_sensor [3]\n#>   id_model             id_sensor data               model \n#>   <chr>                <chr>     <list>             <list>\n#> 1 newton_cooling       a         <tibble [327 × 2]> <nls> \n#> 2 newton_cooling       b         <tibble [327 × 2]> <nls> \n#> 3 newton_cooling       c         <tibble [327 × 2]> <nls> \n#> 4 semi_infinite_simple a         <tibble [327 × 2]> <nls> \n#> 5 semi_infinite_simple b         <tibble [327 × 2]> <nls> \n#> 6 semi_infinite_simple c         <tibble [327 × 2]> <nls>\n# apply over model_nested_new but only three variables\nmodel_parameters <- \n  model_nested_new %>%\n  select(id_model, id_sensor, model) %>%\n  mutate(tidy = map(model, tidy)) %>%\n  select(-model) %>%\n  unnest() %>%\n  print()\n#> Warning: `cols` is now required.\n#> Please use `cols = c(tidy)`\n#> # A tibble: 12 x 7\n#> # Groups:   id_sensor [3]\n#>   id_model      id_sensor term            estimate std.error statistic   p.value\n#>   <chr>         <chr>     <chr>              <dbl>     <dbl>     <dbl>     <dbl>\n#> 1 newton_cooli… a         delta_temperat…   -15.1     0.0526    -286.  0.       \n#> 2 newton_cooli… a         tau_0             500.      4.84       103.  1.07e-250\n#> 3 newton_cooli… b         delta_temperat…    -7.59    0.0676    -112.  6.38e-262\n#> 4 newton_cooli… b         tau_0            1041.     16.2         64.2 9.05e-187\n#> 5 newton_cooli… c         delta_temperat…    -9.87    0.704      -14.0 3.16e- 35\n#> 6 newton_cooli… c         tau_0            3525.    299.          11.8 5.61e- 27\n#> # … with 6 more rows"},{"path":"temperature-modeling-using-nested-dataframes.html","id":"get-a-sense-of-the-coefficients","chapter":"38 Temperature modeling using nested dataframes","heading":"38.7.1 Get a sense of the coefficients","text":"","code":"\nmodel_summary <-\n  model_parameters %>%\n  select(id_model, id_sensor, term, estimate) %>%\n  spread(key = \"term\", value = \"estimate\") %>%\n  print()\n#> # A tibble: 6 x 4\n#> # Groups:   id_sensor [3]\n#>   id_model             id_sensor delta_temperature_0 tau_0\n#>   <chr>                <chr>                   <dbl> <dbl>\n#> 1 newton_cooling       a                      -15.1   500.\n#> 2 newton_cooling       b                       -7.59 1041.\n#> 3 newton_cooling       c                       -9.87 3525.\n#> 4 semi_infinite_simple a                      -21.5   139.\n#> 5 semi_infinite_simple b                      -10.6   287.\n#> 6 semi_infinite_simple c                       -8.04  500."},{"path":"temperature-modeling-using-nested-dataframes.html","id":"summary-1","chapter":"38 Temperature modeling using nested dataframes","heading":"38.7.2 Summary","text":"just smalll part purrrthis just smalll part purrrthere seem parallels tidyr::nest()/purrr::map() dplyr::group_by()/dplyr::()\nmind, purrr framework understandable\nupdate tweet Hadley\nseem parallels tidyr::nest()/purrr::map() dplyr::group_by()/dplyr::()mind, purrr framework understandableupdate tweet HadleyReferences Hadley:purrr 0.1.0 announcementpurrr 0.2.0 announcementchapter Garrett Grolemund Hadley’s forthcoming book","code":""},{"path":"credit-scoring-with-neuralnet.html","id":"credit-scoring-with-neuralnet","chapter":"39 Credit Scoring with neuralnet","heading":"39 Credit Scoring with neuralnet","text":"","code":""},{"path":"credit-scoring-with-neuralnet.html","id":"introduction-21","chapter":"39 Credit Scoring with neuralnet","heading":"39.1 Introduction","text":"Source: https://www.r-bloggers.com/using-neural-networks--credit-scoring--simple-example/","code":""},{"path":"credit-scoring-with-neuralnet.html","id":"motivation-1","chapter":"39 Credit Scoring with neuralnet","heading":"39.2 Motivation","text":"Credit scoring practice analysing persons background credit application order assess creditworthiness person. One can take numerous approaches analysing creditworthiness. end basically comes first selecting correct independent variables (e.g. income, age, gender) lead given level creditworthiness.words: creditworthiness = f(income, age, gender, …).creditscoring system can represented linear regression, logistic regression, machine learning combination . Neural networks situated domain machine learning. following strongly simplified example. actual procedure building credit scoring system much complex resulting model likely consist solely even neural network.’re unsure neural network exactly , find good place start.example R package neuralnet used, -depth view exact workings package see neuralnet: Training Neural Networks F. Günther S. Fritsch.","code":""},{"path":"credit-scoring-with-neuralnet.html","id":"load-the-data","chapter":"39 Credit Scoring with neuralnet","heading":"39.3 load the data","text":"Dataset downloaded: https://gist.github.com/Bart6114/8675941#file-creditset-csv","code":"\nset.seed(1234567890)\n\nlibrary(neuralnet)\n\ndataset <- read.csv(file.path(data_raw_dir, \"creditset.csv\"))\nhead(dataset)\n#>   clientid income  age   loan      LTI default10yr\n#> 1        1  66156 59.0 8106.5 0.122537           0\n#> 2        2  34415 48.1 6564.7 0.190752           0\n#> 3        3  57317 63.1 8021.0 0.139940           0\n#> 4        4  42710 45.8 6103.6 0.142911           0\n#> 5        5  66953 18.6 8770.1 0.130989           1\n#> 6        6  24904 57.5   15.5 0.000622           0\nnames(dataset)\n#> [1] \"clientid\"    \"income\"      \"age\"         \"loan\"        \"LTI\"        \n#> [6] \"default10yr\"\nsummary(dataset)\n#>     clientid        income           age            loan            LTI        \n#>  Min.   :   1   Min.   :20014   Min.   :18.1   Min.   :    1   Min.   :0.0000  \n#>  1st Qu.: 501   1st Qu.:32796   1st Qu.:29.1   1st Qu.: 1940   1st Qu.:0.0479  \n#>  Median :1000   Median :45789   Median :41.4   Median : 3975   Median :0.0994  \n#>  Mean   :1000   Mean   :45332   Mean   :40.9   Mean   : 4444   Mean   :0.0984  \n#>  3rd Qu.:1500   3rd Qu.:57791   3rd Qu.:52.6   3rd Qu.: 6432   3rd Qu.:0.1476  \n#>  Max.   :2000   Max.   :69996   Max.   :64.0   Max.   :13766   Max.   :0.1999  \n#>   default10yr   \n#>  Min.   :0.000  \n#>  1st Qu.:0.000  \n#>  Median :0.000  \n#>  Mean   :0.142  \n#>  3rd Qu.:0.000  \n#>  Max.   :1.000\n# distribution of defaults\ntable(dataset$default10yr)\n#> \n#>    0    1 \n#> 1717  283\nmin(dataset$LTI)\n#> [1] 4.91e-05\nplot(jitter(dataset$default10yr, 1) ~ jitter(dataset$LTI, 2))\n# convert LTI continuous variable to categorical\ndataset$LTIrng <- cut(dataset$LTI, breaks = 10)\nunique(dataset$LTIrng)\n#>  [1] (0.12,0.14]      (0.18,0.2]       (0.14,0.16]      (-0.000151,0.02]\n#>  [5] (0.1,0.12]       (0.04,0.06]      (0.06,0.08]      (0.08,0.1]      \n#>  [9] (0.16,0.18]      (0.02,0.04]     \n#> 10 Levels: (-0.000151,0.02] (0.02,0.04] (0.04,0.06] (0.06,0.08] ... (0.18,0.2]\nplot(dataset$LTIrng, dataset$default10yr)\n# what age and LTI is more likely to default\nlibrary(ggplot2)\n\nggplot(dataset, aes(x = age, y = LTI, col = default10yr)) +\n    geom_point()\n# what age and loan size is more likely to default\nlibrary(ggplot2)\n\nggplot(dataset, aes(x = age, y = loan, col = default10yr)) +\n    geom_point()"},{"path":"credit-scoring-with-neuralnet.html","id":"objective","chapter":"39 Credit Scoring with neuralnet","heading":"39.4 Objective","text":"dataset contains information different clients received loan least 10 years ago. variables income (yearly), age, loan (size euros) LTI (loan yearly income ratio) available. goal devise model predicts, based input variables LTI age, whether default occur within 10 years.","code":""},{"path":"credit-scoring-with-neuralnet.html","id":"steps","chapter":"39 Credit Scoring with neuralnet","heading":"39.5 Steps","text":"dataset split subset used training neural network another set used testing. ordering dataset completely random, extract random rows can just take first x rows.","code":"\n## extract a set to train the NN\ntrainset <- dataset[1:800, ]\n\n## select the test set\ntestset <- dataset[801:2000, ]"},{"path":"credit-scoring-with-neuralnet.html","id":"build-the-neural-network","chapter":"39 Credit Scoring with neuralnet","heading":"39.5.1 Build the neural network","text":"Now ’ll build neural network 4 hidden nodes (neural network comprised input, hidden output nodes). number nodes chosen without clear method, however rules thumb. lifesign option refers verbosity. ouput linear use threshold value 10%. neuralnet package uses resilient backpropagation weight backtracking standard algorithm.neuralnet package also possibility visualize generated model show found weights.","code":"\n## build the neural network (NN)\ncreditnet <- neuralnet(default10yr ~ LTI + age, trainset, \n                       hidden = 4, \n                       lifesign = \"minimal\", \n                       linear.output = FALSE, \n                       threshold = 0.1)\n#> hidden: 4    thresh: 0.1    rep: 1/1    steps:   44487   error: 0.20554  time: 7.65 secs\n## plot the NN\nplot(creditnet, rep = \"best\")"},{"path":"credit-scoring-with-neuralnet.html","id":"test-the-neural-network","chapter":"39 Credit Scoring with neuralnet","heading":"39.6 Test the neural network","text":"’ve trained neural network ready test . use testset subset . compute function applied computing outputs based LTI age inputs testset.temp dataset contains columns LTI age train set. variables used input. set looks follows:Let’s look neural network produced:can round nearest integer improve readability:can see pretty close! already stated, strongly simplified example. might serve basis play around first neural network.","code":"\n## test the resulting output\ntemp_test <- subset(testset, select = c(\"LTI\", \"age\"))\n\ncreditnet.results <- compute(creditnet, temp_test)\nhead(temp_test)\n#>        LTI  age\n#> 801 0.0231 25.9\n#> 802 0.1373 40.8\n#> 803 0.1046 32.5\n#> 804 0.1599 53.2\n#> 805 0.1116 46.5\n#> 806 0.1149 47.1\nresults <- data.frame(actual = testset$default10yr, prediction = creditnet.results$net.result)\nresults[100:115, ]\n#>     actual prediction\n#> 900      0   7.29e-32\n#> 901      0   8.17e-11\n#> 902      0   4.33e-45\n#> 903      1   1.00e+00\n#> 904      0   8.06e-04\n#> 905      0   3.54e-40\n#> 906      0   1.48e-24\n#> 907      1   1.00e+00\n#> 908      0   1.11e-02\n#> 909      0   8.05e-44\n#> 910      0   6.72e-07\n#> 911      1   1.00e+00\n#> 912      0   9.97e-59\n#> 913      1   1.00e+00\n#> 914      0   3.39e-37\n#> 915      0   1.18e-07\nresults$prediction <- round(results$prediction)\nresults[100:115, ]\n#>     actual prediction\n#> 900      0          0\n#> 901      0          0\n#> 902      0          0\n#> 903      1          1\n#> 904      0          0\n#> 905      0          0\n#> 906      0          0\n#> 907      1          1\n#> 908      0          0\n#> 909      0          0\n#> 910      0          0\n#> 911      1          1\n#> 912      0          0\n#> 913      1          1\n#> 914      0          0\n#> 915      0          0\n# how many predictions were wrong\nindices <- which(results$actual != results$prediction)\nindices\n#> [1]  330 1008\n# what are the predictions that failed\nresults[indices,]\n#>      actual prediction\n#> 1130      0          1\n#> 1808      1          0"},{"path":"wine-classification-with-neuralnet.html","id":"wine-classification-with-neuralnet","chapter":"40 Wine classification with neuralnet","heading":"40 Wine classification with neuralnet","text":"Source: https://www.r-bloggers.com/multilabel-classification--neuralnet-package/neuralnet package perhaps best option R using neural networks. ask , starters recognize typical formula y~., support factors, provide lot models standard MLP, great competitors nnet package seems better integrated R can used caret package, MXnet package high level deep learning library provides wide variety neural networks.still, think value ease use neuralnet package, especially beginner, therefore ’ll using .’m going using neuralnet , curiously enough, nnet package. Let’s load :","code":"\n# load libs\nrequire(neuralnet)\n#> Loading required package: neuralnet\nrequire(nnet)\n#> Loading required package: nnet\nrequire(ggplot2)\n#> Loading required package: ggplot2\nset.seed(10)"},{"path":"wine-classification-with-neuralnet.html","id":"the-dataset","chapter":"40 Wine classification with neuralnet","heading":"40.1 The dataset","text":"looked UCI Machine Learning Repository1 found wine dataset.dataset contains results chemical analysis 3 different kind wines. target variable label wine factor 3 (unordered) levels. predictors continuous represent 13 variables obtained result chemical measurements.","code":"\n# get the data file from the package location\nwine_dataset_path <- file.path(data_raw_dir, \"wine.data\")\nwine_dataset_path\n#> [1] \"../data/wine.data\"\nwines <- read.csv(wine_dataset_path)\nwines\n#>     X1 X14.23 X1.71 X2.43 X15.6 X127 X2.8 X3.06 X.28 X2.29 X5.64 X1.04 X3.92\n#> 1    1   13.2  1.78  2.14  11.2  100 2.65  2.76 0.26  1.28  4.38 1.050  3.40\n#> 2    1   13.2  2.36  2.67  18.6  101 2.80  3.24 0.30  2.81  5.68 1.030  3.17\n#> 3    1   14.4  1.95  2.50  16.8  113 3.85  3.49 0.24  2.18  7.80 0.860  3.45\n#> 4    1   13.2  2.59  2.87  21.0  118 2.80  2.69 0.39  1.82  4.32 1.040  2.93\n#> 5    1   14.2  1.76  2.45  15.2  112 3.27  3.39 0.34  1.97  6.75 1.050  2.85\n#> 6    1   14.4  1.87  2.45  14.6   96 2.50  2.52 0.30  1.98  5.25 1.020  3.58\n#> 7    1   14.1  2.15  2.61  17.6  121 2.60  2.51 0.31  1.25  5.05 1.060  3.58\n#> 8    1   14.8  1.64  2.17  14.0   97 2.80  2.98 0.29  1.98  5.20 1.080  2.85\n#> 9    1   13.9  1.35  2.27  16.0   98 2.98  3.15 0.22  1.85  7.22 1.010  3.55\n#> 10   1   14.1  2.16  2.30  18.0  105 2.95  3.32 0.22  2.38  5.75 1.250  3.17\n#> 11   1   14.1  1.48  2.32  16.8   95 2.20  2.43 0.26  1.57  5.00 1.170  2.82\n#> 12   1   13.8  1.73  2.41  16.0   89 2.60  2.76 0.29  1.81  5.60 1.150  2.90\n#> 13   1   14.8  1.73  2.39  11.4   91 3.10  3.69 0.43  2.81  5.40 1.250  2.73\n#> 14   1   14.4  1.87  2.38  12.0  102 3.30  3.64 0.29  2.96  7.50 1.200  3.00\n#> 15   1   13.6  1.81  2.70  17.2  112 2.85  2.91 0.30  1.46  7.30 1.280  2.88\n#> 16   1   14.3  1.92  2.72  20.0  120 2.80  3.14 0.33  1.97  6.20 1.070  2.65\n#> 17   1   13.8  1.57  2.62  20.0  115 2.95  3.40 0.40  1.72  6.60 1.130  2.57\n#> 18   1   14.2  1.59  2.48  16.5  108 3.30  3.93 0.32  1.86  8.70 1.230  2.82\n#> 19   1   13.6  3.10  2.56  15.2  116 2.70  3.03 0.17  1.66  5.10 0.960  3.36\n#> 20   1   14.1  1.63  2.28  16.0  126 3.00  3.17 0.24  2.10  5.65 1.090  3.71\n#> 21   1   12.9  3.80  2.65  18.6  102 2.41  2.41 0.25  1.98  4.50 1.030  3.52\n#> 22   1   13.7  1.86  2.36  16.6  101 2.61  2.88 0.27  1.69  3.80 1.110  4.00\n#> 23   1   12.8  1.60  2.52  17.8   95 2.48  2.37 0.26  1.46  3.93 1.090  3.63\n#> 24   1   13.5  1.81  2.61  20.0   96 2.53  2.61 0.28  1.66  3.52 1.120  3.82\n#> 25   1   13.1  2.05  3.22  25.0  124 2.63  2.68 0.47  1.92  3.58 1.130  3.20\n#> 26   1   13.4  1.77  2.62  16.1   93 2.85  2.94 0.34  1.45  4.80 0.920  3.22\n#> 27   1   13.3  1.72  2.14  17.0   94 2.40  2.19 0.27  1.35  3.95 1.020  2.77\n#> 28   1   13.9  1.90  2.80  19.4  107 2.95  2.97 0.37  1.76  4.50 1.250  3.40\n#> 29   1   14.0  1.68  2.21  16.0   96 2.65  2.33 0.26  1.98  4.70 1.040  3.59\n#> 30   1   13.7  1.50  2.70  22.5  101 3.00  3.25 0.29  2.38  5.70 1.190  2.71\n#> 31   1   13.6  1.66  2.36  19.1  106 2.86  3.19 0.22  1.95  6.90 1.090  2.88\n#> 32   1   13.7  1.83  2.36  17.2  104 2.42  2.69 0.42  1.97  3.84 1.230  2.87\n#> 33   1   13.8  1.53  2.70  19.5  132 2.95  2.74 0.50  1.35  5.40 1.250  3.00\n#> 34   1   13.5  1.80  2.65  19.0  110 2.35  2.53 0.29  1.54  4.20 1.100  2.87\n#> 35   1   13.5  1.81  2.41  20.5  100 2.70  2.98 0.26  1.86  5.10 1.040  3.47\n#> 36   1   13.3  1.64  2.84  15.5  110 2.60  2.68 0.34  1.36  4.60 1.090  2.78\n#> 37   1   13.1  1.65  2.55  18.0   98 2.45  2.43 0.29  1.44  4.25 1.120  2.51\n#> 38   1   13.1  1.50  2.10  15.5   98 2.40  2.64 0.28  1.37  3.70 1.180  2.69\n#> 39   1   14.2  3.99  2.51  13.2  128 3.00  3.04 0.20  2.08  5.10 0.890  3.53\n#> 40   1   13.6  1.71  2.31  16.2  117 3.15  3.29 0.34  2.34  6.13 0.950  3.38\n#> 41   1   13.4  3.84  2.12  18.8   90 2.45  2.68 0.27  1.48  4.28 0.910  3.00\n#> 42   1   13.9  1.89  2.59  15.0  101 3.25  3.56 0.17  1.70  5.43 0.880  3.56\n#> 43   1   13.2  3.98  2.29  17.5  103 2.64  2.63 0.32  1.66  4.36 0.820  3.00\n#> 44   1   13.1  1.77  2.10  17.0  107 3.00  3.00 0.28  2.03  5.04 0.880  3.35\n#> 45   1   14.2  4.04  2.44  18.9  111 2.85  2.65 0.30  1.25  5.24 0.870  3.33\n#> 46   1   14.4  3.59  2.28  16.0  102 3.25  3.17 0.27  2.19  4.90 1.040  3.44\n#> 47   1   13.9  1.68  2.12  16.0  101 3.10  3.39 0.21  2.14  6.10 0.910  3.33\n#> 48   1   14.1  2.02  2.40  18.8  103 2.75  2.92 0.32  2.38  6.20 1.070  2.75\n#> 49   1   13.9  1.73  2.27  17.4  108 2.88  3.54 0.32  2.08  8.90 1.120  3.10\n#> 50   1   13.1  1.73  2.04  12.4   92 2.72  3.27 0.17  2.91  7.20 1.120  2.91\n#> 51   1   13.8  1.65  2.60  17.2   94 2.45  2.99 0.22  2.29  5.60 1.240  3.37\n#> 52   1   13.8  1.75  2.42  14.0  111 3.88  3.74 0.32  1.87  7.05 1.010  3.26\n#> 53   1   13.8  1.90  2.68  17.1  115 3.00  2.79 0.39  1.68  6.30 1.130  2.93\n#> 54   1   13.7  1.67  2.25  16.4  118 2.60  2.90 0.21  1.62  5.85 0.920  3.20\n#> 55   1   13.6  1.73  2.46  20.5  116 2.96  2.78 0.20  2.45  6.25 0.980  3.03\n#> 56   1   14.2  1.70  2.30  16.3  118 3.20  3.00 0.26  2.03  6.38 0.940  3.31\n#> 57   1   13.3  1.97  2.68  16.8  102 3.00  3.23 0.31  1.66  6.00 1.070  2.84\n#> 58   1   13.7  1.43  2.50  16.7  108 3.40  3.67 0.19  2.04  6.80 0.890  2.87\n#> 59   2   12.4  0.94  1.36  10.6   88 1.98  0.57 0.28  0.42  1.95 1.050  1.82\n#> 60   2   12.3  1.10  2.28  16.0  101 2.05  1.09 0.63  0.41  3.27 1.250  1.67\n#> 61   2   12.6  1.36  2.02  16.8  100 2.02  1.41 0.53  0.62  5.75 0.980  1.59\n#> 62   2   13.7  1.25  1.92  18.0   94 2.10  1.79 0.32  0.73  3.80 1.230  2.46\n#> 63   2   12.4  1.13  2.16  19.0   87 3.50  3.10 0.19  1.87  4.45 1.220  2.87\n#> 64   2   12.2  1.45  2.53  19.0  104 1.89  1.75 0.45  1.03  2.95 1.450  2.23\n#> 65   2   12.4  1.21  2.56  18.1   98 2.42  2.65 0.37  2.08  4.60 1.190  2.30\n#> 66   2   13.1  1.01  1.70  15.0   78 2.98  3.18 0.26  2.28  5.30 1.120  3.18\n#> 67   2   12.4  1.17  1.92  19.6   78 2.11  2.00 0.27  1.04  4.68 1.120  3.48\n#> 68   2   13.3  0.94  2.36  17.0  110 2.53  1.30 0.55  0.42  3.17 1.020  1.93\n#> 69   2   12.2  1.19  1.75  16.8  151 1.85  1.28 0.14  2.50  2.85 1.280  3.07\n#> 70   2   12.3  1.61  2.21  20.4  103 1.10  1.02 0.37  1.46  3.05 0.906  1.82\n#> 71   2   13.9  1.51  2.67  25.0   86 2.95  2.86 0.21  1.87  3.38 1.360  3.16\n#> 72   2   13.5  1.66  2.24  24.0   87 1.88  1.84 0.27  1.03  3.74 0.980  2.78\n#> 73   2   13.0  1.67  2.60  30.0  139 3.30  2.89 0.21  1.96  3.35 1.310  3.50\n#> 74   2   12.0  1.09  2.30  21.0  101 3.38  2.14 0.13  1.65  3.21 0.990  3.13\n#> 75   2   11.7  1.88  1.92  16.0   97 1.61  1.57 0.34  1.15  3.80 1.230  2.14\n#> 76   2   13.0  0.90  1.71  16.0   86 1.95  2.03 0.24  1.46  4.60 1.190  2.48\n#> 77   2   11.8  2.89  2.23  18.0  112 1.72  1.32 0.43  0.95  2.65 0.960  2.52\n#> 78   2   12.3  0.99  1.95  14.8  136 1.90  1.85 0.35  2.76  3.40 1.060  2.31\n#> 79   2   12.7  3.87  2.40  23.0  101 2.83  2.55 0.43  1.95  2.57 1.190  3.13\n#> 80   2   12.0  0.92  2.00  19.0   86 2.42  2.26 0.30  1.43  2.50 1.380  3.12\n#> 81   2   12.7  1.81  2.20  18.8   86 2.20  2.53 0.26  1.77  3.90 1.160  3.14\n#> 82   2   12.1  1.13  2.51  24.0   78 2.00  1.58 0.40  1.40  2.20 1.310  2.72\n#> 83   2   13.1  3.86  2.32  22.5   85 1.65  1.59 0.61  1.62  4.80 0.840  2.01\n#> 84   2   11.8  0.89  2.58  18.0   94 2.20  2.21 0.22  2.35  3.05 0.790  3.08\n#> 85   2   12.7  0.98  2.24  18.0   99 2.20  1.94 0.30  1.46  2.62 1.230  3.16\n#> 86   2   12.2  1.61  2.31  22.8   90 1.78  1.69 0.43  1.56  2.45 1.330  2.26\n#> 87   2   11.7  1.67  2.62  26.0   88 1.92  1.61 0.40  1.34  2.60 1.360  3.21\n#> 88   2   11.6  2.06  2.46  21.6   84 1.95  1.69 0.48  1.35  2.80 1.000  2.75\n#> 89   2   12.1  1.33  2.30  23.6   70 2.20  1.59 0.42  1.38  1.74 1.070  3.21\n#> 90   2   12.1  1.83  2.32  18.5   81 1.60  1.50 0.52  1.64  2.40 1.080  2.27\n#> 91   2   12.0  1.51  2.42  22.0   86 1.45  1.25 0.50  1.63  3.60 1.050  2.65\n#> 92   2   12.7  1.53  2.26  20.7   80 1.38  1.46 0.58  1.62  3.05 0.960  2.06\n#> 93   2   12.3  2.83  2.22  18.0   88 2.45  2.25 0.25  1.99  2.15 1.150  3.30\n#> 94   2   11.6  1.99  2.28  18.0   98 3.02  2.26 0.17  1.35  3.25 1.160  2.96\n#> 95   2   12.5  1.52  2.20  19.0  162 2.50  2.27 0.32  3.28  2.60 1.160  2.63\n#> 96   2   11.8  2.12  2.74  21.5  134 1.60  0.99 0.14  1.56  2.50 0.950  2.26\n#> 97   2   12.3  1.41  1.98  16.0   85 2.55  2.50 0.29  1.77  2.90 1.230  2.74\n#> 98   2   12.4  1.07  2.10  18.5   88 3.52  3.75 0.24  1.95  4.50 1.040  2.77\n#> 99   2   12.3  3.17  2.21  18.0   88 2.85  2.99 0.45  2.81  2.30 1.420  2.83\n#> 100  2   12.1  2.08  1.70  17.5   97 2.23  2.17 0.26  1.40  3.30 1.270  2.96\n#> 101  2   12.6  1.34  1.90  18.5   88 1.45  1.36 0.29  1.35  2.45 1.040  2.77\n#> 102  2   12.3  2.45  2.46  21.0   98 2.56  2.11 0.34  1.31  2.80 0.800  3.38\n#> 103  2   11.8  1.72  1.88  19.5   86 2.50  1.64 0.37  1.42  2.06 0.940  2.44\n#> 104  2   12.5  1.73  1.98  20.5   85 2.20  1.92 0.32  1.48  2.94 1.040  3.57\n#> 105  2   12.4  2.55  2.27  22.0   90 1.68  1.84 0.66  1.42  2.70 0.860  3.30\n#> 106  2   12.2  1.73  2.12  19.0   80 1.65  2.03 0.37  1.63  3.40 1.000  3.17\n#> 107  2   12.7  1.75  2.28  22.5   84 1.38  1.76 0.48  1.63  3.30 0.880  2.42\n#> 108  2   12.2  1.29  1.94  19.0   92 2.36  2.04 0.39  2.08  2.70 0.860  3.02\n#> 109  2   11.6  1.35  2.70  20.0   94 2.74  2.92 0.29  2.49  2.65 0.960  3.26\n#> 110  2   11.5  3.74  1.82  19.5  107 3.18  2.58 0.24  3.58  2.90 0.750  2.81\n#> 111  2   12.5  2.43  2.17  21.0   88 2.55  2.27 0.26  1.22  2.00 0.900  2.78\n#> 112  2   11.8  2.68  2.92  20.0  103 1.75  2.03 0.60  1.05  3.80 1.230  2.50\n#> 113  2   11.4  0.74  2.50  21.0   88 2.48  2.01 0.42  1.44  3.08 1.100  2.31\n#> 114  2   12.1  1.39  2.50  22.5   84 2.56  2.29 0.43  1.04  2.90 0.930  3.19\n#> 115  2   11.0  1.51  2.20  21.5   85 2.46  2.17 0.52  2.01  1.90 1.710  2.87\n#> 116  2   11.8  1.47  1.99  20.8   86 1.98  1.60 0.30  1.53  1.95 0.950  3.33\n#> 117  2   12.4  1.61  2.19  22.5  108 2.00  2.09 0.34  1.61  2.06 1.060  2.96\n#> 118  2   12.8  3.43  1.98  16.0   80 1.63  1.25 0.43  0.83  3.40 0.700  2.12\n#> 119  2   12.0  3.43  2.00  19.0   87 2.00  1.64 0.37  1.87  1.28 0.930  3.05\n#> 120  2   11.4  2.40  2.42  20.0   96 2.90  2.79 0.32  1.83  3.25 0.800  3.39\n#> 121  2   11.6  2.05  3.23  28.5  119 3.18  5.08 0.47  1.87  6.00 0.930  3.69\n#> 122  2   12.4  4.43  2.73  26.5  102 2.20  2.13 0.43  1.71  2.08 0.920  3.12\n#> 123  2   13.1  5.80  2.13  21.5   86 2.62  2.65 0.30  2.01  2.60 0.730  3.10\n#> 124  2   11.9  4.31  2.39  21.0   82 2.86  3.03 0.21  2.91  2.80 0.750  3.64\n#> 125  2   12.1  2.16  2.17  21.0   85 2.60  2.65 0.37  1.35  2.76 0.860  3.28\n#> 126  2   12.4  1.53  2.29  21.5   86 2.74  3.15 0.39  1.77  3.94 0.690  2.84\n#> 127  2   11.8  2.13  2.78  28.5   92 2.13  2.24 0.58  1.76  3.00 0.970  2.44\n#> 128  2   12.4  1.63  2.30  24.5   88 2.22  2.45 0.40  1.90  2.12 0.890  2.78\n#> 129  2   12.0  4.30  2.38  22.0   80 2.10  1.75 0.42  1.35  2.60 0.790  2.57\n#> 130  3   12.9  1.35  2.32  18.0  122 1.51  1.25 0.21  0.94  4.10 0.760  1.29\n#> 131  3   12.9  2.99  2.40  20.0  104 1.30  1.22 0.24  0.83  5.40 0.740  1.42\n#> 132  3   12.8  2.31  2.40  24.0   98 1.15  1.09 0.27  0.83  5.70 0.660  1.36\n#> 133  3   12.7  3.55  2.36  21.5  106 1.70  1.20 0.17  0.84  5.00 0.780  1.29\n#> 134  3   12.5  1.24  2.25  17.5   85 2.00  0.58 0.60  1.25  5.45 0.750  1.51\n#> 135  3   12.6  2.46  2.20  18.5   94 1.62  0.66 0.63  0.94  7.10 0.730  1.58\n#> 136  3   12.2  4.72  2.54  21.0   89 1.38  0.47 0.53  0.80  3.85 0.750  1.27\n#> 137  3   12.5  5.51  2.64  25.0   96 1.79  0.60 0.63  1.10  5.00 0.820  1.69\n#> 138  3   13.5  3.59  2.19  19.5   88 1.62  0.48 0.58  0.88  5.70 0.810  1.82\n#> 139  3   12.8  2.96  2.61  24.0  101 2.32  0.60 0.53  0.81  4.92 0.890  2.15\n#> 140  3   12.9  2.81  2.70  21.0   96 1.54  0.50 0.53  0.75  4.60 0.770  2.31\n#> 141  3   13.4  2.56  2.35  20.0   89 1.40  0.50 0.37  0.64  5.60 0.700  2.47\n#> 142  3   13.5  3.17  2.72  23.5   97 1.55  0.52 0.50  0.55  4.35 0.890  2.06\n#> 143  3   13.6  4.95  2.35  20.0   92 2.00  0.80 0.47  1.02  4.40 0.910  2.05\n#> 144  3   12.2  3.88  2.20  18.5  112 1.38  0.78 0.29  1.14  8.21 0.650  2.00\n#> 145  3   13.2  3.57  2.15  21.0  102 1.50  0.55 0.43  1.30  4.00 0.600  1.68\n#> 146  3   13.9  5.04  2.23  20.0   80 0.98  0.34 0.40  0.68  4.90 0.580  1.33\n#> 147  3   12.9  4.61  2.48  21.5   86 1.70  0.65 0.47  0.86  7.65 0.540  1.86\n#> 148  3   13.3  3.24  2.38  21.5   92 1.93  0.76 0.45  1.25  8.42 0.550  1.62\n#> 149  3   13.1  3.90  2.36  21.5  113 1.41  1.39 0.34  1.14  9.40 0.570  1.33\n#> 150  3   13.5  3.12  2.62  24.0  123 1.40  1.57 0.22  1.25  8.60 0.590  1.30\n#> 151  3   12.8  2.67  2.48  22.0  112 1.48  1.36 0.24  1.26 10.80 0.480  1.47\n#> 152  3   13.1  1.90  2.75  25.5  116 2.20  1.28 0.26  1.56  7.10 0.610  1.33\n#> 153  3   13.2  3.30  2.28  18.5   98 1.80  0.83 0.61  1.87 10.52 0.560  1.51\n#> 154  3   12.6  1.29  2.10  20.0  103 1.48  0.58 0.53  1.40  7.60 0.580  1.55\n#> 155  3   13.2  5.19  2.32  22.0   93 1.74  0.63 0.61  1.55  7.90 0.600  1.48\n#> 156  3   13.8  4.12  2.38  19.5   89 1.80  0.83 0.48  1.56  9.01 0.570  1.64\n#> 157  3   12.4  3.03  2.64  27.0   97 1.90  0.58 0.63  1.14  7.50 0.670  1.73\n#> 158  3   14.3  1.68  2.70  25.0   98 2.80  1.31 0.53  2.70 13.00 0.570  1.96\n#> 159  3   13.5  1.67  2.64  22.5   89 2.60  1.10 0.52  2.29 11.75 0.570  1.78\n#> 160  3   12.4  3.83  2.38  21.0   88 2.30  0.92 0.50  1.04  7.65 0.560  1.58\n#> 161  3   13.7  3.26  2.54  20.0  107 1.83  0.56 0.50  0.80  5.88 0.960  1.82\n#> 162  3   12.8  3.27  2.58  22.0  106 1.65  0.60 0.60  0.96  5.58 0.870  2.11\n#> 163  3   13.0  3.45  2.35  18.5  106 1.39  0.70 0.40  0.94  5.28 0.680  1.75\n#> 164  3   13.8  2.76  2.30  22.0   90 1.35  0.68 0.41  1.03  9.58 0.700  1.68\n#> 165  3   13.7  4.36  2.26  22.5   88 1.28  0.47 0.52  1.15  6.62 0.780  1.75\n#> 166  3   13.4  3.70  2.60  23.0  111 1.70  0.92 0.43  1.46 10.68 0.850  1.56\n#> 167  3   12.8  3.37  2.30  19.5   88 1.48  0.66 0.40  0.97 10.26 0.720  1.75\n#> 168  3   13.6  2.58  2.69  24.5  105 1.55  0.84 0.39  1.54  8.66 0.740  1.80\n#> 169  3   13.4  4.60  2.86  25.0  112 1.98  0.96 0.27  1.11  8.50 0.670  1.92\n#> 170  3   12.2  3.03  2.32  19.0   96 1.25  0.49 0.40  0.73  5.50 0.660  1.83\n#> 171  3   12.8  2.39  2.28  19.5   86 1.39  0.51 0.48  0.64  9.90 0.570  1.63\n#> 172  3   14.2  2.51  2.48  20.0   91 1.68  0.70 0.44  1.24  9.70 0.620  1.71\n#> 173  3   13.7  5.65  2.45  20.5   95 1.68  0.61 0.52  1.06  7.70 0.640  1.74\n#> 174  3   13.4  3.91  2.48  23.0  102 1.80  0.75 0.43  1.41  7.30 0.700  1.56\n#> 175  3   13.3  4.28  2.26  20.0  120 1.59  0.69 0.43  1.35 10.20 0.590  1.56\n#> 176  3   13.2  2.59  2.37  20.0  120 1.65  0.68 0.53  1.46  9.30 0.600  1.62\n#> 177  3   14.1  4.10  2.74  24.5   96 2.05  0.76 0.56  1.35  9.20 0.610  1.60\n#>     X1065\n#> 1    1050\n#> 2    1185\n#> 3    1480\n#> 4     735\n#> 5    1450\n#> 6    1290\n#> 7    1295\n#> 8    1045\n#> 9    1045\n#> 10   1510\n#> 11   1280\n#> 12   1320\n#> 13   1150\n#> 14   1547\n#> 15   1310\n#> 16   1280\n#> 17   1130\n#> 18   1680\n#> 19    845\n#> 20    780\n#> 21    770\n#> 22   1035\n#> 23   1015\n#> 24    845\n#> 25    830\n#> 26   1195\n#> 27   1285\n#> 28    915\n#> 29   1035\n#> 30   1285\n#> 31   1515\n#> 32    990\n#> 33   1235\n#> 34   1095\n#> 35    920\n#> 36    880\n#> 37   1105\n#> 38   1020\n#> 39    760\n#> 40    795\n#> 41   1035\n#> 42   1095\n#> 43    680\n#> 44    885\n#> 45   1080\n#> 46   1065\n#> 47    985\n#> 48   1060\n#> 49   1260\n#> 50   1150\n#> 51   1265\n#> 52   1190\n#> 53   1375\n#> 54   1060\n#> 55   1120\n#> 56    970\n#> 57   1270\n#> 58   1285\n#> 59    520\n#> 60    680\n#> 61    450\n#> 62    630\n#> 63    420\n#> 64    355\n#> 65    678\n#> 66    502\n#> 67    510\n#> 68    750\n#> 69    718\n#> 70    870\n#> 71    410\n#> 72    472\n#> 73    985\n#> 74    886\n#> 75    428\n#> 76    392\n#> 77    500\n#> 78    750\n#> 79    463\n#> 80    278\n#> 81    714\n#> 82    630\n#> 83    515\n#> 84    520\n#> 85    450\n#> 86    495\n#> 87    562\n#> 88    680\n#> 89    625\n#> 90    480\n#> 91    450\n#> 92    495\n#> 93    290\n#> 94    345\n#> 95    937\n#> 96    625\n#> 97    428\n#> 98    660\n#> 99    406\n#> 100   710\n#> 101   562\n#> 102   438\n#> 103   415\n#> 104   672\n#> 105   315\n#> 106   510\n#> 107   488\n#> 108   312\n#> 109   680\n#> 110   562\n#> 111   325\n#> 112   607\n#> 113   434\n#> 114   385\n#> 115   407\n#> 116   495\n#> 117   345\n#> 118   372\n#> 119   564\n#> 120   625\n#> 121   465\n#> 122   365\n#> 123   380\n#> 124   380\n#> 125   378\n#> 126   352\n#> 127   466\n#> 128   342\n#> 129   580\n#> 130   630\n#> 131   530\n#> 132   560\n#> 133   600\n#> 134   650\n#> 135   695\n#> 136   720\n#> 137   515\n#> 138   580\n#> 139   590\n#> 140   600\n#> 141   780\n#> 142   520\n#> 143   550\n#> 144   855\n#> 145   830\n#> 146   415\n#> 147   625\n#> 148   650\n#> 149   550\n#> 150   500\n#> 151   480\n#> 152   425\n#> 153   675\n#> 154   640\n#> 155   725\n#> 156   480\n#> 157   880\n#> 158   660\n#> 159   620\n#> 160   520\n#> 161   680\n#> 162   570\n#> 163   675\n#> 164   615\n#> 165   520\n#> 166   695\n#> 167   685\n#> 168   750\n#> 169   630\n#> 170   510\n#> 171   470\n#> 172   660\n#> 173   740\n#> 174   750\n#> 175   835\n#> 176   840\n#> 177   560\nnames(wines) <- c(\"label\",\n                  \"Alcohol\",\n                  \"Malic_acid\",\n                  \"Ash\",\n                  \"Alcalinity_of_ash\",\n                  \"Magnesium\",\n                  \"Total_phenols\",\n                  \"Flavanoids\",\n                  \"Nonflavanoid_phenols\",\n                  \"Proanthocyanins\",\n                  \"Color_intensity\",\n                  \"Hue\",\n                  \"OD280_OD315_of_diluted_wines\",\n                  \"Proline\")\nhead(wines)\n#>   label Alcohol Malic_acid  Ash Alcalinity_of_ash Magnesium Total_phenols\n#> 1     1    13.2       1.78 2.14              11.2       100          2.65\n#> 2     1    13.2       2.36 2.67              18.6       101          2.80\n#> 3     1    14.4       1.95 2.50              16.8       113          3.85\n#> 4     1    13.2       2.59 2.87              21.0       118          2.80\n#> 5     1    14.2       1.76 2.45              15.2       112          3.27\n#> 6     1    14.4       1.87 2.45              14.6        96          2.50\n#>   Flavanoids Nonflavanoid_phenols Proanthocyanins Color_intensity  Hue\n#> 1       2.76                 0.26            1.28            4.38 1.05\n#> 2       3.24                 0.30            2.81            5.68 1.03\n#> 3       3.49                 0.24            2.18            7.80 0.86\n#> 4       2.69                 0.39            1.82            4.32 1.04\n#> 5       3.39                 0.34            1.97            6.75 1.05\n#> 6       2.52                 0.30            1.98            5.25 1.02\n#>   OD280_OD315_of_diluted_wines Proline\n#> 1                         3.40    1050\n#> 2                         3.17    1185\n#> 3                         3.45    1480\n#> 4                         2.93     735\n#> 5                         2.85    1450\n#> 6                         3.58    1290\nplt1 <- ggplot(wines, aes(x = Alcohol, y = Magnesium, colour = as.factor(label))) +\n    geom_point(size=3) +\n    ggtitle(\"Wines\")\n\nplt1\nplt2 <- ggplot(wines, aes(x = Alcohol, y = Proline, colour = as.factor(label))) +\n    geom_point(size=3) +\n    ggtitle(\"Wines\")\nplt2"},{"path":"wine-classification-with-neuralnet.html","id":"preprocessing","chapter":"40 Wine classification with neuralnet","heading":"40.2 Preprocessing","text":"preprocessing phase, least following two things:Encode categorical variables. Standardize predictors. First , let’s encode target variable. encoding categorical variables needed using neuralnet since like factors . shout try feed factor (told nnet likes factors though).wine dataset variable label contains three different labels: 1,2 3.usual practice, far know, encode categorical variables “one hot” vector. instance, three classes, like case, ’d need replace label variable three variables like :case first observation labeled 1, second labeled 2, . Ironically, nnet package provides function perform encoding painless way:way, since predictors continuous, need encode , however, case needed , apply strategy applied categorical predictors. Unless course ’d like try kind custom encoding.Now let’s standardize predictors [0−1]\">[0−1] interval leveraging lapply function:","code":"#   l1,l2,l3\n#   1,0,0\n#   0,0,1\n#   ...\n# Encode as a one hot vector multilabel data\ntrain <- cbind(wines[, 2:14], class.ind(as.factor(wines$label)))\n\n# Set labels name\nnames(train) <- c(names(wines)[2:14],\"l1\",\"l2\",\"l3\")\n# Scale data\nscl <- function(x) { (x - min(x))/(max(x) - min(x)) }\ntrain[, 1:13] <- data.frame(lapply(train[, 1:13], scl))\nhead(train)\n#>   Alcohol Malic_acid   Ash Alcalinity_of_ash Magnesium Total_phenols Flavanoids\n#> 1   0.571      0.206 0.417            0.0309     0.326         0.576      0.511\n#> 2   0.561      0.320 0.701            0.4124     0.337         0.628      0.612\n#> 3   0.879      0.239 0.610            0.3196     0.467         0.990      0.665\n#> 4   0.582      0.366 0.807            0.5361     0.522         0.628      0.496\n#> 5   0.834      0.202 0.583            0.2371     0.457         0.790      0.643\n#> 6   0.884      0.223 0.583            0.2062     0.283         0.524      0.460\n#>   Nonflavanoid_phenols Proanthocyanins Color_intensity   Hue\n#> 1                0.245           0.274           0.265 0.463\n#> 2                0.321           0.757           0.375 0.447\n#> 3                0.208           0.558           0.556 0.309\n#> 4                0.491           0.445           0.259 0.455\n#> 5                0.396           0.492           0.467 0.463\n#> 6                0.321           0.495           0.339 0.439\n#>   OD280_OD315_of_diluted_wines Proline l1 l2 l3\n#> 1                        0.780   0.551  1  0  0\n#> 2                        0.696   0.647  1  0  0\n#> 3                        0.799   0.857  1  0  0\n#> 4                        0.608   0.326  1  0  0\n#> 5                        0.579   0.836  1  0  0\n#> 6                        0.846   0.722  1  0  0"},{"path":"wine-classification-with-neuralnet.html","id":"fitting-the-model-with-neuralnet","chapter":"40 Wine classification with neuralnet","heading":"40.3 Fitting the model with neuralnet","text":"Now finally time fit model. might remember old post wrote, neuralnet like formula y~.. Fear , can build formula used simple step:Note characters vector pasted right “~” symbol. Just remember check formula indeed correct good go.Let’s train neural network full dataset. take little time converge. standardize predictors take lot though.Note set argument linear.output FALSE order tell model want apply activation function act.fct regression task. set activation function logistic (way default option) order apply logistic function. available option tanh model seems perform little worse opted default option. far know two two available options, relu function available although seems common activation function packages.far number hidden neurons, tried combination one used seems perform slightly better others (around 1% accuracy difference cross validation score).using -built plot method can get visual take actually happening inside model, however plot helpful thinkLet’s look accuracy training set:100% bad! wait, may model fitted data, furthermore evaluating accuracy training set kind cheating since model already “knows” (know) answers. order assess “true accuracy” model need perform kind cross validation.","code":"\n# Set up formula\nn <- names(train)\nf <- as.formula(paste(\"l1 + l2 + l3 ~\", paste(n[!n %in% c(\"l1\",\"l2\",\"l3\")], collapse = \" + \")))\nf\n#> l1 + l2 + l3 ~ Alcohol + Malic_acid + Ash + Alcalinity_of_ash + \n#>     Magnesium + Total_phenols + Flavanoids + Nonflavanoid_phenols + \n#>     Proanthocyanins + Color_intensity + Hue + OD280_OD315_of_diluted_wines + \n#>     Proline\nnn <- neuralnet(f,\n                data = train,\n                hidden = c(13, 10, 3),\n                act.fct = \"logistic\",\n                linear.output = FALSE,\n                lifesign = \"minimal\")\n#> hidden: 13, 10, 3    thresh: 0.01    rep: 1/1    steps:      88  error: 0.03039  time: 0.05 secs\nplot(nn)\n# Compute predictions\npr.nn <- compute(nn, train[, 1:13])\n\n# Extract results\npr.nn_ <- pr.nn$net.result\nhead(pr.nn_)\n#>       [,1]    [,2]     [,3]\n#> [1,] 0.990 0.00317 6.99e-06\n#> [2,] 0.991 0.00233 8.69e-06\n#> [3,] 0.991 0.00210 8.65e-06\n#> [4,] 0.986 0.00442 8.74e-06\n#> [5,] 0.992 0.00212 8.32e-06\n#> [6,] 0.992 0.00214 8.34e-06\n# Accuracy (training set)\noriginal_values <- max.col(train[, 14:16])\npr.nn_2 <- max.col(pr.nn_)\nmean(pr.nn_2 == original_values)\n#> [1] 1"},{"path":"wine-classification-with-neuralnet.html","id":"cross-validating-the-classifier","chapter":"40 Wine classification with neuralnet","heading":"40.4 Cross validating the classifier","text":"Let’s cross-validate model using evergreen 10 fold cross validation following train test split: 95% dataset used training set remaining 5% test set.Just curiosity, decided run LOOCV round . case ’d like run cross validation technique, just set proportion variable 0.995: select just one observation test set leave observations training set. Running LOOCV get similar results 10 fold cross validation.98.8%, awesome! Next time invited relaxing evening includes wine tasting competition think definitely bring laptop contestant!Aside poor taste joke, (made !), indeed dataset challenging, think tweaking better cross validation score achieved. Nevertheless hope found tutorial useful. gist entire code tutorial can found .Thank reading article, please feel free leave comment questions suggestions share post others find useful.Notes:","code":"\n# Set seed for reproducibility purposes\nset.seed(500)\n# 10 fold cross validation\nk <- 10\n# Results from cv\nouts <- NULL\n# Train test split proportions\nproportion <- 0.95 # Set to 0.995 for LOOCV\n\n# Crossvalidate, go!\nfor(i in 1:k)\n{\n    index <- sample(1:nrow(train), round(proportion*nrow(train)))\n    train_cv <- train[index, ]\n    test_cv <- train[-index, ]\n    nn_cv <- neuralnet(f,\n                        data = train_cv,\n                        hidden = c(13, 10, 3),\n                        act.fct = \"logistic\",\n                        linear.output = FALSE)\n    \n    # Compute predictions\n    pr.nn <- compute(nn_cv, test_cv[, 1:13])\n    # Extract results\n    pr.nn_ <- pr.nn$net.result\n    # Accuracy (test set)\n    original_values <- max.col(test_cv[, 14:16])\n    pr.nn_2 <- max.col(pr.nn_)\n    outs[i] <- mean(pr.nn_2 == original_values)\n}\n\nmean(outs)\n#> [1] 0.978"},{"path":"predicting-the-rating-of-cereals.html","id":"predicting-the-rating-of-cereals","chapter":"41 Predicting the rating of cereals","heading":"41 Predicting the rating of cereals","text":"Datasets: cereals.csvAlgorithms:\nNeural Networks\nNeural Networks","code":""},{"path":"predicting-the-rating-of-cereals.html","id":"introduction-22","chapter":"41 Predicting the rating of cereals","heading":"41.1 Introduction","text":"Source: https://www.analyticsvidhya.com/blog/2017/09/creating-visualizing-neural-network--r/Neural network information-processing machine can viewed analogous human nervous system. Just like human nervous system, made interconnected neurons, neural network made interconnected information processing units. information processing units work linear manner. fact, neural network draws strength parallel processing information, allows deal non-linearity. Neural network becomes handy infer meaning detect patterns complex data sets.Neural network considered one useful technique world data analytics. However, complex often regarded black box, .e. users view input output neural network remain clueless knowledge generating process. hope article help readers learn internal mechanism neural network get hands-experience implement R.","code":""},{"path":"predicting-the-rating-of-cereals.html","id":"the-basics-of-neural-networks","chapter":"41 Predicting the rating of cereals","heading":"41.2 The Basics of Neural Networks","text":"neural network model characterized activation function, used interconnected information processing units transform input output. neural network always compared human nervous system. Information passed interconnected units analogous information passage neurons humans. first layer neural network receives raw input, processes passes processed information hidden layers. hidden layer passes information last layer, produces output. advantage neural network adaptive nature. learns information provided, .e. trains data, known outcome optimizes weights better prediction situations unknown outcome.perceptron, viz. single layer neural network, basic form neural network. perceptron receives multidimensional input processes using weighted summation activation function. trained using labeled data learning algorithm optimize weights summation processor. major limitation perceptron model inability deal non-linearity. multilayered neural network overcomes limitation helps solve non-linear problems. input layer connects hidden layer, turn connects output layer. connections weighted weights optimized using learning rule.many learning rules used neural network:least mean square;gradient descent;newton’s rule;conjugate gradient etc.learning rules can used conjunction backpropgation error method. learning rule used calculate error output unit. error backpropagated units error unit proportional contribution unit towards total error output unit. errors unit used optimize weight connection. Figure 1 displays structure simple neural network model better understanding.","code":""},{"path":"predicting-the-rating-of-cereals.html","id":"fitting-a-neural-network-in-r","chapter":"41 Predicting the rating of cereals","heading":"41.3 Fitting a Neural Network in R","text":"Now fit neural network model R. article, use subset cereal dataset shared Carnegie Mellon University (CMU). details dataset following link: http://lib.stat.cmu.edu/DASL/Datafiles/Cereals.html. objective predict rating cereals variables calories, proteins, fat etc. R script provided side side commented better understanding user. . data .csv format can downloaded clicking: cereals.Please set working directory R using setwd( ) function, keep cereal.csv working directory. use rating dependent variable calories, proteins, fat, sodium fiber independent variables. divide data training test set. Training set used find relationship dependent independent variables test set assesses performance model. use 60% dataset training set. assignment data training test set done using random sampling. perform random sampling R using sample ( ) function. used set.seed( ) generate random sample everytime maintain consistency. use index variable fitting neural network create training test data sets. R script follows:Now fit neural network data. use neuralnet library analysis. first step scale cereal dataset. scaling data essential otherwise variable may large impact prediction variable scale. Using unscaled may lead meaningless results. common techniques scale data : min-max normalization, Z-score normalization, median MAD, tan-h estimators. min-max normalization transforms data common range, thus removing scaling effect variables. Unlike Z-score normalization median MAD method, min-max method retains original distribution variables. use min-max normalization scale data. R script scaling data follows.\nFigure 41.1: Variation RMSE\nFigure 41.1) shows median RMSE model decreases length training set. important result. reader must remember model accuracy dependent length training set. performance neural network model sensitive training-test split.","code":"\n## Creating index variable \n\n# Read the Data\ndata = read.csv(file.path(data_raw_dir, \"cereals.csv\"), header=T)\n\n# Random sampling\nsamplesize = 0.60 * nrow(data)\nset.seed(80)\nindex = sample( seq_len ( nrow ( data ) ), size = samplesize )\n\n# Create training and test set  \ndatatrain = data[ index, ]\ndatatest = data[ -index, ]\ndplyr::glimpse(data)\n#> Rows: 75\n#> Columns: 6\n#> $ calories <int> 70, 120, 70, 50, 110, 110, 130, 90, 90, 120, 110, 120, 110, …\n#> $ protein  <int> 4, 3, 4, 4, 2, 2, 3, 2, 3, 1, 6, 1, 3, 1, 2, 2, 1, 1, 3, 2, …\n#> $ fat      <int> 1, 5, 1, 0, 2, 0, 2, 1, 0, 2, 2, 3, 2, 1, 0, 0, 0, 1, 3, 0, …\n#> $ sodium   <int> 130, 15, 260, 140, 180, 125, 210, 200, 210, 220, 290, 210, 1…\n#> $ fiber    <dbl> 10.0, 2.0, 9.0, 14.0, 1.5, 1.0, 2.0, 4.0, 5.0, 0.0, 2.0, 0.0…\n#> $ rating   <dbl> 68.4, 34.0, 59.4, 93.7, 29.5, 33.2, 37.0, 49.1, 53.3, 18.0, …\n## Scale data for neural network\n\nmax = apply(data , 2 , max)\nmin = apply(data, 2 , min)\nscaled = as.data.frame(scale(data, center = min, scale = max - min))\n## Fit neural network \n\n# install library\n# install.packages(\"neuralnet \")\n\n# load library\nlibrary(neuralnet)\n\n# creating training and test set\ntrainNN = scaled[index , ]\ntestNN = scaled[-index , ]\n\n# fit neural network\nset.seed(2)\nNN = neuralnet(rating ~ calories + protein + fat + sodium + fiber, \n               trainNN, hidden = 3 , linear.output = T )\n\n# plot neural network\nplot(NN)\n## Prediction using neural network\n\npredict_testNN = compute(NN, testNN[,c(1:5)])\npredict_testNN = (predict_testNN$net.result * (max(data$rating) - min(data$rating))) + min(data$rating)\n\nplot(datatest$rating, predict_testNN, col='blue', pch=16, ylab = \"predicted rating NN\", xlab = \"real rating\")\n\nabline(0,1)\n\n# Calculate Root Mean Square Error (RMSE)\nRMSE.NN = (sum((datatest$rating - predict_testNN)^2) / nrow(datatest)) ^ 0.5\n## Cross validation of neural network model\n\n# install relevant libraries\n# install.packages(\"boot\")\n# install.packages(\"plyr\")\n\n# Load libraries\nlibrary(boot)\nlibrary(plyr)\n\n# Initialize variables\nset.seed(50)\nk = 100\nRMSE.NN = NULL\n\nList = list( )\n\n# Fit neural network model within nested for loop\nfor(j in 10:65){\n    for (i in 1:k) {\n        index = sample(1:nrow(data),j )\n\n        trainNN = scaled[index,]\n        testNN = scaled[-index,]\n        datatest = data[-index,]\n\n        NN = neuralnet(rating ~ calories + protein + fat + sodium + fiber, trainNN, hidden = 3, linear.output= T)\n        predict_testNN = compute(NN,testNN[,c(1:5)])\n        predict_testNN = (predict_testNN$net.result*(max(data$rating)-min(data$rating)))+min(data$rating)\n\n        RMSE.NN [i]<- (sum((datatest$rating - predict_testNN)^2)/nrow(datatest))^0.5\n    }\n    List[[j]] = RMSE.NN\n}\n\nMatrix.RMSE = do.call(cbind, List)\n## Prepare boxplot\nboxplot(Matrix.RMSE[,56], ylab = \"RMSE\", main = \"RMSE BoxPlot (length of traning set = 65)\")\n## Variation of median RMSE \n# install.packages(\"matrixStats\")\nlibrary(matrixStats)\n#> \n#> Attaching package: 'matrixStats'\n#> The following object is masked from 'package:plyr':\n#> \n#>     count\n\nmed = colMedians(Matrix.RMSE)\n\nX = seq(10,65)\n\nplot (med~X, type = \"l\", xlab = \"length of training set\", ylab = \"median RMSE\", main = \"Variation of RMSE with length of training set\")"},{"path":"predicting-the-rating-of-cereals.html","id":"end-notes-1","chapter":"41 Predicting the rating of cereals","heading":"41.4 End Notes","text":"article discusses theoretical aspects neural network, implementation R post training evaluation. Neural network inspired biological nervous system. Similar nervous system information passed layers processors. significance variables represented weights connection. article provides basic understanding back propagation algorithm, used assign weights. article also implement neural network R. use publicly available dataset shared CMU. aim predict rating cereals using information calories, fat, protein etc. constructing neural network evaluate model accuracy robustness. compute RMSE perform cross-validation analysis. cross validation, check variation model accuracy length training set changed. consider training sets length 10 65. length 100 samples random picked median RMSE calculated. show model accuracy increases training set large. using model prediction, important check robustness performance cross validation.article provides quick review neural network useful reference data enthusiasts. provided commented R code throughout article help readers hands experience using neural networks.","code":""},{"path":"fitting-a-linear-model-with-neural-networks.html","id":"fitting-a-linear-model-with-neural-networks","chapter":"42 Fitting a linear model with neural networks","heading":"42 Fitting a linear model with neural networks","text":"Datasets: BostonAlgorithms:\nNeural Networks\nNeural Networks","code":""},{"path":"fitting-a-linear-model-with-neural-networks.html","id":"introduction-23","chapter":"42 Fitting a linear model with neural networks","heading":"42.1 Introduction","text":"https://www.r-bloggers.com/fitting--neural-network--r-neuralnet-package/https://datascienceplus.com/fitting-neural-network--r/Neural networks always one fascinating machine learning models opinion, fancy backpropagation algorithm also complexity (think deep learning many hidden layers) structure inspired brain.Neural networks always popular, partly , still cases, computationally expensive partly seem yield better results compared simpler methods support vector machines (SVMs). Nevertheless, Neural Networks , , raised attention become popular.Update: published another post Network analysis DataScience+ Network analysis Game ThronesIn post, going fit simple neural network using neuralnet package fit linear model comparison.","code":""},{"path":"fitting-a-linear-model-with-neural-networks.html","id":"the-dataset-1","chapter":"42 Fitting a linear model with neural networks","heading":"42.2 The dataset","text":"going use Boston dataset MASS package.\nBoston dataset collection data housing values suburbs Boston. goal predict median value owner-occupied homes (medv) using continuous variables available.First need check data point missing, otherwise need fix dataset.missing data, good. proceed randomly splitting data train test set, fit linear regression model test test set. Note using gml() function instead lm() become useful later cross validating linear model.sample(x,size) function simply outputs vector specified size randomly selected samples vector x. default sampling without replacement: index essentially random vector indeces.\nSince dealing regression problem, going use mean squared error (MSE) measure much predictions far away real data.","code":"\nset.seed(500)\nlibrary(MASS)\ndata <- Boston\ndplyr::glimpse(data)\n#> Rows: 506\n#> Columns: 14\n#> $ crim    <dbl> 0.00632, 0.02731, 0.02729, 0.03237, 0.06905, 0.02985, 0.08829…\n#> $ zn      <dbl> 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.5, 12.5, 12.5, 12.5, 12.5, …\n#> $ indus   <dbl> 2.31, 7.07, 7.07, 2.18, 2.18, 2.18, 7.87, 7.87, 7.87, 7.87, 7…\n#> $ chas    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n#> $ nox     <dbl> 0.538, 0.469, 0.469, 0.458, 0.458, 0.458, 0.524, 0.524, 0.524…\n#> $ rm      <dbl> 6.58, 6.42, 7.18, 7.00, 7.15, 6.43, 6.01, 6.17, 5.63, 6.00, 6…\n#> $ age     <dbl> 65.2, 78.9, 61.1, 45.8, 54.2, 58.7, 66.6, 96.1, 100.0, 85.9, …\n#> $ dis     <dbl> 4.09, 4.97, 4.97, 6.06, 6.06, 6.06, 5.56, 5.95, 6.08, 6.59, 6…\n#> $ rad     <int> 1, 2, 2, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4…\n#> $ tax     <dbl> 296, 242, 242, 222, 222, 222, 311, 311, 311, 311, 311, 311, 3…\n#> $ ptratio <dbl> 15.3, 17.8, 17.8, 18.7, 18.7, 18.7, 15.2, 15.2, 15.2, 15.2, 1…\n#> $ black   <dbl> 397, 397, 393, 395, 397, 394, 396, 397, 387, 387, 393, 397, 3…\n#> $ lstat   <dbl> 4.98, 9.14, 4.03, 2.94, 5.33, 5.21, 12.43, 19.15, 29.93, 17.1…\n#> $ medv    <dbl> 24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 1…\napply(data,2,function(x) sum(is.na(x)))\n#>    crim      zn   indus    chas     nox      rm     age     dis     rad     tax \n#>       0       0       0       0       0       0       0       0       0       0 \n#> ptratio   black   lstat    medv \n#>       0       0       0       0\nindex <- sample(1:nrow(data),round(0.75*nrow(data)))\ntrain <- data[index,]\ntest <- data[-index,]\nlm.fit <- glm(medv~., data=train)\nsummary(lm.fit)\n#> \n#> Call:\n#> glm(formula = medv ~ ., data = train)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -15.211   -2.559   -0.655    1.828   29.711  \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  31.11170    5.45981    5.70  2.5e-08 ***\n#> crim         -0.11137    0.03326   -3.35  0.00090 ***\n#> zn            0.04263    0.01431    2.98  0.00308 ** \n#> indus         0.00148    0.06745    0.02  0.98247    \n#> chas          1.75684    0.98109    1.79  0.07417 .  \n#> nox         -18.18485    4.47157   -4.07  5.8e-05 ***\n#> rm            4.76034    0.48047    9.91  < 2e-16 ***\n#> age          -0.01344    0.01410   -0.95  0.34119    \n#> dis          -1.55375    0.21893   -7.10  6.7e-12 ***\n#> rad           0.28818    0.07202    4.00  7.6e-05 ***\n#> tax          -0.01374    0.00406   -3.38  0.00079 ***\n#> ptratio      -0.94755    0.14012   -6.76  5.4e-11 ***\n#> black         0.00950    0.00290    3.28  0.00115 ** \n#> lstat        -0.38890    0.05973   -6.51  2.5e-10 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for gaussian family taken to be 20.2)\n#> \n#>     Null deviance: 32463.5  on 379  degrees of freedom\n#> Residual deviance:  7407.1  on 366  degrees of freedom\n#> AIC: 2237\n#> \n#> Number of Fisher Scoring iterations: 2\npr.lm <- predict(lm.fit,test)\nMSE.lm <- sum((pr.lm - test$medv)^2)/nrow(test)"},{"path":"fitting-a-linear-model-with-neural-networks.html","id":"preparing-to-fit-the-neural-network","chapter":"42 Fitting a linear model with neural networks","heading":"42.3 Preparing to fit the neural network","text":"fitting neural network, preparation need done. Neural networks easy train tune.first step, going address data pre-processing.\ngood practice normalize data training neural network. emphasize enough important step : depending dataset, avoiding normalization may lead useless results difficult training process (times algorithm converge number maximum iterations allowed). can choose different methods scale data (z-normalization, min-max scale, etc…). chose use min-max method scale data interval [0,1]. Usually scaling intervals [0,1] [-1,1] tends give better results.\ntherefore scale split data moving :Note scale returns matrix needs coerced data.frame.","code":"\nmaxs <- apply(data, 2, max) \nmins <- apply(data, 2, min)\n\nscaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins))\n\ntrain_ <- scaled[index,]\ntest_ <- scaled[-index,]"},{"path":"fitting-a-linear-model-with-neural-networks.html","id":"parameters","chapter":"42 Fitting a linear model with neural networks","heading":"42.4 Parameters","text":"far know fixed rule many layers neurons use although several less accepted rules thumb. Usually, necessary, one hidden layer enough vast numbers applications. far number neurons concerned, input layer size output layer size, usually 2/3 input size. least brief experience testing best solution since guarantee rules fit model best.\nSince toy example, going use 2 hidden layers configuration: 13:5:3:1. input layer 13 inputs, two hidden layers 5 3 neurons output layer , course, single output since regression.\nLet’s fit net:couple notes:reason formula y~. accepted neuralnet() function. need first write formula pass argument fitting function.reason formula y~. accepted neuralnet() function. need first write formula pass argument fitting function.hidden argument accepts vector number neurons hidden layer, argument linear.output used specify whether want regression linear.output=TRUE classification linear.output=FALSEThe hidden argument accepts vector number neurons hidden layer, argument linear.output used specify whether want regression linear.output=TRUE classification linear.output=FALSEThe neuralnet package provides nice tool plot model:graphical representation model weights connection:black lines show connections layer weights connection blue lines show bias term added step. bias can thought intercept linear model.\nnet essentially black box say much fitting, weights model. Suffice say training algorithm converged therefore model ready used.","code":"\nlibrary(neuralnet)\nn <- names(train_)\nf <- as.formula(paste(\"medv ~\", paste(n[!n %in% \"medv\"], collapse = \" + \")))\nnn <- neuralnet(f,data=train_,hidden=c(5,3),linear.output=T)\nplot(nn)"},{"path":"fitting-a-linear-model-with-neural-networks.html","id":"predicting-medv-using-the-neural-network","chapter":"42 Fitting a linear model with neural networks","heading":"42.5 Predicting medv using the neural network","text":"Now can try predict values test set calculate MSE. Remember net output normalized prediction, need scale back order make meaningful comparison (just simple prediction).compare two MSEsApparently, net better work linear model predicting medv. , careful result depends train-test split performed . , visual plot, going perform fast cross validation order confident results.first visual approach performance network linear model test set plotted belowBy visually inspecting plot can see predictions made neural network (general) concetrated around line (perfect alignment line indicate MSE 0 thus ideal perfect prediction) made linear model.","code":"\npr.nn <- compute(nn,test_[,1:13])\n\npr.nn_ <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)\ntest.r <- (test_$medv)*(max(data$medv)-min(data$medv))+min(data$medv)\n\nMSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test_)\nprint(paste(MSE.lm,MSE.nn))\n#> [1] \"31.2630222372615 16.4595537665717\"\npar(mfrow=c(1,2))\n\nplot(test$medv,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)\nabline(0,1,lwd=2)\nlegend('bottomright',legend='NN',pch=18,col='red', bty='n')\n\nplot(test$medv,pr.lm,col='blue',main='Real vs predicted lm',pch=18, cex=0.7)\nabline(0,1,lwd=2)\nlegend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)\nplot(test$medv,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)\npoints(test$medv,pr.lm,col='blue',pch=18,cex=0.7)\nabline(0,1,lwd=2)\nlegend('bottomright',legend=c('NN','LM'),pch=18,col=c('red','blue'))"},{"path":"fitting-a-linear-model-with-neural-networks.html","id":"a-fast-cross-validation","chapter":"42 Fitting a linear model with neural networks","heading":"42.6 A (fast) cross validation","text":"Cross validation another important step building predictive models. different kind cross validation methods, basic idea repeating following process number time:train-test splitDo train-test splitFit model train setTest model test setCalculate prediction errorRepeat process K timesThen calculating average error can get grasp model .going implement fast cross validation using loop neural network cv.glm() function boot package linear model.\nfar know, built-function R perform cross-validation kind neural network, know function, please let know comments. 10 fold cross-validated MSE linear model:Now net. Note splitting data way: 90% train set 10% test set random way 10 times. also initializing progress bar using plyr library want keep eye status process since fitting neural network may take ., process done, calculate average MSE plot results boxplotThe code box plot:\ncode outputs following boxplot:can see, average MSE neural network (10.33) lower one linear model although seems certain degree variation MSEs cross validation. may depend splitting data random initialization weights net. running simulation different times different seeds can get precise point estimate average MSE.","code":"\nlibrary(boot)\nset.seed(200)\nlm.fit <- glm(medv~.,data=data)\ncv.glm(data,lm.fit,K=10)$delta[1]\n#> [1] 23.2set.seed(450)\ncv.error <- NULL\nk <- 10\n\nlibrary(plyr) \npbar <- create_progress_bar('text')\npbar$init(k)\n#> \n  |                                                                            \n  |                                                                      |   0%\n\nfor(i in 1:k){\n    index <- sample(1:nrow(data),round(0.9*nrow(data)))\n    train.cv <- scaled[index,]\n    test.cv <- scaled[-index,]\n    \n    nn <- neuralnet(f,data=train.cv,hidden=c(5,2),linear.output=T)\n    \n    pr.nn <- compute(nn,test.cv[,1:13])\n    pr.nn <- pr.nn$net.result*(max(data$medv)-min(data$medv))+min(data$medv)\n    \n    test.cv.r <- (test.cv$medv)*(max(data$medv)-min(data$medv))+min(data$medv)\n    \n    cv.error[i] <- sum((test.cv.r - pr.nn)^2)/nrow(test.cv)\n    \n    pbar$step()\n}\n#> \n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |======================================================================| 100%\nmean(cv.error)\n#> [1] 7.64\ncv.error\n#>  [1] 13.33  7.10  6.58  5.70  6.84  5.77 10.75  5.38  9.45  5.50\nboxplot(cv.error,xlab='MSE CV',col='cyan',\n        border='blue',names='CV error (MSE)',\n        main='CV error (MSE) for NN',horizontal=TRUE)"},{"path":"fitting-a-linear-model-with-neural-networks.html","id":"a-final-note-on-model-interpretability","chapter":"42 Fitting a linear model with neural networks","heading":"42.7 A final note on model interpretability","text":"Neural networks resemble black boxes lot: explaining outcome much difficult explaining outcome simpler model linear model. Therefore, depending kind application need, might want take account factor . Furthermore, seen , extra care needed fit neural network small changes can lead different results.gist full code post can found .Thank reading post, leave comment question.","code":""},{"path":"visualization-of-neural-networks.html","id":"visualization-of-neural-networks","chapter":"43 Visualization of neural networks","heading":"43 Visualization of neural networks","text":"https://beckmw.wordpress.com/tag/neuralnet/last post said wasn’t going write anymore neural networks (.e., multilayer feedforward perceptron, supervised ANN, etc.). lie. ’ve received several requests update neural network plotting function described original post. previously explained, R provide lot options visualizing neural networks. option know plotting method objects neuralnet package. may opinion, think plot leaves much desired (see ). Also, plotting methods exist neural networks created packages, .e., nnet RSNNS. packages ones listed CRAN task view, ’ve updated original plotting function work three. Additionally, ’ve added new option plotting raw weight vector allow use neural networks created elsewhere. blog describes changes, well new arguments added original function.usual, ’ll simulate data use creating neural networks. dataset contains eight input variables two output variables. final dataset data frame variables, well separate data frames input output variables. ’ve retained separate datasets based syntax package.various neural network packages used create separate models plotting.’ve noticed differences functions lead confusion. simplicity, code represents interpretation direct way create neural network package. aware direct comparison results advised given default arguments differ packages. key differences follows, although many others noted. First, functions differ methods passing primary input variables.nnet function can take separate (combined) x y inputs data frames formula, neuralnet function can use formula input, mlp function can take data frame combined separate variables input. far know, neuralnet function capable modelling multiple response variables, unless response categorical variable uses one node outcome. Additionally, default output neuralnet function linear, whereas opposite true two functions.Specifics aside, ’s use updated plot function. Note syntax used plot modelThe plotting function can also now used arbitrary weight vector, rather specific model object. struct argument must also included option used. thought easiest way use plotting function weights input weights numeric vector, including bias layers. ’ve shown can done using weights directly mod1 simplicity.Note wts.numeric vector length equal expected given architecture (.e., 8 10 2 network, 100 connection weights plus 12 bias weights). plot look plot neural network nnet.weights input vector need specific order correct plotting. realize clear looking directly wt.simplest approach think . weight vector shows weights hidden node sequence, starting bias input node, weights output node sequence, starting bias input output node. Note bias layer included even network created biases. case, simply input random number bias values go use argument bias=F. ’ll show correct order weights using example plot.nn neuralnet package since weights included directly plot.pretend figure wasn’t created R, input mod.argument updated plotting function follows. Also note struct must included using approach.Note comparability figure created using neuralnet package. , larger weights thicker lines color indicates sign (+ black, – grey).One days ’ll actually put functions package. meantime, please let know bugs encountered.","code":"\nlibrary(clusterGeneration)\n#> Loading required package: MASS\nlibrary(tictoc)\n \nseed.val<- 12345\nset.seed(seed.val)\n \nnum.vars<-8\nnum.obs<-1000\n \n# input variables\ncov.mat <-genPositiveDefMat(num.vars,covMethod=c(\"unifcorrmat\"))$Sigma\nrand.vars <-mvrnorm(num.obs,rep(0,num.vars),Sigma=cov.mat)\n \n# output variables\nparms <-runif(num.vars,-10,10)\ny1 <- rand.vars %*% matrix(parms) + rnorm(num.obs,sd=20)\nparms2 <- runif(num.vars,-10,10)\ny2 <- rand.vars %*% matrix(parms2) + rnorm(num.obs,sd=20)\n \n# final datasets\nrand.vars <- data.frame(rand.vars)\nresp <- data.frame(y1,y2)\nnames(resp) <- c('Y1','Y2')\ndat.in <- data.frame(resp, rand.vars)\ndplyr::glimpse(dat.in)\n#> Rows: 1,000\n#> Columns: 10\n#> $ Y1 <dbl> 25.442, -14.578, -36.214, 15.216, -6.393, -20.849, -28.665, -87.36…\n#> $ Y2 <dbl> 16.9, 38.8, 31.2, -31.2, 93.3, 11.7, 59.7, -103.5, -49.8, 50.1, 28…\n#> $ X1 <dbl> 3.138, -0.705, -4.373, 0.837, 0.787, 1.923, -1.419, 1.121, -0.423,…\n#> $ X2 <dbl> 0.1945, -0.3016, 0.7734, 1.3112, 3.5056, 1.2453, 3.8002, -0.1646, …\n#> $ X3 <dbl> -1.795, -2.596, 2.308, 4.081, -3.921, 1.473, -0.926, 7.101, 2.366,…\n#> $ X4 <dbl> -2.7216, 3.0589, 1.2455, 3.4607, 2.3775, -2.9833, 2.6669, -0.3046,…\n#> $ X5 <dbl> 0.0407, 0.7602, -3.0217, -4.2799, 2.0859, 1.4765, 0.0561, 2.8328, …\n#> $ X6 <dbl> -1.4820, -0.5014, 0.0603, -1.8551, 2.2817, 1.7386, 1.7450, -2.8279…\n#> $ X7 <dbl> -0.7169, -0.3618, -1.5283, 4.2026, -6.1548, -0.3545, -6.0284, 9.52…\n#> $ X8 <dbl> 1.152, 1.810, -1.357, 0.598, -1.425, -1.210, -1.004, 2.494, -0.702…\n# first model with nnet\n#nnet function from nnet package\nlibrary(nnet)\nset.seed(seed.val)\ntic()\nmod1 <- nnet(rand.vars, resp, data = dat.in, size = 10, linout = T)\n#> # weights:  112\n#> initial  value 4784162.893260 \n#> iter  10 value 1794537.980652\n#> iter  20 value 1577753.498759\n#> iter  30 value 1485254.945754\n#> iter  40 value 1449238.248497\n#> iter  50 value 1427720.291593\n#> iter  60 value 1416950.633230\n#> iter  70 value 1405977.228751\n#> iter  80 value 1392096.965794\n#> iter  90 value 1365158.497142\n#> iter 100 value 1358854.803800\n#> final  value 1358854.803800 \n#> stopped after 100 iterations\ntoc()\n#> 0.162 sec elapsed\n# nn <- neuralnet(form.in,\n#                 data = dat.sc,\n#                 # hidden = c(13, 10, 3),\n#                 hidden = c(5),\n#                 act.fct = \"tanh\",\n#                 linear.output = FALSE,\n#                 lifesign = \"minimal\")\n# 2nd model with neuralnet\n# neuralnet function from neuralnet package, notice use of only one response\nlibrary(neuralnet)\n\nsoftplus <- function(x) log(1 + exp(x))\nsigmoid  <- function(x) log(1 + exp(-x))\n\ndat.sc <- scale(dat.in)\nform.in <- as.formula('Y1 ~ X1+X2+X3+X4+X5+X6+X7+X8')\nset.seed(seed.val)\ntic()\nmod2 <- neuralnet(form.in, data = dat.sc, hidden = 10, lifesign = \"minimal\",\n                  linear.output = FALSE,\n                  act.fct = \"tanh\")\n#> hidden: 10    thresh: 0.01    rep: 1/1    steps:   26361 error: 160.06372    time: 26.03 secs\ntoc()\n#> 26.043 sec elapsed\n# third model with RSNNS\n# mlp function from RSNNS package\nlibrary(RSNNS)\n#> Loading required package: Rcpp\nset.seed(seed.val)\ntic()\nmod3 <- mlp(rand.vars, resp, size = 10, linOut = T)\ntoc()\n#> 0.305 sec elapsed\n# import the function from Github\nlibrary(devtools)\n#> Loading required package: usethis\nsource_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')\n#> SHA-1 hash of file is 74c80bd5ddbc17ab3ae5ece9c0ed9beb612e87ef\n \n# plot each model\nplot.nnet(mod1)\n#> Loading required package: scales\n#> Loading required package: reshape\nplot.nnet(mod2) \nplot.nnet(mod3)\n#> Warning in plot.nnet(mod3): Bias layer not applicable for rsnns object\nwts.in <- mod1$wts\nstruct <- mod1$n\nplot.nnet(wts.in,struct=struct)\nmod.in<-c(13.12,1.49,0.16,-0.11,-0.19,-0.16,0.56,-0.52,0.81)\nstruct<-c(2,2,1) #two inputs, two hidden, one output \nplot.nnet(mod.in, struct=struct)"},{"path":"visualization-of-neural-networks.html","id":"caret-and-plot-nn","chapter":"43 Visualization of neural networks","heading":"43.1 caret and plot NN","text":"’ve changed function work neural networks created using train function caret package. link updated can also grab .","code":"\nlibrary(caret)\n#> Loading required package: lattice\n#> Loading required package: ggplot2\n#> \n#> Attaching package: 'caret'\n#> The following objects are masked from 'package:RSNNS':\n#> \n#>     confusionMatrix, train\nmod4 <- train(Y1 ~., method='nnet', data=dat.in, linout=T)\nplot.nnet(mod4,nid=T)\n#> Warning in plot.nnet(mod4, nid = T): Using best nnet model from train output\nfact<-factor(sample(c('a','b','c'),size=num.obs,replace=T))\nform.in<-formula('cbind(Y2,Y1)~X1+X2+X3+fact')\nmod5<-nnet(form.in,data=cbind(dat.in,fact),size=10,linout=T)\n#> # weights:  82\n#> initial  value 4799569.423556 \n#> iter  10 value 2864553.218126\n#> iter  20 value 2595828.194160\n#> iter  30 value 2517965.483939\n#> iter  40 value 2464882.178242\n#> iter  50 value 2444238.701539\n#> iter  60 value 2424302.507241\n#> iter  70 value 2395226.893106\n#> iter  80 value 2375603.533929\n#> iter  90 value 2339340.538338\n#> iter 100 value 2300041.821875\n#> final  value 2300041.821875 \n#> stopped after 100 iterations\nplot.nnet(mod5,nid=T)"},{"path":"visualization-of-neural-networks.html","id":"multiple-hidden-layers","chapter":"43 Visualization of neural networks","heading":"43.2 Multiple hidden layers","text":"updates… ’ve now modified function plot multiple hidden layers networks created using mlp function RSNNS package neuralnet neuralnet package. far know, neural network functions R can create multiple hidden layers. others use single hidden layer. tested plotting function using manual input weight vectors multiple hidden layers. guess won’t work can’t bothered change function unless ’s specifically requested. updated function can grabbed (links function also changed).","code":"\nlibrary(RSNNS)\n \n# neural net with three hidden layers, 9, 11, and 8 nodes in each\ntic()\nmod <-mlp(rand.vars, resp, \n          size = c(9,11,8), \n          linOut = T)\ntoc()\n#> 0.461 sec elapsed\npar(mar=numeric(4),family='serif')\nplot.nnet(mod)\n#> Warning in plot.nnet(mod): Bias layer not applicable for rsnns object"},{"path":"visualization-of-neural-networks.html","id":"binary-predictors","chapter":"43 Visualization of neural networks","heading":"43.3 Binary predictors","text":"’s example using neuralnet function binary predictors categorical outputs (credit Tao Ma model code).","code":"\nlibrary(neuralnet)\n \n#response\nAND<-c(rep(0,7),1)\nOR<-c(0,rep(1,7))\n \n# response with predictors\nbinary.data <- data.frame(expand.grid(c(0,1), c(0,1), c(0,1)), AND, OR)\n \n#model\ntic()\nnet <- neuralnet(AND+OR ~ Var1+Var2+Var3,\n                 binary.data, hidden =c(6,12,8), \n                 rep = 10, \n                 err.fct=\"ce\", \n                 linear.output=FALSE)\ntoc()\n#> 0.075 sec elapsed\n#plot ouput\npar(mar=numeric(4),family='serif')\nplot.nnet(net)"},{"path":"visualization-of-neural-networks.html","id":"color-coding-the-input-layer","chapter":"43 Visualization of neural networks","heading":"43.4 color coding the input layer","text":"color vector argument (circle.col) nodes changed allow separate color vector input layer.following example shows can done using relative importance input variables color-code first layer.","code":"\n# example showing use of separate colors for input layer\n# color based on relative importance using 'gar.fun'\n \n##\n#create input data\nseed.val<-3\nset.seed(seed.val)\n  \nnum.vars<-8\nnum.obs<-1000\n  \n#input variables\nlibrary(clusterGeneration)\ncov.mat<-genPositiveDefMat(num.vars,covMethod=c(\"unifcorrmat\"))$Sigma\nrand.vars<-mvrnorm(num.obs,rep(0,num.vars),Sigma=cov.mat)\n  \n# output variables\nparms<-runif(num.vars,-10,10)\ny1<-rand.vars %*% matrix(parms) + rnorm(num.obs,sd=20)\n \n# final datasets\nrand.vars<-data.frame(rand.vars)\nresp<-data.frame(y1)\nnames(resp)<-'Y1'\ndat.in <- data.frame(resp,rand.vars)\n \n##\n# create model\nlibrary(nnet)\nmod1 <- nnet(rand.vars,resp,data=dat.in,size=10,linout=T)\n#> # weights:  101\n#> initial  value 844959.580478 \n#> iter  10 value 543616.101824\n#> iter  20 value 479986.887834\n#> iter  30 value 465607.581653\n#> iter  40 value 454224.889175\n#> iter  50 value 448045.215005\n#> iter  60 value 445024.634696\n#> iter  70 value 442353.392985\n#> iter  80 value 438868.995558\n#> iter  90 value 435409.869319\n#> iter 100 value 432831.005169\n#> final  value 432831.005169 \n#> stopped after 100 iterations\n \n##\n# relative importance function\nlibrary(devtools)\nsource_url('https://gist.github.com/fawda123/6206737/raw/2e1bc9cbc48d1a56d2a79dd1d33f414213f5f1b1/gar_fun.r')\n#> SHA-1 hash of file is 9faa58824c46956c3ff78081696290d9b32d845f\n \n# relative importance of input variables for Y1\nrel.imp <- gar.fun('Y1',mod1,bar.plot=F)$rel.imp\n \n#color vector based on relative importance of input values\ncols<-colorRampPalette(c('green','red'))(num.vars)[rank(rel.imp)]\n \n##\n#plotting function\nsource_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')\n#> SHA-1 hash of file is 74c80bd5ddbc17ab3ae5ece9c0ed9beb612e87ef\n  \n#plot model with new color vector\n#separate colors for input vectors using a list for 'circle.col'\nplot(mod1,circle.col=list(cols,'lightblue'))"},{"path":"build-a-fully-connected-r-neural-network-from-scratch.html","id":"build-a-fully-connected-r-neural-network-from-scratch","chapter":"44 Build a fully connected R neural network from scratch","heading":"44 Build a fully connected R neural network from scratch","text":"","code":""},{"path":"build-a-fully-connected-r-neural-network-from-scratch.html","id":"introduction-24","chapter":"44 Build a fully connected R neural network from scratch","heading":"44.1 Introduction","text":"http://www.parallelr.com/r-deep-neural-network--scratch/","code":"\nlibrary(neuralnet)\n\n# Copyright 2016: www.ParallelR.com\n# Parallel Blog : R For Deep Learning (I): Build Fully Connected Neural Network From Scratch\n# Classification by 2-layers DNN and tested by iris dataset\n# Author: Peng Zhao, patric.zhao@gmail.com\n\n# Prediction\npredict.dnn <- function(model, data = X.test) {\n  # new data, transfer to matrix\n  new.data <- data.matrix(data)\n  \n  # Feed Forwad\n  hidden.layer <- sweep(new.data %*% model$W1 ,2, model$b1, '+')\n  # neurons : Rectified Linear\n  hidden.layer <- pmax(hidden.layer, 0)\n  score <- sweep(hidden.layer %*% model$W2, 2, model$b2, '+')\n  \n  # Loss Function: softmax\n  score.exp <- exp(score)\n  probs <-sweep(score.exp, 1, rowSums(score.exp), '/') \n  \n  # select max possiblity\n  labels.predicted <- max.col(probs)\n  return(labels.predicted)\n}\n\n# Train: build and train a 2-layers neural network \ntrain.dnn <- function(x, y, traindata=data, testdata=NULL,\n                  model = NULL,\n                  # set hidden layers and neurons\n                  # currently, only support 1 hidden layer\n                  hidden=c(6), \n                  # max iteration steps\n                  maxit=2000,\n                  # delta loss \n                  abstol=1e-2,\n                  # learning rate\n                  lr = 1e-2,\n                  # regularization rate\n                  reg = 1e-3,\n                  # show results every 'display' step\n                  display = 100,\n                  random.seed = 1)\n{\n  # to make the case reproducible.\n  set.seed(random.seed)\n  \n  # total number of training set\n  N <- nrow(traindata)\n  \n  # extract the data and label\n  # don't need atribute \n  X <- unname(data.matrix(traindata[,x]))\n  # correct categories represented by integer \n  Y <- traindata[,y]\n  if(is.factor(Y)) { Y <- as.integer(Y) }\n  # create index for both row and col\n  # create index for both row and col\n  Y.len   <- length(unique(Y))\n  Y.set   <- sort(unique(Y))\n  Y.index <- cbind(1:N, match(Y, Y.set))\n\n  # create model or get model from parameter\n  if(is.null(model)) {\n       # number of input features\n       D <- ncol(X)\n       # number of categories for classification\n       K <- length(unique(Y))\n       H <-  hidden\n  \n       # create and init weights and bias \n       W1 <- 0.01*matrix(rnorm(D*H), nrow=D, ncol=H)\n       b1 <- matrix(0, nrow=1, ncol=H)\n  \n       W2 <- 0.01*matrix(rnorm(H*K), nrow=H, ncol=K)\n       b2 <- matrix(0, nrow=1, ncol=K)\n  } else {\n       D  <- model$D\n       K  <- model$K\n       H  <- model$H\n       W1 <- model$W1\n       b1 <- model$b1\n       W2 <- model$W2\n       b2 <- model$b2\n  }\n  \n  # use all train data to update weights since it's a small dataset\n  batchsize <- N\n  # init loss to a very big value\n  loss <- 100000\n  \n  # Training the network\n  i <- 0\n  while(i < maxit && loss > abstol ) {\n    \n    # iteration index\n    i <- i +1\n    \n    # forward ....\n    # 1 indicate row, 2 indicate col\n    hidden.layer <- sweep(X %*% W1 ,2, b1, '+')\n    # neurons : ReLU\n    hidden.layer <- pmax(hidden.layer, 0)\n    score <- sweep(hidden.layer %*% W2, 2, b2, '+')\n    \n    # softmax\n    score.exp <- exp(score)\n    # debug\n    probs <- score.exp/rowSums(score.exp)\n    \n    # compute the loss\n    corect.logprobs <- -log(probs[Y.index])\n    data.loss  <- sum(corect.logprobs)/batchsize\n    reg.loss   <- 0.5*reg* (sum(W1*W1) + sum(W2*W2))\n    loss <- data.loss + reg.loss\n    \n    # display results and update model\n    if( i %% display == 0) {\n        if(!is.null(testdata)) {\n            model <- list( D = D,\n                           H = H,\n                           K = K,\n                           # weights and bias\n                           W1 = W1, \n                           b1 = b1, \n                           W2 = W2, \n                           b2 = b2)\n            labs <- predict.dnn(model, testdata[,-y])\n            accuracy <- mean(as.integer(testdata[,y]) == Y.set[labs])\n            cat(i, loss, accuracy, \"\\n\")\n        } else {\n            cat(i, loss, \"\\n\")\n        }\n    }\n    \n    # backward ....\n    dscores <- probs\n    dscores[Y.index] <- dscores[Y.index] -1\n    dscores <- dscores / batchsize\n    \n    \n    dW2 <- t(hidden.layer) %*% dscores \n    db2 <- colSums(dscores)\n    \n    dhidden <- dscores %*% t(W2)\n    dhidden[hidden.layer <= 0] <- 0\n    \n    dW1 <- t(X) %*% dhidden\n    db1 <- colSums(dhidden) \n    \n    # update ....\n    dW2 <- dW2 + reg*W2\n    dW1 <- dW1  + reg*W1\n    \n    W1 <- W1 - lr * dW1\n    b1 <- b1 - lr * db1\n    \n    W2 <- W2 - lr * dW2\n    b2 <- b2 - lr * db2\n    \n   \n    \n  }\n  \n  # final results\n  # creat list to store learned parameters\n  # you can add more parameters for debug and visualization\n  # such as residuals, fitted.values ...\n  model <- list( D = D,\n                 H = H,\n                 K = K,\n                 # weights and bias\n                 W1= W1, \n                 b1= b1, \n                 W2= W2, \n                 b2= b2)\n    \n  return(model)\n}\n\n########################################################################\n# testing\n#######################################################################\nset.seed(1)\n\n# 0. EDA\nsummary(iris)\n#>   Sepal.Length   Sepal.Width    Petal.Length   Petal.Width        Species  \n#>  Min.   :4.30   Min.   :2.00   Min.   :1.00   Min.   :0.1   setosa    :50  \n#>  1st Qu.:5.10   1st Qu.:2.80   1st Qu.:1.60   1st Qu.:0.3   versicolor:50  \n#>  Median :5.80   Median :3.00   Median :4.35   Median :1.3   virginica :50  \n#>  Mean   :5.84   Mean   :3.06   Mean   :3.76   Mean   :1.2                  \n#>  3rd Qu.:6.40   3rd Qu.:3.30   3rd Qu.:5.10   3rd Qu.:1.8                  \n#>  Max.   :7.90   Max.   :4.40   Max.   :6.90   Max.   :2.5\nplot(iris)\n\n# 1. split data into test/train\nsamp <- c(sample(1:50,25), sample(51:100,25), sample(101:150,25))\n\n# 2. train model\n ir.model <- train.dnn(x=1:4, y=5, traindata=iris[samp,], testdata=iris[-samp,], hidden=10, maxit=2000, display=50)\n#> 50 1.1 0.333 \n#> 100 1.1 0.333 \n#> 150 1.09 0.333 \n#> 200 1.08 0.333 \n#> 250 1.05 0.333 \n#> 300 1 0.333 \n#> 350 0.933 0.667 \n#> 400 0.855 0.667 \n#> 450 0.775 0.667 \n#> 500 0.689 0.667 \n#> 550 0.611 0.68 \n#> 600 0.552 0.693 \n#> 650 0.507 0.747 \n#> 700 0.473 0.84 \n#> 750 0.445 0.88 \n#> 800 0.421 0.92 \n#> 850 0.399 0.947 \n#> 900 0.379 0.96 \n#> 950 0.36 0.96 \n#> 1000 0.341 0.973 \n#> 1050 0.324 0.973 \n#> 1100 0.307 0.973 \n#> 1150 0.292 0.973 \n#> 1200 0.277 0.973 \n#> 1250 0.263 0.973 \n#> 1300 0.25 0.973 \n#> 1350 0.238 0.973 \n#> 1400 0.227 0.973 \n#> 1450 0.216 0.973 \n#> 1500 0.207 0.973 \n#> 1550 0.198 0.973 \n#> 1600 0.19 0.973 \n#> 1650 0.183 0.973 \n#> 1700 0.176 0.973 \n#> 1750 0.17 0.973 \n#> 1800 0.164 0.973 \n#> 1850 0.158 0.973 \n#> 1900 0.153 0.973 \n#> 1950 0.149 0.973 \n#> 2000 0.144 0.973\n# ir.model <- train.dnn(x=1:4, y=5, traindata=iris[samp,], hidden=6, maxit=2000, display=50)\n# 3. prediction\n# NOTE: if the predict is factor, we need to transfer the number into class manually.\n#       To make the code clear, I don't write this change into predict.dnn function.\nlabels.dnn <- predict.dnn(ir.model, iris[-samp, -5])\n\n# 4. verify the results\ntable(iris[-samp,5], labels.dnn)\n#>             labels.dnn\n#>               1  2  3\n#>   setosa     25  0  0\n#>   versicolor  0 23  2\n#>   virginica   0  0 25\n#          labels.dnn\n#            1  2  3\n#setosa     25  0  0\n#versicolor  0 24  1\n#virginica   0  0 25\n#accuracy\nmean(as.integer(iris[-samp, 5]) == labels.dnn)\n#> [1] 0.973\n# 0.98\n\n# 5. compare with nnet\nlibrary(nnet)\nird <- data.frame(rbind(iris3[,,1], iris3[,,2], iris3[,,3]),\n                  species = factor(c(rep(\"s\",50), rep(\"c\", 50), rep(\"v\", 50))))\nir.nn2 <- nnet(species ~ ., data = ird, subset = samp, size = 6, rang = 0.1,\n               decay = 1e-2, maxit = 2000)\n#> # weights:  51\n#> initial  value 82.293110 \n#> iter  10 value 29.196376\n#> iter  20 value 5.446284\n#> iter  30 value 4.782022\n#> iter  40 value 4.379729\n#> iter  50 value 4.188725\n#> iter  60 value 4.120587\n#> iter  70 value 4.091706\n#> iter  80 value 4.086017\n#> iter  90 value 4.081664\n#> iter 100 value 4.074111\n#> iter 110 value 4.072894\n#> iter 120 value 4.069012\n#> iter 130 value 4.067691\n#> iter 140 value 4.067633\n#> final  value 4.067633 \n#> converged\n\n           \nlabels.nnet <- predict(ir.nn2, ird[-samp,], type=\"class\")\ntable(ird$species[-samp], labels.nnet)\n#>    labels.nnet\n#>      c  s  v\n#>   c 23  0  2\n#>   s  0 25  0\n#>   v  0  0 25\n#  labels.nnet\n#   c  s  v\n#c 22  0  3\n#s  0 25  0\n#v  3  0 22\n\n# accuracy\nmean(ird$species[-samp] == labels.nnet)\n#> [1] 0.973\n# 0.96\n\n# Visualization\n# the output from screen, copy and paste here.\ndata1 <- (\"i loss accuracy\n50 1.098421 0.3333333 \n100 1.098021 0.3333333 \n150 1.096843 0.3333333 \n200 1.093393 0.3333333 \n250 1.084069 0.3333333 \n300 1.063278 0.3333333 \n350 1.027273 0.3333333 \n400 0.9707605 0.64 \n450 0.8996356 0.6666667 \n500 0.8335469 0.6666667 \n550 0.7662386 0.6666667 \n600 0.6914156 0.6666667 \n650 0.6195753 0.68 \n700 0.5620381 0.68 \n750 0.5184008 0.7333333 \n800 0.4844815 0.84 \n850 0.4568258 0.8933333 \n900 0.4331083 0.92 \n950 0.4118948 0.9333333 \n1000 0.392368 0.96 \n1050 0.3740457 0.96 \n1100 0.3566594 0.96 \n1150 0.3400993 0.9866667 \n1200 0.3243276 0.9866667 \n1250 0.3093422 0.9866667 \n1300 0.2951787 0.9866667 \n1350 0.2818472 0.9866667 \n1400 0.2693641 0.9866667 \n1450 0.2577245 0.9866667 \n1500 0.2469068 0.9866667 \n1550 0.2368819 0.9866667 \n1600 0.2276124 0.9866667 \n1650 0.2190535 0.9866667 \n1700 0.2111565 0.9866667 \n1750 0.2038719 0.9866667 \n1800 0.1971507 0.9866667 \n1850 0.1909452 0.9866667 \n1900 0.1852105 0.9866667 \n1950 0.1799045 0.9866667 \n2000 0.1749881 0.9866667  \")\n\ndata.v <- read.table(text=data1, header=T)\npar(mar=c(5.1, 4.1, 4.1, 4.1))\nplot(x=data.v$i, y=data.v$loss, type=\"o\", col=\"blue\", pch=16, \n     main=\"IRIS loss and accuracy by 2-layers DNN\",\n     ylim=c(0, 1.2),\n     xlab=\"\",\n     ylab=\"\",\n     axe =F)\nlines(x=data.v$i, y=data.v$accuracy, type=\"o\", col=\"red\", pch=1)\nbox()\naxis(1, at=seq(0,2000,by=200))\naxis(4, at=seq(0,1.0,by=0.1))\naxis(2, at=seq(0,1.2,by=0.1))\nmtext(\"training step\", 1, line=3)\nmtext(\"loss of training set\", 2, line=2.5)\nmtext(\"accuracy of testing set\", 4, line=2)\n\nlegend(\"bottomleft\", \n       legend = c(\"loss\", \"accuracy\"),\n       pch = c(16,1),\n       col = c(\"blue\",\"red\"),\n       lwd=c(1,1)\n)"},{"path":"tuning-hyperparameters-in-a-neural-network.html","id":"tuning-hyperparameters-in-a-neural-network","chapter":"45 Tuning Hyperparameters in a Neural Network","heading":"45 Tuning Hyperparameters in a Neural Network","text":"Datasets: yacht_hydrodynamics.dataAlgorithms:\nNeural Networks\nNeural NetworksTechniques:\nRegression Hyperparameters Tuning\nRegression Hyperparameters Tuning","code":""},{"path":"tuning-hyperparameters-in-a-neural-network.html","id":"introduction-25","chapter":"45 Tuning Hyperparameters in a Neural Network","heading":"45.1 Introduction","text":"Regression ANNs predict output variable function inputs. input features (independent variables) can categorical numeric types, however, regression ANNs, require numeric dependent variable. output variable categorical variable (binary) ANN function classifier (see next tutorial).Source: http://uc-r.github.io/ann_regressionIn tutorial introduce neural network used numeric predictions cover:Replication requirements: ’ll need reproduce analysis tutorial.Data Preparation: Preparing data.1st Regression ANN: Constructing 1-hidden layer ANN 1 neuron.Regression Hyperparameters: Tuning model.Wrapping : Final comments exercises test skills.","code":""},{"path":"tuning-hyperparameters-in-a-neural-network.html","id":"replication-requirements","chapter":"45 Tuning Hyperparameters in a Neural Network","heading":"45.2 Replication Requirements","text":"require following packages analysis.","code":"\nlibrary(tidyverse)\n#> ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──\n#> ✔ ggplot2 3.3.0     ✔ purrr   0.3.4\n#> ✔ tibble  3.0.1     ✔ dplyr   0.8.5\n#> ✔ tidyr   1.0.2     ✔ stringr 1.4.0\n#> ✔ readr   1.3.1     ✔ forcats 0.5.0\n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\nlibrary(neuralnet)\n#> \n#> Attaching package: 'neuralnet'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     compute\nlibrary(GGally)\n#> Registered S3 method overwritten by 'GGally':\n#>   method from   \n#>   +.gg   ggplot2\n#> \n#> Attaching package: 'GGally'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     nasa"},{"path":"tuning-hyperparameters-in-a-neural-network.html","id":"data-preparation","chapter":"45 Tuning Hyperparameters in a Neural Network","heading":"45.3 Data Preparation","text":"regression ANN use Yacht Hydrodynamics data set UCI’s Machine Learning Repository. yacht data provided Dr. Roberto Lopez email. data set contains data contains results 308 full-scale experiments performed Delft Ship Hydromechanics Laboratory test 22 different hull forms. experiment tested effect variations hull geometry ship’s Froude number craft’s residuary resistance per unit weight displacement.begin download data UCI.Prior data analysis lets take look data set.see excellent summary variation feature data set. Draw attention bottom-strip scatter-plots. shows residuary resistance function data set features (independent experimental values). greatest variation appears Froude Number feature. interesting see pattern appears subsequent regression ANNs.Prior regression ANN construction first must split Yacht data set test training data sets. split, first scale feature fall \n[0,1] interval.scale01() function maps data observation onto [0,1] interval called dplyr mutate_all() function. provided seed reproducible results randomly extracted (without replacement) 80% observations build Yacht_Data_Train data set. Using dplyr’s anti_join() function extracted observations within Yacht_Data_Train data set test data set Yacht_Data_Test.","code":"\nurl <- 'http://archive.ics.uci.edu/ml/machine-learning-databases/00243/yacht_hydrodynamics.data'\n\nYacht_Data <- read_table(file = url,\n                         col_names = c('LongPos_COB', 'Prismatic_Coeff',\n                                       'Len_Disp_Ratio', 'Beam_Draut_Ratio', \n                                       'Length_Beam_Ratio','Froude_Num', \n                                       'Residuary_Resist')) %>%\n  na.omit()\n#> Parsed with column specification:\n#> cols(\n#>   LongPos_COB = col_double(),\n#>   Prismatic_Coeff = col_double(),\n#>   Len_Disp_Ratio = col_double(),\n#>   Beam_Draut_Ratio = col_double(),\n#>   Length_Beam_Ratio = col_double(),\n#>   Froude_Num = col_double(),\n#>   Residuary_Resist = col_double()\n#> )\ndplyr::glimpse(Yacht_Data)\n#> Rows: 308\n#> Columns: 7\n#> $ LongPos_COB       <dbl> -2.3, -2.3, -2.3, -2.3, -2.3, -2.3, -2.3, -2.3, -2.…\n#> $ Prismatic_Coeff   <dbl> 0.568, 0.568, 0.568, 0.568, 0.568, 0.568, 0.568, 0.…\n#> $ Len_Disp_Ratio    <dbl> 4.78, 4.78, 4.78, 4.78, 4.78, 4.78, 4.78, 4.78, 4.7…\n#> $ Beam_Draut_Ratio  <dbl> 3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.9…\n#> $ Length_Beam_Ratio <dbl> 3.17, 3.17, 3.17, 3.17, 3.17, 3.17, 3.17, 3.17, 3.1…\n#> $ Froude_Num        <dbl> 0.125, 0.150, 0.175, 0.200, 0.225, 0.250, 0.275, 0.…\n#> $ Residuary_Resist  <dbl> 0.11, 0.27, 0.47, 0.78, 1.18, 1.82, 2.61, 3.76, 4.9…\n# save the dataset locally\nwrite.csv(Yacht_Data, file = file.path(data_raw_dir, \"yach_data.csv\"))\nggpairs(Yacht_Data, title = \"Scatterplot Matrix of the Features of the Yacht Data Set\")\n# Scale the Data\nscale01 <- function(x){\n  (x - min(x)) / (max(x) - min(x))\n}\n\nYacht_Data <- Yacht_Data %>%\n  mutate_all(scale01)\n\n# Split into test and train sets\nset.seed(12345)\nYacht_Data_Train <- sample_frac(tbl = Yacht_Data, replace = FALSE, size = 0.80)\nYacht_Data_Test <- anti_join(Yacht_Data, Yacht_Data_Train)\n#> Joining, by = c(\"LongPos_COB\", \"Prismatic_Coeff\", \"Len_Disp_Ratio\", \"Beam_Draut_Ratio\", \"Length_Beam_Ratio\", \"Froude_Num\", \"Residuary_Resist\")"},{"path":"tuning-hyperparameters-in-a-neural-network.html","id":"st-regression-ann","chapter":"45 Tuning Hyperparameters in a Neural Network","heading":"45.4 1st Regression ANN","text":"begin construct 1-hidden layer ANN 1 neuron, simplest neural networks.Yacht_NN1 list containing parameters regression ANN well results neural network test data set. view diagram Yacht_NN1 use plot() function.plot shows weights learned Yacht_NN1 neural network, displays number iterations convergence, well SSE training data set. manually compute SSE can use following:SSE error associated training data set. superior metric estimating generalization capability ANN SSE test data set. Recall, test data set contains observations used train Yacht_NN1 ANN. calculate test error, first must run test observations Yacht_NN1 ANN. accomplished neuralnet package compute() function, takes first input desired neural network object created neuralnet() function, second argument test data set feature (independent variable(s)) values.compute() function outputs response variable, case Residuary_Resist, estimated neural network. ANN estimated response can compute test SSE. Comparing test error 0.0084 training error 0.0361 see case test error smaller training error.","code":"\nset.seed(12321)\nYacht_NN1 <- neuralnet(Residuary_Resist ~ LongPos_COB + Prismatic_Coeff + \n                         Len_Disp_Ratio + Beam_Draut_Ratio + Length_Beam_Ratio +\n                         Froude_Num, data = Yacht_Data_Train)\nplot(Yacht_NN1, rep = 'best')\nNN1_Train_SSE <- sum((Yacht_NN1$net.result - Yacht_Data_Train[, 7])^2)/2\npaste(\"SSE: \", round(NN1_Train_SSE, 4))\n#> [1] \"SSE:  0.0365\"\n## [1] \"SSE:  0.0361\"\nTest_NN1_Output <- compute(Yacht_NN1, Yacht_Data_Test[, 1:6])$net.result\nNN1_Test_SSE <- sum((Test_NN1_Output - Yacht_Data_Test[, 7])^2)/2\nNN1_Test_SSE\n#> [1] 0.0139\n## [1] 0.008417631461"},{"path":"tuning-hyperparameters-in-a-neural-network.html","id":"regression-hyperparameters","chapter":"45 Tuning Hyperparameters in a Neural Network","heading":"45.5 Regression Hyperparameters","text":"constructed basic regression ANNs without modifying default hyperparameters associated neuralnet() function. try improve network modifying basic structure hyperparameter modification. begin add depth hidden layer network, change activation function logistic tangent hyperbolicus (tanh) determine modifications can improve test data set SSE. using tanh activation function, first must rescale data \\([0,1]\\) \\([-1,1]\\) using rescale package. purposes exercise use random seed reproducible results, generally best practice.evident plot, see best regression ANN found Yacht_NN2 training test SSE 0.0188 0.0057. make determination value training test SSEs . Yacht_NN2’s structure presented :setting seed, prior running 10 repetitions ANNs, force software reproduce exact Yacht_NN2 ANN first replication. subsequent 9 generated ANNs, use different random set starting weights. Comparing ‘best’ 10 repetitions, Yacht_NN2, observe decrease training set error indicating superior set weights.","code":"\n# 2-Hidden Layers, Layer-1 4-neurons, Layer-2, 1-neuron, logistic activation\n# function\nset.seed(12321)\nYacht_NN2 <- neuralnet(Residuary_Resist ~ LongPos_COB + Prismatic_Coeff + Len_Disp_Ratio + Beam_Draut_Ratio + Length_Beam_Ratio + Froude_Num, \n                       data = Yacht_Data_Train, \n                       hidden = c(4, 1), \n                       act.fct = \"logistic\")\n\n## Training Error\nNN2_Train_SSE <- sum((Yacht_NN2$net.result - Yacht_Data_Train[, 7])^2)/2\n\n## Test Error\nTest_NN2_Output <- compute(Yacht_NN2, Yacht_Data_Test[, 1:6])$net.result\nNN2_Test_SSE <- sum((Test_NN2_Output - Yacht_Data_Test[, 7])^2)/2\n\n# Rescale for tanh activation function\nscale11 <- function(x) {\n    (2 * ((x - min(x))/(max(x) - min(x)))) - 1\n}\nYacht_Data_Train <- Yacht_Data_Train %>% mutate_all(scale11)\nYacht_Data_Test <- Yacht_Data_Test %>% mutate_all(scale11)\n\n# 2-Hidden Layers, Layer-1 4-neurons, Layer-2, 1-neuron, tanh activation\n# function\nset.seed(12321)\nYacht_NN3 <- neuralnet(Residuary_Resist ~ LongPos_COB + Prismatic_Coeff + Len_Disp_Ratio + Beam_Draut_Ratio + Length_Beam_Ratio + Froude_Num, \n                       data = Yacht_Data_Train, \n                       hidden = c(4, 1), \n                       act.fct = \"tanh\")\n\n## Training Error\nNN3_Train_SSE <- sum((Yacht_NN3$net.result - Yacht_Data_Train[, 7])^2)/2\n\n## Test Error\nTest_NN3_Output <- compute(Yacht_NN3, Yacht_Data_Test[, 1:6])$net.result\nNN3_Test_SSE <- sum((Test_NN3_Output - Yacht_Data_Test[, 7])^2)/2\n\n# 1-Hidden Layer, 1-neuron, tanh activation function\nset.seed(12321)\nYacht_NN4 <- neuralnet(Residuary_Resist ~ LongPos_COB + Prismatic_Coeff + Len_Disp_Ratio + Beam_Draut_Ratio + Length_Beam_Ratio + Froude_Num, \n                       data = Yacht_Data_Train, \n                       act.fct = \"tanh\")\n\n## Training Error\nNN4_Train_SSE <- sum((Yacht_NN4$net.result - Yacht_Data_Train[, 7])^2)/2\n\n## Test Error\nTest_NN4_Output <- compute(Yacht_NN4, Yacht_Data_Test[, 1:6])$net.result\nNN4_Test_SSE <- sum((Test_NN4_Output - Yacht_Data_Test[, 7])^2)/2\n# Bar plot of results\nRegression_NN_Errors <- tibble(Network = rep(c(\"NN1\", \"NN2\", \"NN3\", \"NN4\"), each = 2), \n                               DataSet = rep(c(\"Train\", \"Test\"), time = 4), \n                               SSE = c(NN1_Train_SSE, NN1_Test_SSE, \n                                       NN2_Train_SSE, NN2_Test_SSE, \n                                       NN3_Train_SSE, NN3_Test_SSE, \n                                       NN4_Train_SSE, NN4_Test_SSE))\n\nRegression_NN_Errors %>% \n  ggplot(aes(Network, SSE, fill = DataSet)) + \n  geom_col(position = \"dodge\") + \n  ggtitle(\"Regression ANN's SSE\")\nplot(Yacht_NN2, rep = \"best\")\nset.seed(12321)\nYacht_NN2 <- neuralnet(Residuary_Resist ~ LongPos_COB + Prismatic_Coeff + Len_Disp_Ratio + Beam_Draut_Ratio + Length_Beam_Ratio + Froude_Num, \n                       data = Yacht_Data_Train, \n                       hidden = c(4, 1), \n                       act.fct = \"logistic\", \n                       rep = 10)\n\nplot(Yacht_NN2, rep = \"best\")"},{"path":"tuning-hyperparameters-in-a-neural-network.html","id":"wrapping-up-2","chapter":"45 Tuning Hyperparameters in a Neural Network","heading":"45.6 Wrapping Up","text":"briefly covered regression ANNs tutorial. next tutorial cover classification ANNs. neuralnet package used tutorial one many tools available ANN implementation R. Others include:nnetautoencodercaretRSNNSh2oBefore move next tutorial, test new knowledge exercises follow.split yacht data training test data sets?split yacht data training test data sets?Re-load Yacht Data UCI Machine learning repository yacht data without scaling. Run regression ANN. happens? think happens?Re-load Yacht Data UCI Machine learning repository yacht data without scaling. Run regression ANN. happens? think happens?completing exercise question 1, re-scale yacht data. Perform simple linear regression fitting Residuary_Resist function features. Now run regression neural network (see 1st Regression ANN section). Plot regression ANN compare weights features ANN p-values regressors.completing exercise question 1, re-scale yacht data. Perform simple linear regression fitting Residuary_Resist function features. Now run regression neural network (see 1st Regression ANN section). Plot regression ANN compare weights features ANN p-values regressors.Build regression ANN using scaled yacht data modifying one hyperparameter. Use ?neuralnet see function options. Plot ANN.Build regression ANN using scaled yacht data modifying one hyperparameter. Use ?neuralnet see function options. Plot ANN.","code":""},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"deep-learning-tips-for-classification-and-regression","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46 Deep Learning tips for Classification and Regression","text":"Datasets: spiral.csv, grid.csv, covtype.full.csvAlgorithms:\nDeep Learning h2o\nDeep Learning h2oTechniques:\nDecision Boundaries\nHyper-parameter Tuning Grid Search\nCheckpointing\nCross-Validation\nDecision BoundariesHyper-parameter Tuning Grid SearchCheckpointingCross-Validation","code":""},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"introduction-26","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46.1 Introduction","text":"Source: http://docs.h2o.ai/h2o-tutorials/latest-stable/tutorials/deeplearning/index.htmlRepo: https://github.com/h2oai/h2o-tutorialsThis tutorial shows H2O Deep Learning model can used supervised classification regression. great tutorial Deep Learning given Quoc Le . tutorial covers usage H2O R. python version tutorial available well separate document. file available plain R, R markdown regular markdown formats, plots available PDF files. documents available Github.run plain R, execute R directory script. run RStudio, sure setwd() location script.h2o.init() starts H2O R’s current working directory. h2o.importFile() looks files perspective H2O started.examples explanations can found H2O Deep Learning booklet H2O Github Repository. PDF slide deck can found Github.","code":""},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"h2o-r-package","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46.2 H2O R Package","text":"Load H2O R package:Source: http://docs.h2o.ai/h2o-tutorials/latest-stable/tutorials/deeplearning/index.html","code":"\n## R installation instructions are at http://h2o.ai/download\nlibrary(h2o)\n#> \n#> ----------------------------------------------------------------------\n#> \n#> Your next step is to start H2O:\n#>     > h2o.init()\n#> \n#> For H2O package documentation, ask for help:\n#>     > ??h2o\n#> \n#> After starting H2O, you can use the Web UI at http://localhost:54321\n#> For more information visit http://docs.h2o.ai\n#> \n#> ----------------------------------------------------------------------\n#> \n#> Attaching package: 'h2o'\n#> The following objects are masked from 'package:stats':\n#> \n#>     cor, sd, var\n#> The following objects are masked from 'package:base':\n#> \n#>     &&, %*%, %in%, ||, apply, as.factor, as.numeric, colnames,\n#>     colnames<-, ifelse, is.character, is.factor, is.numeric, log,\n#>     log10, log1p, log2, round, signif, trunc"},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"start-h2o","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46.3 Start H2O","text":"Start 1-node H2O server local machine, allow use CPU cores 2GB memory:h2o.deeplearning function fits H2O’s Deep Learning models within R. can run example man page using example function, run longer demonstration h2o package using demo function::H2O Deep Learning many parameters, designed just easy use supervised training methods H2O. Early stopping, automatic data standardization handling categorical variables missing values adaptive learning rates (per weight) reduce amount parameters user specify. Often, ’s just number sizes hidden layers, number epochs activation function maybe regularization techniques.","code":"\nh2o.init(nthreads=-1, max_mem_size=\"2G\")\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         36 minutes 39 seconds \n#>     H2O cluster timezone:       Etc/UTC \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.30.0.1 \n#>     H2O cluster version age:    7 months and 16 days !!! \n#>     H2O cluster name:           H2O_started_from_R_root_mwl453 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   7.04 GB \n#>     H2O cluster total cores:    8 \n#>     H2O cluster allowed cores:  8 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4 \n#>     R Version:                  R version 3.6.3 (2020-02-29)\n#> Warning in h2o.clusterInfo(): \n#> Your H2O cluster version is too old (7 months and 16 days)!\n#> Please download and install the latest version from http://h2o.ai/download/\nh2o.removeAll() ## clean slate - just in case the cluster was already running\nargs(h2o.deeplearning)\n#> function (x, y, training_frame, model_id = NULL, validation_frame = NULL, \n#>     nfolds = 0, keep_cross_validation_models = TRUE, keep_cross_validation_predictions = FALSE, \n#>     keep_cross_validation_fold_assignment = FALSE, fold_assignment = c(\"AUTO\", \n#>         \"Random\", \"Modulo\", \"Stratified\"), fold_column = NULL, \n#>     ignore_const_cols = TRUE, score_each_iteration = FALSE, weights_column = NULL, \n#>     offset_column = NULL, balance_classes = FALSE, class_sampling_factors = NULL, \n#>     max_after_balance_size = 5, max_hit_ratio_k = 0, checkpoint = NULL, \n#>     pretrained_autoencoder = NULL, overwrite_with_best_model = TRUE, \n#>     use_all_factor_levels = TRUE, standardize = TRUE, activation = c(\"Tanh\", \n#>         \"TanhWithDropout\", \"Rectifier\", \"RectifierWithDropout\", \n#>         \"Maxout\", \"MaxoutWithDropout\"), hidden = c(200, 200), \n#>     epochs = 10, train_samples_per_iteration = -2, target_ratio_comm_to_comp = 0.05, \n#>     seed = -1, adaptive_rate = TRUE, rho = 0.99, epsilon = 1e-08, \n#>     rate = 0.005, rate_annealing = 1e-06, rate_decay = 1, momentum_start = 0, \n#>     momentum_ramp = 1e+06, momentum_stable = 0, nesterov_accelerated_gradient = TRUE, \n#>     input_dropout_ratio = 0, hidden_dropout_ratios = NULL, l1 = 0, \n#>     l2 = 0, max_w2 = 3.4028235e+38, initial_weight_distribution = c(\"UniformAdaptive\", \n#>         \"Uniform\", \"Normal\"), initial_weight_scale = 1, initial_weights = NULL, \n#>     initial_biases = NULL, loss = c(\"Automatic\", \"CrossEntropy\", \n#>         \"Quadratic\", \"Huber\", \"Absolute\", \"Quantile\"), distribution = c(\"AUTO\", \n#>         \"bernoulli\", \"multinomial\", \"gaussian\", \"poisson\", \"gamma\", \n#>         \"tweedie\", \"laplace\", \"quantile\", \"huber\"), quantile_alpha = 0.5, \n#>     tweedie_power = 1.5, huber_alpha = 0.9, score_interval = 5, \n#>     score_training_samples = 10000, score_validation_samples = 0, \n#>     score_duty_cycle = 0.1, classification_stop = 0, regression_stop = 1e-06, \n#>     stopping_rounds = 5, stopping_metric = c(\"AUTO\", \"deviance\", \n#>         \"logloss\", \"MSE\", \"RMSE\", \"MAE\", \"RMSLE\", \"AUC\", \"AUCPR\", \n#>         \"lift_top_group\", \"misclassification\", \"mean_per_class_error\", \n#>         \"custom\", \"custom_increasing\"), stopping_tolerance = 0, \n#>     max_runtime_secs = 0, score_validation_sampling = c(\"Uniform\", \n#>         \"Stratified\"), diagnostics = TRUE, fast_mode = TRUE, \n#>     force_load_balance = TRUE, variable_importances = TRUE, replicate_training_data = TRUE, \n#>     single_node_mode = FALSE, shuffle_training_data = FALSE, \n#>     missing_values_handling = c(\"MeanImputation\", \"Skip\"), quiet_mode = FALSE, \n#>     autoencoder = FALSE, sparse = FALSE, col_major = FALSE, average_activation = 0, \n#>     sparsity_beta = 0, max_categorical_features = 2147483647, \n#>     reproducible = FALSE, export_weights_and_biases = FALSE, \n#>     mini_batch_size = 1, categorical_encoding = c(\"AUTO\", \"Enum\", \n#>         \"OneHotInternal\", \"OneHotExplicit\", \"Binary\", \"Eigen\", \n#>         \"LabelEncoder\", \"SortByResponse\", \"EnumLimited\"), elastic_averaging = FALSE, \n#>     elastic_averaging_moving_rate = 0.9, elastic_averaging_regularization = 0.001, \n#>     export_checkpoints_dir = NULL, verbose = FALSE) \n#> NULL\nif (interactive()) help(h2o.deeplearning)\nexample(h2o.deeplearning)\n#> \n#> h2.dpl> ## Not run: \n#> h2.dpl> ##D library(h2o)\n#> h2.dpl> ##D h2o.init()\n#> h2.dpl> ##D iris_hf <- as.h2o(iris)\n#> h2.dpl> ##D iris_dl <- h2o.deeplearning(x = 1:4, y = 5, training_frame = iris_hf, seed=123456)\n#> h2.dpl> ##D \n#> h2.dpl> ##D # now make a prediction\n#> h2.dpl> ##D predictions <- h2o.predict(iris_dl, iris_hf)\n#> h2.dpl> ## End(Not run)\n#> h2.dpl> \n#> h2.dpl> \n#> h2.dpl>\nif (interactive()) demo(h2o.deeplearning)  #requires user interaction"},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"lets-have-some-fun-first-decision-boundaries","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46.4 Let’s have some fun first: Decision Boundaries","text":"start small dataset representing red black dots plane, arranged shape two nested spirals. task H2O’s machine learning methods separate red black dots, .e., recognize spiral assigning point plane one two spirals.visualize nature H2O Deep Learning (DL), H2O’s tree methods (GBM/DRF) H2O’s generalized linear modeling (GLM) plotting decision boundary red black spirals:build different models:can see network learns structure spirals enough training time. explore different network architectures next:clear different configurations can achieve similar performance, tuning required optimal performance. Next, compare different activation functions, including one 50% dropout regularization hidden layers:Clearly, dropout rate high number epochs low last configuration, often ends performing best larger datasets generalization important.information parameters can found H2O Deep Learning booklet.","code":"# setwd(\"~/h2o-tutorials/tutorials/deeplearning\") ##For RStudio\nspiral <- h2o.importFile(path = normalizePath(file.path(data_raw_dir, \"spiral.csv\")))\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\ngrid   <- h2o.importFile(path = normalizePath(file.path(data_raw_dir, \"grid.csv\")))\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |======================================================================| 100%\n\n# Define helper to plot contours\nplotC <- function(name, model, data=spiral, g=grid) {\n  data <- as.data.frame(data) #get data from into R\n  pred <- as.data.frame(h2o.predict(model, g))\n  n=0.5*(sqrt(nrow(g))-1); d <- 1.5; h <- d*(-n:n)/n\n  plot(data[,-3],pch=19,col=data[,3],cex=0.5,\n       xlim=c(-d,d),ylim=c(-d,d),main=name)\n  contour(h,h,z=array(ifelse(pred[,1]==\"Red\",0,1),\n          dim=c(2*n+1,2*n+1)),col=\"blue\",lwd=2,add=T)\n}\n#dev.new(noRStudioGD=FALSE) #direct plotting output to a new window\npar(mfrow=c(2,2)) #set up the canvas for 2x2 plots\nplotC( \"DL\", h2o.deeplearning(1:2,3,spiral,epochs=1e3))\nplotC(\"GBM\", h2o.gbm         (1:2,3,spiral))\nplotC(\"DRF\", h2o.randomForest(1:2,3,spiral))\nplotC(\"GLM\", h2o.glm         (1:2,3,spiral,family=\"binomial\"))\n#dev.new(noRStudioGD=FALSE) #direct plotting output to a new window\npar(mfrow=c(2,2)) #set up the canvas for 2x2 plots\nep <- c(1,250,500,750)\nplotC(paste0(\"DL \",ep[1],\" epochs\"),\n      h2o.deeplearning(1:2,3,spiral,epochs=ep[1],\n                              model_id=\"dl_1\"))\nplotC(paste0(\"DL \",ep[2],\" epochs\"),\n      h2o.deeplearning(1:2,3,spiral,epochs=ep[2],\n            checkpoint=\"dl_1\",model_id=\"dl_2\"))\nplotC(paste0(\"DL \",ep[3],\" epochs\"),\n      h2o.deeplearning(1:2,3,spiral,epochs=ep[3],\n            checkpoint=\"dl_2\",model_id=\"dl_3\"))\nplotC(paste0(\"DL \",ep[4],\" epochs\"),\n      h2o.deeplearning(1:2,3,spiral,epochs=ep[4],\n            checkpoint=\"dl_3\",model_id=\"dl_4\"))\n#dev.new(noRStudioGD=FALSE) #direct plotting output to a new window\npar(mfrow=c(2,2)) #set up the canvas for 2x2 plots\nfor (hidden in list(c(11,13,17,19),c(42,42,42),c(200,200),c(1000))) {\n  plotC(paste0(\"DL hidden=\",paste0(hidden, collapse=\"x\")),\n        h2o.deeplearning(1:2,3 ,spiral, hidden=hidden, epochs=500))\n}\n#dev.new(noRStudioGD=FALSE) #direct plotting output to a new window\npar(mfrow=c(2,2)) #set up the canvas for 2x2 plots\n\nfor (act in c(\"Tanh\", \"Maxout\", \"Rectifier\", \"RectifierWithDropout\")) {\n  plotC(paste0(\"DL \",act,\" activation\"), \n        h2o.deeplearning(1:2,3, spiral,\n              activation = act, \n              hidden = c(100,100), \n              epochs = 1000))\n}"},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"cover-type-dataset","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46.5 Cover Type Dataset","text":"important full cover type dataset (581k rows, 13 columns, 10 numerical, 3 categorical). also split data 3 ways: 60% training, 20% validation (hyper parameter tuning) 20% final testing.’s scalable way scatter plots via binning (works categorical numeric columns) get familiar dataset.","code":"df <- h2o.importFile(path = normalizePath(file.path(data_raw_dir, \"covtype.full.csv\")))\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\ndim(df)\n#> [1] 581012     13\ndf\n#>   Elevation Aspect Slope Horizontal_Distance_To_Hydrology\n#> 1      3066    124     5                                0\n#> 2      3136     32    20                              450\n#> 3      2655     28    14                               42\n#> 4      3191     45    19                              323\n#> 5      3217     80    13                               30\n#> 6      3119    293    13                               30\n#>   Vertical_Distance_To_Hydrology Horizontal_Distance_To_Roadways Hillshade_9am\n#> 1                              0                            1533           229\n#> 2                            -38                            1290           211\n#> 3                              8                            1890           214\n#> 4                             88                            3932           221\n#> 5                              1                            3901           237\n#> 6                             10                            4810           182\n#>   Hillshade_Noon Hillshade_3pm Horizontal_Distance_To_Fire_Points\n#> 1            236           141                                459\n#> 2            193           111                               1112\n#> 3            209           128                               1001\n#> 4            195           100                               2919\n#> 5            217           109                               2859\n#> 6            237           194                               1200\n#>   Wilderness_Area Soil_Type Cover_Type\n#> 1          area_0   type_22    class_1\n#> 2          area_0   type_28    class_1\n#> 3          area_2    type_9    class_2\n#> 4          area_0   type_39    class_2\n#> 5          area_0   type_22    class_7\n#> 6          area_0   type_21    class_1\n#> \n#> [581012 rows x 13 columns]\nsplits <- h2o.splitFrame(df, c(0.6, 0.2), seed=1234)\ntrain  <- h2o.assign(splits[[1]], \"train.hex\") # 60%\nvalid  <- h2o.assign(splits[[2]], \"valid.hex\") # 20%\ntest   <- h2o.assign(splits[[3]], \"test.hex\")  # 20%\n#dev.new(noRStudioGD=FALSE) #direct plotting output to a new window\npar(mfrow=c(1,1)) # reset canvas\nplot(h2o.tabulate(df, \"Elevation\",                       \"Cover_Type\"))\nplot(h2o.tabulate(df, \"Horizontal_Distance_To_Roadways\", \"Cover_Type\"))\nplot(h2o.tabulate(df, \"Soil_Type\",                       \"Cover_Type\"))\nplot(h2o.tabulate(df, \"Horizontal_Distance_To_Roadways\", \"Elevation\" ))"},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"first-run-of-h2o-deep-learning","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46.5.1 First Run of H2O Deep Learning","text":"Let’s run first Deep Learning model covtype dataset. want predict Cover_Type column, categorical feature 7 levels, Deep Learning model tasked perform (multi-class) classification. uses 12 predictors dataset, 10 numerical, 2 categorical total 44 levels. can expect Deep Learning model 56 input neurons (automatic one-hot encoding).keep fast, run one epoch (one pass training data).Inspect model Flow information model building etc. issuing cell content getModel “dl_model_first”, pressing Ctrl-Enter.","code":"\nresponse <- \"Cover_Type\"\npredictors <- setdiff(names(df), response)\npredictors\n#>  [1] \"Elevation\"                          \"Aspect\"                            \n#>  [3] \"Slope\"                              \"Horizontal_Distance_To_Hydrology\"  \n#>  [5] \"Vertical_Distance_To_Hydrology\"     \"Horizontal_Distance_To_Roadways\"   \n#>  [7] \"Hillshade_9am\"                      \"Hillshade_Noon\"                    \n#>  [9] \"Hillshade_3pm\"                      \"Horizontal_Distance_To_Fire_Points\"\n#> [11] \"Wilderness_Area\"                    \"Soil_Type\"\ntrain_df <- as.data.frame(train)\nstr(train_df)\n#> 'data.frame':    349015 obs. of  13 variables:\n#>  $ Elevation                         : int  3136 3217 3119 2679 3261 2885 3227 2843 2853 2883 ...\n#>  $ Aspect                            : int  32 80 293 48 322 26 32 12 124 177 ...\n#>  $ Slope                             : int  20 13 13 7 13 9 6 18 12 9 ...\n#>  $ Horizontal_Distance_To_Hydrology  : int  450 30 30 150 30 192 108 335 30 426 ...\n#>  $ Vertical_Distance_To_Hydrology    : int  -38 1 10 24 5 38 13 50 -5 126 ...\n#>  $ Horizontal_Distance_To_Roadways   : int  1290 3901 4810 1588 5701 3271 5542 2642 1485 2139 ...\n#>  $ Hillshade_9am                     : int  211 237 182 223 186 216 219 199 240 225 ...\n#>  $ Hillshade_Noon                    : int  193 217 237 224 226 220 227 201 231 246 ...\n#>  $ Hillshade_3pm                     : int  111 109 194 136 180 140 145 135 119 153 ...\n#>  $ Horizontal_Distance_To_Fire_Points: int  1112 2859 1200 6265 769 2643 765 1719 2497 713 ...\n#>  $ Wilderness_Area                   : Factor w/ 4 levels \"area_0\",\"area_1\",..: 1 1 1 1 1 1 1 3 3 3 ...\n#>  $ Soil_Type                         : Factor w/ 40 levels \"type_0\",\"type_1\",..: 22 16 15 4 15 22 15 27 12 25 ...\n#>  $ Cover_Type                        : Factor w/ 7 levels \"class_1\",\"class_2\",..: 1 7 1 2 1 2 1 2 1 2 ...\nvalid_df <- as.data.frame(valid)\nstr(valid_df)\n#> 'data.frame':    116018 obs. of  13 variables:\n#>  $ Elevation                         : int  3066 2655 2902 2994 2697 2990 3237 2884 2972 2696 ...\n#>  $ Aspect                            : int  124 28 304 61 93 59 135 71 100 169 ...\n#>  $ Slope                             : int  5 14 22 9 9 12 14 9 4 10 ...\n#>  $ Horizontal_Distance_To_Hydrology  : int  0 42 511 391 306 108 240 459 175 323 ...\n#>  $ Vertical_Distance_To_Hydrology    : int  0 8 18 57 -2 10 -11 141 13 149 ...\n#>  $ Horizontal_Distance_To_Roadways   : int  1533 1890 1273 4286 553 2190 1189 1214 5031 2452 ...\n#>  $ Hillshade_9am                     : int  229 214 155 227 234 229 241 231 227 228 ...\n#>  $ Hillshade_Noon                    : int  236 209 223 222 227 215 233 222 234 244 ...\n#>  $ Hillshade_3pm                     : int  141 128 206 128 125 117 118 124 142 148 ...\n#>  $ Horizontal_Distance_To_Fire_Points: int  459 1001 1347 1928 1716 1048 2748 1355 6198 1044 ...\n#>  $ Wilderness_Area                   : Factor w/ 4 levels \"area_0\",\"area_1\",..: 1 3 3 1 1 3 1 3 1 3 ...\n#>  $ Soil_Type                         : Factor w/ 39 levels \"type_0\",\"type_1\",..: 15 39 25 4 4 25 14 25 11 23 ...\n#>  $ Cover_Type                        : Factor w/ 7 levels \"class_1\",\"class_2\",..: 1 2 2 2 2 2 1 2 1 3 ...m1 <- h2o.deeplearning(\n  model_id=\"dl_model_first\", \n  training_frame = train, \n  validation_frame = valid,   ## validation dataset: used for scoring and early stopping\n  x = predictors,\n  y = response,\n  #activation=\"Rectifier\",  ## default\n  #hidden=c(200,200),       ## default: 2 hidden layers with 200 neurons each\n  epochs = 1,\n  variable_importances=T    ## not enabled by default\n)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |======================================================================| 100%\nsummary(m1)\n#> Model Details:\n#> ==============\n#> \n#> H2OMultinomialModel: deeplearning\n#> Model Key:  dl_model_first \n#> Status of Neuron Layers: predicting Cover_Type, 7-class classification, multinomial distribution, CrossEntropy loss, 53,007 weights/biases, 633.2 KB, 382,887 training samples, mini-batch size 1\n#>   layer units      type dropout       l1       l2 mean_rate rate_rms momentum\n#> 1     1    56     Input  0.00 %       NA       NA        NA       NA       NA\n#> 2     2   200 Rectifier  0.00 % 0.000000 0.000000  0.051130 0.212618 0.000000\n#> 3     3   200 Rectifier  0.00 % 0.000000 0.000000  0.008042 0.006936 0.000000\n#> 4     4     7   Softmax      NA 0.000000 0.000000  0.100831 0.268094 0.000000\n#>   mean_weight weight_rms mean_bias bias_rms\n#> 1          NA         NA        NA       NA\n#> 2   -0.009454   0.121201  0.046923 0.121500\n#> 3   -0.026873   0.118570  0.720955 0.355757\n#> 4   -0.326300   0.493430 -0.562048 0.171816\n#> \n#> H2OMultinomialMetrics: deeplearning\n#> ** Reported on training data. **\n#> ** Metrics reported on temporary training frame with 10014 samples **\n#> \n#> Training Set Metrics: \n#> =====================\n#> \n#> MSE: (Extract with `h2o.mse`) 0.129\n#> RMSE: (Extract with `h2o.rmse`) 0.359\n#> Logloss: (Extract with `h2o.logloss`) 0.417\n#> Mean Per-Class Error: 0.346\n#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,train = TRUE)`)\n#> =========================================================================\n#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error\n#> class_1    3080     507       0       0       5       0      16 0.1463\n#> class_2     627    4151      56       1      30      12       1 0.1490\n#> class_3       0      35     595       3       0      10       0 0.0747\n#> class_4       0       0      28      24       0       1       0 0.5472\n#> class_5       1      74       9       0      81       0       0 0.5091\n#> class_6       0      54     165       3       0      80       0 0.7351\n#> class_7      88       6       0       0       0       0     271 0.2575\n#> Totals     3796    4827     853      31     116     103     288 0.1730\n#>                     Rate\n#> class_1 =    528 / 3,608\n#> class_2 =    727 / 4,878\n#> class_3 =       48 / 643\n#> class_4 =        29 / 53\n#> class_5 =       84 / 165\n#> class_6 =      222 / 302\n#> class_7 =       94 / 365\n#> Totals  = 1,732 / 10,014\n#> \n#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,train = TRUE)`\n#> =======================================================================\n#> Top-7 Hit Ratios: \n#>   k hit_ratio\n#> 1 1  0.827042\n#> 2 2  0.986219\n#> 3 3  0.998103\n#> 4 4  0.999800\n#> 5 5  1.000000\n#> 6 6  1.000000\n#> 7 7  1.000000\n#> \n#> \n#> H2OMultinomialMetrics: deeplearning\n#> ** Reported on validation data. **\n#> ** Metrics reported on full validation frame **\n#> \n#> Validation Set Metrics: \n#> =====================\n#> \n#> Extract validation frame with `h2o.getFrame(\"valid.hex\")`\n#> MSE: (Extract with `h2o.mse`) 0.132\n#> RMSE: (Extract with `h2o.rmse`) 0.363\n#> Logloss: (Extract with `h2o.logloss`) 0.422\n#> Mean Per-Class Error: 0.338\n#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,valid = TRUE)`)\n#> =========================================================================\n#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error\n#> class_1   36026    6217       1       0      55       7     194 0.1523\n#> class_2    7619   47602     661       4     336     131      27 0.1557\n#> class_3       1     371    6565      58       2     146       0 0.0809\n#> class_4       0       0     260     295       0       7       0 0.4751\n#> class_5      42     946      49       0     827       6       0 0.5578\n#> class_6       7     655    1693      33       0    1076       0 0.6894\n#> class_7     997      60       0       0       0       0    3042 0.2579\n#> Totals    44692   55851    9229     390    1220    1373    3263 0.1774\n#>                       Rate\n#> class_1 =   6,474 / 42,500\n#> class_2 =   8,778 / 56,380\n#> class_3 =      578 / 7,143\n#> class_4 =        267 / 562\n#> class_5 =    1,043 / 1,870\n#> class_6 =    2,388 / 3,464\n#> class_7 =    1,057 / 4,099\n#> Totals  = 20,585 / 116,018\n#> \n#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,valid = TRUE)`\n#> =======================================================================\n#> Top-7 Hit Ratios: \n#>   k hit_ratio\n#> 1 1  0.822571\n#> 2 2  0.984321\n#> 3 3  0.998035\n#> 4 4  0.999664\n#> 5 5  1.000000\n#> 6 6  1.000000\n#> 7 7  1.000000\n#> \n#> \n#> \n#> \n#> Scoring History: \n#>             timestamp   duration training_speed  epochs iterations\n#> 1 2020-11-20 03:24:58  0.000 sec             NA 0.00000          0\n#> 2 2020-11-20 03:25:02  5.157 sec  10945 obs/sec 0.09947          1\n#> 3 2020-11-20 03:25:22 25.865 sec  14123 obs/sec 0.89690          9\n#> 4 2020-11-20 03:25:28 31.683 sec  14493 obs/sec 1.09705         11\n#>         samples training_rmse training_logloss training_r2\n#> 1      0.000000            NA               NA          NA\n#> 2  34718.000000       0.44203          0.62188     0.90148\n#> 3 313032.000000       0.36538          0.42521     0.93269\n#> 4 382887.000000       0.35945          0.41667     0.93485\n#>   training_classification_error validation_rmse validation_logloss\n#> 1                            NA              NA                 NA\n#> 2                       0.26143         0.44393            0.62273\n#> 3                       0.17845         0.36982            0.43433\n#> 4                       0.17296         0.36296            0.42196\n#>   validation_r2 validation_classification_error\n#> 1            NA                              NA\n#> 2       0.89898                         0.26367\n#> 3       0.92989                         0.18155\n#> 4       0.93247                         0.17743\n#> \n#> Variable Importances: (Extract with `h2o.varimp`) \n#> =================================================\n#> \n#> Variable Importances: \n#>                             variable relative_importance scaled_importance\n#> 1                          Elevation            1.000000          1.000000\n#> 2 Horizontal_Distance_To_Fire_Points            0.937127          0.937127\n#> 3    Horizontal_Distance_To_Roadways            0.925575          0.925575\n#> 4             Wilderness_Area.area_0            0.923445          0.923445\n#> 5             Wilderness_Area.area_2            0.840108          0.840108\n#>   percentage\n#> 1   0.031124\n#> 2   0.029167\n#> 3   0.028807\n#> 4   0.028741\n#> 5   0.026147\n#> \n#> ---\n#>                       variable relative_importance scaled_importance percentage\n#> 51           Soil_Type.type_14            0.442107          0.442107   0.013760\n#> 52           Soil_Type.type_17            0.433302          0.433302   0.013486\n#> 53               Hillshade_3pm            0.383027          0.383027   0.011921\n#> 54                      Aspect            0.309722          0.309722   0.009640\n#> 55       Soil_Type.missing(NA)            0.000000          0.000000   0.000000\n#> 56 Wilderness_Area.missing(NA)            0.000000          0.000000   0.000000"},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"variable-importances","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46.5.2 Variable Importances","text":"Variable importances Neural Network models notoriously difficult compute, many pitfalls. H2O Deep Learning implemented method Gedeon, returns relative variable importances descending order importance.","code":"\nhead(as.data.frame(h2o.varimp(m1)))\n#>                             variable relative_importance scaled_importance\n#> 1                          Elevation               1.000             1.000\n#> 2 Horizontal_Distance_To_Fire_Points               0.937             0.937\n#> 3    Horizontal_Distance_To_Roadways               0.926             0.926\n#> 4             Wilderness_Area.area_0               0.923             0.923\n#> 5             Wilderness_Area.area_2               0.840             0.840\n#> 6             Wilderness_Area.area_1               0.783             0.783\n#>   percentage\n#> 1     0.0311\n#> 2     0.0292\n#> 3     0.0288\n#> 4     0.0287\n#> 5     0.0261\n#> 6     0.0244"},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"early-stopping","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46.5.3 Early Stopping","text":"Now run another, smaller network, let stop automatically misclassification rate converges (specifically, moving average length 2 improve least 1% 2 consecutive scoring events). also sample validation set 10,000 rows faster scoring.","code":"m2 <- h2o.deeplearning(\n  model_id=\"dl_model_faster\", \n  training_frame=train, \n  validation_frame=valid,\n  x=predictors,\n  y=response,\n  hidden=c(32,32,32),                  ## small network, runs faster\n  epochs=1000000,                      ## hopefully converges earlier...\n  score_validation_samples=10000,      ## sample the validation dataset (faster)\n  stopping_rounds=2,\n  stopping_metric=\"misclassification\", ## could be \"MSE\",\"logloss\",\"r2\"\n  stopping_tolerance=0.01\n)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\nsummary(m2)\n#> Model Details:\n#> ==============\n#> \n#> H2OMultinomialModel: deeplearning\n#> Model Key:  dl_model_faster \n#> Status of Neuron Layers: predicting Cover_Type, 7-class classification, multinomial distribution, CrossEntropy loss, 4,167 weights/biases, 57.9 KB, 6,100,399 training samples, mini-batch size 1\n#>   layer units      type dropout       l1       l2 mean_rate rate_rms momentum\n#> 1     1    56     Input  0.00 %       NA       NA        NA       NA       NA\n#> 2     2    32 Rectifier  0.00 % 0.000000 0.000000  0.047239 0.209274 0.000000\n#> 3     3    32 Rectifier  0.00 % 0.000000 0.000000  0.000316 0.000145 0.000000\n#> 4     4    32 Rectifier  0.00 % 0.000000 0.000000  0.000624 0.000524 0.000000\n#> 5     5     7   Softmax      NA 0.000000 0.000000  0.131989 0.320338 0.000000\n#>   mean_weight weight_rms mean_bias bias_rms\n#> 1          NA         NA        NA       NA\n#> 2   -0.007746   0.308353  0.318408 0.318204\n#> 3   -0.048561   0.385372  0.532908 0.780092\n#> 4    0.048894   0.559214  0.703946 0.808040\n#> 5   -3.758495   3.397758 -2.136927 0.630783\n#> \n#> H2OMultinomialMetrics: deeplearning\n#> ** Reported on training data. **\n#> ** Metrics reported on temporary training frame with 10051 samples **\n#> \n#> Training Set Metrics: \n#> =====================\n#> \n#> MSE: (Extract with `h2o.mse`) 0.115\n#> RMSE: (Extract with `h2o.rmse`) 0.339\n#> Logloss: (Extract with `h2o.logloss`) 0.389\n#> Mean Per-Class Error: 0.249\n#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,train = TRUE)`)\n#> =========================================================================\n#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error\n#> class_1    2915     759       1       0      13       2      52 0.2210\n#> class_2     234    4474      23       0      55      30       6 0.0722\n#> class_3       0      33     494       6       5      49       0 0.1584\n#> class_4       0       0      28      36       0       5       0 0.4783\n#> class_5       1      50       2       0     116       1       0 0.3176\n#> class_6       0      28      78       1       3     188       0 0.3691\n#> class_7      42       3       0       0       0       0     318 0.1240\n#> Totals     3192    5347     626      43     192     275     376 0.1502\n#>                     Rate\n#> class_1 =    827 / 3,742\n#> class_2 =    348 / 4,822\n#> class_3 =       93 / 587\n#> class_4 =        33 / 69\n#> class_5 =       54 / 170\n#> class_6 =      110 / 298\n#> class_7 =       45 / 363\n#> Totals  = 1,510 / 10,051\n#> \n#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,train = TRUE)`\n#> =======================================================================\n#> Top-7 Hit Ratios: \n#>   k hit_ratio\n#> 1 1  0.849766\n#> 2 2  0.986568\n#> 3 3  0.997712\n#> 4 4  0.999503\n#> 5 5  1.000000\n#> 6 6  1.000000\n#> 7 7  1.000000\n#> \n#> \n#> H2OMultinomialMetrics: deeplearning\n#> ** Reported on validation data. **\n#> ** Metrics reported on temporary validation frame with 10061 samples **\n#> \n#> Validation Set Metrics: \n#> =====================\n#> \n#> MSE: (Extract with `h2o.mse`) 0.117\n#> RMSE: (Extract with `h2o.rmse`) 0.342\n#> Logloss: (Extract with `h2o.logloss`) 0.392\n#> Mean Per-Class Error: 0.232\n#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,valid = TRUE)`)\n#> =========================================================================\n#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error\n#> class_1    2862     765       0       0      16       1      56 0.2265\n#> class_2     221    4487      25       1      75      32       7 0.0745\n#> class_3       0      32     503       7       5      66       0 0.1794\n#> class_4       0       1      12      45       0       5       0 0.2857\n#> class_5       1      54       3       0     104       0       0 0.3580\n#> class_6       0      33      90       0       3     198       0 0.3889\n#> class_7      33       7       0       0       0       0     311 0.1140\n#> Totals     3117    5379     633      53     203     302     374 0.1542\n#>                     Rate\n#> class_1 =    838 / 3,700\n#> class_2 =    361 / 4,848\n#> class_3 =      110 / 613\n#> class_4 =        18 / 63\n#> class_5 =       58 / 162\n#> class_6 =      126 / 324\n#> class_7 =       40 / 351\n#> Totals  = 1,551 / 10,061\n#> \n#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,valid = TRUE)`\n#> =======================================================================\n#> Top-7 Hit Ratios: \n#>   k hit_ratio\n#> 1 1  0.845840\n#> 2 2  0.985091\n#> 3 3  0.998112\n#> 4 4  0.999901\n#> 5 5  1.000000\n#> 6 6  1.000000\n#> 7 7  1.000000\n#> \n#> \n#> \n#> \n#> Scoring History: \n#>              timestamp   duration training_speed   epochs iterations\n#> 1  2020-11-20 03:25:30  0.000 sec             NA  0.00000          0\n#> 2  2020-11-20 03:25:31  1.218 sec  86537 obs/sec  0.28663          1\n#> 3  2020-11-20 03:25:37  6.904 sec 117540 obs/sec  2.29412          8\n#> 4  2020-11-20 03:25:42 12.216 sec 124136 obs/sec  4.30227         15\n#> 5  2020-11-20 03:25:48 17.465 sec 127160 obs/sec  6.30821         22\n#> 6  2020-11-20 03:25:53 22.522 sec 129849 obs/sec  8.31226         29\n#> 7  2020-11-20 03:25:58 28.169 sec 132359 obs/sec 10.60271         37\n#> 8  2020-11-20 03:26:04 33.810 sec 134053 obs/sec 12.89398         45\n#> 9  2020-11-20 03:26:10 39.374 sec 135568 obs/sec 15.18461         53\n#> 10 2020-11-20 03:26:15 45.121 sec 136151 obs/sec 17.47890         61\n#> 11 2020-11-20 03:26:15 45.156 sec 136145 obs/sec 17.47890         61\n#>           samples training_rmse training_logloss training_r2\n#> 1        0.000000            NA               NA          NA\n#> 2   100037.000000       0.44433          0.61635     0.90055\n#> 3   800683.000000       0.37721          0.45233     0.92833\n#> 4  1501557.000000       0.36405          0.42387     0.93324\n#> 5  2201659.000000       0.35280          0.41546     0.93730\n#> 6  2901104.000000       0.35123          0.41707     0.93786\n#> 7  3700506.000000       0.34083          0.38757     0.94148\n#> 8  4500191.000000       0.34116          0.38573     0.94137\n#> 9  5299658.000000       0.33887          0.38902     0.94216\n#> 10 6100399.000000       0.34641          0.41733     0.93955\n#> 11 6100399.000000       0.33887          0.38902     0.94216\n#>    training_classification_error validation_rmse validation_logloss\n#> 1                             NA              NA                 NA\n#> 2                        0.26435         0.44000            0.60607\n#> 3                        0.19013         0.37728            0.45445\n#> 4                        0.17948         0.36368            0.42504\n#> 5                        0.16705         0.35426            0.41375\n#> 6                        0.16516         0.35363            0.41177\n#> 7                        0.15680         0.34155            0.38641\n#> 8                        0.15322         0.34093            0.38759\n#> 9                        0.15023         0.34191            0.39202\n#> 10                       0.16287         0.34426            0.40771\n#> 11                       0.15023         0.34191            0.39202\n#>    validation_r2 validation_classification_error\n#> 1             NA                              NA\n#> 2        0.90236                         0.25862\n#> 3        0.92821                         0.19133\n#> 4        0.93329                         0.17563\n#> 5        0.93670                         0.16539\n#> 6        0.93693                         0.17145\n#> 7        0.94116                         0.15525\n#> 8        0.94138                         0.15664\n#> 9        0.94104                         0.15416\n#> 10       0.94023                         0.16261\n#> 11       0.94104                         0.15416\n#> \n#> Variable Importances: (Extract with `h2o.varimp`) \n#> =================================================\n#> \n#> Variable Importances: \n#>                          variable relative_importance scaled_importance\n#> 1 Horizontal_Distance_To_Roadways            1.000000          1.000000\n#> 2          Wilderness_Area.area_3            0.976070          0.976070\n#> 3                       Elevation            0.919578          0.919578\n#> 4          Wilderness_Area.area_1            0.870796          0.870796\n#> 5          Wilderness_Area.area_0            0.854139          0.854139\n#>   percentage\n#> 1   0.036580\n#> 2   0.035705\n#> 3   0.033639\n#> 4   0.031854\n#> 5   0.031245\n#> \n#> ---\n#>                            variable relative_importance scaled_importance\n#> 51 Horizontal_Distance_To_Hydrology            0.307296          0.307296\n#> 52                Soil_Type.type_14            0.305442          0.305442\n#> 53                            Slope            0.143808          0.143808\n#> 54                           Aspect            0.045524          0.045524\n#> 55            Soil_Type.missing(NA)            0.000000          0.000000\n#> 56      Wilderness_Area.missing(NA)            0.000000          0.000000\n#>    percentage\n#> 51   0.011241\n#> 52   0.011173\n#> 53   0.005261\n#> 54   0.001665\n#> 55   0.000000\n#> 56   0.000000\nplot(m2)"},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"adaptive-learning-rate","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46.5.4 Adaptive Learning Rate","text":"default, H2O Deep Learning uses adaptive learning rate (ADADELTA) stochastic gradient descent optimization. two tuning parameters method: rho epsilon, balance global local search efficiencies. rho similarity prior weight updates (similar momentum), epsilon parameter prevents optimization get stuck local optima. Defaults rho=0.99 epsilon=1e-8. cases convergence speed important, might make sense perform runs optimize two parameters (e.g., rho c(0.9,0.95,0.99,0.999) epsilon c(1e-10,1e-8,1e-6,1e-4)). course, always grid searches, caution applied extrapolating grid search results different parameter regime (e.g., epochs different layer topologies activation functions, etc.).adaptive_rate disabled, several manual learning rate parameters become important: rate, rate_annealing, rate_decay, momentum_start, momentum_ramp, momentum_stable nesterov_accelerated_gradient, discussion leave H2O Deep Learning booklet.","code":""},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"tuning","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46.5.5 Tuning","text":"tuning, possible obtain less 10% test set error rate one minute. Error rates 5% possible larger models. Note deep tree methods can effective dataset Deep Learning, directly partition space sectors, seems needed .Let’s compare training error validation test set errorsTo confirm reported confusion matrix validation set (, test set) correct, make prediction test set compare confusion matrices explicitly:","code":"m3 <- h2o.deeplearning(\n  model_id=\"dl_model_tuned\", \n  training_frame=train, \n  validation_frame=valid, \n  x=predictors, \n  y=response, \n  overwrite_with_best_model=F,    ## Return final model after 10 epochs, even if not the best\n  hidden=c(128,128,128),          ## more hidden layers -> more complex interactions\n  epochs=10,                      ## to keep it short enough\n  score_validation_samples=10000, ## downsample validation set for faster scoring\n  score_duty_cycle=0.025,         ## don't score more than 2.5% of the wall time\n  adaptive_rate=F,                ## manually tuned learning rate\n  rate=0.01, \n  rate_annealing=2e-6,            \n  momentum_start=0.2,             ## manually tuned momentum\n  momentum_stable=0.4, \n  momentum_ramp=1e7, \n  l1=1e-5,                        ## add some L1/L2 regularization\n  l2=1e-5,\n  max_w2=10                       ## helps stability for Rectifier\n) \n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |======================================================================| 100%\nsummary(m3)\n#> Model Details:\n#> ==============\n#> \n#> H2OMultinomialModel: deeplearning\n#> Model Key:  dl_model_tuned \n#> Status of Neuron Layers: predicting Cover_Type, 7-class classification, multinomial distribution, CrossEntropy loss, 41,223 weights/biases, 332.9 KB, 3,499,847 training samples, mini-batch size 1\n#>   layer units      type dropout       l1       l2 mean_rate rate_rms momentum\n#> 1     1    56     Input  0.00 %       NA       NA        NA       NA       NA\n#> 2     2   128 Rectifier  0.00 % 0.000010 0.000010  0.001250 0.000000 0.269997\n#> 3     3   128 Rectifier  0.00 % 0.000010 0.000010  0.001250 0.000000 0.269997\n#> 4     4   128 Rectifier  0.00 % 0.000010 0.000010  0.001250 0.000000 0.269997\n#> 5     5     7   Softmax      NA 0.000010 0.000010  0.001250 0.000000 0.269997\n#>   mean_weight weight_rms mean_bias bias_rms\n#> 1          NA         NA        NA       NA\n#> 2   -0.010395   0.316163 -0.004841 0.280280\n#> 3   -0.054721   0.221230  0.927883 0.359542\n#> 4   -0.062446   0.213405  0.807113 0.186559\n#> 5   -0.019938   0.270728 -0.001981 1.060441\n#> \n#> H2OMultinomialMetrics: deeplearning\n#> ** Reported on training data. **\n#> ** Metrics reported on temporary training frame with 10060 samples **\n#> \n#> Training Set Metrics: \n#> =====================\n#> \n#> MSE: (Extract with `h2o.mse`) 0.0559\n#> RMSE: (Extract with `h2o.rmse`) 0.236\n#> Logloss: (Extract with `h2o.logloss`) 0.184\n#> Mean Per-Class Error: 0.126\n#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,train = TRUE)`)\n#> =========================================================================\n#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error\n#> class_1    3225     354       0       0       3       0      16 0.1037\n#> class_2     169    4808      17       0      17       4       5 0.0422\n#> class_3       0      11     557       1       2      18       0 0.0543\n#> class_4       0       0       4      31       0       4       0 0.2051\n#> class_5       3      32       1       0     112       0       0 0.2432\n#> class_6       1      10      32       0       0     257       0 0.1433\n#> class_7      26       6       0       0       0       0     334 0.0874\n#> Totals     3424    5221     611      32     134     283     355 0.0732\n#>                   Rate\n#> class_1 =  373 / 3,598\n#> class_2 =  212 / 5,020\n#> class_3 =     32 / 589\n#> class_4 =       8 / 39\n#> class_5 =     36 / 148\n#> class_6 =     43 / 300\n#> class_7 =     32 / 366\n#> Totals  = 736 / 10,060\n#> \n#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,train = TRUE)`\n#> =======================================================================\n#> Top-7 Hit Ratios: \n#>   k hit_ratio\n#> 1 1  0.926839\n#> 2 2  0.995527\n#> 3 3  0.999901\n#> 4 4  1.000000\n#> 5 5  1.000000\n#> 6 6  1.000000\n#> 7 7  1.000000\n#> \n#> \n#> H2OMultinomialMetrics: deeplearning\n#> ** Reported on validation data. **\n#> ** Metrics reported on temporary validation frame with 10005 samples **\n#> \n#> Validation Set Metrics: \n#> =====================\n#> \n#> MSE: (Extract with `h2o.mse`) 0.0637\n#> RMSE: (Extract with `h2o.rmse`) 0.252\n#> Logloss: (Extract with `h2o.logloss`) 0.208\n#> Mean Per-Class Error: 0.163\n#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,valid = TRUE)`)\n#> =========================================================================\n#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error\n#> class_1    3277     370       0       0       4       0      23 0.1081\n#> class_2     176    4616      16       0      20       8       8 0.0471\n#> class_3       0      16     560       5       1      28       0 0.0820\n#> class_4       0       1       8      39       0       6       0 0.2778\n#> class_5       4      48       2       0     108       0       0 0.3333\n#> class_6       0      22      44       0       0     241       0 0.2150\n#> class_7      25       3       0       0       0       0     326 0.0791\n#> Totals     3482    5076     630      44     133     283     357 0.0838\n#>                   Rate\n#> class_1 =  397 / 3,674\n#> class_2 =  228 / 4,844\n#> class_3 =     50 / 610\n#> class_4 =      15 / 54\n#> class_5 =     54 / 162\n#> class_6 =     66 / 307\n#> class_7 =     28 / 354\n#> Totals  = 838 / 10,005\n#> \n#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,valid = TRUE)`\n#> =======================================================================\n#> Top-7 Hit Ratios: \n#>   k hit_ratio\n#> 1 1  0.916242\n#> 2 2  0.994403\n#> 3 3  0.999600\n#> 4 4  1.000000\n#> 5 5  1.000000\n#> 6 6  1.000000\n#> 7 7  1.000000\n#> \n#> \n#> \n#> \n#> Scoring History: \n#>              timestamp          duration training_speed   epochs iterations\n#> 1  2020-11-20 03:26:16         0.000 sec             NA  0.00000          0\n#> 2  2020-11-20 03:26:23         6.786 sec  15466 obs/sec  0.28676          1\n#> 3  2020-11-20 03:26:35        18.632 sec  22122 obs/sec  1.14690          4\n#> 4  2020-11-20 03:26:46        29.836 sec  24066 obs/sec  2.00553          7\n#> 5  2020-11-20 03:26:57        40.597 sec  25266 obs/sec  2.86748         10\n#> 6  2020-11-20 03:27:08        51.665 sec  25784 obs/sec  3.72709         13\n#> 7  2020-11-20 03:27:18  1 min  2.131 sec  26357 obs/sec  4.58514         16\n#> 8  2020-11-20 03:27:29  1 min 12.694 sec  26737 obs/sec  5.44445         19\n#> 9  2020-11-20 03:27:39  1 min 23.185 sec  27035 obs/sec  6.30235         22\n#> 10 2020-11-20 03:27:50  1 min 33.904 sec  27209 obs/sec  7.16167         25\n#> 11 2020-11-20 03:28:01  1 min 45.092 sec  27221 obs/sec  8.02231         28\n#> 12 2020-11-20 03:28:12  1 min 55.566 sec  27410 obs/sec  8.88425         31\n#> 13 2020-11-20 03:28:22  2 min  5.815 sec  27604 obs/sec  9.74215         34\n#> 14 2020-11-20 03:28:26  2 min  9.524 sec  27631 obs/sec 10.02778         35\n#>           samples training_rmse training_logloss training_r2\n#> 1        0.000000            NA               NA          NA\n#> 2   100084.000000       0.43054          0.57040     0.90484\n#> 3   400286.000000       0.36238          0.41464     0.93258\n#> 4   699960.000000       0.32572          0.34221     0.94553\n#> 5  1000792.000000       0.30290          0.29892     0.95290\n#> 6  1300810.000000       0.29379          0.28232     0.95569\n#> 7  1600283.000000       0.27523          0.24840     0.96111\n#> 8  1900194.000000       0.26909          0.23856     0.96283\n#> 9  2199614.000000       0.26574          0.23293     0.96375\n#> 10 2499530.000000       0.25792          0.21815     0.96585\n#> 11 2799908.000000       0.24960          0.20567     0.96802\n#> 12 3100736.000000       0.24127          0.19288     0.97012\n#> 13 3400156.000000       0.23989          0.19054     0.97046\n#> 14 3499847.000000       0.23634          0.18423     0.97132\n#>    training_classification_error validation_rmse validation_logloss\n#> 1                             NA              NA                 NA\n#> 2                        0.23738         0.43214            0.57193\n#> 3                        0.17584         0.36707            0.41940\n#> 4                        0.13817         0.33385            0.35832\n#> 5                        0.12237         0.31106            0.31207\n#> 6                        0.11541         0.29951            0.29316\n#> 7                        0.10000         0.28369            0.26356\n#> 8                        0.09583         0.27820            0.25226\n#> 9                        0.09394         0.27289            0.24474\n#> 10                       0.08877         0.26676            0.23336\n#> 11                       0.08449         0.26008            0.22383\n#> 12                       0.07694         0.25782            0.21981\n#> 13                       0.07594         0.25191            0.21062\n#> 14                       0.07316         0.25231            0.20847\n#>    validation_r2 validation_classification_error\n#> 1             NA                              NA\n#> 2        0.90512                         0.24438\n#> 3        0.93154                         0.18391\n#> 4        0.94337                         0.14583\n#> 5        0.95084                         0.13173\n#> 6        0.95442                         0.11874\n#> 7        0.95911                         0.10675\n#> 8        0.96068                         0.10155\n#> 9        0.96217                         0.09865\n#> 10       0.96384                         0.09455\n#> 11       0.96563                         0.08876\n#> 12       0.96623                         0.08796\n#> 13       0.96776                         0.08426\n#> 14       0.96766                         0.08376\n#> \n#> Variable Importances: (Extract with `h2o.varimp`) \n#> =================================================\n#> \n#> Variable Importances: \n#>                             variable relative_importance scaled_importance\n#> 1                          Elevation            1.000000          1.000000\n#> 2 Horizontal_Distance_To_Fire_Points            0.954387          0.954387\n#> 3    Horizontal_Distance_To_Roadways            0.893218          0.893218\n#> 4             Wilderness_Area.area_0            0.751640          0.751640\n#> 5             Wilderness_Area.area_3            0.613239          0.613239\n#>   percentage\n#> 1   0.047677\n#> 2   0.045503\n#> 3   0.042586\n#> 4   0.035836\n#> 5   0.029238\n#> \n#> ---\n#>                       variable relative_importance scaled_importance percentage\n#> 51           Soil_Type.type_17            0.161714          0.161714   0.007710\n#> 52            Soil_Type.type_6            0.159901          0.159901   0.007624\n#> 53           Soil_Type.type_14            0.153352          0.153352   0.007311\n#> 54           Soil_Type.type_24            0.145064          0.145064   0.006916\n#> 55       Soil_Type.missing(NA)            0.000000          0.000000   0.000000\n#> 56 Wilderness_Area.missing(NA)            0.000000          0.000000   0.000000\nh2o.performance(m3, train=T)          ## sampled training data (from model building)\n#> H2OMultinomialMetrics: deeplearning\n#> ** Reported on training data. **\n#> ** Metrics reported on temporary training frame with 10060 samples **\n#> \n#> Training Set Metrics: \n#> =====================\n#> \n#> MSE: (Extract with `h2o.mse`) 0.0559\n#> RMSE: (Extract with `h2o.rmse`) 0.236\n#> Logloss: (Extract with `h2o.logloss`) 0.184\n#> Mean Per-Class Error: 0.126\n#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,train = TRUE)`)\n#> =========================================================================\n#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error\n#> class_1    3225     354       0       0       3       0      16 0.1037\n#> class_2     169    4808      17       0      17       4       5 0.0422\n#> class_3       0      11     557       1       2      18       0 0.0543\n#> class_4       0       0       4      31       0       4       0 0.2051\n#> class_5       3      32       1       0     112       0       0 0.2432\n#> class_6       1      10      32       0       0     257       0 0.1433\n#> class_7      26       6       0       0       0       0     334 0.0874\n#> Totals     3424    5221     611      32     134     283     355 0.0732\n#>                   Rate\n#> class_1 =  373 / 3,598\n#> class_2 =  212 / 5,020\n#> class_3 =     32 / 589\n#> class_4 =       8 / 39\n#> class_5 =     36 / 148\n#> class_6 =     43 / 300\n#> class_7 =     32 / 366\n#> Totals  = 736 / 10,060\n#> \n#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,train = TRUE)`\n#> =======================================================================\n#> Top-7 Hit Ratios: \n#>   k hit_ratio\n#> 1 1  0.926839\n#> 2 2  0.995527\n#> 3 3  0.999901\n#> 4 4  1.000000\n#> 5 5  1.000000\n#> 6 6  1.000000\n#> 7 7  1.000000\nh2o.performance(m3, valid=T)          ## sampled validation data (from model building)\n#> H2OMultinomialMetrics: deeplearning\n#> ** Reported on validation data. **\n#> ** Metrics reported on temporary validation frame with 10005 samples **\n#> \n#> Validation Set Metrics: \n#> =====================\n#> \n#> MSE: (Extract with `h2o.mse`) 0.0637\n#> RMSE: (Extract with `h2o.rmse`) 0.252\n#> Logloss: (Extract with `h2o.logloss`) 0.208\n#> Mean Per-Class Error: 0.163\n#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,valid = TRUE)`)\n#> =========================================================================\n#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error\n#> class_1    3277     370       0       0       4       0      23 0.1081\n#> class_2     176    4616      16       0      20       8       8 0.0471\n#> class_3       0      16     560       5       1      28       0 0.0820\n#> class_4       0       1       8      39       0       6       0 0.2778\n#> class_5       4      48       2       0     108       0       0 0.3333\n#> class_6       0      22      44       0       0     241       0 0.2150\n#> class_7      25       3       0       0       0       0     326 0.0791\n#> Totals     3482    5076     630      44     133     283     357 0.0838\n#>                   Rate\n#> class_1 =  397 / 3,674\n#> class_2 =  228 / 4,844\n#> class_3 =     50 / 610\n#> class_4 =      15 / 54\n#> class_5 =     54 / 162\n#> class_6 =     66 / 307\n#> class_7 =     28 / 354\n#> Totals  = 838 / 10,005\n#> \n#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,valid = TRUE)`\n#> =======================================================================\n#> Top-7 Hit Ratios: \n#>   k hit_ratio\n#> 1 1  0.916242\n#> 2 2  0.994403\n#> 3 3  0.999600\n#> 4 4  1.000000\n#> 5 5  1.000000\n#> 6 6  1.000000\n#> 7 7  1.000000\nh2o.performance(m3, newdata=train)    ## full training data\n#> H2OMultinomialMetrics: deeplearning\n#> \n#> Test Set Metrics: \n#> =====================\n#> \n#> MSE: (Extract with `h2o.mse`) 0.0555\n#> RMSE: (Extract with `h2o.rmse`) 0.236\n#> Logloss: (Extract with `h2o.logloss`) 0.184\n#> Mean Per-Class Error: 0.132\n#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>, <data>)`)\n#> =========================================================================\n#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error\n#> class_1  114487   11863       8       0     109      12     641 0.0994\n#> class_2    5661  163277     423       0     630     267      84 0.0415\n#> class_3       1     402   19734     115      65    1125       0 0.0797\n#> class_4       0       1     293    1268       0      96       0 0.2352\n#> class_5     113    1094      94       0    4401      18       0 0.2306\n#> class_6      31     521    1031      43      10    8797       0 0.1568\n#> class_7     835     153       0       0       1       0   11311 0.0804\n#> Totals   121128  177311   21583    1426    5216   10315   12036 0.0738\n#>                       Rate\n#> class_1 = 12,633 / 127,120\n#> class_2 =  7,065 / 170,342\n#> class_3 =   1,708 / 21,442\n#> class_4 =      390 / 1,658\n#> class_5 =    1,319 / 5,720\n#> class_6 =   1,636 / 10,433\n#> class_7 =     989 / 12,300\n#> Totals  = 25,740 / 349,015\n#> \n#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>, <data>)`\n#> =======================================================================\n#> Top-7 Hit Ratios: \n#>   k hit_ratio\n#> 1 1  0.926250\n#> 2 2  0.996496\n#> 3 3  0.999716\n#> 4 4  0.999977\n#> 5 5  0.999997\n#> 6 6  1.000000\n#> 7 7  1.000000\nh2o.performance(m3, newdata=valid)    ## full validation data\n#> H2OMultinomialMetrics: deeplearning\n#> \n#> Test Set Metrics: \n#> =====================\n#> \n#> MSE: (Extract with `h2o.mse`) 0.0623\n#> RMSE: (Extract with `h2o.rmse`) 0.25\n#> Logloss: (Extract with `h2o.logloss`) 0.207\n#> Mean Per-Class Error: 0.146\n#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>, <data>)`)\n#> =========================================================================\n#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error\n#> class_1   37823    4392       3       0      41       5     236 0.1100\n#> class_2    2154   53692     149       0     251     104      30 0.0477\n#> class_3       0     168    6510      49      26     390       0 0.0886\n#> class_4       0       1     109     421       0      31       0 0.2509\n#> class_5      38     415      44       0    1369       4       0 0.2679\n#> class_6      12     205     358      21       3    2865       0 0.1729\n#> class_7     308      45       0       0       1       0    3745 0.0864\n#> Totals    40335   58918    7173     491    1691    3399    4011 0.0827\n#>                      Rate\n#> class_1 =  4,677 / 42,500\n#> class_2 =  2,688 / 56,380\n#> class_3 =     633 / 7,143\n#> class_4 =       141 / 562\n#> class_5 =     501 / 1,870\n#> class_6 =     599 / 3,464\n#> class_7 =     354 / 4,099\n#> Totals  = 9,593 / 116,018\n#> \n#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>, <data>)`\n#> =======================================================================\n#> Top-7 Hit Ratios: \n#>   k hit_ratio\n#> 1 1  0.917315\n#> 2 2  0.995475\n#> 3 3  0.999543\n#> 4 4  0.999948\n#> 5 5  1.000000\n#> 6 6  1.000000\n#> 7 7  1.000000\nh2o.performance(m3, newdata=test)     ## full test data\n#> H2OMultinomialMetrics: deeplearning\n#> \n#> Test Set Metrics: \n#> =====================\n#> \n#> MSE: (Extract with `h2o.mse`) 0.0612\n#> RMSE: (Extract with `h2o.rmse`) 0.247\n#> Logloss: (Extract with `h2o.logloss`) 0.203\n#> Mean Per-Class Error: 0.137\n#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>, <data>)`)\n#> =========================================================================\n#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error\n#> class_1   37673    4247       1       0      38       2     259 0.1077\n#> class_2    2142   53844     165       0     257     127      44 0.0483\n#> class_3       0     158    6532      48      19     412       0 0.0889\n#> class_4       0       0      80     420       0      27       0 0.2030\n#> class_5      41     384      32       0    1437       9       0 0.2449\n#> class_6      14     198     376      18       7    2857       0 0.1767\n#> class_7     307      48       0       0       0       0    3756 0.0864\n#> Totals    40177   58879    7186     486    1758    3434    4059 0.0816\n#>                      Rate\n#> class_1 =  4,547 / 42,220\n#> class_2 =  2,735 / 56,579\n#> class_3 =     637 / 7,169\n#> class_4 =       107 / 527\n#> class_5 =     466 / 1,903\n#> class_6 =     613 / 3,470\n#> class_7 =     355 / 4,111\n#> Totals  = 9,460 / 115,979\n#> \n#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>, <data>)`\n#> =======================================================================\n#> Top-7 Hit Ratios: \n#>   k hit_ratio\n#> 1 1  0.918434\n#> 2 2  0.995775\n#> 3 3  0.999526\n#> 4 4  0.999914\n#> 5 5  0.999983\n#> 6 6  1.000000\n#> 7 7  1.000000pred <- h2o.predict(m3, test)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\npred\n#>   predict class_1  class_2  class_3  class_4  class_5  class_6  class_7\n#> 1 class_2 0.03950 0.960471 2.43e-05 1.49e-06 5.43e-08 1.15e-06 3.60e-07\n#> 2 class_1 0.99794 0.001032 3.35e-04 1.27e-04 1.37e-05 6.56e-05 4.84e-04\n#> 3 class_1 0.99970 0.000303 6.82e-09 7.40e-10 2.12e-12 1.98e-09 1.27e-06\n#> 4 class_1 0.99973 0.000273 1.21e-07 4.28e-08 1.43e-09 1.74e-09 2.90e-07\n#> 5 class_2 0.02622 0.973709 4.23e-07 7.00e-07 4.62e-05 1.98e-05 1.10e-06\n#> 6 class_5 0.00122 0.304013 4.37e-05 1.27e-05 6.95e-01 1.92e-09 3.06e-10\n#> \n#> [115979 rows x 8 columns]\ntest$Accuracy <- pred$predict == test$Cover_Type\n1-mean(test$Accuracy)\n#> [1] 0.0816"},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"hyper-parameter-tuning-with-grid-search","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46.5.6 Hyper-parameter Tuning with Grid Search","text":"Since lot parameters can impact model accuracy, hyper-parameter tuning especially important Deep Learning:speed, train first 10,000 rows training dataset:simplest hyperparameter search method brute-force scan full Cartesian product combinations specified grid search:Let’s see model lowest validation error:","code":"\nsampled_train=train[1:10000,]hyper_params <- list(\n  hidden=list(c(32,32,32),c(64,64)),\n  input_dropout_ratio=c(0,0.05),\n  rate=c(0.01,0.02),\n  rate_annealing=c(1e-8,1e-7,1e-6)\n)\nhyper_params\n#> $hidden\n#> $hidden[[1]]\n#> [1] 32 32 32\n#> \n#> $hidden[[2]]\n#> [1] 64 64\n#> \n#> \n#> $input_dropout_ratio\n#> [1] 0.00 0.05\n#> \n#> $rate\n#> [1] 0.01 0.02\n#> \n#> $rate_annealing\n#> [1] 1e-08 1e-07 1e-06\ngrid <- h2o.grid(\n  algorithm=\"deeplearning\",\n  grid_id=\"dl_grid\", \n  training_frame=sampled_train,\n  validation_frame=valid, \n  x=predictors, \n  y=response,\n  epochs=10,\n  stopping_metric=\"misclassification\",\n  stopping_tolerance=1e-2,        ## stop when misclassification does not improve by >=1% for 2 scoring events\n  stopping_rounds=2,\n  score_validation_samples=10000, ## downsample validation set for faster scoring\n  score_duty_cycle=0.025,         ## don't score more than 2.5% of the wall time\n  adaptive_rate=F,                ## manually tuned learning rate\n  momentum_start=0.5,             ## manually tuned momentum\n  momentum_stable=0.9, \n  momentum_ramp=1e7, \n  l1=1e-5,\n  l2=1e-5,\n  activation=c(\"Rectifier\"),\n  max_w2=10,                      ## can help improve stability for Rectifier\n  hyper_params=hyper_params\n)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\ngrid\n#> H2O Grid Details\n#> ================\n#> \n#> Grid ID: dl_grid \n#> Used hyper parameters: \n#>   -  hidden \n#>   -  input_dropout_ratio \n#>   -  rate \n#>   -  rate_annealing \n#> Number of models: 24 \n#> Number of failed models: 0 \n#> \n#> Hyper-Parameter Search Summary: ordered by increasing logloss\n#>     hidden input_dropout_ratio rate rate_annealing        model_ids\n#> 1 [64, 64]                0.05 0.02         1.0E-6 dl_grid_model_24\n#> 2 [64, 64]                 0.0 0.01         1.0E-6 dl_grid_model_18\n#> 3 [64, 64]                 0.0 0.02         1.0E-7 dl_grid_model_14\n#> 4 [64, 64]                0.05 0.01         1.0E-8  dl_grid_model_4\n#> 5 [64, 64]                 0.0 0.02         1.0E-6 dl_grid_model_22\n#>              logloss\n#> 1 0.5653311098099636\n#> 2 0.5660312226119422\n#> 3 0.5711945397917454\n#> 4 0.5794875616211669\n#> 5 0.5803665424070287\n#> \n#> ---\n#>          hidden input_dropout_ratio rate rate_annealing        model_ids\n#> 19     [64, 64]                 0.0 0.02         1.0E-8  dl_grid_model_6\n#> 20 [32, 32, 32]                 0.0 0.02         1.0E-8  dl_grid_model_5\n#> 21     [64, 64]                 0.0 0.01         1.0E-8  dl_grid_model_2\n#> 22 [32, 32, 32]                0.05 0.02         1.0E-8  dl_grid_model_7\n#> 23 [32, 32, 32]                0.05 0.02         1.0E-7 dl_grid_model_15\n#> 24 [32, 32, 32]                 0.0 0.02         1.0E-7 dl_grid_model_13\n#>               logloss\n#> 19   0.63187628812144\n#> 20 0.6319144770646112\n#> 21 0.6371643186572534\n#> 22 0.6389680247906112\n#> 23 0.6514794473621144\n#> 24 0.6607289594973879\ngrid <- h2o.getGrid(\"dl_grid\",sort_by=\"err\",decreasing=FALSE)\ngrid\n#> H2O Grid Details\n#> ================\n#> \n#> Grid ID: dl_grid \n#> Used hyper parameters: \n#>   -  hidden \n#>   -  input_dropout_ratio \n#>   -  rate \n#>   -  rate_annealing \n#> Number of models: 24 \n#> Number of failed models: 0 \n#> \n#> Hyper-Parameter Search Summary: ordered by increasing err\n#>         hidden input_dropout_ratio rate rate_annealing        model_ids\n#> 1     [64, 64]                 0.0 0.01         1.0E-6 dl_grid_model_18\n#> 2     [64, 64]                0.05 0.02         1.0E-6 dl_grid_model_24\n#> 3     [64, 64]                 0.0 0.02         1.0E-7 dl_grid_model_14\n#> 4 [32, 32, 32]                 0.0 0.01         1.0E-8  dl_grid_model_1\n#> 5     [64, 64]                0.05 0.01         1.0E-8  dl_grid_model_4\n#>                   err\n#> 1  0.2380003983270265\n#> 2 0.24264485888102125\n#> 3  0.2466067864271457\n#> 4  0.2502484595507851\n#> 5  0.2507450824557918\n#> \n#> ---\n#>          hidden input_dropout_ratio rate rate_annealing        model_ids\n#> 19 [32, 32, 32]                 0.0 0.02         1.0E-8  dl_grid_model_5\n#> 20 [32, 32, 32]                0.05 0.01         1.0E-6 dl_grid_model_19\n#> 21 [32, 32, 32]                0.05 0.02         1.0E-8  dl_grid_model_7\n#> 22 [32, 32, 32]                0.05 0.01         1.0E-7 dl_grid_model_11\n#> 23 [32, 32, 32]                 0.0 0.02         1.0E-7 dl_grid_model_13\n#> 24 [32, 32, 32]                0.05 0.02         1.0E-7 dl_grid_model_15\n#>                    err\n#> 19  0.2712696662992284\n#> 20  0.2732702837232454\n#> 21  0.2745569240012016\n#> 22 0.27541049662536515\n#> 23 0.28618884638614855\n#> 24  0.2910365548322484\n\n## To see what other \"sort_by\" criteria are allowed\n#grid <- h2o.getGrid(\"dl_grid\",sort_by=\"wrong_thing\",decreasing=FALSE)\n\n## Sort by logloss\nh2o.getGrid(\"dl_grid\",sort_by=\"logloss\",decreasing=FALSE)\n#> H2O Grid Details\n#> ================\n#> \n#> Grid ID: dl_grid \n#> Used hyper parameters: \n#>   -  hidden \n#>   -  input_dropout_ratio \n#>   -  rate \n#>   -  rate_annealing \n#> Number of models: 24 \n#> Number of failed models: 0 \n#> \n#> Hyper-Parameter Search Summary: ordered by increasing logloss\n#>     hidden input_dropout_ratio rate rate_annealing        model_ids\n#> 1 [64, 64]                0.05 0.02         1.0E-6 dl_grid_model_24\n#> 2 [64, 64]                 0.0 0.01         1.0E-6 dl_grid_model_18\n#> 3 [64, 64]                 0.0 0.02         1.0E-7 dl_grid_model_14\n#> 4 [64, 64]                0.05 0.01         1.0E-8  dl_grid_model_4\n#> 5 [64, 64]                 0.0 0.02         1.0E-6 dl_grid_model_22\n#>              logloss\n#> 1 0.5653311098099636\n#> 2 0.5660312226119422\n#> 3 0.5711945397917454\n#> 4 0.5794875616211669\n#> 5 0.5803665424070287\n#> \n#> ---\n#>          hidden input_dropout_ratio rate rate_annealing        model_ids\n#> 19     [64, 64]                 0.0 0.02         1.0E-8  dl_grid_model_6\n#> 20 [32, 32, 32]                 0.0 0.02         1.0E-8  dl_grid_model_5\n#> 21     [64, 64]                 0.0 0.01         1.0E-8  dl_grid_model_2\n#> 22 [32, 32, 32]                0.05 0.02         1.0E-8  dl_grid_model_7\n#> 23 [32, 32, 32]                0.05 0.02         1.0E-7 dl_grid_model_15\n#> 24 [32, 32, 32]                 0.0 0.02         1.0E-7 dl_grid_model_13\n#>               logloss\n#> 19   0.63187628812144\n#> 20 0.6319144770646112\n#> 21 0.6371643186572534\n#> 22 0.6389680247906112\n#> 23 0.6514794473621144\n#> 24 0.6607289594973879\n\n## Find the best model and its full set of parameters\ngrid@summary_table[1,]\n#> Hyper-Parameter Search Summary: ordered by increasing err\n#>     hidden input_dropout_ratio rate rate_annealing        model_ids\n#> 1 [64, 64]                 0.0 0.01         1.0E-6 dl_grid_model_18\n#>                  err\n#> 1 0.2380003983270265\nbest_model <- h2o.getModel(grid@model_ids[[1]])\nbest_model\n#> Model Details:\n#> ==============\n#> \n#> H2OMultinomialModel: deeplearning\n#> Model ID:  dl_grid_model_18 \n#> Status of Neuron Layers: predicting Cover_Type, 7-class classification, multinomial distribution, CrossEntropy loss, 8,263 weights/biases, 72.2 KB, 100,000 training samples, mini-batch size 1\n#>   layer units      type dropout       l1       l2 mean_rate rate_rms momentum\n#> 1     1    56     Input  0.00 %       NA       NA        NA       NA       NA\n#> 2     2    64 Rectifier  0.00 % 0.000010 0.000010  0.009091 0.000000 0.504000\n#> 3     3    64 Rectifier  0.00 % 0.000010 0.000010  0.009091 0.000000 0.504000\n#> 4     4     7   Softmax      NA 0.000010 0.000010  0.009091 0.000000 0.504000\n#>   mean_weight weight_rms mean_bias bias_rms\n#> 1          NA         NA        NA       NA\n#> 2   -0.007389   0.210256  0.208580 0.159783\n#> 3   -0.059410   0.187647  0.881156 0.121903\n#> 4   -0.016843   0.393353  0.014311 0.532353\n#> \n#> \n#> H2OMultinomialMetrics: deeplearning\n#> ** Reported on training data. **\n#> ** Metrics reported on full training frame **\n#> \n#> Training Set Metrics: \n#> =====================\n#> \n#> Extract training frame with `h2o.getFrame(\"RTMP_sid_af9c_9\")`\n#> MSE: (Extract with `h2o.mse`) 0.163\n#> RMSE: (Extract with `h2o.rmse`) 0.403\n#> Logloss: (Extract with `h2o.logloss`) 0.507\n#> Mean Per-Class Error: 0.47\n#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,train = TRUE)`)\n#> =========================================================================\n#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error\n#> class_1    2672     951       1       0       0       1      63 0.2755\n#> class_2     564    4219      42       0       1       4       5 0.1274\n#> class_3       0      63     565       1       0       1       0 0.1032\n#> class_4       0       0      34      10       0       0       0 0.7727\n#> class_5      10     126       3       0      17       0       0 0.8910\n#> class_6       0      90     177       0       0      42       0 0.8641\n#> class_7      85       2       0       0       0       0     251 0.2574\n#> Totals     3331    5451     822      11      18      48     319 0.2224\n#>                     Rate\n#> class_1 =  1,016 / 3,688\n#> class_2 =    616 / 4,835\n#> class_3 =       65 / 630\n#> class_4 =        34 / 44\n#> class_5 =      139 / 156\n#> class_6 =      267 / 309\n#> class_7 =       87 / 338\n#> Totals  = 2,224 / 10,000\n#> \n#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,train = TRUE)`\n#> =======================================================================\n#> Top-7 Hit Ratios: \n#>   k hit_ratio\n#> 1 1  0.777600\n#> 2 2  0.980100\n#> 3 3  0.997700\n#> 4 4  0.999400\n#> 5 5  1.000000\n#> 6 6  1.000000\n#> 7 7  1.000000\n#> \n#> \n#> H2OMultinomialMetrics: deeplearning\n#> ** Reported on validation data. **\n#> ** Metrics reported on temporary validation frame with 10042 samples **\n#> \n#> Validation Set Metrics: \n#> =====================\n#> \n#> MSE: (Extract with `h2o.mse`) 0.178\n#> RMSE: (Extract with `h2o.rmse`) 0.422\n#> Logloss: (Extract with `h2o.logloss`) 0.566\n#> Mean Per-Class Error: 0.506\n#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,valid = TRUE)`)\n#> =========================================================================\n#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error\n#> class_1    2549    1003       0       0       0       1      67 0.2959\n#> class_2     621    4240      45       0       2      12       7 0.1394\n#> class_3       0      75     544       2       0       5       0 0.1310\n#> class_4       0       1      49       7       0       0       0 0.8772\n#> class_5       6     122       3       0      13       0       0 0.9097\n#> class_6       0      89     189       0       0      16       0 0.9456\n#> class_7      89       2       0       0       0       0     283 0.2433\n#> Totals     3265    5532     830       9      15      34     357 0.2380\n#>                     Rate\n#> class_1 =  1,071 / 3,620\n#> class_2 =    687 / 4,927\n#> class_3 =       82 / 626\n#> class_4 =        50 / 57\n#> class_5 =      131 / 144\n#> class_6 =      278 / 294\n#> class_7 =       91 / 374\n#> Totals  = 2,390 / 10,042\n#> \n#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,valid = TRUE)`\n#> =======================================================================\n#> Top-7 Hit Ratios: \n#>   k hit_ratio\n#> 1 1  0.762000\n#> 2 2  0.973013\n#> 3 3  0.994921\n#> 4 4  0.998805\n#> 5 5  1.000000\n#> 6 6  1.000000\n#> 7 7  1.000000\n\nprint(best_model@allparameters)\n#> $model_id\n#> [1] \"dl_grid_model_18\"\n#> \n#> $training_frame\n#> [1] \"RTMP_sid_af9c_9\"\n#> \n#> $validation_frame\n#> [1] \"valid.hex\"\n#> \n#> $nfolds\n#> [1] 0\n#> \n#> $keep_cross_validation_models\n#> [1] TRUE\n#> \n#> $keep_cross_validation_predictions\n#> [1] FALSE\n#> \n#> $keep_cross_validation_fold_assignment\n#> [1] FALSE\n#> \n#> $fold_assignment\n#> [1] \"AUTO\"\n#> \n#> $ignore_const_cols\n#> [1] TRUE\n#> \n#> $score_each_iteration\n#> [1] FALSE\n#> \n#> $balance_classes\n#> [1] FALSE\n#> \n#> $max_after_balance_size\n#> [1] 5\n#> \n#> $max_confusion_matrix_size\n#> [1] 20\n#> \n#> $max_hit_ratio_k\n#> [1] 0\n#> \n#> $overwrite_with_best_model\n#> [1] TRUE\n#> \n#> $use_all_factor_levels\n#> [1] TRUE\n#> \n#> $standardize\n#> [1] TRUE\n#> \n#> $activation\n#> [1] \"Rectifier\"\n#> \n#> $hidden\n#> [1] 64 64\n#> \n#> $epochs\n#> [1] 10\n#> \n#> $train_samples_per_iteration\n#> [1] -2\n#> \n#> $target_ratio_comm_to_comp\n#> [1] 0.05\n#> \n#> $seed\n#> [1] \"3141284810210899683\"\n#> \n#> $adaptive_rate\n#> [1] FALSE\n#> \n#> $rho\n#> [1] 0.99\n#> \n#> $epsilon\n#> [1] 1e-08\n#> \n#> $rate\n#> [1] 0.01\n#> \n#> $rate_annealing\n#> [1] 1e-06\n#> \n#> $rate_decay\n#> [1] 1\n#> \n#> $momentum_start\n#> [1] 0.5\n#> \n#> $momentum_ramp\n#> [1] 1e+07\n#> \n#> $momentum_stable\n#> [1] 0.9\n#> \n#> $nesterov_accelerated_gradient\n#> [1] TRUE\n#> \n#> $input_dropout_ratio\n#> [1] 0\n#> \n#> $l1\n#> [1] 1e-05\n#> \n#> $l2\n#> [1] 1e-05\n#> \n#> $max_w2\n#> [1] 10\n#> \n#> $initial_weight_distribution\n#> [1] \"UniformAdaptive\"\n#> \n#> $initial_weight_scale\n#> [1] 1\n#> \n#> $loss\n#> [1] \"Automatic\"\n#> \n#> $distribution\n#> [1] \"AUTO\"\n#> \n#> $quantile_alpha\n#> [1] 0.5\n#> \n#> $tweedie_power\n#> [1] 1.5\n#> \n#> $huber_alpha\n#> [1] 0.9\n#> \n#> $score_interval\n#> [1] 5\n#> \n#> $score_training_samples\n#> [1] 10000\n#> \n#> $score_validation_samples\n#> [1] 10000\n#> \n#> $score_duty_cycle\n#> [1] 0.025\n#> \n#> $classification_stop\n#> [1] 0\n#> \n#> $regression_stop\n#> [1] 1e-06\n#> \n#> $stopping_rounds\n#> [1] 2\n#> \n#> $stopping_metric\n#> [1] \"misclassification\"\n#> \n#> $stopping_tolerance\n#> [1] 0.01\n#> \n#> $max_runtime_secs\n#> [1] 1.8e+308\n#> \n#> $score_validation_sampling\n#> [1] \"Uniform\"\n#> \n#> $diagnostics\n#> [1] TRUE\n#> \n#> $fast_mode\n#> [1] TRUE\n#> \n#> $force_load_balance\n#> [1] TRUE\n#> \n#> $variable_importances\n#> [1] TRUE\n#> \n#> $replicate_training_data\n#> [1] TRUE\n#> \n#> $single_node_mode\n#> [1] FALSE\n#> \n#> $shuffle_training_data\n#> [1] FALSE\n#> \n#> $missing_values_handling\n#> [1] \"MeanImputation\"\n#> \n#> $quiet_mode\n#> [1] FALSE\n#> \n#> $autoencoder\n#> [1] FALSE\n#> \n#> $sparse\n#> [1] FALSE\n#> \n#> $col_major\n#> [1] FALSE\n#> \n#> $average_activation\n#> [1] 0\n#> \n#> $sparsity_beta\n#> [1] 0\n#> \n#> $max_categorical_features\n#> [1] 2147483647\n#> \n#> $reproducible\n#> [1] FALSE\n#> \n#> $export_weights_and_biases\n#> [1] FALSE\n#> \n#> $mini_batch_size\n#> [1] 1\n#> \n#> $categorical_encoding\n#> [1] \"AUTO\"\n#> \n#> $elastic_averaging\n#> [1] FALSE\n#> \n#> $elastic_averaging_moving_rate\n#> [1] 0.9\n#> \n#> $elastic_averaging_regularization\n#> [1] 0.001\n#> \n#> $x\n#>  [1] \"Soil_Type\"                          \"Wilderness_Area\"                   \n#>  [3] \"Elevation\"                          \"Aspect\"                            \n#>  [5] \"Slope\"                              \"Horizontal_Distance_To_Hydrology\"  \n#>  [7] \"Vertical_Distance_To_Hydrology\"     \"Horizontal_Distance_To_Roadways\"   \n#>  [9] \"Hillshade_9am\"                      \"Hillshade_Noon\"                    \n#> [11] \"Hillshade_3pm\"                      \"Horizontal_Distance_To_Fire_Points\"\n#> \n#> $y\n#> [1] \"Cover_Type\"\nprint(h2o.performance(best_model, valid=T))\n#> H2OMultinomialMetrics: deeplearning\n#> ** Reported on validation data. **\n#> ** Metrics reported on temporary validation frame with 10042 samples **\n#> \n#> Validation Set Metrics: \n#> =====================\n#> \n#> MSE: (Extract with `h2o.mse`) 0.178\n#> RMSE: (Extract with `h2o.rmse`) 0.422\n#> Logloss: (Extract with `h2o.logloss`) 0.566\n#> Mean Per-Class Error: 0.506\n#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,valid = TRUE)`)\n#> =========================================================================\n#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error\n#> class_1    2549    1003       0       0       0       1      67 0.2959\n#> class_2     621    4240      45       0       2      12       7 0.1394\n#> class_3       0      75     544       2       0       5       0 0.1310\n#> class_4       0       1      49       7       0       0       0 0.8772\n#> class_5       6     122       3       0      13       0       0 0.9097\n#> class_6       0      89     189       0       0      16       0 0.9456\n#> class_7      89       2       0       0       0       0     283 0.2433\n#> Totals     3265    5532     830       9      15      34     357 0.2380\n#>                     Rate\n#> class_1 =  1,071 / 3,620\n#> class_2 =    687 / 4,927\n#> class_3 =       82 / 626\n#> class_4 =        50 / 57\n#> class_5 =      131 / 144\n#> class_6 =      278 / 294\n#> class_7 =       91 / 374\n#> Totals  = 2,390 / 10,042\n#> \n#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,valid = TRUE)`\n#> =======================================================================\n#> Top-7 Hit Ratios: \n#>   k hit_ratio\n#> 1 1  0.762000\n#> 2 2  0.973013\n#> 3 3  0.994921\n#> 4 4  0.998805\n#> 5 5  1.000000\n#> 6 6  1.000000\n#> 7 7  1.000000\nprint(h2o.logloss(best_model, valid=T))\n#> [1] 0.566"},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"random-hyper-parameter-search","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46.5.7 Random Hyper-Parameter Search","text":"Often, hyper-parameter search 4 parameters can done efficiently random parameter search grid search. Basically, chances good find one many good models less time performing exhaustive grid search. simply build max_models models parameters drawn randomly user-specified distributions (, uniform). example, use adaptive learning rate focus tuning network architecture regularization parameters. also let grid search stop automatically performance top leaderboard doesn’t change much anymore, .e., search converged.Let’s look model lowest validation misclassification rate:","code":"\nhyper_params <- list(\n  activation=c(\"Rectifier\",\"Tanh\",\"Maxout\",\"RectifierWithDropout\",\"TanhWithDropout\",\"MaxoutWithDropout\"),\n  hidden=list(c(20,20),c(50,50),c(30,30,30),c(25,25,25,25)),\n  input_dropout_ratio=c(0,0.05),\n  l1=seq(0,1e-4,1e-6),\n  l2=seq(0,1e-4,1e-6)\n)\nhyper_params\n\n## Stop once the top 5 models are within 1% of each other (i.e., the windowed average varies less than 1%)\nsearch_criteria = list(strategy = \"RandomDiscrete\", max_runtime_secs = 360, max_models = 100, seed=1234567, stopping_rounds=5, stopping_tolerance=1e-2)\ndl_random_grid <- h2o.grid(\n  algorithm=\"deeplearning\",\n  grid_id = \"dl_grid_random\",\n  training_frame=sampled_train,\n  validation_frame=valid, \n  x=predictors, \n  y=response,\n  epochs=1,\n  stopping_metric=\"logloss\",\n  stopping_tolerance=1e-2,        ## stop when logloss does not improve by >=1% for 2 scoring events\n  stopping_rounds=2,\n  score_validation_samples=10000, ## downsample validation set for faster scoring\n  score_duty_cycle=0.025,         ## don't score more than 2.5% of the wall time\n  max_w2=10,                      ## can help improve stability for Rectifier\n  hyper_params = hyper_params,\n  search_criteria = search_criteria\n)                                \ngrid <- h2o.getGrid(\"dl_grid_random\",sort_by=\"logloss\",decreasing=FALSE)\ngrid\n\ngrid@summary_table[1,]\nbest_model <- h2o.getModel(grid@model_ids[[1]]) ## model with lowest logloss\nbest_model\ngrid <- h2o.getGrid(\"dl_grid\",sort_by=\"err\",decreasing=FALSE)\nbest_model <- h2o.getModel(grid@model_ids[[1]]) ## model with lowest classification error (on validation, since it was available during training)\nh2o.confusionMatrix(best_model,valid=T)\n#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error\n#> class_1    2549    1003       0       0       0       1      67 0.2959\n#> class_2     621    4240      45       0       2      12       7 0.1394\n#> class_3       0      75     544       2       0       5       0 0.1310\n#> class_4       0       1      49       7       0       0       0 0.8772\n#> class_5       6     122       3       0      13       0       0 0.9097\n#> class_6       0      89     189       0       0      16       0 0.9456\n#> class_7      89       2       0       0       0       0     283 0.2433\n#> Totals     3265    5532     830       9      15      34     357 0.2380\n#>                     Rate\n#> class_1 =  1,071 / 3,620\n#> class_2 =    687 / 4,927\n#> class_3 =       82 / 626\n#> class_4 =        50 / 57\n#> class_5 =      131 / 144\n#> class_6 =      278 / 294\n#> class_7 =       91 / 374\n#> Totals  = 2,390 / 10,042\nbest_params <- best_model@allparameters\nbest_params$activation\n#> [1] \"Rectifier\"\nbest_params$hidden\n#> [1] 64 64\nbest_params$input_dropout_ratio\n#> [1] 0\nbest_params$l1\n#> [1] 1e-05\nbest_params$l2\n#> [1] 1e-05"},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"checkpointing","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46.5.8 Checkpointing","text":"Let’s continue training manually tuned model , 2 epochs. Note since many important parameters epochs, l1, l2, max_w2, score_interval, train_samples_per_iteration, input_dropout_ratio, hidden_dropout_ratios, score_duty_cycle, classification_stop, regression_stop, variable_importances, force_load_balance can modified checkpoint restarts, best specify many parameters possible explicitly.satisfied results, can save model disk (cluster). example, store model directory called mybest_deeplearning_covtype_model, created us since force=TRUE.can loaded later following command:model fully functional can inspected, restarted, used score dataset, etc. Note binary compatibility H2O versions currently guaranteed.","code":"\nmax_epochs <- 12 ## Add two more epochs\nm_cont <- h2o.deeplearning(\n  model_id=\"dl_model_tuned_continued\", \n  checkpoint=\"dl_model_tuned\", \n  training_frame=train, \n  validation_frame=valid, \n  x=predictors, \n  y=response, \n  hidden=c(128,128,128),          ## more hidden layers -> more complex interactions\n  epochs=max_epochs,              ## hopefully long enough to converge (otherwise restart again)\n  stopping_metric=\"logloss\",      ## logloss is directly optimized by Deep Learning\n  stopping_tolerance=1e-2,        ## stop when validation logloss does not improve by >=1% for 2 scoring events\n  stopping_rounds=2,\n  score_validation_samples=10000, ## downsample validation set for faster scoring\n  score_duty_cycle=0.025,         ## don't score more than 2.5% of the wall time\n  adaptive_rate=F,                ## manually tuned learning rate\n  rate=0.01, \n  rate_annealing=2e-6,            \n  momentum_start=0.2,             ## manually tuned momentum\n  momentum_stable=0.4, \n  momentum_ramp=1e7, \n  l1=1e-5,                        ## add some L1/L2 regularization\n  l2=1e-5,\n  max_w2=10                       ## helps stability for Rectifier\n) \nsummary(m_cont)\nplot(m_cont)\npath <- h2o.saveModel(m_cont, \n          path = file.path(data_out_dir, \"mybest_deeplearning_covtype_model\"), force=TRUE)\nprint(path)\n#> [1] \"/home/rstudio/all/output/data/mybest_deeplearning_covtype_model/dl_model_tuned_continued\"\nm_loaded <- h2o.loadModel(path)\nsummary(m_loaded)\n#> Model Details:\n#> ==============\n#> \n#> H2OMultinomialModel: deeplearning\n#> Model Key:  dl_model_tuned_continued \n#> Status of Neuron Layers: predicting Cover_Type, 7-class classification, multinomial distribution, CrossEntropy loss, 41,223 weights/biases, 333.0 KB, 4,200,713 training samples, mini-batch size 1\n#>   layer units      type dropout       l1       l2 mean_rate rate_rms momentum\n#> 1     1    56     Input  0.00 %       NA       NA        NA       NA       NA\n#> 2     2   128 Rectifier  0.00 % 0.000010 0.000010  0.001064 0.000000 0.284014\n#> 3     3   128 Rectifier  0.00 % 0.000010 0.000010  0.001064 0.000000 0.284014\n#> 4     4   128 Rectifier  0.00 % 0.000010 0.000010  0.001064 0.000000 0.284014\n#> 5     5     7   Softmax      NA 0.000010 0.000010  0.001064 0.000000 0.284014\n#>   mean_weight weight_rms mean_bias bias_rms\n#> 1          NA         NA        NA       NA\n#> 2   -0.010689   0.325116 -0.009821 0.289280\n#> 3   -0.054759   0.226153  0.933718 0.372898\n#> 4   -0.063011   0.217923  0.805753 0.189808\n#> 5   -0.020857   0.270380 -0.000980 1.062795\n#> \n#> H2OMultinomialMetrics: deeplearning\n#> ** Reported on training data. **\n#> ** Metrics reported on temporary training frame with 9866 samples **\n#> \n#> Training Set Metrics: \n#> =====================\n#> \n#> MSE: (Extract with `h2o.mse`) 0.0535\n#> RMSE: (Extract with `h2o.rmse`) 0.231\n#> Logloss: (Extract with `h2o.logloss`) 0.178\n#> Mean Per-Class Error: 0.133\n#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,train = TRUE)`)\n#> =========================================================================\n#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error\n#> class_1    3321     254       0       0       2       0      17 0.0760\n#> class_2     225    4565       8       0      10      13       3 0.0537\n#> class_3       0      23     546       4       2      37       0 0.1078\n#> class_4       0       0       8      42       0       4       0 0.2222\n#> class_5       5      32       1       0     108       3       0 0.2752\n#> class_6       0       8      24       1       0     253       0 0.1154\n#> class_7      27       0       0       0       0       0     320 0.0778\n#> Totals     3578    4882     587      47     122     310     340 0.0721\n#>                  Rate\n#> class_1 = 273 / 3,594\n#> class_2 = 259 / 4,824\n#> class_3 =    66 / 612\n#> class_4 =     12 / 54\n#> class_5 =    41 / 149\n#> class_6 =    33 / 286\n#> class_7 =    27 / 347\n#> Totals  = 711 / 9,866\n#> \n#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,train = TRUE)`\n#> =======================================================================\n#> Top-7 Hit Ratios: \n#>   k hit_ratio\n#> 1 1  0.927934\n#> 2 2  0.997567\n#> 3 3  0.999595\n#> 4 4  1.000000\n#> 5 5  1.000000\n#> 6 6  1.000000\n#> 7 7  1.000000\n#> \n#> \n#> H2OMultinomialMetrics: deeplearning\n#> ** Reported on validation data. **\n#> ** Metrics reported on temporary validation frame with 9997 samples **\n#> \n#> Validation Set Metrics: \n#> =====================\n#> \n#> MSE: (Extract with `h2o.mse`) 0.0588\n#> RMSE: (Extract with `h2o.rmse`) 0.243\n#> Logloss: (Extract with `h2o.logloss`) 0.194\n#> Mean Per-Class Error: 0.133\n#> Confusion Matrix: Extract with `h2o.confusionMatrix(<model>,valid = TRUE)`)\n#> =========================================================================\n#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n#>         class_1 class_2 class_3 class_4 class_5 class_6 class_7  Error\n#> class_1    3310     302       0       0       1       0      14 0.0874\n#> class_2     246    4637       6       0      15      13       3 0.0575\n#> class_3       0      17     554       2       1      43       0 0.1021\n#> class_4       0       0       3      39       0       3       0 0.1333\n#> class_5       5      43       4       0     103       0       0 0.3355\n#> class_6       1      12      22       4       0     259       0 0.1309\n#> class_7      26       2       0       0       0       0     307 0.0836\n#> Totals     3588    5013     589      45     120     318     324 0.0788\n#>                  Rate\n#> class_1 = 317 / 3,627\n#> class_2 = 283 / 4,920\n#> class_3 =    63 / 617\n#> class_4 =      6 / 45\n#> class_5 =    52 / 155\n#> class_6 =    39 / 298\n#> class_7 =    28 / 335\n#> Totals  = 788 / 9,997\n#> \n#> Hit Ratio Table: Extract with `h2o.hit_ratio_table(<model>,valid = TRUE)`\n#> =======================================================================\n#> Top-7 Hit Ratios: \n#>   k hit_ratio\n#> 1 1  0.921176\n#> 2 2  0.995699\n#> 3 3  0.999400\n#> 4 4  0.999900\n#> 5 5  1.000000\n#> 6 6  1.000000\n#> 7 7  1.000000\n#> \n#> \n#> \n#> \n#> Scoring History: \n#>              timestamp          duration training_speed   epochs iterations\n#> 1  2020-11-20 03:26:16         0.000 sec             NA  0.00000          0\n#> 2  2020-11-20 03:26:23         6.786 sec  15466 obs/sec  0.28676          1\n#> 3  2020-11-20 03:26:35        18.632 sec  22122 obs/sec  1.14690          4\n#> 4  2020-11-20 03:26:46        29.836 sec  24066 obs/sec  2.00553          7\n#> 5  2020-11-20 03:26:57        40.597 sec  25266 obs/sec  2.86748         10\n#> 6  2020-11-20 03:27:08        51.665 sec  25784 obs/sec  3.72709         13\n#> 7  2020-11-20 03:27:18  1 min  2.131 sec  26357 obs/sec  4.58514         16\n#> 8  2020-11-20 03:27:29  1 min 12.694 sec  26737 obs/sec  5.44445         19\n#> 9  2020-11-20 03:27:39  1 min 23.185 sec  27035 obs/sec  6.30235         22\n#> 10 2020-11-20 03:27:50  1 min 33.904 sec  27209 obs/sec  7.16167         25\n#> 11 2020-11-20 03:28:01  1 min 45.092 sec  27221 obs/sec  8.02231         28\n#> 12 2020-11-20 03:28:12  1 min 55.566 sec  27410 obs/sec  8.88425         31\n#> 13 2020-11-20 03:28:22  2 min  5.815 sec  27604 obs/sec  9.74215         34\n#> 14 2020-11-20 03:28:26  2 min  9.524 sec  27631 obs/sec 10.02778         35\n#> 15 2020-11-20 03:29:34  2 min 13.240 sec  27662 obs/sec 10.31546         36\n#> 16 2020-11-20 03:29:42  2 min 21.230 sec  27553 obs/sec 10.89011         38\n#> 17 2020-11-20 03:29:53  2 min 32.027 sec  27608 obs/sec 11.75131         41\n#> 18 2020-11-20 03:29:57  2 min 35.687 sec  27633 obs/sec 12.03591         42\n#>           samples training_rmse training_logloss training_r2\n#> 1        0.000000            NA               NA          NA\n#> 2   100084.000000       0.43054          0.57040     0.90484\n#> 3   400286.000000       0.36238          0.41464     0.93258\n#> 4   699960.000000       0.32572          0.34221     0.94553\n#> 5  1000792.000000       0.30290          0.29892     0.95290\n#> 6  1300810.000000       0.29379          0.28232     0.95569\n#> 7  1600283.000000       0.27523          0.24840     0.96111\n#> 8  1900194.000000       0.26909          0.23856     0.96283\n#> 9  2199614.000000       0.26574          0.23293     0.96375\n#> 10 2499530.000000       0.25792          0.21815     0.96585\n#> 11 2799908.000000       0.24960          0.20567     0.96802\n#> 12 3100736.000000       0.24127          0.19288     0.97012\n#> 13 3400156.000000       0.23989          0.19054     0.97046\n#> 14 3499847.000000       0.23634          0.18423     0.97132\n#> 15 3600249.000000       0.23974          0.18952     0.97014\n#> 16 3800813.000000       0.23554          0.18496     0.97118\n#> 17 4101382.000000       0.22893          0.17532     0.97278\n#> 18 4200713.000000       0.23129          0.17767     0.97221\n#>    training_classification_error validation_rmse validation_logloss\n#> 1                             NA              NA                 NA\n#> 2                        0.23738         0.43214            0.57193\n#> 3                        0.17584         0.36707            0.41940\n#> 4                        0.13817         0.33385            0.35832\n#> 5                        0.12237         0.31106            0.31207\n#> 6                        0.11541         0.29951            0.29316\n#> 7                        0.10000         0.28369            0.26356\n#> 8                        0.09583         0.27820            0.25226\n#> 9                        0.09394         0.27289            0.24474\n#> 10                       0.08877         0.26676            0.23336\n#> 11                       0.08449         0.26008            0.22383\n#> 12                       0.07694         0.25782            0.21981\n#> 13                       0.07594         0.25191            0.21062\n#> 14                       0.07316         0.25231            0.20847\n#> 15                       0.07653         0.25506            0.21307\n#> 16                       0.07176         0.24595            0.19942\n#> 17                       0.06902         0.24357            0.19463\n#> 18                       0.07207         0.24256            0.19427\n#>    validation_r2 validation_classification_error\n#> 1             NA                              NA\n#> 2        0.90512                         0.24438\n#> 3        0.93154                         0.18391\n#> 4        0.94337                         0.14583\n#> 5        0.95084                         0.13173\n#> 6        0.95442                         0.11874\n#> 7        0.95911                         0.10675\n#> 8        0.96068                         0.10155\n#> 9        0.96217                         0.09865\n#> 10       0.96384                         0.09455\n#> 11       0.96563                         0.08876\n#> 12       0.96623                         0.08796\n#> 13       0.96776                         0.08426\n#> 14       0.96766                         0.08376\n#> 15       0.96567                         0.08693\n#> 16       0.96808                         0.08182\n#> 17       0.96869                         0.07752\n#> 18       0.96895                         0.07882\n#> \n#> Variable Importances: (Extract with `h2o.varimp`) \n#> =================================================\n#> \n#> Variable Importances: \n#>                             variable relative_importance scaled_importance\n#> 1                          Elevation            1.000000          1.000000\n#> 2 Horizontal_Distance_To_Fire_Points            0.956722          0.956722\n#> 3    Horizontal_Distance_To_Roadways            0.889055          0.889055\n#> 4             Wilderness_Area.area_0            0.740660          0.740660\n#> 5             Wilderness_Area.area_3            0.607293          0.607293\n#>   percentage\n#> 1   0.048252\n#> 2   0.046164\n#> 3   0.042899\n#> 4   0.035738\n#> 5   0.029303\n#> \n#> ---\n#>                       variable relative_importance scaled_importance percentage\n#> 51           Soil_Type.type_17            0.157231          0.157231   0.007587\n#> 52            Soil_Type.type_6            0.154352          0.154352   0.007448\n#> 53           Soil_Type.type_14            0.147832          0.147832   0.007133\n#> 54           Soil_Type.type_24            0.140650          0.140650   0.006787\n#> 55       Soil_Type.missing(NA)            0.000000          0.000000   0.000000\n#> 56 Wilderness_Area.missing(NA)            0.000000          0.000000   0.000000"},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"cross-validation","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46.5.9 Cross-Validation","text":"N-fold cross-validation, specify nfolds>1 instead (addition ) validation frame, N+1 models built: 1 model full training data, N models 1/N-th data held (different holdout strategies). N models score held data, combined predictions full training data scored get cross-validation metrics.N-fold cross-validation especially useful early stopping, main model pick ideal number epochs convergence behavior cross-validation models.","code":"\ndlmodel <- h2o.deeplearning(\n  x=predictors,\n  y=response, \n  training_frame=train,\n  hidden=c(10,10),\n  epochs=1,\n  nfolds=5,\n  fold_assignment=\"Modulo\" # can be \"AUTO\", \"Modulo\", \"Random\" or \"Stratified\"\n  )\ndlmodel"},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"regression-and-binary-classification","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46.6 Regression and Binary Classification","text":"Assume want turn multi-class problem binary classification problem. create binary response follows:Let’s build quick model inspect model:Instead binary classification model, find regression model (H2ORegressionModel) contains 1 output neuron (instead 2). reason response numerical feature (ordinal numbers 0 1), H2O Deep Learning run distribution=AUTO, defaulted Gaussian regression problem real-valued response. H2O Deep Learning supports regression distributions Gaussian Poisson, Gamma, Tweedie, Laplace. also supports Huber loss per-row offsets specified via offset_column. refer H2O Deep Learning regression code examples information.perform classification, response must first turned categorical (factor) feature:Now model performs (binary) classification, multiple (2) output neurons.","code":"\ntrain$bin_response <- ifelse(train[,response] == \"class_1\", 0, 1)\ndlmodel <- h2o.deeplearning(\n  x=predictors,\n  y=\"bin_response\", \n  training_frame=train,\n  hidden=c(10,10),\n  epochs=0.1\n)\nsummary(dlmodel)\ntrain$bin_response <- as.factor(train$bin_response) ##make categorical\ndlmodel <- h2o.deeplearning(\n  x=predictors,\n  y=\"bin_response\", \n  training_frame=train,\n  hidden=c(10,10),\n  epochs=0.1\n  #balance_classes=T    ## enable this for high class imbalance\n)\nsummary(dlmodel) ## Now the model metrics contain AUC for binary classification\nplot(h2o.performance(dlmodel)) ## display ROC curve"},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"unsupervised-anomaly-detection","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46.7 Unsupervised Anomaly detection","text":"instructions build unsupervised models H2O Deep Learning, refer previous Tutorial Anomaly Detection H2O Deep Learning MNIST Anomaly detection code example, well Stacked AutoEncoder R code example another one Unsupervised Pretraining AutoEncoder R code example.","code":""},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"h2o-deep-learning-tips-tricks","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46.8 H2O Deep Learning Tips & Tricks","text":"","code":""},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"performance-tuning","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46.8.1 Performance Tuning","text":"Definitive H2O Deep Learning Performance Tuning blog post covers many following points affect computational efficiency, ’s highly recommended.","code":""},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"activation-functions","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46.8.2 Activation Functions","text":"sigmoids used historically neural networks, H2O Deep Learning implements Tanh, scaled shifted variant sigmoid symmetric around 0. Since output values bounded -1..1, stability neural network rarely endangered. However, derivative tanh function always non-zero back-propagation (training) weights computationally expensive rectified linear units, Rectifier, max(0,x) vanishing gradient x<=0, leading much faster training speed large networks often fastest path accuracy larger problems. case encounter instabilities Rectifier (case model building automatically aborted), try limited value re-scale weights: max_w2=10. Maxout activation function computationally expensive, can lead higher accuracy. generalized version Rectifier two non-zero channels. practice, Rectifier (RectifierWithDropout, see ) versatile performant option problems.","code":""},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"generalization-techniques","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46.8.3 Generalization Techniques","text":"L1 L2 penalties can applied specifying l1 l2 parameters. Intuition: L1 lets strong weights survive (constant pulling force towards zero), L2 prevents single weight getting big. Dropout recently introduced powerful generalization technique, available parameter per layer, including input layer. input_dropout_ratio controls amount input layer neurons randomly dropped (set zero), hidden_dropout_ratios specified hidden layer. former controls overfitting respect input data (useful high-dimensional noisy data), latter controls overfitting learned features. Note hidden_dropout_ratios require activation function end …WithDropout.","code":""},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"early-stopping-and-optimizing-for-lowest-validation-error","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46.8.4 Early stopping and optimizing for lowest validation error","text":"default, Deep Learning training stops stopping_metric improve least stopping_tolerance (0.01 means 1% improvement) stopping_rounds consecutive scoring events training (validation) data. default, overwrite_with_best_model enabled model returned training specified number epochs (stopping early due convergence) model best training set error (according metric specified stopping_metric), , validation set provided, lowest validation set error. Note training validation set errors can based subset training validation data, depending values score_validation_samples score_training_samples, see . early stopping predefined error rate training data (accuracy classification MSE regression), specify classification_stop regression_stop.","code":""},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"training-samples-per-mapreduce-iteration","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46.8.5 Training Samples per MapReduce Iteration","text":"parameter train_samples_per_iteration matters especially multi-node operation. controls number rows trained MapReduce iteration. Depending value selected, one MapReduce pass can sample observations, multiple passes needed train one epoch. H2O compute nodes communicate agree best model coefficients (weights/biases) far, model may scored (controlled parameters ). default value -2 indicates auto-tuning, attemps keep communication overhead 5% total runtime. parameter target_ratio_comm_to_comp controls ratio. parameter explained detail H2O Deep Learning booklet,","code":""},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"categorical-data","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46.8.6 Categorical Data","text":"categorical data, feature K factor levels automatically one-hot encoded (horizontalized) K-1 input neurons. Hence, input neuron layer can grow substantially datasets high factor counts. cases, might make sense reduce number hidden neurons first hidden layer, large numbers factor levels can handled. limit 1 neuron first hidden layer, resulting model similar logistic regression stochastic gradient descent, except classification problems, ’s still softmax output layer, activation function necessarily sigmoid (Tanh). variable importances computed, recommended turn use_all_factor_levels (K input neurons K levels). experimental option max_categorical_features uses feature hashing reduce number input neurons via hash trick expense hash collisions reduced accuracy. Another way reduce dimensionality (categorical) features use h2o.glrm(), refer GLRM tutorial details.","code":""},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"sparse-data","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46.8.7 Sparse Data","text":"input data sparse (many zeros), might make sense enable sparse option. result input standardized (0 mean, 1 variance), de-scaled (1 variance) 0 values remain 0, leading efficient back-propagation. Sparsity also reason CPU implementations can faster GPU implementations, can take advantage /else statements effectively.","code":""},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"missing-values","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46.8.8 Missing Values","text":"H2O Deep Learning automatically mean imputation missing values training (leaving input layer activation 0 standardizing values). testing, missing test set values also treated way default. See h2o.impute function mean imputation.","code":""},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"loss-functions-distributions-offsets-observation-weights","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46.8.9 Loss functions, Distributions, Offsets, Observation Weights","text":"H2O Deep Learning supports advanced statistical features multiple loss functions, non-Gaussian distributions, per-row offsets observation weights. addition Gaussian distributions Squared loss, H2O Deep Learning supports Poisson, Gamma, Tweedie Laplace distributions. also supports Absolute Huber loss per-row offsets specified via offset_column. Observation weights supported via user-specified weights_column.refer H2O Deep Learning R test code examples information.","code":""},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"exporting-weights-and-biases","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46.8.10 Exporting Weights and Biases","text":"model parameters (weights connecting two adjacent layers per-neuron bias terms) can stored H2O Frames (like dataset) enabling export_weights_and_biases, can accessed follows:","code":"iris_dl <- h2o.deeplearning(1:4,5,as.h2o(iris),\n             export_weights_and_biases=T)\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |======================================================================| 100%\nh2o.weights(iris_dl, matrix_id=1)\n#>   Sepal.Length Sepal.Width Petal.Length Petal.Width\n#> 1      0.00796      0.0524     -0.08179     -0.0547\n#> 2      0.03736      0.1534      0.13268      0.1454\n#> 3     -0.01872      0.1114     -0.18782     -0.1408\n#> 4     -0.11893     -0.0144      0.06212      0.1130\n#> 5     -0.00984      0.1280     -0.15188      0.1385\n#> 6     -0.05972      0.0916      0.00419     -0.1071\n#> \n#> [200 rows x 4 columns]\nh2o.weights(iris_dl, matrix_id=2)\n#>        C1      C2      C3       C4      C5      C6      C7       C8      C9\n#> 1  0.0465 -0.1095  0.0436  0.07882  0.0711 -0.0186 -0.0820 -0.09476 -0.0133\n#> 2 -0.0310  0.0407 -0.0462  0.00213 -0.0241 -0.0179  0.0104 -0.09055  0.0741\n#> 3 -0.0995 -0.0281 -0.1295  0.03595 -0.1058  0.0261  0.0455 -0.09053  0.0415\n#> 4  0.0814  0.0774  0.0520  0.03100  0.1085  0.0889 -0.0411  0.00283 -0.0949\n#> 5  0.0999  0.0817 -0.0449 -0.02073 -0.0957 -0.0999 -0.0264  0.05706 -0.0543\n#> 6 -0.1114  0.1008 -0.1016 -0.11798  0.0561  0.0789 -0.0798  0.05676 -0.0738\n#>        C10     C11      C12      C13     C14     C15     C16     C17      C18\n#> 1  0.00517  0.0498 -0.01020  0.02896 -0.0122 -0.1116  0.0756 -0.1171  0.06473\n#> 2  0.10793  0.0213  0.04943  0.00473 -0.0684 -0.0210  0.1328 -0.0723 -0.08566\n#> 3 -0.07389 -0.0101  0.00656 -0.01021 -0.0897  0.0994  0.0871  0.0611  0.00803\n#> 4  0.07844 -0.0675 -0.02354  0.07888  0.0178 -0.1056  0.0146 -0.0238  0.11052\n#> 5 -0.01753  0.0389  0.09626 -0.07570 -0.0258 -0.0967 -0.0850  0.0606 -0.10822\n#> 6  0.00324  0.0222 -0.05676 -0.09071 -0.0873  0.0659 -0.1169  0.1081  0.07081\n#>       C19     C20     C21     C22      C23      C24      C25      C26      C27\n#> 1 -0.1046 -0.0166 -0.1213 -0.0996 -0.02018 -0.09506  0.00439  0.03220  0.00958\n#> 2  0.1030 -0.1301 -0.0886  0.0432 -0.06524 -0.00245  0.05370  0.01215  0.06714\n#> 3  0.0395 -0.0842 -0.0827 -0.0423 -0.09799  0.09814  0.02524 -0.01772 -0.00768\n#> 4 -0.0485 -0.1591  0.0494 -0.0646 -0.06662 -0.07631  0.08138  0.04345  0.05303\n#> 5 -0.0906 -0.0616 -0.0837  0.1135  0.00207 -0.02798 -0.09369 -0.00993 -0.05584\n#> 6 -0.0400  0.0872 -0.0220 -0.0977 -0.07580 -0.07600 -0.00641  0.02352 -0.05926\n#>       C28     C29      C30     C31     C32     C33      C34       C35      C36\n#> 1  0.0272 -0.0195 -0.00218 -0.0816  0.0920  0.0891 -0.07083 -0.074438 -0.06834\n#> 2 -0.0491 -0.0777 -0.04750  0.0163 -0.0841 -0.0158 -0.03827  0.039149 -0.06379\n#> 3  0.1053  0.0530 -0.03415 -0.0539  0.0542  0.0918 -0.03704  0.000316  0.02297\n#> 4 -0.0296 -0.0215 -0.05852 -0.0147  0.1126  0.0757 -0.10917 -0.104685  0.00504\n#> 5  0.0111 -0.0246 -0.07814  0.1209 -0.1105  0.0497 -0.00612  0.034458 -0.05902\n#> 6 -0.0795  0.1106  0.07811  0.1262  0.0821  0.0399  0.07686  0.112702  0.03121\n#>        C37     C38      C39     C40     C41      C42     C43     C44     C45\n#> 1 -0.05408  0.0225 -0.00842 -0.1187  0.0739  0.00635 -0.0805  0.0921  0.0235\n#> 2  0.04124  0.0915 -0.12458 -0.0487 -0.0163 -0.12214  0.0778 -0.1051  0.0916\n#> 3 -0.00215 -0.0228 -0.01176  0.0196  0.0910  0.09060 -0.0803  0.0592  0.0652\n#> 4  0.10315 -0.0233  0.01607  0.0319  0.0656 -0.10637  0.1208 -0.1292  0.0777\n#> 5 -0.03806  0.1265 -0.02216  0.1261 -0.0040 -0.12519  0.0231 -0.0469  0.1046\n#> 6  0.03472  0.1054 -0.02221  0.1120  0.0304 -0.01493 -0.0504  0.0788 -0.0763\n#>        C46     C47     C48     C49     C50      C51      C52     C53      C54\n#> 1 -0.07186 -0.0450  0.0366 -0.0193 -0.0283 -0.00323  0.00888  0.0441  0.06183\n#> 2  0.03675  0.1078  0.0522  0.0988  0.0592 -0.03600 -0.02050  0.0546 -0.06854\n#> 3 -0.03591 -0.0993  0.1261 -0.1079  0.0892  0.07524 -0.00341 -0.0794 -0.08465\n#> 4  0.13047 -0.0820 -0.1394  0.1091 -0.1071 -0.08141  0.00751 -0.0196  0.09403\n#> 5  0.11292 -0.0134  0.0835 -0.0728 -0.1032  0.03095  0.05855  0.0795  0.04242\n#> 6 -0.00516  0.0723 -0.0913 -0.0119 -0.0520 -0.06228  0.03579  0.0205 -0.00257\n#>       C55      C56      C57     C58     C59     C60     C61     C62       C63\n#> 1 -0.0419  0.06653 -0.02064 -0.0873 -0.0496 -0.0519  0.0893 -0.0551  0.090068\n#> 2 -0.0934 -0.03070  0.03025  0.0153 -0.0473 -0.0450 -0.0658 -0.0875 -0.081945\n#> 3 -0.0664 -0.09085 -0.00822  0.0295 -0.0800  0.0102  0.0199  0.0525 -0.093481\n#> 4  0.0550  0.00429 -0.11825 -0.0882  0.0268 -0.1007 -0.0972 -0.0905 -0.000336\n#> 5  0.0223  0.03773 -0.11006  0.0874  0.0416 -0.0476 -0.0492  0.1090  0.131046\n#> 6  0.0262 -0.01780 -0.02332 -0.0656 -0.0041 -0.0937 -0.0325 -0.0546  0.021579\n#>       C64     C65     C66      C67     C68     C69     C70      C71       C72\n#> 1 -0.0281 -0.1037  0.0510 -0.02837  0.0544 -0.0402 -0.0550  0.03376 -0.047851\n#> 2 -0.0581 -0.0906 -0.0513  0.01167  0.0580  0.1065  0.1368  0.06565  0.016624\n#> 3  0.0678 -0.0770  0.0310  0.10816 -0.0307  0.0457 -0.0242 -0.01571  0.059846\n#> 4  0.0214  0.0834 -0.0556  0.01432 -0.1186 -0.0546 -0.0487 -0.07035 -0.102846\n#> 5 -0.0970  0.0571 -0.0952  0.04021  0.0132  0.0987  0.0636  0.00649 -0.083315\n#> 6 -0.0225  0.0755 -0.0641  0.00621  0.0047  0.0728 -0.1122  0.03780 -0.000375\n#>        C73      C74     C75     C76      C77      C78    C79     C80       C81\n#> 1 -0.11732  0.07100 -0.0903  0.0508  0.03185  0.02267 0.1123  0.0742 -3.55e-02\n#> 2 -0.00512 -0.04240 -0.1177  0.1109  0.04258 -0.11612 0.0312  0.0826 -2.71e-02\n#> 3  0.06567  0.00884  0.0633  0.0131 -0.04068 -0.02366 0.1199 -0.0867 -1.93e-02\n#> 4  0.01926  0.00886 -0.0544 -0.0276  0.11046  0.06483 0.0512 -0.1099 -1.47e-02\n#> 5  0.05164  0.04142  0.0368 -0.0952 -0.07763  0.00915 0.0467  0.0303 -2.72e-05\n#> 6  0.12767  0.07201 -0.0630 -0.0597  0.00775  0.06622 0.0284 -0.0867  2.95e-02\n#>       C82     C83     C84      C85     C86      C87     C88     C89     C90\n#> 1  0.1248 -0.0298  0.0245 -0.07174 -0.0527 -0.10474 -0.0419  0.0957  0.1277\n#> 2  0.0863 -0.0643 -0.0791  0.08628  0.0242  0.04843 -0.0679  0.0524  0.1303\n#> 3 -0.1135 -0.0415 -0.0607  0.08333  0.0310 -0.00957 -0.0295  0.1271 -0.0662\n#> 4  0.0342 -0.0952 -0.0394  0.00664  0.0428 -0.03103 -0.1223  0.0160 -0.0318\n#> 5  0.0178  0.1108 -0.0236  0.02130 -0.0846 -0.06329  0.0636 -0.1057  0.0753\n#> 6  0.0103  0.0630  0.0766 -0.09502  0.1178  0.08095  0.0758 -0.1083  0.0612\n#>       C91      C92     C93     C94     C95     C96     C97     C98     C99\n#> 1  0.0462  0.08691  0.0897 -0.0143  0.0264 -0.0654  0.0377  0.0306 -0.0429\n#> 2 -0.0987  0.02555 -0.0787 -0.0641  0.0939  0.1071  0.0400  0.0848 -0.1121\n#> 3 -0.0185  0.10440  0.0594  0.0850  0.0807  0.0742 -0.0145  0.1164  0.0532\n#> 4  0.0228  0.00242  0.0576  0.0192 -0.0641 -0.0679 -0.0734 -0.0834 -0.0216\n#> 5 -0.0327 -0.11211 -0.0312  0.0335 -0.0525 -0.1217  0.0437 -0.1403 -0.1242\n#> 6  0.1193 -0.04983  0.0755  0.1163 -0.0436 -0.0276  0.0767 -0.0139 -0.0653\n#>      C100    C101     C102    C103    C104    C105    C106    C107    C108\n#> 1 -0.1141  0.1261 -0.03210 -0.0938 -0.1061  0.1139 -0.0498  0.0689 -0.0565\n#> 2 -0.0964  0.0423 -0.00512 -0.0652  0.1264  0.0327  0.1129  0.0532  0.0294\n#> 3  0.1193  0.0665 -0.01009 -0.0185 -0.0286 -0.1081  0.0831 -0.0191 -0.0270\n#> 4  0.1139 -0.0714 -0.11221  0.0196  0.0303 -0.0001 -0.0345 -0.1524 -0.0512\n#> 5  0.0412 -0.0120  0.01501  0.0491  0.0882 -0.0535  0.0785 -0.0754 -0.0429\n#> 6 -0.0862  0.0671  0.09162 -0.0248  0.0439  0.0380 -0.0200 -0.1123 -0.0396\n#>       C109     C110    C111     C112     C113    C114    C115    C116    C117\n#> 1  0.11503  0.07412 -0.0420 -0.02711  0.09977  0.0823  0.0378  0.0866  0.0492\n#> 2 -0.02634 -0.00723 -0.0667  0.02429 -0.00372  0.1116 -0.0156  0.0911 -0.0560\n#> 3 -0.08935 -0.04766  0.0465 -0.06340  0.06570 -0.1124 -0.1201  0.0514 -0.0405\n#> 4 -0.04889  0.04083  0.0783 -0.00769 -0.01486  0.0330  0.0806 -0.0637 -0.0688\n#> 5  0.05483  0.04193 -0.0643  0.06601  0.04655 -0.0204  0.0687  0.0510 -0.0692\n#> 6 -0.00251 -0.00414 -0.0899 -0.08483 -0.04445  0.0646  0.0675 -0.0982 -0.0887\n#>      C118     C119    C120    C121     C122    C123    C124     C125    C126\n#> 1  0.0612 -0.00107 0.02497  0.1162 -0.01583 -0.0657 -0.0411  0.12093  0.0158\n#> 2  0.0978  0.07340 0.11923 -0.0618 -0.04678  0.0762  0.0218 -0.05517  0.0891\n#> 3  0.0797  0.13426 0.00742  0.0315  0.00669  0.0824 -0.0559 -0.05673 -0.1109\n#> 4 -0.0748 -0.05128 0.06179 -0.0902  0.12589  0.0503  0.1347  0.02471 -0.0566\n#> 5  0.1263  0.07567 0.00516 -0.0951  0.00320  0.0454  0.0953  0.00395 -0.0845\n#> 6 -0.0282 -0.02412 0.00310 -0.0938  0.10188  0.0992 -0.0060  0.00587 -0.1206\n#>        C127     C128    C129    C130    C131     C132     C133    C134    C135\n#> 1  0.090928  0.00486  0.0566  0.0684  0.1093  0.00677  0.02756  0.0338 -0.0832\n#> 2  0.076693 -0.04962  0.0812 -0.1009  0.0712 -0.03638  0.00412 -0.1111 -0.0922\n#> 3 -0.010088  0.05781 -0.0644  0.0339 -0.0777 -0.07366  0.02398  0.0985 -0.1089\n#> 4  0.043642  0.05265  0.0978  0.0568 -0.0676 -0.03234  0.11928 -0.1029 -0.0591\n#> 5  0.000126 -0.09309 -0.0997  0.0732 -0.0101  0.11830 -0.05768  0.0431  0.0622\n#> 6  0.040048  0.00438 -0.0514  0.0863 -0.1153  0.09065  0.11514 -0.0477  0.0641\n#>       C136    C137     C138     C139     C140    C141     C142     C143    C144\n#> 1  0.04905  0.0669 -0.00892 -0.01743 -0.01033 -0.0976  0.00747  0.04476 -0.0289\n#> 2 -0.00962 -0.0806  0.12274 -0.09662 -0.02635  0.0168  0.06538  0.01851 -0.1096\n#> 3 -0.06242 -0.0884 -0.03374 -0.08997 -0.08453 -0.0102  0.01346 -0.01596  0.0645\n#> 4 -0.10898  0.0791  0.05837  0.00861  0.10179 -0.0655 -0.07928  0.02056  0.0247\n#> 5 -0.05495  0.1125 -0.05631 -0.04248  0.00896  0.1147 -0.10580  0.00887 -0.0151\n#> 6  0.03876  0.1222  0.07789 -0.03381 -0.03409 -0.0238  0.04382 -0.06370 -0.0674\n#>       C145     C146     C147      C148    C149     C150     C151    C152\n#> 1  0.00721  0.05120 -0.07245 -0.000306 -0.1053  0.00926 -0.09905  0.1209\n#> 2 -0.07532  0.00276 -0.09409  0.119070  0.0914 -0.03677 -0.03606 -0.1187\n#> 3  0.08570  0.04919 -0.00791 -0.044381  0.0467  0.01959 -0.05031  0.1196\n#> 4  0.09734 -0.00110 -0.09585 -0.011292  0.0698  0.12555  0.02274 -0.0403\n#> 5  0.04589 -0.01145  0.01207 -0.027290  0.0164  0.12170 -0.00585  0.0282\n#> 6 -0.01175  0.05045  0.10169 -0.015577 -0.0949  0.05535  0.10587 -0.1389\n#>       C153    C154     C155    C156     C157    C158    C159    C160    C161\n#> 1  0.06538 -0.1007  0.09324  0.1187  0.01293  0.1047  0.0440  0.0274  0.0448\n#> 2 -0.07462  0.0524 -0.00618  0.0859  0.06160 -0.0167 -0.0573 -0.0696  0.0993\n#> 3 -0.13834 -0.0229  0.00823 -0.0927  0.10409  0.0734  0.0310 -0.1217  0.1021\n#> 4  0.01341 -0.0104 -0.09765 -0.0901 -0.00982  0.0769 -0.0882  0.0116 -0.1180\n#> 5  0.00774 -0.0809 -0.09593 -0.0516 -0.11520 -0.1044 -0.0909  0.1006 -0.0803\n#> 6 -0.09055 -0.0722 -0.00549 -0.1079  0.02840  0.0503  0.0305 -0.1056 -0.0325\n#>      C162    C163     C164    C165     C166    C167    C168     C169    C170\n#> 1 -0.0770  0.0211  0.00251  0.0452 -0.03784  0.0553 -0.0809 -0.00343  0.0834\n#> 2  0.0926 -0.0689 -0.01158  0.0478 -0.11060  0.0441 -0.1129  0.00859 -0.0722\n#> 3 -0.0860  0.0686  0.02537 -0.1053  0.03189 -0.1133 -0.0378  0.09477 -0.0352\n#> 4 -0.1149 -0.0860 -0.06355 -0.0608  0.05246 -0.0377  0.0293  0.02084  0.0146\n#> 5  0.0149 -0.0477  0.08182  0.0323 -0.09866  0.0210  0.0790 -0.01953 -0.1253\n#> 6 -0.0303 -0.0820 -0.04820  0.0786  0.00416  0.1112  0.0430  0.05029 -0.1441\n#>       C171    C172    C173    C174    C175    C176    C177    C178    C179\n#> 1  0.02757 -0.1148  0.0550 -0.0101 -0.1003  0.0601  0.0225 -0.1106 -0.0915\n#> 2  0.11150 -0.0123 -0.0961 -0.0216 -0.0568  0.0739  0.0120  0.0270 -0.0339\n#> 3  0.01311  0.1147  0.0564  0.0476 -0.0756  0.0366 -0.0735  0.0399  0.0590\n#> 4 -0.00972 -0.0937  0.0616  0.0321 -0.1145 -0.0110 -0.0814  0.0361  0.0210\n#> 5  0.01359 -0.0153  0.0134 -0.0889  0.0517 -0.0464  0.0347 -0.0651  0.0264\n#> 6 -0.06683  0.0158  0.0779 -0.1010 -0.1170  0.0351 -0.0258  0.0400 -0.0693\n#>      C180    C181      C182    C183    C184    C185     C186    C187    C188\n#> 1  0.0831 -0.0834  7.87e-02 -0.0755  0.0130  0.0636  0.08468  0.0054  0.0922\n#> 2  0.0783 -0.0530 -3.80e-02  0.0036  0.0551 -0.0108  0.00275  0.0831  0.0460\n#> 3  0.0700 -0.0340 -7.55e-02 -0.0936  0.0830 -0.0450 -0.00995  0.0102  0.0350\n#> 4  0.1263 -0.1112  1.25e-01  0.0600 -0.1234  0.0765 -0.09840  0.0446  0.0729\n#> 5 -0.0348  0.0740  3.77e-05 -0.0960  0.0827  0.1147  0.04449 -0.1361  0.0728\n#> 6 -0.1145 -0.0543  2.08e-02  0.0466  0.0363  0.0107 -0.10155 -0.0485 -0.0543\n#>      C189     C190    C191    C192    C193     C194     C195    C196    C197\n#> 1 -0.0702  0.09825  0.1152  0.0161  0.1129  0.07579  0.06587  0.0933  0.0639\n#> 2 -0.1122  0.06139  0.1182 -0.0117  0.1116  0.11156  0.02069 -0.1009 -0.0872\n#> 3  0.0876  0.05208 -0.0576  0.1145 -0.0487  0.01280 -0.09603 -0.1153  0.0385\n#> 4  0.0698 -0.06449  0.0967  0.0691 -0.0197 -0.01377  0.04898  0.1180 -0.0629\n#> 5 -0.0849 -0.09553 -0.0951  0.0223  0.0948 -0.05589  0.01636  0.0302 -0.1344\n#> 6  0.0142 -0.00847 -0.0389 -0.0752 -0.0502  0.00129  0.00672  0.1169  0.0258\n#>      C198     C199    C200\n#> 1  0.0364  0.07103  0.0803\n#> 2  0.0022  0.10607 -0.1129\n#> 3  0.0956  0.00358  0.1221\n#> 4  0.0554 -0.06344 -0.0148\n#> 5 -0.0633  0.08324  0.0842\n#> 6  0.0515  0.02187  0.0790\n#> \n#> [200 rows x 200 columns]\nh2o.weights(iris_dl, matrix_id=3)\n#>       C1     C2      C3     C4     C5      C6      C7    C8      C9    C10\n#> 1 -0.600 -0.034  0.0648  0.518  0.137  0.2003  0.3685 0.241 -0.1409 0.1230\n#> 2  0.434  0.121 -0.1345 -0.360 -0.469 -0.1170 -0.5432 0.670 -0.3621 0.5953\n#> 3 -0.390 -0.457  0.6642 -0.507 -0.393 -0.0759 -0.0357 0.236 -0.0787 0.0134\n#>      C11     C12    C13    C14     C15    C16   C17    C18    C19    C20    C21\n#> 1  0.135  0.0166 0.0547 -0.585 -0.1309  0.251 0.596  0.479  0.284 -0.232  0.415\n#> 2 -0.446 -0.2548 0.2520 -0.579 -0.2655  0.216 0.313 -0.399  0.442  0.622  0.514\n#> 3 -0.544  0.2491 0.5121 -0.106 -0.0109 -0.422 0.421  0.282 -0.190 -0.320 -0.589\n#>      C22    C23    C24    C25    C26    C27     C28     C29     C30    C31\n#> 1  0.352 -0.669  0.291 0.0416  0.046 -0.354  0.1140 -0.2854 -0.0985 -0.510\n#> 2 -0.389 -0.574 -0.338 0.6237 -0.456  0.497 -0.0339 -0.0538  0.4771  0.493\n#> 3 -0.198 -0.326 -0.106 0.3211 -0.582 -0.111  0.5719  0.6656  0.3636 -0.683\n#>       C32    C33    C34    C35    C36     C37    C38    C39     C40    C41\n#> 1 -0.5764 0.0976  0.293  0.278 -0.475  0.5719  0.681 -0.490  0.0105 -0.442\n#> 2 -0.0646 0.2823  0.637 -0.397 -0.265 -0.0489  0.400 -0.515 -0.3607  0.374\n#> 3  0.4944 0.3393 -0.566 -0.550  0.480  0.1938 -0.524 -0.659 -0.1355 -0.621\n#>       C42     C43    C44    C45    C46    C47     C48    C49    C50    C51\n#> 1 -0.6330  0.5466  0.492 0.4509 -0.202  0.612  0.0583  0.615 -0.029 -0.213\n#> 2  0.0197  0.0797  0.126 0.4147  0.194 -0.650  0.2747 -0.208 -0.407 -0.221\n#> 3  0.4438 -0.0380 -0.389 0.0871  0.285 -0.127 -0.5634  0.182 -0.441 -0.140\n#>      C52    C53     C54     C55   C56     C57     C58    C59    C60    C61\n#> 1 -0.600 -0.460  0.5427 -0.3430 0.315 -0.1174 -0.0588 -0.395 -0.513 -0.531\n#> 2 -0.130 -0.300 -0.2125  0.3765 0.170 -0.0142  0.2672 -0.543 -0.247 -0.320\n#> 3 -0.538  0.287 -0.0514 -0.0737 0.583 -0.3330 -0.5998 -0.583  0.325 -0.253\n#>      C62     C63     C64    C65    C66    C67   C68    C69   C70   C71    C72\n#> 1 -0.191  0.3235  0.4566 -0.333  0.575 0.3329 0.296 -0.600 0.591 0.437 -0.549\n#> 2 -0.677 -0.0967  0.5469  0.527  0.224 0.0949 0.630 -0.637 0.619 0.054  0.331\n#> 3  0.105 -0.5705 -0.0725 -0.268 -0.104 0.0777 0.109  0.085 0.293 0.584 -0.555\n#>       C73     C74    C75    C76    C77    C78    C79    C80    C81   C82    C83\n#> 1  0.6522 -0.3810 -0.600 -0.539  0.351 -0.287 -0.594  0.396 -0.430 0.328  0.353\n#> 2 -0.1306  0.5129 -0.497  0.483 -0.234 -0.473  0.152 -0.431 -0.307 0.363  0.206\n#> 3  0.0844 -0.0782 -0.306  0.280  0.649  0.665 -0.485  0.572 -0.559 0.240 -0.161\n#>         C84    C85     C86     C87    C88    C89    C90    C91    C92    C93\n#> 1  0.226810  0.282 -0.5522  0.2455 -0.407  0.452 -0.215  0.449 -0.106 -0.250\n#> 2 -0.000313 -0.355 -0.4411  0.2810  0.651 -0.372  0.254 -0.301  0.479  0.426\n#> 3  0.115621 -0.264 -0.0365 -0.0629  0.312 -0.273  0.164  0.251 -0.493  0.266\n#>     C94    C95    C96   C97     C98     C99   C100    C101    C102   C103\n#> 1 0.400  0.413  0.252 0.230 -0.5025 -0.1547 -0.446 -0.0758 -0.0717  0.334\n#> 2 0.384 -0.412  0.118 0.255 -0.1173 -0.0825  0.465  0.1318 -0.4162  0.613\n#> 3 0.024  0.504 -0.143 0.551 -0.0818  0.0203 -0.576 -0.4444  0.5721 -0.108\n#>     C104    C105    C106    C107    C108   C109    C110   C111    C112   C113\n#> 1 -0.284 -0.6307 -0.0368 -0.0389 -0.6355  0.594 -0.2257  0.555 -0.6732  0.470\n#> 2  0.432 -0.0855 -0.6143  0.6428  0.2378 -0.357  0.5485  0.523 -0.0368 -0.578\n#> 3  0.660 -0.4420  0.4393  0.4980 -0.0359  0.462 -0.0291 -0.358 -0.2280  0.221\n#>     C114    C115   C116    C117   C118   C119    C120   C121  C122    C123\n#> 1 -0.252  0.0362 -0.502 -0.0704 -0.484 0.0002 -0.0313 -0.613 0.302 -0.0131\n#> 2  0.493 -0.2665 -0.631  0.0762 -0.553 0.6201  0.2895  0.606 0.173 -0.3939\n#> 3 -0.471  0.1518 -0.218 -0.3191  0.251 0.1345 -0.6085 -0.153 0.109 -0.3652\n#>    C124   C125   C126   C127  C128   C129   C130   C131    C132    C133    C134\n#> 1 0.365 -0.671  0.471 -0.260 0.329 -0.126 -0.485 -0.186 -0.6058 -0.0641 -0.2790\n#> 2 0.443  0.596  0.485  0.293 0.503 -0.133  0.309 -0.673  0.0117  0.6763 -0.0673\n#> 3 0.509  0.557 -0.526  0.646 0.293  0.525 -0.673  0.393 -0.2555  0.1266 -0.4196\n#>     C135    C136   C137    C138   C139    C140    C141   C142   C143   C144\n#> 1 -0.405 -0.1379 -0.500 -0.1522 -0.140  0.0101 0.09004 -0.422  0.343  0.465\n#> 2  0.285 -0.4502  0.267  0.0704  0.489 -0.6379 0.39523 -0.334  0.186 -0.273\n#> 3  0.648  0.0613  0.419 -0.0347 -0.414 -0.0820 0.00975  0.261 -0.681 -0.169\n#>     C145    C146   C147    C148   C149   C150    C151   C152    C153   C154\n#> 1 -0.193 -0.1779 -0.221  0.5112 -0.378  0.607  0.2944 -0.544 -0.2129  0.333\n#> 2 -0.296 -0.1473  0.380  0.0468  0.267 -0.289 -0.5452 -0.324 -0.0456  0.594\n#> 3 -0.446 -0.0256 -0.657 -0.4119 -0.549  0.617  0.0981  0.428  0.3343 -0.529\n#>     C155   C156    C157     C158   C159   C160   C161   C162   C163   C164\n#> 1 0.3730 -0.255 -0.0904  0.33481 -0.459  0.208 -0.498  0.541  0.321  0.389\n#> 2 0.2748 -0.349  0.4207  0.00871 -0.363 -0.176 -0.594 -0.157  0.388 -0.024\n#> 3 0.0638 -0.246 -0.3141 -0.10845 -0.358 -0.354  0.202  0.672 -0.639  0.627\n#>     C165   C166  C167   C168    C169    C170  C171     C172     C173    C174\n#> 1  0.117 -0.228 0.503  0.457  0.2055  0.6260 0.687  0.16732  0.22204  0.1521\n#> 2  0.104 -0.566 0.471 -0.435  0.4983  0.0754 0.518 -0.54281  0.37667  0.0134\n#> 3 -0.142  0.597 0.489  0.040 -0.0596 -0.5953 0.186 -0.00423 -0.00865 -0.1778\n#>      C175   C176   C177   C178   C179   C180    C181   C182   C183    C184\n#> 1  0.1172  0.438 -0.227 -0.452  0.367 -0.693  0.5106  0.439 -0.580  0.0815\n#> 2  0.0544 -0.195 -0.231  0.131 -0.571 -0.637 -0.1254 -0.354  0.564 -0.1983\n#> 3 -0.2982 -0.584  0.309 -0.155 -0.274  0.640 -0.0663  0.424 -0.282  0.5671\n#>     C185   C186   C187   C188   C189    C190   C191   C192   C193   C194   C195\n#> 1 0.3890 -0.182  0.350  0.554  0.271 -0.4645  0.528 -0.246  0.656 -0.355 -0.594\n#> 2 0.6534 -0.232 -0.244 -0.545  0.110 -0.5231  0.291 -0.330 -0.120 -0.420 -0.664\n#> 3 0.0729  0.458 -0.249  0.463 -0.301  0.0937 -0.336  0.364  0.243  0.329  0.678\n#>    C196   C197   C198    C199   C200\n#> 1 0.151 -0.563 -0.399 -0.2783 0.3663\n#> 2 0.459 -0.343  0.378 -0.0525 0.2024\n#> 3 0.457  0.153 -0.024  0.3552 0.0864\n#> \n#> [3 rows x 200 columns]\nh2o.biases(iris_dl,  vector_id=1)\n#>      C1\n#> 1 0.455\n#> 2 0.450\n#> 3 0.506\n#> 4 0.470\n#> 5 0.450\n#> 6 0.507\n#> \n#> [200 rows x 1 column]\nh2o.biases(iris_dl,  vector_id=2)\n#>      C1\n#> 1 1.000\n#> 2 1.003\n#> 3 0.996\n#> 4 1.002\n#> 5 0.999\n#> 6 0.997\n#> \n#> [200 rows x 1 column]\nh2o.biases(iris_dl,  vector_id=3)\n#>          C1\n#> 1 -0.000702\n#> 2  0.000292\n#> 3 -0.002267\n#> \n#> [3 rows x 1 column]\n#plot weights connecting `Sepal.Length` to first hidden neurons\nplot(as.data.frame(h2o.weights(iris_dl,  matrix_id=1))[,1])"},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"reproducibility","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46.8.11 Reproducibility","text":"Every run DeepLearning results different results since multithreading done via Hogwild! benefits intentional lock-free race conditions threads. get reproducible results small datasets testing purposes, set reproducible=T set seed=1337 (pick integer). work big data technical reasons, probably also desired significant slowdown (runs 1 core ).","code":""},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"scoring-on-trainingvalidation-sets-during-training","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46.8.12 Scoring on Training/Validation Sets During Training","text":"training /validation set errors can based subset training validation data, depending values score_validation_samples (defaults 0: ) score_training_samples (defaults 10,000 rows, since training error used early stopping monitoring). large datasets, Deep Learning can automatically sample validation set avoid spending much time scoring training, especially since scoring results currently displayed model returned R.Note default value score_duty_cycle=0.1 limits amount time spent scoring 10%, large number scoring samples won’t slow overall training progress much, always score first MapReduce iteration, end training.Stratified sampling validation dataset can help scoring datasets class imbalance. Note option also requires balance_classes enabled (used /-sample training dataset, based max. relative size resulting training dataset, max_after_balance_size):information can found H2O Deep Learning booklet, H2O SlideShare Presentations, H2O YouTube channel, well H2O Github Repository, especially H2O Deep Learning R tests, H2O Deep Learning Python tests.","code":""},{"path":"deep-learning-tips-for-classification-and-regression.html","id":"all-done-shutdown-h2o","chapter":"46 Deep Learning tips for Classification and Regression","heading":"46.9 All done, shutdown H2O","text":"","code":"\nh2o.shutdown(prompt=FALSE)"},{"path":"what-is-dot-hat-in-a-regression-output.html","id":"what-is-dot-hat-in-a-regression-output","chapter":"A What is dot hat in a regression output","heading":"A What is dot hat in a regression output","text":"https://stats.stackexchange.com//256364/154908Q. augment() function broom package R creates dataframe predicted values regression model. Columns created include fitted values, standard error fit Cook’s distance. also include something ’m familar column .hat.Can anyone explain value , different linear regression logistic regression?. diagonal elements hat-matrix describe leverage point fitted values.one fits:\\[\\vec{Y} = \\mathbf{X} \\vec {\\beta} + \\vec {\\epsilon}\\]:\\[\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\]\nexample:\\[\n\\begin{pmatrix}Y_1\\\\\n\\vdots\\\\\nY_{32}\\end{pmatrix} = \\begin{pmatrix}\n1 & 2.620\\\\\n\\vdots\\\\\n1 & 2.780\n\\end{pmatrix} \\cdot \\begin{pmatrix}\n\\beta_0\\\\\n\\beta_1\n\\end{pmatrix} + \\begin{pmatrix}\\epsilon_1\\\\\n\\vdots\\\\\n\\epsilon_{32}\\end{pmatrix}\n\\]calculating \\(\\mathbf{H}\\) matrix results :last matrix \\(32 \\times 32\\) matrix contains hat values diagonal.","code":"\nlibrary(broom)\ndata(mtcars)\n\nm1 <- lm(mpg ~ wt, data = mtcars)\n\nhead(augment(m1))\n#> # A tibble: 6 x 10\n#>   .rownames    mpg    wt .fitted .se.fit .resid   .hat .sigma .cooksd .std.resid\n#>   <chr>      <dbl> <dbl>   <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>      <dbl>\n#> 1 Mazda RX4   21    2.62    23.3   0.634 -2.28  0.0433   3.07 1.33e-2    -0.766 \n#> 2 Mazda RX4…  21    2.88    21.9   0.571 -0.920 0.0352   3.09 1.72e-3    -0.307 \n#> 3 Datsun 710  22.8  2.32    24.9   0.736 -2.09  0.0584   3.07 1.54e-2    -0.706 \n#> 4 Hornet 4 …  21.4  3.22    20.1   0.538  1.30  0.0313   3.09 3.02e-3     0.433 \n#> 5 Hornet Sp…  18.7  3.44    18.9   0.553 -0.200 0.0329   3.10 7.60e-5    -0.0668\n#> 6 Valiant     18.1  3.46    18.8   0.555 -0.693 0.0332   3.10 9.21e-4    -0.231\n# .hat vector\naugment(m1)$.hat\n#>  [1] 0.0433 0.0352 0.0584 0.0313 0.0329 0.0332 0.0354 0.0313 0.0314 0.0329\n#> [11] 0.0329 0.0558 0.0401 0.0419 0.1705 0.1953 0.1838 0.0661 0.1177 0.0956\n#> [21] 0.0503 0.0343 0.0328 0.0443 0.0445 0.0866 0.0704 0.1291 0.0313 0.0380\n#> [31] 0.0354 0.0377\nlibrary(MASS)\n\nwt <- mtcars[, 6]\n\nX <- matrix(cbind(rep(1, length(wt)), wt), ncol=2)\n\nH <- X %*% ginv(t(X) %*% X) %*% t(X)X                           32x2\nt(X)                        2x32\nX %*% t(X)                  32x32\nt(X) %*% X                  2x2\nginv(t(X) %*% X)            2x2\nginv(t(X) %*% X) %*% t(X)   2x32\nX %*% ginv(t(X) %*% X)      32x2\ndim(ginv(t(X) %*% X) %*% t(X))\n#> [1]  2 32\nx1 <- X %*% ginv(t(X) %*% X)\ndim(x1)\n#> [1] 32  2\ndim(x1 %*% t(X))\n#> [1] 32 32\nx2 <- ginv(t(X) %*% X) %*% t(X)\ndim(x2)\n#> [1]  2 32\ndim(X %*% x2)\n#> [1] 32 32\n# this last matrix is a 32×32 matrix and contains these hat values on the diagonal.\ndiag(H)\n#>  [1] 0.0433 0.0352 0.0584 0.0313 0.0329 0.0332 0.0354 0.0313 0.0314 0.0329\n#> [11] 0.0329 0.0558 0.0401 0.0419 0.1705 0.1953 0.1838 0.0661 0.1177 0.0956\n#> [21] 0.0503 0.0343 0.0328 0.0443 0.0445 0.0866 0.0704 0.1291 0.0313 0.0380\n#> [31] 0.0354 0.0377"},{"path":"q-q-normal-to-compare-data-to-distributions.html","id":"q-q-normal-to-compare-data-to-distributions","chapter":"B Q-Q normal to compare data to distributions","heading":"B Q-Q normal to compare data to distributions","text":"","code":""},{"path":"q-q-normal-to-compare-data-to-distributions.html","id":"introduction-27","chapter":"B Q-Q normal to compare data to distributions","heading":"B.1 Introduction","text":"https://mgimond.github.io/ES218/Week06a.htmlThus far, used quantile-quantile plots compare distributions two empirical (.e. observational) datasets. sometimes referred empirical Q-Q plot. can also use q-q plot compare empirical observation theoretical observation (.e. one defined mathematically). plot usually referred theoretical Q-Q plot. Examples popular theoretical observations normal distribution (aka Gaussian distribution), chi-square distribution, exponential distribution just name .","code":""},{"path":"q-q-normal-to-compare-data-to-distributions.html","id":"why-we-want-to-compare-emprirical-vs-theoretical-distributions","chapter":"B Q-Q normal to compare data to distributions","heading":"B.2 Why we want to compare emprirical vs theoretical distributions","text":"many reasons might want compare empirical data theoretical distributions:theoretical distribution easy parameterize. example, shape distribution batch numbers can approximated normal distribution can reduce complexity data just two values: mean standard deviation.theoretical distribution easy parameterize. example, shape distribution batch numbers can approximated normal distribution can reduce complexity data just two values: mean standard deviation.data can approximated certain theoretical distributions, many mainstream statistical procedures can applied data.data can approximated certain theoretical distributions, many mainstream statistical procedures can applied data.inferential statistics, knowing sample derived population whose distribution follows theoretical distribution allows us derive certain properties population sample. example, know sample comes normally distributed population, can define confidence intervals sample mean using t-distribution.inferential statistics, knowing sample derived population whose distribution follows theoretical distribution allows us derive certain properties population sample. example, know sample comes normally distributed population, can define confidence intervals sample mean using t-distribution.Modeling distribution observed data can provide insight underlying process generated data.Modeling distribution observed data can provide insight underlying process generated data.empirical datasets follow theoretical distributions exactly. questions usually ends “well theoretical distribution X fit data?”theoretical quantile-quantile plot tool explore batch numbers deviates theoretical distribution visually assess whether difference significant purpose analysis. following examples, compare empirical data normal distribution using normal quantile-quantile plot.","code":""},{"path":"q-q-normal-to-compare-data-to-distributions.html","id":"the-normal-q-q-plot","chapter":"B Q-Q normal to compare data to distributions","heading":"B.3 The normal q-q plot","text":"normal q-q plot just special case empirical q-q plot ’ve explored far; difference assign normal distribution quantiles x-axis.","code":""},{"path":"q-q-normal-to-compare-data-to-distributions.html","id":"drawing-a-normal-q-q-plot-from-scratch","chapter":"B Q-Q normal to compare data to distributions","heading":"B.3.1 Drawing a normal q-q plot from scratch","text":"following example, ’ll compare Alto 1 group normal distribution. First, ’ll extract Alto 1 height values save atomic vector object using dplyr’s piping operations.However, dplyr’s operations return dataframe–even single column selected. force output atomic vector, ’ll pipe subset pull(height) extract height column plain vector element.Next, need find matching normal distribution quantiles. first find f-values alto, use qnorm find matching normal distribution values f-valuesNow can plot sorted alto values normal values.comparing batch numbers theoretical distribution q-q plot, looking significant deviation straight line. make easier judge straightness, can fit line points. Note creating 45° (x=y) slope; range values sets numbers match. , seeking straightness points.many ways one can fit line data, Cleveland opts fit line first third quartile q-q plot. following chunk code identifies quantiles alto dataset theoretical normal distribution. computes slope intercept coordinates.Next, add line plot.","code":"\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following object is masked from 'package:gridExtra':\n#> \n#>     combine\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\n\ndf   <- lattice::singer\nalto <- df %>%  \n    filter(voice.part == \"Alto 1\") %>% \n    arrange(height) %>% \n    pull(height) %>% \n    print\n#>  [1] 60 61 61 61 61 62 62 62 63 63 63 63 64 64 64 65 65 65 65 66 66 66 66 66 66\n#> [26] 66 67 67 67 67 68 68 69 70 72\ni <- 1:length(alto)\nfi <- (i - 0.5) / length(alto)\nfi\n#>  [1] 0.0143 0.0429 0.0714 0.1000 0.1286 0.1571 0.1857 0.2143 0.2429 0.2714\n#> [11] 0.3000 0.3286 0.3571 0.3857 0.4143 0.4429 0.4714 0.5000 0.5286 0.5571\n#> [21] 0.5857 0.6143 0.6429 0.6714 0.7000 0.7286 0.7571 0.7857 0.8143 0.8429\n#> [31] 0.8714 0.9000 0.9286 0.9571 0.9857\nx.norm <- qnorm(fi)\nx.norm\n#>  [1] -2.1893 -1.7185 -1.4652 -1.2816 -1.1332 -1.0063 -0.8938 -0.7916 -0.6971\n#> [10] -0.6085 -0.5244 -0.4439 -0.3661 -0.2905 -0.2165 -0.1437 -0.0717  0.0000\n#> [19]  0.0717  0.1437  0.2165  0.2905  0.3661  0.4439  0.5244  0.6085  0.6971\n#> [28]  0.7916  0.8938  1.0063  1.1332  1.2816  1.4652  1.7185  2.1893\nplot(x.norm)\nplot( alto ~ x.norm, type=\"p\", xlab=\"Normal quantiles\", pch=20)\n# Find 1st and 3rd quartile for the Alto 1 data\ny <- quantile(alto, c(0.25, 0.75), type=5)\ny\n#>  25%  75% \n#> 63.0 66.8\n# Find the 1st and 3rd quartile of the normal distribution\nx <- qnorm( c(0.25, 0.75))\nx\n#> [1] -0.674  0.674\n# Now we can compute the intercept and slope of the line that passes\n# through these points\nslope <- diff(y) / diff(x)\nint   <- y[1] - slope * x[1]\nplot( alto ~ x.norm, type=\"p\", xlab=\"Normal quantiles\", pch=20)\nabline(a=int, b=slope )"},{"path":"q-q-normal-to-compare-data-to-distributions.html","id":"using-rs-built-in-functions","chapter":"B Q-Q normal to compare data to distributions","heading":"B.4 Using R’s built-in functions","text":"R two built-functions facilitate plot building task comparing batch normal distribution: qqnorm qqline. Note function qqline allows user define quantile method via qtype= parameter. , set 5 match choice f-value calculation.’s . Just two lines code!","code":"\nqqnorm(alto)           # plot the points\nqqline(alto, qtype=5)  # plot the line"},{"path":"q-q-normal-to-compare-data-to-distributions.html","id":"using-the-ggplot2-plotting-environment","chapter":"B Q-Q normal to compare data to distributions","heading":"B.5 Using the ggplot2 plotting environment","text":"can take advantage stat_qq() function plot points, equation line must computed manually (done earlier). steps repeated .can, course, make use ggplot’s faceting function generate trellised plots. example, following plot replicates Cleveland’s figure 2.11 (except layout ’ll setup single row plots instead). first, need compute slopes singer group. ’ll use dplyr’s piping operations create new dataframe singer group name, slope intercept.’s important voice.part names match df letter--letter ggplot called, know facet assign slope intercept values via geom_abline.","code":"\n# normal distribution\nlibrary(ggplot2)\n\n# Find the slope and intercept of the line that passes through the 1st and 3rd\n# quartile of the normal q-q plot\n\ny     <- quantile(alto, c(0.25, 0.75), type=5) # Find the 1st and 3rd quartiles\nx     <- qnorm( c(0.25, 0.75))                 # Find the matching normal values on the x-axis\nslope <- diff(y) / diff(x)                     # Compute the line slope\nint   <- y[1] - slope * x[1]                   # Compute the line intercept\n\n# Generate normal q-q plot\nggplot() + aes(sample=alto) + \n    stat_qq(distribution=qnorm) + \n    geom_abline(intercept=int, slope=slope) + \n    ylab(\"Height\") \n\nqq_any <- function(var, f) {\n    # Find the slope and intercept of the line that passes through the 1st and 3rd\n    # quartile of the normal q-q plot\n    \n    y     <- quantile(var, c(0.25, 0.75), type=5) # Find the 1st and 3rd quartiles\n    x     <- f( c(0.25, 0.75))                 # Find the matching normal values x-axis\n    slope <- diff(y) / diff(x)                     # Compute the line slope\n    int   <- y[1] - slope * x[1]                   # Compute the line intercept\n    ggplot() + aes(sample = var) + \n    stat_qq(distribution = f) + \n    geom_abline(intercept=int, slope=slope)\n}\n\n# two function only, for the moment\nqq_any(alto, qexp)\nqq_any(alto, qnorm)\nlibrary(dplyr)\n\nintsl <- df %>% \n    group_by(voice.part) %>% \n       summarize(q25    = quantile(height,0.25, type=5),\n                 q75    = quantile(height,0.75, type=5),\n                 norm25 = qnorm( 0.25),\n                 norm75 = qnorm( 0.75),\n                 slope  = (q25 - q75) / (norm25 - norm75),\n                 int    = q25 - slope * norm25) %>%\n       select(voice.part, slope, int) %>% \n    print\n#> # A tibble: 8 x 3\n#>   voice.part slope   int\n#>   <fct>      <dbl> <dbl>\n#> 1 Bass 2      2.97  72  \n#> 2 Bass 1      2.22  70.5\n#> 3 Tenor 2     1.48  70  \n#> 4 Tenor 1     3.89  68.6\n#> 5 Alto 2      2.22  65.5\n#> 6 Alto 1      2.78  64.9\n#> # … with 2 more rows\nggplot(df, aes(sample = height)) + \n    stat_qq(distribution = qnorm) + \n    geom_abline(data=intsl, aes(intercept=int, slope=slope), col=\"blue\") +\n    facet_wrap(~voice.part, nrow=1) + \n    ylab(\"Height\") "},{"path":"qq-and-pp-plots.html","id":"qq-and-pp-plots","chapter":"C QQ and PP Plots","heading":"C QQ and PP Plots","text":"https://homepage.divms.uiowa.edu/~luke/classes/STAT4580/qqpp.html","code":""},{"path":"qq-and-pp-plots.html","id":"qq-plot","chapter":"C QQ and PP Plots","heading":"C.1 QQ Plot","text":"One way assess well particular theoretical model describes data distribution plot data quantiles theoretical quantiles.Base graphics provides qqnorm, lattice qqmath, ggplot2 geom_qq.default theoretical distribution used standard normal, , except qqnorm, allow specify alternative.large sample theoretical distribution plot straight line origin slope 1:plot straight line different slope intercept, data distribution corresponds location-scale transformation theoretical distribution.slope scale intercept location:QQ plot can constructed directly scatterplot sorted sample \\(= 1, \\dots, n\\) quantiles \\[p_i = \\frac{}{n} - \\frac{1}{2n}\\]","code":"\nlibrary(ggplot2)\n\nn <- 10000\nggplot() + geom_qq(aes(sample = rnorm(n)))\nggplot() +\n    geom_qq(aes(sample = rnorm(n, 10, 4))) +\n    geom_abline(intercept = 10, slope = 4,\n                color = \"red\", size = 1.5, alpha = 0.8)\np <- (1 : n) / n - 0.5 / n\ny <- rnorm(n, 10, 4)\nggplot() + geom_point(aes(x = qnorm(p), y = sort(y)))"},{"path":"qq-and-pp-plots.html","id":"some-examples","chapter":"C QQ and PP Plots","heading":"C.2 Some Examples","text":"histograms density estimates duration variable geyser data set showed distribution far normal distribution, normal QQ plot shows well:Except rounding parent heights Galton data seemed fat normally distributed:Rounding interferes visualization histogram density plot.Rounding visible visualization histogram density plot.Another Gatlton dataset available UsingR package less rounding father.son:middle seems fairly straight, ends somewhat wiggly.can calibrate judgment?","code":"\nlibrary(MASS)\nggplot(geyser) + geom_qq(aes(sample = duration))\nlibrary(psych)\nlibrary(UsingR)\n\nggplot(galton) + geom_qq(aes(sample = parent))\nlibrary(UsingR)\nggplot(father.son) + geom_qq(aes(sample = fheight))"},{"path":"qq-and-pp-plots.html","id":"calibrating-the-variability","chapter":"C QQ and PP Plots","heading":"C.3 Calibrating the Variability","text":"One approach use simulation, sometimes called graphical bootstrap.nboot function simulate R samples normal distribution match variable x sample size, sample mean, sample SD.result returned dataframe suitable plotting:Plotting lines shows variability shapes can expect sampling theoretical normal distribution:can insert simulation behind data help calibrate visualization:","code":"\nnsim <- function(n, m = 0, s = 1) {\n    z <- rnorm(n)\n    m + s * ((z - mean(z)) / sd(z))\n}\n\nnboot <- function(x, R) {\n    n <- length(x)\n    m <- mean(x)\n    s <- sd(x)\n    do.call(rbind,\n            lapply(1 : R,\n                   function(i) {\n                       xx <- sort(nsim(n, m, s))\n                       p <- seq_along(x) / n - 0.5 / n\n                       data.frame(x = xx, p = p, sim = i)\n    }))\n}\ngb <- nboot(father.son$fheight, 50)\ntibble::as_tibble(gb)\n#> # A tibble: 53,900 x 3\n#>       x        p   sim\n#>   <dbl>    <dbl> <int>\n#> 1  59.8 0.000464     1\n#> 2  59.9 0.00139      1\n#> 3  59.9 0.00232      1\n#> 4  60.8 0.00325      1\n#> 5  60.8 0.00417      1\n#> 6  60.9 0.00510      1\n#> # … with 53,894 more rows\nggplot() +\n    geom_line(aes(x = qnorm(p), y = x, group = sim),\n              color = \"gray\", data = gb)\nggplot(father.son) +\n    geom_line(aes(x = qnorm(p), y = x, group = sim),\n              color = \"gray\", data = gb) +\n    geom_qq(aes(sample = fheight))"},{"path":"qq-and-pp-plots.html","id":"scalability","chapter":"C QQ and PP Plots","heading":"C.4 Scalability","text":"large sample sizes overplotting occur:can alleviated using grid quantiles:reasonable model might exponential distribution:","code":"\nggplot(diamonds) + geom_qq(aes(sample = price))\nnq <- 100\np <- (1 : nq) / nq - 0.5 / nq\nggplot() + geom_point(aes(x = qnorm(p), y = quantile(diamonds$price, p)))\nggplot() + geom_point(aes(x = qexp(p), y = quantile(diamonds$price, p)))"},{"path":"qq-and-pp-plots.html","id":"comparing-two-distributions","chapter":"C QQ and PP Plots","heading":"C.5 Comparing Two Distributions","text":"QQ plot can also used compare two distributions based sample .samples size just plot ordered sample values .Choosing fixed set quantiles allows samples unequal size compared.Using small set quantiles can compare distributions waiting times eruptions Old Faithful two different data sets looked :","code":"\nnq <- 31  # user defined\nnq <- min(length(geyser$waiting), length(faithful$waiting)) # or take the minimum\np <- (1 : nq) / nq - 0.5 / nq\n\nwg <- geyser$waiting\nwf <- faithful$waiting\n\nggplot() + geom_point(aes(x = quantile(wg, p), y = quantile(wf, p)))"},{"path":"qq-and-pp-plots.html","id":"pp-plots","chapter":"C QQ and PP Plots","heading":"C.6 PP Plots","text":"PP plot comparing sample theoretical model plots theoretical proportion less equal observed value actual proportion.theoretical cumulative distribution function F means plotting\\[F(x())∼pi\\]fheight variable father.son data:values vertical axis probability integral transform data theoretical distribution.values vertical axis probability integral transform data theoretical distribution.data sample theoretical distribution transforms uniformly distributed [0,1].data sample theoretical distribution transforms uniformly distributed [0,1].PP plot QQ plot transformed values uniform distribution.PP plot QQ plot transformed values uniform distribution.PP plot goes points (0,0) (1,1) much less variable tails:PP plot goes points (0,0) (1,1) much less variable tails:Adding data:PP plot also less sensitive deviations tails.compromise QQ PP plots uses arcsine square root variance-stabilizing transformation, makes variability approximately constant across range plot:Adding data:","code":"\nm <- mean(father.son$fheight)\ns <- sd(father.son$fheight)\nn <- nrow(father.son)\np <- (1 : n) / n - 0.5 / n\nggplot(father.son) + geom_point(aes(x = p, y = sort(pnorm(fheight, m, s))))\npp <- ggplot() +\ngeom_line(aes(x = p, y = pnorm(x, m, s), group = sim),\n          color = \"gray\", data = gb)\npp\npp + \ngeom_point(aes(x = p, y = sort(pnorm(fheight, m, s))), data = (father.son))\nvpp <- ggplot() +\ngeom_line(aes(x = asin(sqrt(p)), y = asin(sqrt(pnorm(x, m, s))), group = sim),\n          color = \"gray\", data = gb)\nvpp\nvpp +\ngeom_point(aes(x = asin(sqrt(p)), y = sort(asin(sqrt(pnorm(fheight, m, s))))),\n           data = (father.son))"},{"path":"qq-and-pp-plots.html","id":"plots-for-assessing-model-fit","chapter":"C QQ and PP Plots","heading":"C.7 Plots For Assessing Model Fit","text":"QQ PP plots can used asses well theoretical family models fits data, residuals.QQ PP plots can used asses well theoretical family models fits data, residuals.use PP plot estimate parameters first.use PP plot estimate parameters first.location-scale family, like normal distribution family, can use QQ plot standard member family.location-scale family, like normal distribution family, can use QQ plot standard member family.families can use transformations lead straight lines family members:families can use transformations lead straight lines family members:Weibull family widely used reliability modeling; CDF \n\\[F(t) = 1 - \\exp\\left\\{-\\left(\\frac{t}{b}\\right)^\\right\\}\\]logarithms Weibull random variables form location-scale family.logarithms Weibull random variables form location-scale family.Special paper used available Weibull probability plots.Special paper used available Weibull probability plots.Weibull QQ plot price diamonds data:lower tail match Weibull distribution.lower tail match Weibull distribution.important?important?engineering applications often .engineering applications often .selecting reasonable model capture shape distribution may .selecting reasonable model capture shape distribution may .QQ plots helpful understanding departures theoretical model.QQ plots helpful understanding departures theoretical model.data fit theoretical model perfectly.data fit theoretical model perfectly.Case-specific judgment needed decide whether departures important.Case-specific judgment needed decide whether departures important.George Box: models wrong useful.George Box: models wrong useful.","code":"\nn <- nrow(diamonds)\np <- (1 : n) / n - 0.5 / n\nggplot(diamonds) +\n    geom_point(aes(x = log10(qweibull(p, 1, 1)), y = log10(sort(price))))"},{"path":"visualizing-residuals.html","id":"visualizing-residuals","chapter":"D Visualizing residuals","heading":"D Visualizing residuals","text":"Source: https://www.r-bloggers.com/visualising-residuals/","code":"\nfit <- lm(mpg ~ hp, data = mtcars)  # Fit the model\nsummary(fit)  # Report the results\n#> \n#> Call:\n#> lm(formula = mpg ~ hp, data = mtcars)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -5.712 -2.112 -0.885  1.582  8.236 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  30.0989     1.6339   18.42  < 2e-16 ***\n#> hp           -0.0682     0.0101   -6.74  1.8e-07 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 3.86 on 30 degrees of freedom\n#> Multiple R-squared:  0.602,  Adjusted R-squared:  0.589 \n#> F-statistic: 45.5 on 1 and 30 DF,  p-value: 1.79e-07\npar(mfrow = c(2, 2))  # Split the plotting panel into a 2 x 2 grid\nplot(fit)  # Plot the model information\n\npar(mfrow = c(1, 1))  # Return plotting panel to 1 section"},{"path":"visualizing-residuals.html","id":"simple-linear-regression","chapter":"D Visualizing residuals","heading":"D.1 Simple Linear Regression","text":"","code":"\nd <- mtcars\nfit <- lm(mpg ~ hp, data = d)\nd$predicted <- predict(fit)   # Save the predicted values\nd$residuals <- residuals(fit) # Save the residual values\n\n# Quick look at the actual, predicted, and residual values\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\nd %>% select(mpg, predicted, residuals) %>% head()\n#>                    mpg predicted residuals\n#> Mazda RX4         21.0      22.6    -1.594\n#> Mazda RX4 Wag     21.0      22.6    -1.594\n#> Datsun 710        22.8      23.8    -0.954\n#> Hornet 4 Drive    21.4      22.6    -1.194\n#> Hornet Sportabout 18.7      18.2     0.541\n#> Valiant           18.1      22.9    -4.835"},{"path":"visualizing-residuals.html","id":"step-3-plot-the-actual-and-predicted-values","chapter":"D Visualizing residuals","heading":"D.1.1 Step 3: plot the actual and predicted values","text":"plot first actual dataNext, plot predicted values way ’re distinguishable actual values. example, let’s change shape:track, ’s difficult see actual predicted values related. Let’s connect actual data points corresponding predicted value using geom_segment():’ll make final adjustments:\n* Clean overall look theme_bw().\n* Fade connection lines adjusting alpha.\n* Add regression slope geom_smooth():","code":"\nlibrary(ggplot2)\nggplot(d, aes(x = hp, y = mpg)) +  # Set up canvas with outcome variable on y-axis\n  geom_point()  # Plot the actual points\nggplot(d, aes(x = hp, y = mpg)) +\n  geom_point() +\n  geom_point(aes(y = predicted), shape = 1)  # Add the predicted values\nggplot(d, aes(x = hp, y = mpg)) +\n  geom_segment(aes(xend = hp, yend = predicted)) +\n  geom_point() +\n  geom_point(aes(y = predicted), shape = 1)\n\nlibrary(ggplot2)\nggplot(d, aes(x = hp, y = mpg)) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"lightgrey\") +  # Plot regression slope\n  geom_segment(aes(xend = hp, yend = predicted), alpha = .2) +  # alpha to fade lines\n  geom_point() +\n  geom_point(aes(y = predicted), shape = 1) +\n  theme_bw()  # Add theme for cleaner look\n#> `geom_smooth()` using formula 'y ~ x'"},{"path":"visualizing-residuals.html","id":"step-4-use-residuals-to-adjust","chapter":"D Visualizing residuals","heading":"D.2 Step 4: use residuals to adjust","text":"Finally, want make adjustment highlight size residual. MANY options. make comparisons easy, ’ll make adjustments actual values, just easily apply , changes, predicted values. examples building previous plot:particularly like last example, colours nicely help identify non-linearity data. example, can see red extreme values hp actual values greater predicted. blue centre, however, indicating actual values less predicted. Together, suggests relationship variables non-linear, might better modelled including quadratic term regression equation.","code":"\n\n\n# ALPHA\n# Changing alpha of actual values based on absolute value of residuals\nggplot(d, aes(x = hp, y = mpg)) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"lightgrey\") +\n  geom_segment(aes(xend = hp, yend = predicted), alpha = .2) +\n\n  # > Alpha adjustments made here...\n  geom_point(aes(alpha = abs(residuals))) +  # Alpha mapped to abs(residuals)\n  guides(alpha = FALSE) +  # Alpha legend removed\n  # <\n\n  geom_point(aes(y = predicted), shape = 1) +\n  theme_bw()\n#> `geom_smooth()` using formula 'y ~ x'\n# COLOR\n# High residuals (in abolsute terms) made more red on actual values.\nggplot(d, aes(x = hp, y = mpg)) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"lightgrey\") +\n  geom_segment(aes(xend = hp, yend = predicted), alpha = .2) +\n\n  # > Color adjustments made here...\n  geom_point(aes(color = abs(residuals))) + # Color mapped to abs(residuals)\n  scale_color_continuous(low = \"black\", high = \"red\") +  # Colors to use here\n  guides(color = FALSE) +  # Color legend removed\n  # <\n\n  geom_point(aes(y = predicted), shape = 1) +\n  theme_bw()\n#> `geom_smooth()` using formula 'y ~ x'\n# SIZE AND COLOR\n# Same coloring as above, size corresponding as well\nggplot(d, aes(x = hp, y = mpg)) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"lightgrey\") +\n  geom_segment(aes(xend = hp, yend = predicted), alpha = .2) +\n\n  # > Color AND size adjustments made here...\n  geom_point(aes(color = abs(residuals), size = abs(residuals))) + # size also mapped\n  scale_color_continuous(low = \"black\", high = \"red\") +\n  guides(color = FALSE, size = FALSE) +  # Size legend also removed\n  # <\n\n  geom_point(aes(y = predicted), shape = 1) +\n  theme_bw()\n#> `geom_smooth()` using formula 'y ~ x'\n# COLOR UNDER/OVER\n# Color mapped to residual with sign taken into account.\n# i.e., whether actual value is greater or less than predicted\nggplot(d, aes(x = hp, y = mpg)) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"lightgrey\") +\n  geom_segment(aes(xend = hp, yend = predicted), alpha = .2) +\n\n  # > Color adjustments made here...\n  geom_point(aes(color = residuals)) +  # Color mapped here\n  scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\") +  # Colors to use here\n  guides(color = FALSE) +\n  # <\n\n  geom_point(aes(y = predicted), shape = 1) +\n  theme_bw()\n#> `geom_smooth()` using formula 'y ~ x'"}]
