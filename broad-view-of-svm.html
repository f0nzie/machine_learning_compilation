<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 11 Broad view of SVM | A Machine Learning Compilation</title>
<meta name="author" content="Several authors. Compiled by Alfonso R. Reyes">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.5.3/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.5.3/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.2.9000/tabs.js"></script><script src="libs/bs3compat-0.2.2.9000/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">A Machine Learning Compilation</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Preface</a></li>
<li class="book-part">The Basics of Machine Learning</li>
<li><a class="" href="introduction-to-pca.html"><span class="header-section-number">2</span> Introduction to PCA</a></li>
<li><a class="" href="comparison-of-two-pca-packages.html"><span class="header-section-number">3</span> Comparison of two PCA packages</a></li>
<li><a class="" href="detailed-study-of-principal-component-analysis.html"><span class="header-section-number">4</span> Detailed study of Principal Component Analysis</a></li>
<li><a class="" href="detection-of-diabetes-using-logistic-regression.html"><span class="header-section-number">5</span> Detection of diabetes using Logistic Regression</a></li>
<li><a class="" href="sensitivity-analysis-for-a-neural-network.html"><span class="header-section-number">6</span> Sensitivity analysis for a neural network</a></li>
<li><a class="" href="data-visualization-for-ml-models.html"><span class="header-section-number">7</span> Data Visualization for ML models</a></li>
<li class="book-part">Feature Engineering</li>
<li><a class="" href="ten-methods-to-assess-variable-importance.html"><span class="header-section-number">8</span> Ten methods to assess Variable Importance</a></li>
<li><a class="" href="employee-attrition-using-feature-importance.html"><span class="header-section-number">9</span> Employee Attrition using Feature Importance</a></li>
<li class="book-part">Classification</li>
<li><a class="" href="a-gentle-introduction-to-support-vector-machines.html"><span class="header-section-number">10</span> A gentle introduction to Support Vector Machines</a></li>
<li><a class="active" href="broad-view-of-svm.html"><span class="header-section-number">11</span> Broad view of SVM</a></li>
<li><a class="" href="feature-selection-to-enhance-cancer-detection.html"><span class="header-section-number">12</span> Feature Selection to enhance cancer detection</a></li>
<li><a class="" href="dealing-with-unbalanced-data.html"><span class="header-section-number">13</span> Dealing with unbalanced data</a></li>
<li><a class="" href="imputting-missing-values-with-random-forest.html"><span class="header-section-number">14</span> Imputting missing values with Random Forest</a></li>
<li><a class="" href="tuning-of-support-vector-machine-prediction.html"><span class="header-section-number">15</span> Tuning of Support Vector Machine prediction</a></li>
<li class="book-part">Classification</li>
<li><a class="" href="introduction-to-algorithms-for-classification.html"><span class="header-section-number">16</span> Introduction to algorithms for Classification</a></li>
<li><a class="" href="comparing-classification-algorithms.html"><span class="header-section-number">17</span> Comparing Classification algorithms</a></li>
<li><a class="" href="who-buys-social-network-ads.html"><span class="header-section-number">18</span> Who buys Social Network ads</a></li>
<li><a class="" href="predicting-ozone-levels.html"><span class="header-section-number">19</span> Predicting Ozone levels</a></li>
<li><a class="" href="building-a-naive-bayes-classifier.html"><span class="header-section-number">20</span> Building a Naive Bayes Classifier</a></li>
<li><a class="" href="linear-and-non-linear-algorithms-for-classification.html"><span class="header-section-number">21</span> Linear and Non-Linear Algorithms for Classification</a></li>
<li><a class="" href="detect-mines-vs-rocks-with-random-forest.html"><span class="header-section-number">22</span> Detect mines vs rocks with Random Forest</a></li>
<li><a class="" href="predicting-the-type-of-glass.html"><span class="header-section-number">23</span> Predicting the type of glass</a></li>
<li><a class="" href="naive-bayes-for-sms-spam.html"><span class="header-section-number">24</span> Naive Bayes for SMS spam</a></li>
<li><a class="" href="vehicles-classiification-with-decision-trees.html"><span class="header-section-number">25</span> Vehicles classiification with Decision Trees</a></li>
<li><a class="" href="applying-naive-bayes-on-the-titanic-case.html"><span class="header-section-number">26</span> Applying Naive-Bayes on the Titanic case</a></li>
<li><a class="" href="classification-on-bad-loans.html"><span class="header-section-number">27</span> Classification on bad loans</a></li>
<li><a class="" href="predicting-flu-outcome-comparing-eight-classification-algorithms.html"><span class="header-section-number">28</span> Predicting Flu outcome comparing eight classification algorithms</a></li>
<li><a class="" href="a-detailed-study-of-bike-sharing-demand.html"><span class="header-section-number">29</span> A detailed study of bike sharing demand</a></li>
<li><a class="" href="prediction-of-arrhythmia-with-deep-neural-nets.html"><span class="header-section-number">30</span> Prediction of arrhythmia with deep neural nets</a></li>
<li class="book-part">Linear Regression</li>
<li><a class="" href="linear-regression-with-islr.html"><span class="header-section-number">31</span> Linear Regression with ISLR</a></li>
<li><a class="" href="evaluation-of-three-linear-regression-models.html"><span class="header-section-number">32</span> Evaluation of three linear regression models</a></li>
<li><a class="" href="comparison-of-six-linear-regression-algorithms.html"><span class="header-section-number">33</span> Comparison of six Linear Regression algorithms</a></li>
<li><a class="" href="comparing-regression-models.html"><span class="header-section-number">34</span> Comparing regression models</a></li>
<li><a class="" href="finding-the-factors-of-happiness.html"><span class="header-section-number">35</span> Finding the factors of happiness</a></li>
<li><a class="" href="regression-with-a-neural-network.html"><span class="header-section-number">36</span> Regression with a neural network</a></li>
<li><a class="" href="comparing-multiple-regression-vs-a-neural-network.html"><span class="header-section-number">37</span> Comparing Multiple Regression vs a Neural Network</a></li>
<li><a class="" href="temperature-modeling-using-nested-dataframes.html"><span class="header-section-number">38</span> Temperature modeling using nested dataframes</a></li>
<li class="book-part">Neural Networks</li>
<li><a class="" href="credit-scoring-with-neuralnet.html"><span class="header-section-number">39</span> Credit Scoring with neuralnet</a></li>
<li><a class="" href="wine-classification-with-neuralnet.html"><span class="header-section-number">40</span> Wine classification with neuralnet</a></li>
<li><a class="" href="predicting-the-rating-of-cereals.html"><span class="header-section-number">41</span> Predicting the rating of cereals</a></li>
<li><a class="" href="fitting-a-linear-model-with-neural-networks.html"><span class="header-section-number">42</span> Fitting a linear model with neural networks</a></li>
<li><a class="" href="visualization-of-neural-networks.html"><span class="header-section-number">43</span> Visualization of neural networks</a></li>
<li><a class="" href="build-a-fully-connected-r-neural-network-from-scratch.html"><span class="header-section-number">44</span> Build a fully connected R neural network from scratch</a></li>
<li><a class="" href="tuning-hyperparameters-in-a-neural-network.html"><span class="header-section-number">45</span> Tuning Hyperparameters in a Neural Network</a></li>
<li><a class="" href="deep-learning-tips-for-classification-and-regression.html"><span class="header-section-number">46</span> Deep Learning tips for Classification and Regression</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="what-is-dot-hat-in-a-regression-output.html"><span class="header-section-number">A</span> What is dot hat in a regression output</a></li>
<li><a class="" href="q-q-normal-to-compare-data-to-distributions.html"><span class="header-section-number">B</span> Q-Q normal to compare data to distributions</a></li>
<li><a class="" href="qq-and-pp-plots.html"><span class="header-section-number">C</span> QQ and PP Plots</a></li>
<li><a class="" href="visualizing-residuals.html"><span class="header-section-number">D</span> Visualizing residuals</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/f0nzie/machine_learning_compilation">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="broad-view-of-svm" class="section level1">
<h1>
<span class="header-section-number">11</span> Broad view of SVM<a class="anchor" aria-label="anchor" href="#broad-view-of-svm"><i class="fas fa-link"></i></a>
</h1>
<div id="introduction-5" class="section level2">
<h2>
<span class="header-section-number">11.1</span> Introduction<a class="anchor" aria-label="anchor" href="#introduction-5"><i class="fas fa-link"></i></a>
</h2>
<p>Source: <a href="http://uc-r.github.io/svm" class="uri">http://uc-r.github.io/svm</a></p>
<div class="sourceCode" id="cb272"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># set pseudorandom number generator</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">10</span><span class="op">)</span>

<span class="co"># Attach Packages</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>    <span class="co"># data manipulation and visualization</span>
<span class="co">#&gt; ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──</span>
<span class="co">#&gt; ✔ ggplot2 3.3.0     ✔ purrr   0.3.4</span>
<span class="co">#&gt; ✔ tibble  3.0.1     ✔ dplyr   0.8.5</span>
<span class="co">#&gt; ✔ tidyr   1.0.2     ✔ stringr 1.4.0</span>
<span class="co">#&gt; ✔ readr   1.3.1     ✔ forcats 0.5.0</span>
<span class="co">#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──</span>
<span class="co">#&gt; ✖ dplyr::filter() masks stats::filter()</span>
<span class="co">#&gt; ✖ dplyr::lag()    masks stats::lag()</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">kernlab</span><span class="op">)</span>      <span class="co"># SVM methodology</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Attaching package: 'kernlab'</span>
<span class="co">#&gt; The following object is masked from 'package:purrr':</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     cross</span>
<span class="co">#&gt; The following object is masked from 'package:ggplot2':</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     alpha</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">e1071</span><span class="op">)</span>        <span class="co"># SVM methodology</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.StatLearning.com">ISLR</a></span><span class="op">)</span>         <span class="co"># contains example data set "Khan"</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">RColorBrewer</span><span class="op">)</span> <span class="co"># customized coloring of plots</span></code></pre></div>
<p>The data sets used in the tutorial (with the exception of Khan) will be generated using built-in R commands. The Support Vector Machine methodology is sound for any number of dimensions, but becomes difficult to visualize for more than 2. As previously mentioned, SVMs are robust for any number of classes, but we will stick to no more than 3 for the duration of this tutorial.</p>
</div>
<div id="maximal-margin-classifier" class="section level2">
<h2>
<span class="header-section-number">11.2</span> Maximal Margin Classifier<a class="anchor" aria-label="anchor" href="#maximal-margin-classifier"><i class="fas fa-link"></i></a>
</h2>
<p>If the classes are separable by a linear boundary, we can use a Maximal Margin Classifier to find the classification boundary. To visualize an example of separated data, we generate 40 random observations and assign them to two classes. Upon visual inspection, we can see that infinitely many lines exist that split the two classes.</p>
<div class="sourceCode" id="cb273"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Construct sample data set - completely separated</span>
<span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">20</span><span class="op">*</span><span class="fl">2</span><span class="op">)</span>, ncol <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>
<span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>,<span class="fl">10</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">10</span><span class="op">)</span><span class="op">)</span>
<span class="va">x</span><span class="op">[</span><span class="va">y</span><span class="op">==</span><span class="fl">1</span>,<span class="op">]</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">[</span><span class="va">y</span><span class="op">==</span><span class="fl">1</span>,<span class="op">]</span> <span class="op">+</span> <span class="fl">3</span><span class="op">/</span><span class="fl">2</span>
<span class="va">dat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">x</span>, y<span class="op">=</span><span class="fu"><a href="https://rdrr.io/pkg/h2o/man/as.factor.html">as.factor</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span>

<span class="co"># Plot data</span>
<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">dat</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x.2</span>, y <span class="op">=</span> <span class="va">x.1</span>, color <span class="op">=</span> <span class="va">y</span>, shape <span class="op">=</span> <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_manual.html">scale_color_manual</a></span><span class="op">(</span>values<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"#000000"</span>, <span class="st">"#FF0000"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html">theme</a></span><span class="op">(</span>legend.position <span class="op">=</span> <span class="st">"none"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="103-classification_113-broad_view-SVM_files/figure-html/unnamed-chunk-2-1.png" width="70%" style="display: block; margin: auto;"></div>
<p>The goal of the maximal margin classifier is to identify the linear boundary that maximizes the total distance between the line and the closest point in each class. We can use the svm() function in the e1071 package to find this boundary.</p>
<div class="sourceCode" id="cb274"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Fit Support Vector Machine model to data set</span>
<span class="va">svmfit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/e1071/man/svm.html">svm</a></span><span class="op">(</span><span class="va">y</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">dat</span>, kernel <span class="op">=</span> <span class="st">"linear"</span>, scale <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span>
<span class="co"># Plot Results</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span><span class="op">(</span><span class="va">svmfit</span>, <span class="va">dat</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="103-classification_113-broad_view-SVM_files/figure-html/unnamed-chunk-3-1.png" width="70%" style="display: block; margin: auto;"></div>
<p>In the plot, points that are represented by an “X” are the support vectors, or the points that directly affect the classification line. The points marked with an “o” are the other points, which don’t affect the calculation of the line. This principle will lay the foundation for support vector machines. The same plot can be generated using the kernlab package, with the following results:</p>
<div class="sourceCode" id="cb275"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># fit model and produce plot</span>
<span class="va">kernfit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/kernlab/man/ksvm.html">ksvm</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, type <span class="op">=</span> <span class="st">"C-svc"</span>, kernel <span class="op">=</span> <span class="st">'vanilladot'</span><span class="op">)</span>
<span class="co">#&gt;  Setting default kernel parameters</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span><span class="op">(</span><span class="va">kernfit</span>, data <span class="op">=</span> <span class="va">x</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="103-classification_113-broad_view-SVM_files/figure-html/unnamed-chunk-4-1.png" width="70%" style="display: block; margin: auto;"></div>
<p><code>kernlab</code> shows a little more detail than e1071, showing a color gradient that indicates how confidently a new point would be classified based on its features. Just as in the first plot, the support vectors are marked, in this case as filled-in points, while the classes are denoted by different shapes.</p>
</div>
<div id="support-vector-classifiers" class="section level2">
<h2>
<span class="header-section-number">11.3</span> Support Vector Classifiers<a class="anchor" aria-label="anchor" href="#support-vector-classifiers"><i class="fas fa-link"></i></a>
</h2>
<p>As convenient as the maximal marginal classifier is to understand, most real data sets will not be fully separable by a linear boundary. To handle such data, we must use modified methodology. We simulate a new data set where the classes are more mixed.</p>
<div class="sourceCode" id="cb276"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Construct sample data set - not completely separated</span>
<span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">20</span><span class="op">*</span><span class="fl">2</span><span class="op">)</span>, ncol <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>
<span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>,<span class="fl">10</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">10</span><span class="op">)</span><span class="op">)</span>
<span class="va">x</span><span class="op">[</span><span class="va">y</span><span class="op">==</span><span class="fl">1</span>,<span class="op">]</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">[</span><span class="va">y</span><span class="op">==</span><span class="fl">1</span>,<span class="op">]</span> <span class="op">+</span> <span class="fl">1</span>
<span class="va">dat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">x</span>, y<span class="op">=</span><span class="fu"><a href="https://rdrr.io/pkg/h2o/man/as.factor.html">as.factor</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span>

<span class="co"># Plot data set</span>
<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">dat</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x.2</span>, y <span class="op">=</span> <span class="va">x.1</span>, color <span class="op">=</span> <span class="va">y</span>, shape <span class="op">=</span> <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_manual.html">scale_color_manual</a></span><span class="op">(</span>values<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"#000000"</span>, <span class="st">"#FF0000"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html">theme</a></span><span class="op">(</span>legend.position <span class="op">=</span> <span class="st">"none"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="103-classification_113-broad_view-SVM_files/figure-html/unnamed-chunk-5-1.png" width="70%" style="display: block; margin: auto;"></div>
<p>Whether the data is separable or not, the <code><a href="https://rdrr.io/pkg/e1071/man/svm.html">svm()</a></code> command syntax is the same. In the case of data that is not linearly separable, however, the cost = argument takes on real importance. This quantifies the penalty associated with having an observation on the wrong side of the classification boundary. We can plot the fit in the same way as the completely separable case. We first use <code>e1071</code>:</p>
<div class="sourceCode" id="cb277"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Fit Support Vector Machine model to data set</span>
<span class="va">svmfit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/e1071/man/svm.html">svm</a></span><span class="op">(</span><span class="va">y</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">dat</span>, kernel <span class="op">=</span> <span class="st">"linear"</span>, cost <span class="op">=</span> <span class="fl">10</span><span class="op">)</span>
<span class="co"># Plot Results</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span><span class="op">(</span><span class="va">svmfit</span>, <span class="va">dat</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="103-classification_113-broad_view-SVM_files/figure-html/unnamed-chunk-6-1.png" width="70%" style="display: block; margin: auto;"></div>
<p>By upping the cost of misclassification from 10 to 100, you can see the difference in the classification line. We repeat the process of plotting the SVM using the <code>kernlab</code> package:</p>
<div class="sourceCode" id="cb278"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Fit Support Vector Machine model to data set</span>
<span class="va">kernfit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/kernlab/man/ksvm.html">ksvm</a></span><span class="op">(</span><span class="va">x</span>,<span class="va">y</span>, type <span class="op">=</span> <span class="st">"C-svc"</span>, kernel <span class="op">=</span> <span class="st">'vanilladot'</span>, C <span class="op">=</span> <span class="fl">100</span><span class="op">)</span>
<span class="co">#&gt;  Setting default kernel parameters</span>
<span class="co"># Plot results</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span><span class="op">(</span><span class="va">kernfit</span>, data <span class="op">=</span> <span class="va">x</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="103-classification_113-broad_view-SVM_files/figure-html/unnamed-chunk-7-1.png" width="70%" style="display: block; margin: auto;"></div>
<p>But how do we decide how costly these misclassifications actually are? Instead of specifying a cost up front, we can use the tune() function from e1071 to test various costs and identify which value produces the best fitting model.</p>
<div class="sourceCode" id="cb279"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># find optimal cost of misclassification</span>
<span class="va">tune.out</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/e1071/man/tune.html">tune</a></span><span class="op">(</span><span class="va">svm</span>, <span class="va">y</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">dat</span>, kernel <span class="op">=</span> <span class="st">"linear"</span>,
                 ranges <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>cost <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.001</span>, <span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="fl">1</span>, <span class="fl">5</span>, <span class="fl">10</span>, <span class="fl">100</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>
<span class="co"># extract the best model</span>
<span class="op">(</span><span class="va">bestmod</span> <span class="op">&lt;-</span> <span class="va">tune.out</span><span class="op">$</span><span class="va">best.model</span><span class="op">)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; best.tune(method = svm, train.x = y ~ ., data = dat, ranges = list(cost = c(0.001, </span>
<span class="co">#&gt;     0.01, 0.1, 1, 5, 10, 100)), kernel = "linear")</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Parameters:</span>
<span class="co">#&gt;    SVM-Type:  C-classification </span>
<span class="co">#&gt;  SVM-Kernel:  linear </span>
<span class="co">#&gt;        cost:  0.1 </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Number of Support Vectors:  16</span></code></pre></div>
<p>For our data set, the optimal cost (from amongst the choices we provided) is calculated to be 0.1, which doesn’t penalize the model much for misclassified observations. Once this model has been identified, we can construct a table of predicted classes against true classes using the predict() command as follows:</p>
<div class="sourceCode" id="cb280"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Create a table of misclassified observations</span>
<span class="va">ypred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">bestmod</span>, <span class="va">dat</span><span class="op">)</span>
<span class="op">(</span><span class="va">misclass</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span>predict <span class="op">=</span> <span class="va">ypred</span>, truth <span class="op">=</span> <span class="va">dat</span><span class="op">$</span><span class="va">y</span><span class="op">)</span><span class="op">)</span>
<span class="co">#&gt;        truth</span>
<span class="co">#&gt; predict -1 1</span>
<span class="co">#&gt;      -1  9 3</span>
<span class="co">#&gt;      1   1 7</span></code></pre></div>
<p>Using this support vector classifier, 80% of the observations were correctly classified, which matches what we see in the plot. If we wanted to test our classifier more rigorously, we could split our data into training and testing sets and then see how our SVC performed with the observations not used to construct the model. We will use this training-testing method later in this tutorial to validate our SVMs.</p>
</div>
<div id="support-vector-machines" class="section level2">
<h2>
<span class="header-section-number">11.4</span> Support Vector Machines<a class="anchor" aria-label="anchor" href="#support-vector-machines"><i class="fas fa-link"></i></a>
</h2>
<p>Support Vector Classifiers are a subset of the group of classification structures known as Support Vector Machines. Support Vector Machines can construct classification boundaries that are nonlinear in shape. The options for classification structures using the svm() command from the e1071 package are linear, polynomial, radial, and sigmoid. To demonstrate a nonlinear classification boundary, we will construct a new data set.</p>
<div class="sourceCode" id="cb281"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># construct larger random data set</span>
<span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">200</span><span class="op">*</span><span class="fl">2</span><span class="op">)</span>, ncol <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>
<span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">100</span>,<span class="op">]</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">100</span>,<span class="op">]</span> <span class="op">+</span> <span class="fl">2.5</span>
<span class="va">x</span><span class="op">[</span><span class="fl">101</span><span class="op">:</span><span class="fl">150</span>,<span class="op">]</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">[</span><span class="fl">101</span><span class="op">:</span><span class="fl">150</span>,<span class="op">]</span> <span class="op">-</span> <span class="fl">2.5</span>
<span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">150</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">50</span><span class="op">)</span><span class="op">)</span>
<span class="va">dat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">x</span>,y<span class="op">=</span><span class="fu"><a href="https://rdrr.io/pkg/h2o/man/as.factor.html">as.factor</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span>

<span class="co"># Plot data</span>
<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">dat</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x.2</span>, y <span class="op">=</span> <span class="va">x.1</span>, color <span class="op">=</span> <span class="va">y</span>, shape <span class="op">=</span> <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_manual.html">scale_color_manual</a></span><span class="op">(</span>values<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"#000000"</span>, <span class="st">"#FF0000"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html">theme</a></span><span class="op">(</span>legend.position <span class="op">=</span> <span class="st">"none"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="103-classification_113-broad_view-SVM_files/figure-html/unnamed-chunk-10-1.png" width="70%" style="display: block; margin: auto;"></div>
<p>Notice that the data is not linearly separable, and furthermore, isn’t all clustered together in a single group. There are two sections of class 1 observations with a cluster of class 2 observations in between. To demonstrate the power of SVMs, we’ll take 100 random observations from the set and use them to construct our boundary. We set kernel = “radial” based on the shape of our data and plot the results.</p>
<div class="sourceCode" id="cb282"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># set pseudorandom number generator</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span>
<span class="co"># sample training data and fit model</span>
<span class="va">train</span> <span class="op">&lt;-</span> <span class="fu">base</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="fl">200</span>,<span class="fl">100</span>, replace <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span>
<span class="va">svmfit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/e1071/man/svm.html">svm</a></span><span class="op">(</span><span class="va">y</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">dat</span><span class="op">[</span><span class="va">train</span>,<span class="op">]</span>, kernel <span class="op">=</span> <span class="st">"radial"</span>, gamma <span class="op">=</span> <span class="fl">1</span>, cost <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>
<span class="co"># plot classifier</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span><span class="op">(</span><span class="va">svmfit</span>, <span class="va">dat</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="103-classification_113-broad_view-SVM_files/figure-html/unnamed-chunk-11-1.png" width="70%" style="display: block; margin: auto;"></div>
<p>The same procedure can be run using the kernlab package, which has far more kernel options than the corresponding function in e1071. In addition to the four choices in e1071, this package allows use of a hyperbolic tangent, Laplacian, Bessel, Spline, String, or ANOVA RBF kernel. To fit this data, we set the cost to be the same as it was before, 1.</p>
<div class="sourceCode" id="cb283"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Fit radial-based SVM in kernlab</span>
<span class="va">kernfit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/kernlab/man/ksvm.html">ksvm</a></span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="va">train</span>,<span class="op">]</span>,<span class="va">y</span><span class="op">[</span><span class="va">train</span><span class="op">]</span>, type <span class="op">=</span> <span class="st">"C-svc"</span>, kernel <span class="op">=</span> <span class="st">'rbfdot'</span>, C <span class="op">=</span> <span class="fl">1</span>, scaled <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span>
<span class="co"># Plot training data</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span><span class="op">(</span><span class="va">kernfit</span>, data <span class="op">=</span> <span class="va">x</span><span class="op">[</span><span class="va">train</span>,<span class="op">]</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="103-classification_113-broad_view-SVM_files/figure-html/unnamed-chunk-12-1.png" width="70%" style="display: block; margin: auto;"></div>
<p>We see that, at least visually, the SVM does a reasonable job of separating the two classes. To fit the model, we used <code>cost = 1</code>, but as mentioned previously, it isn’t usually obvious which cost will produce the optimal classification boundary. We can use the tune() command to try several different values of cost as well as several different values of <span class="math inline">\(\gamma\)</span>, a scaling parameter used to fit nonlinear boundaries.</p>
<div class="sourceCode" id="cb284"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># tune model to find optimal cost, gamma values</span>
<span class="va">tune.out</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/e1071/man/tune.html">tune</a></span><span class="op">(</span><span class="va">svm</span>, <span class="va">y</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">dat</span><span class="op">[</span><span class="va">train</span>,<span class="op">]</span>, kernel <span class="op">=</span> <span class="st">"radial"</span>,
                 ranges <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>cost <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.1</span>,<span class="fl">1</span>,<span class="fl">10</span>,<span class="fl">100</span>,<span class="fl">1000</span><span class="op">)</span>,
                 gamma <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.5</span>,<span class="fl">1</span>,<span class="fl">2</span>,<span class="fl">3</span>,<span class="fl">4</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>
<span class="co"># show best model</span>
<span class="va">tune.out</span><span class="op">$</span><span class="va">best.model</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; best.tune(method = svm, train.x = y ~ ., data = dat[train, ], ranges = list(cost = c(0.1, </span>
<span class="co">#&gt;     1, 10, 100, 1000), gamma = c(0.5, 1, 2, 3, 4)), kernel = "radial")</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Parameters:</span>
<span class="co">#&gt;    SVM-Type:  C-classification </span>
<span class="co">#&gt;  SVM-Kernel:  radial </span>
<span class="co">#&gt;        cost:  1 </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Number of Support Vectors:  30</span></code></pre></div>
<p>The model that reduces the error the most in the training data uses a cost of 1 and <span class="math inline">\(\gamma\)</span>
value of 0.5. We can now see how well the SVM performs by predicting the class of the 100 testing observations:</p>
<div class="sourceCode" id="cb285"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># validate model performance</span>
<span class="op">(</span><span class="va">valid</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span>true <span class="op">=</span> <span class="va">dat</span><span class="op">[</span><span class="op">-</span><span class="va">train</span>,<span class="st">"y"</span><span class="op">]</span>, pred <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">tune.out</span><span class="op">$</span><span class="va">best.model</span>,
                                             newx <span class="op">=</span> <span class="va">dat</span><span class="op">[</span><span class="op">-</span><span class="va">train</span>,<span class="op">]</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>
<span class="co">#&gt;     pred</span>
<span class="co">#&gt; true  1  2</span>
<span class="co">#&gt;    1 55 28</span>
<span class="co">#&gt;    2 12  5</span>
<span class="co">##     pred</span>
<span class="co">## true  1  2</span>
<span class="co">##    1 58 19</span>
<span class="co">##    2 16  7</span></code></pre></div>
<p>Our best-fitting model produces 65% accuracy in identifying classes. For such a complicated shape of observations, this performed reasonably well. We can challenge this method further by adding additional classes of observations.</p>
</div>
<div id="svms-for-multiple-classes" class="section level2">
<h2>
<span class="header-section-number">11.5</span> SVMs for Multiple Classes<a class="anchor" aria-label="anchor" href="#svms-for-multiple-classes"><i class="fas fa-link"></i></a>
</h2>
<p>The procedure does not change for data sets that involve more than two classes of observations. We construct our data set the same way as we have previously, only now specifying three classes instead of two:</p>
<div class="sourceCode" id="cb286"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># construct data set</span>
<span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://Rdatatable.gitlab.io/data.table/reference/rbindlist.html">rbind</a></span><span class="op">(</span><span class="va">x</span>, <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">50</span><span class="op">*</span><span class="fl">2</span><span class="op">)</span>, ncol <span class="op">=</span> <span class="fl">2</span><span class="op">)</span><span class="op">)</span>
<span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">y</span>, <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">50</span><span class="op">)</span><span class="op">)</span>
<span class="va">x</span><span class="op">[</span><span class="va">y</span><span class="op">==</span><span class="fl">0</span>,<span class="fl">2</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">[</span><span class="va">y</span><span class="op">==</span><span class="fl">0</span>,<span class="fl">2</span><span class="op">]</span> <span class="op">+</span> <span class="fl">2.5</span>
<span class="va">dat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">x</span>, y<span class="op">=</span><span class="fu"><a href="https://rdrr.io/pkg/h2o/man/as.factor.html">as.factor</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span>
<span class="co"># plot data set</span>
<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">dat</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x.2</span>, y <span class="op">=</span> <span class="va">x.1</span>, color <span class="op">=</span> <span class="va">y</span>, shape <span class="op">=</span> <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_manual.html">scale_color_manual</a></span><span class="op">(</span>values<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"#000000"</span>,<span class="st">"#FF0000"</span>,<span class="st">"#00BA00"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html">theme</a></span><span class="op">(</span>legend.position <span class="op">=</span> <span class="st">"none"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="103-classification_113-broad_view-SVM_files/figure-html/unnamed-chunk-15-1.png" width="70%" style="display: block; margin: auto;"></div>
<p>The commands don’t change for the e1071 package. We specify a cost and tuning parameter
<span class="math inline">\(\gamma\)</span>
and fit a support vector machine. The results and interpretation are similar to two-class classification.</p>
<div class="sourceCode" id="cb287"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># fit model</span>
<span class="va">svmfit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/e1071/man/svm.html">svm</a></span><span class="op">(</span><span class="va">y</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">dat</span>, kernel <span class="op">=</span> <span class="st">"radial"</span>, cost <span class="op">=</span> <span class="fl">10</span>, gamma <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>
<span class="co"># plot results</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span><span class="op">(</span><span class="va">svmfit</span>, <span class="va">dat</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="103-classification_113-broad_view-SVM_files/figure-html/unnamed-chunk-16-1.png" width="70%" style="display: block; margin: auto;"></div>
<p>We can check to see how well our model fit the data by using the <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> command, as follows:</p>
<div class="sourceCode" id="cb288"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co">#construct table</span>
<span class="va">ypred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">svmfit</span>, <span class="va">dat</span><span class="op">)</span>
<span class="op">(</span><span class="va">misclass</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span>predict <span class="op">=</span> <span class="va">ypred</span>, truth <span class="op">=</span> <span class="va">dat</span><span class="op">$</span><span class="va">y</span><span class="op">)</span><span class="op">)</span>
<span class="co">#&gt;        truth</span>
<span class="co">#&gt; predict   0   1   2</span>
<span class="co">#&gt;       0  38   2   5</span>
<span class="co">#&gt;       1   7 145   2</span>
<span class="co">#&gt;       2   5   3  43</span>
<span class="co">##        truth</span>
<span class="co">## predict   0   1   2</span>
<span class="co">##       0  38   2   4</span>
<span class="co">##       1   8 143   4</span>
<span class="co">##       2   4   5  42</span></code></pre></div>
<p>As shown in the resulting table, 89% of our training observations were correctly classified. However, since we didn’t break our data into training and testing sets, we didn’t truly validate our results.</p>
<p>The kernlab package, on the other hand, can fit more than 2 classes, but cannot plot the results. To visualize the results of the ksvm function, we take the steps listed below to create a grid of points, predict the value of each point, and plot the results:</p>
<div class="sourceCode" id="cb289"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># fit and plot</span>
<span class="va">kernfit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/kernlab/man/ksvm.html">ksvm</a></span><span class="op">(</span><span class="fu"><a href="https://Rdatatable.gitlab.io/data.table/reference/as.matrix.html">as.matrix</a></span><span class="op">(</span><span class="va">dat</span><span class="op">[</span>,<span class="fl">2</span><span class="op">:</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span>,<span class="va">dat</span><span class="op">$</span><span class="va">y</span>, type <span class="op">=</span> <span class="st">"C-svc"</span>, kernel <span class="op">=</span> <span class="st">'rbfdot'</span>, 
                C <span class="op">=</span> <span class="fl">100</span>, scaled <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span>

<span class="co"># Create a fine grid of the feature space</span>
<span class="va">x.1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span>from <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">dat</span><span class="op">$</span><span class="va">x.1</span><span class="op">)</span>, to <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">dat</span><span class="op">$</span><span class="va">x.1</span><span class="op">)</span>, length <span class="op">=</span> <span class="fl">100</span><span class="op">)</span>
<span class="va">x.2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span>from <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">dat</span><span class="op">$</span><span class="va">x.2</span><span class="op">)</span>, to <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">dat</span><span class="op">$</span><span class="va">x.2</span><span class="op">)</span>, length <span class="op">=</span> <span class="fl">100</span><span class="op">)</span>
<span class="va">x.grid</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/expand.grid.html">expand.grid</a></span><span class="op">(</span><span class="va">x.2</span>, <span class="va">x.1</span><span class="op">)</span>

<span class="co"># Get class predictions over grid</span>
<span class="va">pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">kernfit</span>, newdata <span class="op">=</span> <span class="va">x.grid</span><span class="op">)</span>

<span class="co"># Plot the results</span>
<span class="va">cols</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/RColorBrewer/man/ColorBrewer.html">brewer.pal</a></span><span class="op">(</span><span class="fl">3</span>, <span class="st">"Set1"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.html">plot</a></span><span class="op">(</span><span class="va">x.grid</span>, pch <span class="op">=</span> <span class="fl">19</span>, col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/grDevices/adjustcolor.html">adjustcolor</a></span><span class="op">(</span><span class="va">cols</span><span class="op">[</span><span class="va">pred</span><span class="op">]</span>, alpha.f <span class="op">=</span> <span class="fl">0.05</span><span class="op">)</span><span class="op">)</span>

<span class="va">classes</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="va">pred</span>, nrow <span class="op">=</span> <span class="fl">100</span>, ncol <span class="op">=</span> <span class="fl">100</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/contour.html">contour</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x.2</span>, y <span class="op">=</span> <span class="va">x.1</span>, z <span class="op">=</span> <span class="va">classes</span>, levels <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">3</span>, labels <span class="op">=</span> <span class="st">""</span>, add <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>

<span class="fu"><a href="https://rdrr.io/r/graphics/points.html">points</a></span><span class="op">(</span><span class="va">dat</span><span class="op">[</span>, <span class="fl">2</span><span class="op">:</span><span class="fl">1</span><span class="op">]</span>, pch <span class="op">=</span> <span class="fl">19</span>, col <span class="op">=</span> <span class="va">cols</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">kernfit</span><span class="op">)</span><span class="op">]</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="103-classification_113-broad_view-SVM_files/figure-html/unnamed-chunk-18-1.png" width="70%" style="display: block; margin: auto;"></div>
</div>
<div id="application" class="section level2">
<h2>
<span class="header-section-number">11.6</span> Application<a class="anchor" aria-label="anchor" href="#application"><i class="fas fa-link"></i></a>
</h2>
<p>The Khan data set contains data on 83 tissue samples with 2308 gene expression measurements on each sample. These were split into 63 training observations and 20 testing observations, and there are four distinct classes in the set. It would be impossible to visualize such data, so we choose the simplest classifier (linear) to construct our model. We will use the svm command from <code>e1071</code> to conduct our analysis.</p>
<div class="sourceCode" id="cb290"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># fit model</span>
<span class="va">dat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">Khan</span><span class="op">$</span><span class="va">xtrain</span>, y<span class="op">=</span><span class="fu"><a href="https://rdrr.io/pkg/h2o/man/as.factor.html">as.factor</a></span><span class="op">(</span><span class="va">Khan</span><span class="op">$</span><span class="va">ytrain</span><span class="op">)</span><span class="op">)</span>
<span class="op">(</span><span class="va">out</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/e1071/man/svm.html">svm</a></span><span class="op">(</span><span class="va">y</span><span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">dat</span>, kernel <span class="op">=</span> <span class="st">"linear"</span>, cost<span class="op">=</span><span class="fl">10</span><span class="op">)</span><span class="op">)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt; svm(formula = y ~ ., data = dat, kernel = "linear", cost = 10)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Parameters:</span>
<span class="co">#&gt;    SVM-Type:  C-classification </span>
<span class="co">#&gt;  SVM-Kernel:  linear </span>
<span class="co">#&gt;        cost:  10 </span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Number of Support Vectors:  58</span></code></pre></div>
<p>First of all, we can check how well our model did at classifying the training observations. This is usually high, but again, doesn’t validate the model. If the model doesn’t do a very good job of classifying the training set, it could be a red flag. In our case, all 63 training observations were correctly classified.</p>
<div class="sourceCode" id="cb291"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># check model performance on training set</span>
<span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">out</span><span class="op">$</span><span class="va">fitted</span>, <span class="va">dat</span><span class="op">$</span><span class="va">y</span><span class="op">)</span>
<span class="co">#&gt;    </span>
<span class="co">#&gt;      1  2  3  4</span>
<span class="co">#&gt;   1  8  0  0  0</span>
<span class="co">#&gt;   2  0 23  0  0</span>
<span class="co">#&gt;   3  0  0 12  0</span>
<span class="co">#&gt;   4  0  0  0 20</span></code></pre></div>
<p>To perform validation, we can check how the model performs on the testing set:</p>
<div class="sourceCode" id="cb292"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># validate model performance</span>
<span class="va">dat.te</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">Khan</span><span class="op">$</span><span class="va">xtest</span>, y<span class="op">=</span><span class="fu"><a href="https://rdrr.io/pkg/h2o/man/as.factor.html">as.factor</a></span><span class="op">(</span><span class="va">Khan</span><span class="op">$</span><span class="va">ytest</span><span class="op">)</span><span class="op">)</span>
<span class="va">pred.te</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">out</span>, newdata<span class="op">=</span><span class="va">dat.te</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">pred.te</span>, <span class="va">dat.te</span><span class="op">$</span><span class="va">y</span><span class="op">)</span>
<span class="co">#&gt;        </span>
<span class="co">#&gt; pred.te 1 2 3 4</span>
<span class="co">#&gt;       1 3 0 0 0</span>
<span class="co">#&gt;       2 0 6 2 0</span>
<span class="co">#&gt;       3 0 0 4 0</span>
<span class="co">#&gt;       4 0 0 0 5</span></code></pre></div>
<p>The model correctly identifies 18 of the 20 testing observations. SVMs and the boundaries they impose are more difficult to interpret at higher dimensions, but these results seem to suggest that our model is a good classifier for the gene data.</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="a-gentle-introduction-to-support-vector-machines.html"><span class="header-section-number">10</span> A gentle introduction to Support Vector Machines</a></div>
<div class="next"><a href="feature-selection-to-enhance-cancer-detection.html"><span class="header-section-number">12</span> Feature Selection to enhance cancer detection</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#broad-view-of-svm"><span class="header-section-number">11</span> Broad view of SVM</a></li>
<li><a class="nav-link" href="#introduction-5"><span class="header-section-number">11.1</span> Introduction</a></li>
<li><a class="nav-link" href="#maximal-margin-classifier"><span class="header-section-number">11.2</span> Maximal Margin Classifier</a></li>
<li><a class="nav-link" href="#support-vector-classifiers"><span class="header-section-number">11.3</span> Support Vector Classifiers</a></li>
<li><a class="nav-link" href="#support-vector-machines"><span class="header-section-number">11.4</span> Support Vector Machines</a></li>
<li><a class="nav-link" href="#svms-for-multiple-classes"><span class="header-section-number">11.5</span> SVMs for Multiple Classes</a></li>
<li><a class="nav-link" href="#application"><span class="header-section-number">11.6</span> Application</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/f0nzie/machine_learning_compilation/blob/master/103-classification_113-broad_view-SVM.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/f0nzie/machine_learning_compilation/edit/master/103-classification_113-broad_view-SVM.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>A Machine Learning Compilation</strong>" was written by Several authors. Compiled by Alfonso R. Reyes. It was last built on 2020-11-19.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
